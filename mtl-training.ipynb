{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6525280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory management setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Aggressive Memory Management\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Aggressively clear GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Reset memory stats\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "        \n",
    "        print(f\"🧹 Memory cleaned!\")\n",
    "        print(f\"  Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"  Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Set memory optimization environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"✅ Memory management setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1df2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f861828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated dataset preparation functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Updated Dataset Class and External Data Loading (FIXED)\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from collections import Counter\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        sentiment_label_encoder=None,\n",
    "        emotion_label_encoder=None\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sentiment_label_encoder = sentiment_label_encoder\n",
    "        self.emotion_label_encoder = emotion_label_encoder\n",
    "        \n",
    "        # Validate data\n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have the same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "def load_external_datasets() -> Tuple[Dict, Dict]:\n",
    "    print(\"Loading external datasets for training...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        sentiment_data = {\n",
    "            'train': sst2_dataset['train'],\n",
    "            'validation': sst2_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sentiment_data['train'])} train, {len(sentiment_data['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load SST-2: {e}. Using dummy data.\")\n",
    "        sentiment_data = _create_dummy_sentiment_data()\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        emotion_data = {\n",
    "            'train': emotions_dataset['train'],\n",
    "            'validation': emotions_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotion_data['train'])} train, {len(emotion_data['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load GoEmotions: {e}. Using dummy data.\")\n",
    "        emotion_data = _create_dummy_emotion_data()\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def _create_dummy_sentiment_data() -> Dict:\n",
    "    \"\"\"Create dummy sentiment data for testing\"\"\"\n",
    "    dummy_texts = [\n",
    "        \"I love this product!\", \"This is terrible\", \"It's okay\",\n",
    "        \"Amazing quality\", \"Worst experience ever\", \"Not bad\"\n",
    "    ] * 200\n",
    "    dummy_labels = [1, 0, 1, 1, 0, 1] * 200\n",
    "    \n",
    "    dummy_data = {\n",
    "        'sentence': dummy_texts,\n",
    "        'label': dummy_labels\n",
    "    }\n",
    "    \n",
    "    dataset = HFDataset.from_dict(dummy_data)\n",
    "    return {'train': dataset, 'validation': dataset.select(range(200))}\n",
    "\n",
    "def _create_dummy_emotion_data() -> Dict:\n",
    "    \"\"\"Create dummy emotion data for testing\"\"\"\n",
    "    dummy_texts = [\n",
    "        \"I'm so happy!\", \"This is sad\", \"I'm angry\", \"That's scary\",\n",
    "        \"What a surprise!\", \"This is neutral\", \"I love this!\", \"Great stuff\"\n",
    "    ] * 200\n",
    "    dummy_labels = [0, 1, 2, 3, 4, 5, 0, 0] * 200  # Map to 6 classes\n",
    "    \n",
    "    dummy_data = {\n",
    "        'text': dummy_texts,\n",
    "        'labels': dummy_labels\n",
    "    }\n",
    "    \n",
    "    dataset = HFDataset.from_dict(dummy_data)\n",
    "    return {'train': dataset, 'validation': dataset.select(range(200))}\n",
    "\n",
    "def prepare_external_data_for_multitask(\n",
    "    sentiment_data: Dict,\n",
    "    emotion_data: Dict,\n",
    "    max_samples: int = 10000\n",
    ") -> Tuple[Dict, LabelEncoder, LabelEncoder]:\n",
    "    \"\"\"\n",
    "    Prepare external datasets for multitask training\n",
    "    \"\"\"\n",
    "    print(\"🔄 Preparing external datasets for multitask training...\")\n",
    "    \n",
    "    # Filter emotion data to first 6 classes only (to match your Reddit data)\n",
    "    def filter_emotion_classes(example):\n",
    "        # Handle both single-label and multi-label\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    emotion_data['train'] = emotion_data['train'].filter(filter_emotion_classes)\n",
    "    emotion_data['validation'] = emotion_data['validation'].filter(filter_emotion_classes)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    # Sentiment (SST-2)\n",
    "    sentiment_texts = sentiment_data['train']['sentence'][:max_samples]\n",
    "    sentiment_labels = sentiment_data['train']['label'][:max_samples]\n",
    "    \n",
    "    # Emotion (GoEmotions) \n",
    "    emotion_texts = emotion_data['train']['text'][:max_samples]\n",
    "    emotion_labels_raw = emotion_data['train']['labels'][:max_samples]\n",
    "    \n",
    "    # Handle multi-label to single-label conversion for emotions\n",
    "    emotion_labels = []\n",
    "    for label in emotion_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            emotion_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            emotion_labels.append(label)\n",
    "    \n",
    "    # Create label encoders based on your Reddit data classes\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # Fit with the classes that match your Reddit data\n",
    "    # SST-2: 0=negative, 1=positive. We need: Negative, Neutral, Positive\n",
    "    # Map SST labels to 3-class: 0->0 (Negative), 1->2 (Positive), add 1 (Neutral) artificially\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    # GoEmotions: Map to your 6 classes\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    # Convert SST labels: 0->0 (Negative), 1->2 (Positive)\n",
    "    # We'll add some neutral examples by randomly converting some to class 1\n",
    "    converted_sentiment_labels = []\n",
    "    for label in sentiment_labels:\n",
    "        if label == 0:  # Negative\n",
    "            converted_sentiment_labels.append(0)\n",
    "        elif label == 1:  # Positive\n",
    "            # Randomly assign some as neutral (class 1) to have all 3 classes\n",
    "            if np.random.random() < 0.1:  # 10% chance\n",
    "                converted_sentiment_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                converted_sentiment_labels.append(2)  # Positive\n",
    "    \n",
    "    # Ensure we have all 3 sentiment classes\n",
    "    if 1 not in converted_sentiment_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(converted_sentiment_labels), size=50, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            converted_sentiment_labels[idx] = 1\n",
    "    \n",
    "    # Balance the datasets - use minimum length\n",
    "    min_length = min(len(sentiment_texts), len(emotion_texts))\n",
    "    \n",
    "    final_texts = sentiment_texts[:min_length]\n",
    "    final_sentiment_labels = converted_sentiment_labels[:min_length]\n",
    "    final_emotion_labels = emotion_labels[:min_length]\n",
    "    \n",
    "    # Create train/val splits\n",
    "    split_idx = int(0.8 * min_length)\n",
    "    \n",
    "    data_splits = {\n",
    "        'train': {\n",
    "            'texts': final_texts[:split_idx],\n",
    "            'sentiment_labels': final_sentiment_labels[:split_idx],\n",
    "            'emotion_labels': final_emotion_labels[:split_idx]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': final_texts[split_idx:],\n",
    "            'sentiment_labels': final_sentiment_labels[split_idx:],\n",
    "            'emotion_labels': final_emotion_labels[split_idx:]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ External data prepared:\")\n",
    "    print(f\"  Train samples: {len(data_splits['train']['texts'])}\")\n",
    "    print(f\"  Validation samples: {len(data_splits['val']['texts'])}\")\n",
    "    print(f\"  Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"  Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    train_sentiment_counts = Counter(data_splits['train']['sentiment_labels'])\n",
    "    train_emotion_counts = Counter(data_splits['train']['emotion_labels'])\n",
    "    \n",
    "    print(f\"\\n📈 Training set class distribution:\")\n",
    "    for i, class_name in enumerate(sentiment_encoder.classes_):\n",
    "        count = train_sentiment_counts.get(i, 0)\n",
    "        print(f\"  Sentiment '{class_name}': {count} samples\")\n",
    "    \n",
    "    for i, class_name in enumerate(emotion_encoder.classes_):\n",
    "        count = train_emotion_counts.get(i, 0)\n",
    "        print(f\"  Emotion '{class_name}': {count} samples\")\n",
    "    \n",
    "    return data_splits, sentiment_encoder, emotion_encoder\n",
    "\n",
    "def prepare_reddit_data_for_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    sentiment_encoder: LabelEncoder,\n",
    "    emotion_encoder: LabelEncoder,\n",
    "    sentiment_column: str = 'sentiment',\n",
    "    emotion_column: str = 'emotion',\n",
    "    text_column: str = 'text_content'\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Prepare Reddit data for evaluation only (not training)\n",
    "    \"\"\"\n",
    "    print(\"🔄 Preparing Reddit data for evaluation...\")\n",
    "    \n",
    "    # Extract data\n",
    "    texts = df[text_column].tolist()\n",
    "    sentiment_labels_text = df[sentiment_column].tolist()\n",
    "    emotion_labels_text = df[emotion_column].tolist()\n",
    "    \n",
    "    # Transform labels using pre-fitted encoders\n",
    "    try:\n",
    "        sentiment_labels = sentiment_encoder.transform(sentiment_labels_text)\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Sentiment label mismatch: {e}\")\n",
    "        # Handle unknown labels by mapping them to existing classes\n",
    "        sentiment_labels = []\n",
    "        for label in sentiment_labels_text:\n",
    "            if label in sentiment_encoder.classes_:\n",
    "                sentiment_labels.append(sentiment_encoder.transform([label])[0])\n",
    "            else:\n",
    "                print(f\"⚠️ Unknown sentiment label '{label}', mapping to 'Neutral'\")\n",
    "                sentiment_labels.append(sentiment_encoder.transform(['Neutral'])[0])\n",
    "        sentiment_labels = np.array(sentiment_labels)\n",
    "    \n",
    "    try:\n",
    "        emotion_labels = emotion_encoder.transform(emotion_labels_text)\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Emotion label mismatch: {e}\")\n",
    "        # Handle unknown labels\n",
    "        emotion_labels = []\n",
    "        for label in emotion_labels_text:\n",
    "            if label in emotion_encoder.classes_:\n",
    "                emotion_labels.append(emotion_encoder.transform([label])[0])\n",
    "            else:\n",
    "                print(f\"⚠️ Unknown emotion label '{label}', mapping to 'No Emotion'\")\n",
    "                emotion_labels.append(emotion_encoder.transform(['No Emotion'])[0])\n",
    "        emotion_labels = np.array(emotion_labels)\n",
    "    \n",
    "    evaluation_data = {\n",
    "        'texts': texts,\n",
    "        'sentiment_labels': sentiment_labels.tolist(),\n",
    "        'emotion_labels': emotion_labels.tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Reddit evaluation data prepared: {len(texts)} samples\")\n",
    "    \n",
    "    return evaluation_data\n",
    "\n",
    "def create_stratified_sampler(sentiment_labels: List[int], emotion_labels: List[int]) -> WeightedRandomSampler:\n",
    "    \"\"\"\n",
    "    Create a weighted random sampler for stratified sampling\n",
    "    considering both sentiment and emotion class distributions\n",
    "    \"\"\"\n",
    "    # Combine labels to create compound classes for stratification\n",
    "    compound_labels = [f\"{s}_{e}\" for s, e in zip(sentiment_labels, emotion_labels)]\n",
    "    \n",
    "    # Calculate class weights\n",
    "    unique_labels = list(set(compound_labels))\n",
    "    \n",
    "    # FIX: Convert to numpy array as required by compute_class_weight\n",
    "    unique_labels_array = np.array(unique_labels)\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels_array,  # Now it's a numpy array\n",
    "        y=compound_labels\n",
    "    )\n",
    "    \n",
    "    # Create weight dictionary\n",
    "    weight_dict = dict(zip(unique_labels, class_weights))\n",
    "    \n",
    "    # Assign weights to each sample\n",
    "    sample_weights = [weight_dict[label] for label in compound_labels]\n",
    "    \n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "print(\"✅ Updated dataset preparation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf5c6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ultra-lightweight training function ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Ultra Memory-Optimized Training\n",
    "def run_ultra_lightweight_training(\n",
    "    reddit_data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    model_name: str = \"microsoft/deberta-base\",\n",
    "    output_dir: str = \"./multitask_model_ultra_light\",\n",
    "    max_external_samples: int = 1000  # Very small dataset\n",
    "):\n",
    "    \"\"\"\n",
    "    Ultra-lightweight training for 8GB GPU\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Ultra-Lightweight Training\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Aggressive memory cleanup first\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n1️⃣ Loading minimal external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare very small external data\n",
    "    external_data_splits, sentiment_encoder, emotion_encoder = prepare_external_data_for_multitask(\n",
    "        sentiment_data, emotion_data, max_samples=max_external_samples\n",
    "    )\n",
    "    \n",
    "    # Load Reddit data\n",
    "    print(\"\\n2️⃣ Loading Reddit data...\")\n",
    "    reddit_df = pd.read_csv(reddit_data_path)\n",
    "    reddit_evaluation_data = prepare_reddit_data_for_evaluation(\n",
    "        reddit_df, sentiment_encoder, emotion_encoder\n",
    "    )\n",
    "    \n",
    "    # Ultra-lightweight config\n",
    "    config = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir,\n",
    "        num_epochs=2,       # Minimal epochs\n",
    "        batch_size=1,       # Smallest possible batch\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.05,  # Minimal warmup\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        alpha=0.5,\n",
    "        adaptive_alpha=False,  # Disable to save memory\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_dropout_prob=0.1,\n",
    "        classifier_dropout=0.1,\n",
    "        max_length=128,     # Very short sequences\n",
    "        save_total_limit=1  # Keep only 1 checkpoint\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n3️⃣ Ultra-lightweight config:\")\n",
    "    print(f\"  Batch size: {config.batch_size}\")\n",
    "    print(f\"  Max length: {config.max_length}\")\n",
    "    print(f\"  Training samples: {len(external_data_splits['train']['texts'])}\")\n",
    "    print(f\"  Epochs: {config.num_epochs}\")\n",
    "    \n",
    "    # Clear memory before model\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Initialize trainer\n",
    "    print(f\"\\n4️⃣ Initializing trainer...\")\n",
    "    trainer = MultiTaskTrainer(\n",
    "        config=config,\n",
    "        sentiment_num_classes=len(sentiment_encoder.classes_),\n",
    "        emotion_num_classes=len(emotion_encoder.classes_)\n",
    "    )\n",
    "    \n",
    "    # Setup with gradient checkpointing\n",
    "    print(f\"\\n5️⃣ Setting up with memory optimizations...\")\n",
    "    trainer.setup(external_data_splits, sentiment_encoder, emotion_encoder)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    if hasattr(trainer.model.shared_encoder, 'gradient_checkpointing_enable'):\n",
    "        trainer.model.shared_encoder.gradient_checkpointing_enable()\n",
    "        print(\"✅ Gradient checkpointing enabled\")\n",
    "    \n",
    "    # Train with memory monitoring\n",
    "    print(f\"\\n6️⃣ Training with memory monitoring...\")\n",
    "    try:\n",
    "        history = trainer.train()\n",
    "        print(\"✅ Training completed!\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"❌ Still out of memory: {e}\")\n",
    "            print(\"💡 Try restarting kernel and using even smaller batch_size=1\")\n",
    "            return None, None\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Clear memory before evaluation\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n7️⃣ Evaluating...\")\n",
    "    evaluator = MultiTaskEvaluator(\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.tokenizer,\n",
    "        sentiment_encoder=sentiment_encoder,\n",
    "        emotion_encoder=emotion_encoder,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    reddit_results = evaluator.evaluate_dataset(\n",
    "        texts=reddit_evaluation_data['texts'],\n",
    "        sentiment_labels=reddit_evaluation_data['sentiment_labels'],\n",
    "        emotion_labels=reddit_evaluation_data['emotion_labels'],\n",
    "        batch_size=1  # Ultra small batch for evaluation\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    save_model_and_encoders(\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.tokenizer,\n",
    "        sentiment_encoder=sentiment_encoder,\n",
    "        emotion_encoder=emotion_encoder,\n",
    "        output_dir=os.path.join(output_dir, 'final_model')\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n📈 Results:\")\n",
    "    print(f\"Sentiment Accuracy: {reddit_results['sentiment']['accuracy']:.4f}\")\n",
    "    print(f\"Emotion Accuracy: {reddit_results['emotion']['accuracy']:.4f}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    return trainer.model, reddit_results\n",
    "\n",
    "print(\"✅ Ultra-lightweight training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a01b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training utilities defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Utilities (FIXED)\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration class for training parameters\"\"\"\n",
    "    \n",
    "    def __init__(  # ← FIXED: was _init_ now __init__\n",
    "        self,\n",
    "        model_name: str = \"microsoft/deberta-base\",\n",
    "        max_length: int = 256,  # Reduced from 512\n",
    "        batch_size: int = 4,    # Much smaller batch size\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 3,    # Reduced epochs\n",
    "        warmup_ratio: float = 0.1,\n",
    "        weight_decay: float = 0.01,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        alpha: float = 0.5,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        adaptive_alpha: bool = True,\n",
    "        save_strategy: str = \"epoch\",\n",
    "        evaluation_strategy: str = \"epoch\",\n",
    "        output_dir: str = \"./multitask_model\",\n",
    "        logging_steps: int = 20,  # Reduced logging frequency\n",
    "        save_total_limit: int = 1  # Keep only 1 checkpoint\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.alpha = alpha\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_dropout_prob = attention_dropout_prob\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.adaptive_alpha = adaptive_alpha\n",
    "        self.save_strategy = save_strategy\n",
    "        self.evaluation_strategy = evaluation_strategy\n",
    "        self.output_dir = output_dir\n",
    "        self.logging_steps = logging_steps\n",
    "        self.save_total_limit = save_total_limit\n",
    "\n",
    "def create_optimizer_and_scheduler(\n",
    "    model: nn.Module,\n",
    "    config: TrainingConfig,\n",
    "    num_training_steps: int\n",
    ") -> Tuple[AdamW, LambdaLR]:\n",
    "    \"\"\"\n",
    "    Create optimizer and learning rate scheduler\n",
    "    \"\"\"\n",
    "    # Separate parameters for different learning rates\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() \n",
    "                      if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() \n",
    "                      if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # AdamW optimizer\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=config.learning_rate,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Linear warmup scheduler\n",
    "    num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 3, min_delta: float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        \n",
    "    def __call__(self, score: float) -> bool:\n",
    "        \"\"\"Returns True if training should be stopped\"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "class ModelCheckpointer:\n",
    "    \"\"\"Model checkpointing utility\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str, save_total_limit: int = 3):\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.saved_checkpoints = []\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        tokenizer,\n",
    "        optimizer: AdamW,\n",
    "        scheduler: LambdaLR,\n",
    "        epoch: int,\n",
    "        metrics: Dict,\n",
    "        is_best: bool = False\n",
    "    ):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-epoch-{epoch}\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model and tokenizer\n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        tokenizer.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save training state\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "        \n",
    "        # Save best model separately\n",
    "        if is_best:\n",
    "            best_dir = os.path.join(self.output_dir, 'best_model')\n",
    "            os.makedirs(best_dir, exist_ok=True)\n",
    "            model.save_pretrained(best_dir)\n",
    "            tokenizer.save_pretrained(best_dir)\n",
    "        \n",
    "        # Manage checkpoint limit\n",
    "        self.saved_checkpoints.append(checkpoint_dir)\n",
    "        if len(self.saved_checkpoints) > self.save_total_limit:\n",
    "            old_checkpoint = self.saved_checkpoints.pop(0)\n",
    "            if os.path.exists(old_checkpoint) and 'best_model' not in old_checkpoint:\n",
    "                import shutil\n",
    "                shutil.rmtree(old_checkpoint)\n",
    "\n",
    "print(\"✅ Training utilities defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "814e5f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training loop defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training Loop\n",
    "class MultiTaskTrainer:\n",
    "    \"\"\"\n",
    "    Main trainer class for multitask learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfig,\n",
    "        sentiment_num_classes: int,\n",
    "        emotion_num_classes: int\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize components\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = None\n",
    "        self.loss_fn = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.alpha_scheduler = None\n",
    "        self.early_stopping = None\n",
    "        self.checkpointer = None\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_loss': [],\n",
    "            'train_emotion_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_loss': [],\n",
    "            'val_emotion_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'alpha': [],\n",
    "            'learning_rate': []\n",
    "        }\n",
    "    \n",
    "    def setup(\n",
    "        self,\n",
    "        data_splits: Dict,\n",
    "        sentiment_encoder: LabelEncoder,\n",
    "        emotion_encoder: LabelEncoder\n",
    "    ):\n",
    "        \"\"\"Setup model, loss function, and training components\"\"\"\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = MultiTaskTransformer(\n",
    "            model_name=self.config.model_name,\n",
    "            sentiment_num_classes=self.sentiment_num_classes,\n",
    "            emotion_num_classes=self.emotion_num_classes,\n",
    "            hidden_dropout_prob=self.config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=self.config.attention_dropout_prob,\n",
    "            classifier_dropout=self.config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Compute class weights\n",
    "        sentiment_weights = compute_class_weights_from_labels(\n",
    "            data_splits['train']['sentiment_labels'], self.device\n",
    "        )\n",
    "        emotion_weights = compute_class_weights_from_labels(\n",
    "            data_splits['train']['emotion_labels'], self.device\n",
    "        )\n",
    "        \n",
    "        # Initialize loss function\n",
    "        self.loss_fn = MultiTaskLoss(\n",
    "            alpha=self.config.alpha,\n",
    "            sentiment_class_weights=sentiment_weights,\n",
    "            emotion_class_weights=emotion_weights,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        self.train_dataset = MultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length,\n",
    "            sentiment_label_encoder=sentiment_encoder,\n",
    "            emotion_label_encoder=emotion_encoder\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = MultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length,\n",
    "            sentiment_label_encoder=sentiment_encoder,\n",
    "            emotion_label_encoder=emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_sampler = create_stratified_sampler(\n",
    "            data_splits['train']['sentiment_labels'],\n",
    "            data_splits['train']['emotion_labels']\n",
    "        ) if len(data_splits['train']['texts']) > 50 else None\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            sampler=train_sampler,\n",
    "            shuffle=(train_sampler is None),\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer, self.scheduler = create_optimizer_and_scheduler(\n",
    "            self.model, self.config, num_training_steps\n",
    "        )\n",
    "        \n",
    "        # Initialize utilities\n",
    "        if self.config.adaptive_alpha:\n",
    "            self.alpha_scheduler = AdaptiveAlphaScheduler(\n",
    "                initial_alpha=self.config.alpha\n",
    "            )\n",
    "        \n",
    "        self.early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "        self.checkpointer = ModelCheckpointer(\n",
    "            self.config.output_dir,\n",
    "            self.config.save_total_limit\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Setup complete!\")\n",
    "        print(f\"  Model: {self.config.model_name}\")\n",
    "        print(f\"  Training samples: {len(self.train_dataset)}\")\n",
    "        print(f\"  Validation samples: {len(self.val_dataset)}\")\n",
    "        print(f\"  Training steps per epoch: {len(self.train_loader)}\")\n",
    "        print(f\"  Total training steps: {num_training_steps}\")\n",
    "    \n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_sentiment_loss = 0.0\n",
    "        total_emotion_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss_dict = self.loss_fn(\n",
    "                sentiment_logits=outputs['sentiment_logits'],\n",
    "                emotion_logits=outputs['emotion_logits'],\n",
    "                sentiment_labels=sentiment_labels,\n",
    "                emotion_labels=emotion_labels\n",
    "            )\n",
    "            \n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                self.config.max_grad_norm\n",
    "            )\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            total_loss += loss.item()\n",
    "            total_sentiment_loss += loss_dict['sentiment_loss'].item()\n",
    "            total_emotion_loss += loss_dict['emotion_loss'].item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Logging\n",
    "            if (batch_idx + 1) % self.config.logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                current_lr = self.scheduler.get_last_lr()[0]\n",
    "                print(f\"  Batch {batch_idx + 1}/{len(self.train_loader)} | \"\n",
    "                      f\"Loss: {avg_loss:.4f} | \"\n",
    "                      f\"LR: {current_lr:.2e} | \"\n",
    "                      f\"Alpha: {self.loss_fn.alpha:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'train_loss': total_loss / num_batches,\n",
    "            'train_sentiment_loss': total_sentiment_loss / num_batches,\n",
    "            'train_emotion_loss': total_emotion_loss / num_batches\n",
    "        }\n",
    "    \n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_sentiment_loss = 0.0\n",
    "        total_emotion_loss = 0.0\n",
    "        \n",
    "        sentiment_predictions = []\n",
    "        sentiment_true_labels = []\n",
    "        emotion_predictions = []\n",
    "        emotion_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss_dict = self.loss_fn(\n",
    "                    sentiment_logits=outputs['sentiment_logits'],\n",
    "                    emotion_logits=outputs['emotion_logits'],\n",
    "                    sentiment_labels=sentiment_labels,\n",
    "                    emotion_labels=emotion_labels\n",
    "                )\n",
    "                \n",
    "                # Accumulate losses\n",
    "                total_loss += loss_dict['total_loss'].item()\n",
    "                total_sentiment_loss += loss_dict['sentiment_loss'].item()\n",
    "                total_emotion_loss += loss_dict['emotion_loss'].item()\n",
    "                \n",
    "                # Predictions\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                sentiment_true_labels.extend(sentiment_labels.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                emotion_true_labels.extend(emotion_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        num_batches = len(self.val_loader)\n",
    "        sentiment_accuracy = accuracy_score(sentiment_true_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_true_labels, emotion_predictions)\n",
    "        \n",
    "        return {\n",
    "            'val_loss': total_loss / num_batches,\n",
    "            'val_sentiment_loss': total_sentiment_loss / num_batches,\n",
    "            'val_emotion_loss': total_emotion_loss / num_batches,\n",
    "            'val_sentiment_accuracy': sentiment_accuracy,\n",
    "            'val_emotion_accuracy': emotion_accuracy,\n",
    "            'sentiment_predictions': sentiment_predictions,\n",
    "            'sentiment_true_labels': sentiment_true_labels,\n",
    "            'emotion_predictions': emotion_predictions,\n",
    "            'emotion_true_labels': emotion_true_labels\n",
    "        }\n",
    "    \n",
    "    def train(self) -> Dict[str, List]:\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"🚀 Starting training for {self.config.num_epochs} epochs...\")\n",
    "        \n",
    "        best_combined_score = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Train for one epoch\n",
    "            train_metrics = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_metrics = self.evaluate()\n",
    "            \n",
    "            # Update alpha if adaptive\n",
    "            if self.alpha_scheduler:\n",
    "                new_alpha = self.alpha_scheduler.step(\n",
    "                    val_metrics['val_sentiment_accuracy'],\n",
    "                    val_metrics['val_emotion_accuracy']\n",
    "                )\n",
    "                self.loss_fn.update_alpha(new_alpha)\n",
    "            \n",
    "            # Calculate combined score for checkpointing\n",
    "            combined_score = (\n",
    "                val_metrics['val_sentiment_accuracy'] + \n",
    "                val_metrics['val_emotion_accuracy']\n",
    "            ) / 2\n",
    "            \n",
    "            is_best = combined_score > best_combined_score\n",
    "            if is_best:\n",
    "                best_combined_score = combined_score\n",
    "            \n",
    "            # Log metrics\n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            \n",
    "            print(f\"📊 Epoch {epoch + 1} Results:\")\n",
    "            print(f\"  Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "            print(f\"  Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "            print(f\"  Sentiment Accuracy: {val_metrics['val_sentiment_accuracy']:.4f}\")\n",
    "            print(f\"  Emotion Accuracy: {val_metrics['val_emotion_accuracy']:.4f}\")\n",
    "            print(f\"  Combined Score: {combined_score:.4f}\")\n",
    "            print(f\"  Alpha: {self.loss_fn.alpha:.3f}\")\n",
    "            print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "            \n",
    "            # Save history\n",
    "            self.training_history['epoch'].append(epoch + 1)\n",
    "            self.training_history['train_loss'].append(train_metrics['train_loss'])\n",
    "            self.training_history['train_sentiment_loss'].append(train_metrics['train_sentiment_loss'])\n",
    "            self.training_history['train_emotion_loss'].append(train_metrics['train_emotion_loss'])\n",
    "            self.training_history['val_loss'].append(val_metrics['val_loss'])\n",
    "            self.training_history['val_sentiment_loss'].append(val_metrics['val_sentiment_loss'])\n",
    "            self.training_history['val_emotion_loss'].append(val_metrics['val_emotion_loss'])\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_metrics['val_sentiment_accuracy'])\n",
    "            self.training_history['val_emotion_accuracy'].append(val_metrics['val_emotion_accuracy'])\n",
    "            self.training_history['alpha'].append(self.loss_fn.alpha)\n",
    "            self.training_history['learning_rate'].append(current_lr)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if self.config.save_strategy == \"epoch\":\n",
    "                self.checkpointer.save_checkpoint(\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    optimizer=self.optimizer,\n",
    "                    scheduler=self.scheduler,\n",
    "                    epoch=epoch + 1,\n",
    "                    metrics=val_metrics,\n",
    "                    is_best=is_best\n",
    "                )\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(combined_score):\n",
    "                print(f\"⏹️ Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed!\")\n",
    "        print(f\"Best combined score: {best_combined_score:.4f}\")\n",
    "        \n",
    "        return self.training_history\n",
    "\n",
    "print(\"✅ Training loop defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92c8a262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multitask model architecture defined!\n",
      "Available models: ['bertweet', 'deberta']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Multitask Model Architecture (FIXED)\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multitask Learning Framework for Sentiment and Emotion Classification\n",
    "    \n",
    "    Features:\n",
    "    - Shared transformer encoder (BERTweet, DeBERTa)\n",
    "    - Task-specific attention heads\n",
    "    - Parallel classification heads\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"microsoft/deberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        freeze_encoder: bool = False\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load configuration and adjust dropout\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared transformer encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Freeze encoder if specified\n",
    "        if freeze_encoder:\n",
    "            for param in self.shared_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for shared + task-specific\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for shared + task-specific\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        task: Optional[str] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs [batch_size, seq_len]\n",
    "            attention_mask: Attention mask [batch_size, seq_len]\n",
    "            task: Optional task specification (\"sentiment\", \"emotion\", or None for both)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing logits for requested tasks\n",
    "        \"\"\"\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get sequence output [batch_size, seq_len, hidden_size]\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention to capture common linguistic features\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        \n",
    "        # Pool shared features (use [CLS] token or mean pooling)\n",
    "        shared_pooled = shared_attended[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        if task is None or task == \"sentiment\":\n",
    "            # Task-specific attention for sentiment\n",
    "            sentiment_attended, sentiment_weights = self.sentiment_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "            sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "            \n",
    "            # Pool sentiment features\n",
    "            sentiment_pooled = sentiment_attended[:, 0, :]  # [CLS] token\n",
    "            \n",
    "            # Combine shared and task-specific features\n",
    "            sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "            \n",
    "            # Sentiment classification\n",
    "            sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "            outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "            outputs[\"sentiment_attention_weights\"] = sentiment_weights\n",
    "        \n",
    "        # Emotion branch\n",
    "        if task is None or task == \"emotion\":\n",
    "            # Task-specific attention for emotion\n",
    "            emotion_attended, emotion_weights = self.emotion_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "            emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "            \n",
    "            # Pool emotion features\n",
    "            emotion_pooled = emotion_attended[:, 0, :]  # [CLS] token\n",
    "            \n",
    "            # Combine shared and task-specific features\n",
    "            emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "            \n",
    "            # Emotion classification\n",
    "            emotion_logits = self.emotion_classifier(emotion_features)\n",
    "            outputs[\"emotion_logits\"] = emotion_logits\n",
    "            outputs[\"emotion_attention_weights\"] = emotion_weights\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    # ✅ ADD THESE MISSING HUGGING FACE COMPATIBLE METHODS\n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        \"\"\"Save the model in Hugging Face compatible format\"\"\"\n",
    "        import os\n",
    "        import json\n",
    "        \n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"sentiment_num_classes\": self.sentiment_num_classes,\n",
    "            \"emotion_num_classes\": self.emotion_num_classes,\n",
    "            \"model_type\": \"MultiTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"Model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load the model in Hugging Face compatible format\"\"\"\n",
    "        import os\n",
    "        import json\n",
    "        \n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            sentiment_num_classes=config[\"sentiment_num_classes\"],\n",
    "            emotion_num_classes=config[\"emotion_num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        return model\n",
    "\n",
    "# Model configuration options\n",
    "MODEL_CONFIGS = {\n",
    "    \"bertweet\": {\n",
    "        \"name\": \"vinai/bertweet-base\",\n",
    "        \"description\": \"BERTweet optimized for social media text\"\n",
    "    },\n",
    "    \"deberta\": {\n",
    "        \"name\": \"microsoft/deberta-base\",\n",
    "        \"description\": \"DeBERTa with enhanced attention mechanism\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Multitask model architecture defined!\")\n",
    "print(\"Available models:\", list(MODEL_CONFIGS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb25b79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss functions and schedulers defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Loss Function with Weighting\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted loss function for multitask learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.5,\n",
    "        sentiment_class_weights: Optional[torch.Tensor] = None,\n",
    "        emotion_class_weights: Optional[torch.Tensor] = None,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight parameter between sentiment and emotion loss (0.3-0.7)\n",
    "            sentiment_class_weights: Class weights for sentiment imbalance\n",
    "            emotion_class_weights: Class weights for emotion imbalance\n",
    "        \"\"\"\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.device = device or torch.device('cpu')\n",
    "        \n",
    "        # Initialize loss functions with class weights\n",
    "        self.sentiment_loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=sentiment_class_weights.to(self.device) if sentiment_class_weights is not None else None\n",
    "        )\n",
    "        self.emotion_loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=emotion_class_weights.to(self.device) if emotion_class_weights is not None else None\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        sentiment_logits: torch.Tensor,\n",
    "        emotion_logits: torch.Tensor,\n",
    "        sentiment_labels: torch.Tensor,\n",
    "        emotion_labels: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate weighted multitask loss\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing individual and combined losses\n",
    "        \"\"\"\n",
    "        # Calculate individual losses\n",
    "        sentiment_loss = self.sentiment_loss_fn(sentiment_logits, sentiment_labels)\n",
    "        emotion_loss = self.emotion_loss_fn(emotion_logits, emotion_labels)\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * sentiment_loss + (1 - self.alpha) * emotion_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'sentiment_loss': sentiment_loss,\n",
    "            'emotion_loss': emotion_loss,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "    \n",
    "    def update_alpha(self, new_alpha: float):\n",
    "        \"\"\"Update alpha parameter during training\"\"\"\n",
    "        self.alpha = max(0.3, min(0.7, new_alpha))  # Constrain to [0.3, 0.7]\n",
    "\n",
    "def compute_class_weights_from_labels(labels: List[int], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Compute class weights for imbalanced datasets\"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels,\n",
    "        y=labels\n",
    "    )\n",
    "    return torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "class AdaptiveAlphaScheduler:\n",
    "    \"\"\"\n",
    "    Adaptive alpha scheduler that adjusts the loss weighting based on task performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_alpha: float = 0.5, adaptation_rate: float = 0.1):\n",
    "        self.alpha = initial_alpha\n",
    "        self.adaptation_rate = adaptation_rate\n",
    "        self.sentiment_history = []\n",
    "        self.emotion_history = []\n",
    "    \n",
    "    def step(self, sentiment_accuracy: float, emotion_accuracy: float) -> float:\n",
    "        \"\"\"\n",
    "        Adjust alpha based on relative task performance\n",
    "        Better performing task gets lower weight to balance learning\n",
    "        \"\"\"\n",
    "        self.sentiment_history.append(sentiment_accuracy)\n",
    "        self.emotion_history.append(emotion_accuracy)\n",
    "        \n",
    "        if len(self.sentiment_history) >= 2:\n",
    "            # Calculate performance difference\n",
    "            sentiment_trend = sentiment_accuracy - np.mean(self.sentiment_history[-3:])\n",
    "            emotion_trend = emotion_accuracy - np.mean(self.emotion_history[-3:])\n",
    "            \n",
    "            # Adjust alpha: if sentiment is improving faster, decrease its weight\n",
    "            if sentiment_trend > emotion_trend:\n",
    "                self.alpha -= self.adaptation_rate\n",
    "            elif emotion_trend > sentiment_trend:\n",
    "                self.alpha += self.adaptation_rate\n",
    "            \n",
    "            # Constrain alpha to [0.3, 0.7]\n",
    "            self.alpha = max(0.3, min(0.7, self.alpha))\n",
    "        \n",
    "        return self.alpha\n",
    "\n",
    "print(\"✅ Loss functions and schedulers defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "016a53f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Simplified evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions\n",
    "class MultiTaskEvaluator:\n",
    "    \"\"\"\n",
    "    Simplified evaluation for multitask models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: MultiTaskTransformer,\n",
    "        tokenizer,\n",
    "        sentiment_encoder: LabelEncoder,\n",
    "        emotion_encoder: LabelEncoder,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentiment_encoder = sentiment_encoder\n",
    "        self.emotion_encoder = emotion_encoder\n",
    "        self.device = device\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def evaluate_dataset(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        batch_size: int = 32\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Evaluate model on a dataset\n",
    "        \"\"\"\n",
    "        dataset = MultiTaskDataset(\n",
    "            texts=texts,\n",
    "            sentiment_labels=sentiment_labels,\n",
    "            emotion_labels=emotion_labels,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=512,\n",
    "            sentiment_label_encoder=self.sentiment_encoder,\n",
    "            emotion_label_encoder=self.emotion_encoder\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_true_labels = []\n",
    "        emotion_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Get predictions\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                # Store results\n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_true_labels.extend(batch['sentiment_labels'].numpy())\n",
    "                emotion_true_labels.extend(batch['emotion_labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = self._calculate_metrics(\n",
    "            sentiment_predictions=sentiment_predictions,\n",
    "            emotion_predictions=emotion_predictions,\n",
    "            sentiment_true_labels=sentiment_true_labels,\n",
    "            emotion_true_labels=emotion_true_labels\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_metrics(\n",
    "        self,\n",
    "        sentiment_predictions: List[int],\n",
    "        emotion_predictions: List[int],\n",
    "        sentiment_true_labels: List[int],\n",
    "        emotion_true_labels: List[int]\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"Calculate simplified metrics: only accuracy and macro F1\"\"\"\n",
    "        \n",
    "        # Sentiment metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_true_labels, sentiment_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_true_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        # Emotion metrics\n",
    "        emotion_accuracy = accuracy_score(emotion_true_labels, emotion_predictions)\n",
    "        emotion_f1_macro = f1_score(emotion_true_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment': {\n",
    "                'accuracy': sentiment_accuracy,\n",
    "                'f1_macro': sentiment_f1_macro,\n",
    "                'predictions': sentiment_predictions,\n",
    "                'true_labels': sentiment_true_labels\n",
    "            },\n",
    "            'emotion': {\n",
    "                'accuracy': emotion_accuracy,\n",
    "                'f1_macro': emotion_f1_macro,\n",
    "                'predictions': emotion_predictions,\n",
    "                'true_labels': emotion_true_labels\n",
    "            },\n",
    "            'combined': {\n",
    "                'average_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "                'average_f1': (sentiment_f1_macro + emotion_f1_macro) / 2\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"✅ Simplified evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ce67156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Inference Functions\n",
    "class MultiTaskPredictor:\n",
    "    \"\"\"\n",
    "    Inference class for multitask model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        sentiment_encoder_path: str,\n",
    "        emotion_encoder_path: str,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = MultiTaskTransformer.from_pretrained(model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load label encoders\n",
    "        import joblib\n",
    "        self.sentiment_encoder = joblib.load(sentiment_encoder_path)\n",
    "        self.emotion_encoder = joblib.load(emotion_encoder_path)\n",
    "        \n",
    "        print(f\"✅ Model loaded successfully!\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Sentiment classes: {list(self.sentiment_encoder.classes_)}\")\n",
    "        print(f\"Emotion classes: {list(self.emotion_encoder.classes_)}\")\n",
    "    \n",
    "    def predict_single(\n",
    "        self,\n",
    "        text: str,\n",
    "        return_probabilities: bool = True,\n",
    "        return_attention: bool = False\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Predict sentiment and emotion for a single text\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get predictions\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            \n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            sentiment_pred_id = torch.argmax(sentiment_logits, dim=-1).item()\n",
    "            emotion_pred_id = torch.argmax(emotion_logits, dim=-1).item()\n",
    "            \n",
    "            # Decode predictions\n",
    "            sentiment_label = self.sentiment_encoder.inverse_transform([sentiment_pred_id])[0]\n",
    "            emotion_label = self.emotion_encoder.inverse_transform([emotion_pred_id])[0]\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'sentiment': {\n",
    "                    'label': sentiment_label,\n",
    "                    'confidence': sentiment_probs[0][sentiment_pred_id].item(),\n",
    "                    'class_id': sentiment_pred_id\n",
    "                },\n",
    "                'emotion': {\n",
    "                    'label': emotion_label,\n",
    "                    'confidence': emotion_probs[0][emotion_pred_id].item(),\n",
    "                    'class_id': emotion_pred_id\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if return_probabilities:\n",
    "                result['sentiment']['probabilities'] = {\n",
    "                    class_name: prob.item() for class_name, prob in \n",
    "                    zip(self.sentiment_encoder.classes_, sentiment_probs[0])\n",
    "                }\n",
    "                result['emotion']['probabilities'] = {\n",
    "                    class_name: prob.item() for class_name, prob in \n",
    "                    zip(self.emotion_encoder.classes_, emotion_probs[0])\n",
    "                }\n",
    "            \n",
    "            if return_attention:\n",
    "                result['sentiment']['attention_weights'] = outputs['sentiment_attention_weights']\n",
    "                result['emotion']['attention_weights'] = outputs['emotion_attention_weights']\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_batch(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        batch_size: int = 32,\n",
    "        return_probabilities: bool = False\n",
    "    ) -> List[Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        Predict sentiment and emotion for a batch of texts\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                \n",
    "                sentiment_logits = outputs['sentiment_logits']\n",
    "                emotion_logits = outputs['emotion_logits']\n",
    "                \n",
    "                sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "                emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "                emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                # Process each item in batch\n",
    "                for j in range(len(batch_texts)):\n",
    "                    sentiment_pred_id = sentiment_preds[j].item()\n",
    "                    emotion_pred_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    sentiment_label = self.sentiment_encoder.inverse_transform([sentiment_pred_id])[0]\n",
    "                    emotion_label = self.emotion_encoder.inverse_transform([emotion_pred_id])[0]\n",
    "                    \n",
    "                    result = {\n",
    "                        'text': batch_texts[j],\n",
    "                        'sentiment': {\n",
    "                            'label': sentiment_label,\n",
    "                            'confidence': sentiment_probs[j][sentiment_pred_id].item(),\n",
    "                            'class_id': sentiment_pred_id\n",
    "                        },\n",
    "                        'emotion': {\n",
    "                            'label': emotion_label,\n",
    "                            'confidence': emotion_probs[j][emotion_pred_id].item(),\n",
    "                            'class_id': emotion_pred_id\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    if return_probabilities:\n",
    "                        result['sentiment']['probabilities'] = {\n",
    "                            class_name: prob.item() for class_name, prob in \n",
    "                            zip(self.sentiment_encoder.classes_, sentiment_probs[j])\n",
    "                        }\n",
    "                        result['emotion']['probabilities'] = {\n",
    "                            class_name: prob.item() for class_name, prob in \n",
    "                            zip(self.emotion_encoder.classes_, emotion_probs[j])\n",
    "                        }\n",
    "                    \n",
    "                    results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def save_model_and_encoders(\n",
    "    model: MultiTaskTransformer,\n",
    "    tokenizer,\n",
    "    sentiment_encoder: LabelEncoder,\n",
    "    emotion_encoder: LabelEncoder,\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"Save complete model with encoders\"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save encoders\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    # Save model configuration\n",
    "    config = {\n",
    "        'sentiment_classes': list(sentiment_encoder.classes_),\n",
    "        'emotion_classes': list(emotion_encoder.classes_),\n",
    "        'sentiment_num_classes': len(sentiment_encoder.classes_),\n",
    "        'emotion_num_classes': len(emotion_encoder.classes_)\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'model_config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Model and encoders saved to: {output_dir}\")\n",
    "\n",
    "print(\"✅ Inference functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efde64",
   "metadata": {},
   "source": [
    "# DeBERTa Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05dffa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 2.24 GB\n",
      "  Cached: 4.82 GB\n",
      "🚀 Starting Ultra-Lightweight Training for 8GB GPU...\n",
      "🚀 Starting Ultra-Lightweight Training\n",
      "==================================================\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 2.24 GB\n",
      "  Cached: 2.51 GB\n",
      "\n",
      "1️⃣ Loading minimal external datasets...\n",
      "Loading external datasets for training...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 val\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 val\n",
      "🔄 Preparing external datasets for multitask training...\n",
      "✅ External data prepared:\n",
      "  Train samples: 400\n",
      "  Validation samples: 100\n",
      "  Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "  Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "📈 Training set class distribution:\n",
      "  Sentiment 'Negative': 192 samples\n",
      "  Sentiment 'Neutral': 22 samples\n",
      "  Sentiment 'Positive': 186 samples\n",
      "  Emotion 'Anger': 129 samples\n",
      "  Emotion 'Fear': 69 samples\n",
      "  Emotion 'Joy': 50 samples\n",
      "  Emotion 'No Emotion': 58 samples\n",
      "  Emotion 'Sadness': 71 samples\n",
      "  Emotion 'Surprise': 23 samples\n",
      "\n",
      "2️⃣ Loading Reddit data...\n",
      "🔄 Preparing Reddit data for evaluation...\n",
      "✅ Reddit evaluation data prepared: 95 samples\n",
      "\n",
      "3️⃣ Ultra-lightweight config:\n",
      "  Batch size: 1\n",
      "  Max length: 128\n",
      "  Training samples: 400\n",
      "  Epochs: 2\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 2.24 GB\n",
      "  Cached: 2.51 GB\n",
      "\n",
      "4️⃣ Initializing trainer...\n",
      "\n",
      "5️⃣ Setting up with memory optimizations...\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 400\n",
      "  Validation samples: 100\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 800\n",
      "✅ Gradient checkpointing enabled\n",
      "\n",
      "6️⃣ Training with memory monitoring...\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.8893 | LR: 1.00e-05 | Alpha: 0.500\n",
      "  Batch 40/400 | Loss: 1.7163 | LR: 2.00e-05 | Alpha: 0.500\n",
      "  Batch 60/400 | Loss: 1.6818 | LR: 1.95e-05 | Alpha: 0.500\n",
      "  Batch 80/400 | Loss: 1.6985 | LR: 1.89e-05 | Alpha: 0.500\n",
      "  Batch 100/400 | Loss: 1.6888 | LR: 1.84e-05 | Alpha: 0.500\n",
      "  Batch 120/400 | Loss: 1.6827 | LR: 1.79e-05 | Alpha: 0.500\n",
      "  Batch 140/400 | Loss: 1.6425 | LR: 1.74e-05 | Alpha: 0.500\n",
      "  Batch 160/400 | Loss: 1.6083 | LR: 1.68e-05 | Alpha: 0.500\n",
      "  Batch 180/400 | Loss: 1.5987 | LR: 1.63e-05 | Alpha: 0.500\n",
      "  Batch 200/400 | Loss: 1.5870 | LR: 1.58e-05 | Alpha: 0.500\n",
      "  Batch 220/400 | Loss: 1.5401 | LR: 1.53e-05 | Alpha: 0.500\n",
      "  Batch 240/400 | Loss: 1.5154 | LR: 1.47e-05 | Alpha: 0.500\n",
      "  Batch 260/400 | Loss: 1.5077 | LR: 1.42e-05 | Alpha: 0.500\n",
      "  Batch 280/400 | Loss: 1.5053 | LR: 1.37e-05 | Alpha: 0.500\n",
      "  Batch 300/400 | Loss: 1.4892 | LR: 1.32e-05 | Alpha: 0.500\n",
      "  Batch 320/400 | Loss: 1.4800 | LR: 1.26e-05 | Alpha: 0.500\n",
      "  Batch 340/400 | Loss: 1.4817 | LR: 1.21e-05 | Alpha: 0.500\n",
      "  Batch 360/400 | Loss: 1.4528 | LR: 1.16e-05 | Alpha: 0.500\n",
      "  Batch 380/400 | Loss: 1.4386 | LR: 1.11e-05 | Alpha: 0.500\n",
      "  Batch 400/400 | Loss: 1.4136 | LR: 1.05e-05 | Alpha: 0.500\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.4136\n",
      "  Val Loss: 1.5122\n",
      "  Sentiment Accuracy: 0.6400\n",
      "  Emotion Accuracy: 0.1300\n",
      "  Combined Score: 0.3850\n",
      "  Alpha: 0.500\n",
      "  Learning Rate: 1.05e-05\n",
      "Model saved to ./multitask_model_ultra_light\\checkpoint-epoch-1\n",
      "Model saved to ./multitask_model_ultra_light\\best_model\n",
      "\n",
      "📍 Epoch 2/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.0342 | LR: 1.00e-05 | Alpha: 0.500\n",
      "  Batch 40/400 | Loss: 1.1416 | LR: 9.47e-06 | Alpha: 0.500\n",
      "  Batch 60/400 | Loss: 1.0107 | LR: 8.95e-06 | Alpha: 0.500\n",
      "  Batch 80/400 | Loss: 0.9777 | LR: 8.42e-06 | Alpha: 0.500\n",
      "  Batch 100/400 | Loss: 0.8983 | LR: 7.89e-06 | Alpha: 0.500\n",
      "  Batch 120/400 | Loss: 0.8877 | LR: 7.37e-06 | Alpha: 0.500\n",
      "  Batch 140/400 | Loss: 0.8705 | LR: 6.84e-06 | Alpha: 0.500\n",
      "  Batch 160/400 | Loss: 0.8755 | LR: 6.32e-06 | Alpha: 0.500\n",
      "  Batch 180/400 | Loss: 0.8646 | LR: 5.79e-06 | Alpha: 0.500\n",
      "  Batch 200/400 | Loss: 0.8549 | LR: 5.26e-06 | Alpha: 0.500\n",
      "  Batch 220/400 | Loss: 0.8574 | LR: 4.74e-06 | Alpha: 0.500\n",
      "  Batch 240/400 | Loss: 0.8817 | LR: 4.21e-06 | Alpha: 0.500\n",
      "  Batch 260/400 | Loss: 0.8701 | LR: 3.68e-06 | Alpha: 0.500\n",
      "  Batch 280/400 | Loss: 0.8722 | LR: 3.16e-06 | Alpha: 0.500\n",
      "  Batch 300/400 | Loss: 0.8483 | LR: 2.63e-06 | Alpha: 0.500\n",
      "  Batch 320/400 | Loss: 0.8436 | LR: 2.11e-06 | Alpha: 0.500\n",
      "  Batch 340/400 | Loss: 0.8377 | LR: 1.58e-06 | Alpha: 0.500\n",
      "  Batch 360/400 | Loss: 0.8318 | LR: 1.05e-06 | Alpha: 0.500\n",
      "  Batch 380/400 | Loss: 0.8232 | LR: 5.26e-07 | Alpha: 0.500\n",
      "  Batch 400/400 | Loss: 0.8038 | LR: 0.00e+00 | Alpha: 0.500\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 0.8038\n",
      "  Val Loss: 1.5479\n",
      "  Sentiment Accuracy: 0.7400\n",
      "  Emotion Accuracy: 0.1800\n",
      "  Combined Score: 0.4600\n",
      "  Alpha: 0.500\n",
      "  Learning Rate: 0.00e+00\n",
      "Model saved to ./multitask_model_ultra_light\\checkpoint-epoch-2\n",
      "Model saved to ./multitask_model_ultra_light\\best_model\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4600\n",
      "✅ Training completed!\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 4.46 GB\n",
      "  Cached: 4.83 GB\n",
      "\n",
      "7️⃣ Evaluating...\n",
      "Model saved to ./multitask_model_ultra_light\\final_model\n",
      "✅ Model and encoders saved to: ./multitask_model_ultra_light\\final_model\n",
      "\n",
      "📈 Results:\n",
      "Sentiment Accuracy: 0.5579\n",
      "Emotion Accuracy: 0.0842\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 4.46 GB\n",
      "  Cached: 4.83 GB\n",
      "\n",
      "🎉 Training successful!\n",
      "Sentiment Accuracy: 0.5579\n",
      "Emotion Accuracy: 0.0842\n"
     ]
    }
   ],
   "source": [
    "# Clear everything first\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Run ultra-lightweight training\n",
    "print(\"🚀 Starting Ultra-Lightweight Training for 8GB GPU...\")\n",
    "model, results = run_ultra_lightweight_training(\n",
    "    reddit_data_path=\"annotated_reddit_posts.csv\",\n",
    "    model_name=\"microsoft/deberta-base\",\n",
    "    output_dir=\"./multitask_model_ultra_light\",\n",
    "    max_external_samples=500  # Very small training set\n",
    ")\n",
    "\n",
    "if model is not None:\n",
    "    print(\"\\n🎉 Training successful!\")\n",
    "    print(f\"Sentiment Accuracy: {results['sentiment']['accuracy']:.4f}\")\n",
    "    print(f\"Emotion Accuracy: {results['emotion']['accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"\\n💡 If still failing, try:\")\n",
    "    print(\"1. Restart kernel completely\")\n",
    "    print(\"2. Use CPU training: device = torch.device('cpu')\")\n",
    "    print(\"3. Use an even smaller model like 'distilbert-base-uncased'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9570150",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0507cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed hyperparameter tuning functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Fixed Hyperparameter Tuning for External Datasets\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "class MultiTaskHyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning for multitask learning using Optuna\n",
    "    Now properly uses external datasets for training!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        reddit_data_path: str,\n",
    "        n_trials: int = 15,  # Reduced for faster tuning\n",
    "        model_name: str = \"microsoft/deberta-base\",\n",
    "        max_external_samples: int = 2000  # Small for fast tuning\n",
    "    ):\n",
    "        self.reddit_data_path = reddit_data_path\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        self.max_external_samples = max_external_samples\n",
    "        \n",
    "        # Validate model choice\n",
    "        if model_name not in [config[\"name\"] for config in MODEL_CONFIGS.values()]:\n",
    "            available_models = [config[\"name\"] for config in MODEL_CONFIGS.values()]\n",
    "            raise ValueError(f\"Model must be one of: {available_models}\")\n",
    "        \n",
    "        # Load external datasets for training (like your main training)\n",
    "        print(\"🔄 Loading external datasets for hyperparameter tuning...\")\n",
    "        sentiment_data, emotion_data = load_external_datasets()\n",
    "        \n",
    "        # Prepare external data splits\n",
    "        self.external_data_splits, self.sentiment_encoder, self.emotion_encoder = prepare_external_data_for_multitask(\n",
    "            sentiment_data, emotion_data, max_samples=max_external_samples\n",
    "        )\n",
    "        \n",
    "        # Load Reddit data for evaluation\n",
    "        reddit_df = pd.read_csv(reddit_data_path)\n",
    "        self.reddit_evaluation_data = prepare_reddit_data_for_evaluation(\n",
    "            reddit_df, self.sentiment_encoder, self.emotion_encoder\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Hyperparameter tuner initialized\")\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Training data: {len(self.external_data_splits['train']['texts'])} external samples\")\n",
    "        print(f\"Evaluation data: {len(self.reddit_evaluation_data['texts'])} Reddit samples\")\n",
    "        print(f\"Trials: {n_trials}\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters - focused on key ones\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [2, 4, 8])  # Small for memory\n",
    "        alpha = trial.suggest_float('alpha', 0.3, 0.7)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.05, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.4)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.001, 0.1)\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.05, 0.2)\n",
    "        num_epochs = trial.suggest_int('num_epochs', 2, 5)  # Few epochs for speed\n",
    "        max_length = trial.suggest_categorical('max_length', [128, 256])  # Memory optimization\n",
    "        \n",
    "        # Create configuration\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            alpha=alpha,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            adaptive_alpha=False,  # Disable for consistent comparison\n",
    "            output_dir=f\"./temp_trial_{trial.number}\",\n",
    "            save_strategy=\"no\",  # Don't save during tuning\n",
    "            save_total_limit=1\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory before trial\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = MultiTaskTrainer(\n",
    "                config=config,\n",
    "                sentiment_num_classes=len(self.sentiment_encoder.classes_),\n",
    "                emotion_num_classes=len(self.emotion_encoder.classes_)\n",
    "            )\n",
    "            \n",
    "            # Setup with external data (not Reddit data!)\n",
    "            trainer.setup(self.external_data_splits, self.sentiment_encoder, self.emotion_encoder)\n",
    "            \n",
    "            # Train model on external data\n",
    "            history = trainer.train()\n",
    "            \n",
    "            # Clear memory before evaluation\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Evaluate on Reddit data (the real test!)\n",
    "            evaluator = MultiTaskEvaluator(\n",
    "                model=trainer.model,\n",
    "                tokenizer=trainer.tokenizer,\n",
    "                sentiment_encoder=self.sentiment_encoder,\n",
    "                emotion_encoder=self.emotion_encoder,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            reddit_results = evaluator.evaluate_dataset(\n",
    "                texts=self.reddit_evaluation_data['texts'],\n",
    "                sentiment_labels=self.reddit_evaluation_data['sentiment_labels'],\n",
    "                emotion_labels=self.reddit_evaluation_data['emotion_labels'],\n",
    "                batch_size=2  # Small batch for evaluation\n",
    "            )\n",
    "            \n",
    "            # Combined score based on Reddit evaluation (what we really care about)\n",
    "            combined_score = (\n",
    "                reddit_results['sentiment']['accuracy'] + \n",
    "                reddit_results['emotion']['accuracy']\n",
    "            ) / 2\n",
    "            \n",
    "            print(f\"Trial {trial.number}: Combined Score = {combined_score:.4f} \"\n",
    "                  f\"(Sentiment: {reddit_results['sentiment']['accuracy']:.4f}, \"\n",
    "                  f\"Emotion: {reddit_results['emotion']['accuracy']:.4f})\")\n",
    "            \n",
    "            # Clean up\n",
    "            del trainer, evaluator\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            return combined_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            aggressive_memory_cleanup()\n",
    "            return 0.0\n",
    "    \n",
    "    def tune(self) -> optuna.Study:\n",
    "        \"\"\"Run hyperparameter optimization\"\"\"\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
    "        )\n",
    "        \n",
    "        print(f\"🔍 Starting hyperparameter optimization...\")\n",
    "        print(f\"This will run {self.n_trials} trials - each training and evaluating a model\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n🏆 Optimization completed!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best combined score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "def run_hyperparameter_tuning_fixed(\n",
    "    reddit_data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    n_trials: int = 15,\n",
    "    model_name: str = \"microsoft/deberta-base\"\n",
    "):\n",
    "    \"\"\"Run hyperparameter tuning and train final model with best params\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting Hyperparameter Tuning with External Training Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run tuning\n",
    "    tuner = MultiTaskHyperparameterTuner(\n",
    "        reddit_data_path=reddit_data_path,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name,\n",
    "        max_external_samples=2000  # Keep small for fast tuning\n",
    "    )\n",
    "    \n",
    "    study = tuner.tune()\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(f\"\\n🚀 Training final model with best hyperparameters...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Run the optimized training\n",
    "    model, results = run_ultra_lightweight_training(\n",
    "        reddit_data_path=reddit_data_path,\n",
    "        model_name=model_name,\n",
    "        output_dir=\"./multitask_model_optimized\",\n",
    "        max_external_samples=5000  # Use more data for final model\n",
    "    )\n",
    "    \n",
    "    # Apply best hyperparameters to final training config\n",
    "    print(f\"\\n📋 Best hyperparameters found:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save tuning results\n",
    "    import pickle\n",
    "    os.makedirs(\"./multitask_model_optimized\", exist_ok=True)\n",
    "    with open(\"./multitask_model_optimized/hyperparameter_study.pkl\", 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "    \n",
    "    print(f\"\\n📈 Final optimized results:\")\n",
    "    print(f\"Sentiment Accuracy: {results['sentiment']['accuracy']:.4f}\")\n",
    "    print(f\"Emotion Accuracy: {results['emotion']['accuracy']:.4f}\")\n",
    "    \n",
    "    return model, results, study\n",
    "\n",
    "print(\"✅ Fixed hyperparameter tuning functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae1b8f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Optimization...\n",
      "🚀 Starting Hyperparameter Tuning with External Training Data\n",
      "============================================================\n",
      "🔄 Loading external datasets for hyperparameter tuning...\n",
      "Loading external datasets for training...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 val\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 val\n",
      "🔄 Preparing external datasets for multitask training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 22:32:11,946] A new study created in memory with name: no-name-a6add346-b4ec-4566-8e1e-0b0b9a6df221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ External data prepared:\n",
      "  Train samples: 1600\n",
      "  Validation samples: 400\n",
      "  Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "  Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "📈 Training set class distribution:\n",
      "  Sentiment 'Negative': 721 samples\n",
      "  Sentiment 'Neutral': 92 samples\n",
      "  Sentiment 'Positive': 787 samples\n",
      "  Emotion 'Anger': 522 samples\n",
      "  Emotion 'Fear': 267 samples\n",
      "  Emotion 'Joy': 186 samples\n",
      "  Emotion 'No Emotion': 231 samples\n",
      "  Emotion 'Sadness': 281 samples\n",
      "  Emotion 'Surprise': 113 samples\n",
      "🔄 Preparing Reddit data for evaluation...\n",
      "✅ Reddit evaluation data prepared: 95 samples\n",
      "✅ Hyperparameter tuner initialized\n",
      "Model: microsoft/deberta-base\n",
      "Training data: 1600 external samples\n",
      "Evaluation data: 95 Reddit samples\n",
      "Trials: 15\n",
      "🔍 Starting hyperparameter optimization...\n",
      "This will run 15 trials - each training and evaluating a model\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 3200\n",
      "🚀 Starting training for 4 epochs...\n",
      "\n",
      "📍 Epoch 1/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.8833 | LR: 1.06e-06 | Alpha: 0.362\n",
      "  Batch 40/800 | Loss: 2.0093 | LR: 2.12e-06 | Alpha: 0.362\n",
      "  Batch 60/800 | Loss: 2.0011 | LR: 3.17e-06 | Alpha: 0.362\n",
      "  Batch 80/800 | Loss: 1.9288 | LR: 4.23e-06 | Alpha: 0.362\n",
      "  Batch 100/800 | Loss: 1.8873 | LR: 5.29e-06 | Alpha: 0.362\n",
      "  Batch 120/800 | Loss: 1.8620 | LR: 6.35e-06 | Alpha: 0.362\n",
      "  Batch 140/800 | Loss: 1.8181 | LR: 7.40e-06 | Alpha: 0.362\n",
      "  Batch 160/800 | Loss: 1.7895 | LR: 8.46e-06 | Alpha: 0.362\n",
      "  Batch 180/800 | Loss: 1.7745 | LR: 9.52e-06 | Alpha: 0.362\n",
      "  Batch 200/800 | Loss: 1.7577 | LR: 1.06e-05 | Alpha: 0.362\n",
      "  Batch 220/800 | Loss: 1.7347 | LR: 1.16e-05 | Alpha: 0.362\n",
      "  Batch 240/800 | Loss: 1.7226 | LR: 1.27e-05 | Alpha: 0.362\n",
      "  Batch 260/800 | Loss: 1.7240 | LR: 1.37e-05 | Alpha: 0.362\n",
      "  Batch 280/800 | Loss: 1.7194 | LR: 1.48e-05 | Alpha: 0.362\n",
      "  Batch 300/800 | Loss: 1.7064 | LR: 1.59e-05 | Alpha: 0.362\n",
      "  Batch 320/800 | Loss: 1.7063 | LR: 1.69e-05 | Alpha: 0.362\n",
      "  Batch 340/800 | Loss: 1.6986 | LR: 1.80e-05 | Alpha: 0.362\n",
      "  Batch 360/800 | Loss: 1.6870 | LR: 1.90e-05 | Alpha: 0.362\n",
      "  Batch 380/800 | Loss: 1.6862 | LR: 2.01e-05 | Alpha: 0.362\n",
      "  Batch 400/800 | Loss: 1.6818 | LR: 2.12e-05 | Alpha: 0.362\n",
      "  Batch 420/800 | Loss: 1.6680 | LR: 2.22e-05 | Alpha: 0.362\n",
      "  Batch 440/800 | Loss: 1.6533 | LR: 2.33e-05 | Alpha: 0.362\n",
      "  Batch 460/800 | Loss: 1.6464 | LR: 2.36e-05 | Alpha: 0.362\n",
      "  Batch 480/800 | Loss: 1.6456 | LR: 2.34e-05 | Alpha: 0.362\n",
      "  Batch 500/800 | Loss: 1.6373 | LR: 2.32e-05 | Alpha: 0.362\n",
      "  Batch 520/800 | Loss: 1.6191 | LR: 2.31e-05 | Alpha: 0.362\n",
      "  Batch 540/800 | Loss: 1.6111 | LR: 2.29e-05 | Alpha: 0.362\n",
      "  Batch 560/800 | Loss: 1.5991 | LR: 2.27e-05 | Alpha: 0.362\n",
      "  Batch 580/800 | Loss: 1.5972 | LR: 2.26e-05 | Alpha: 0.362\n",
      "  Batch 600/800 | Loss: 1.5812 | LR: 2.24e-05 | Alpha: 0.362\n",
      "  Batch 620/800 | Loss: 1.5758 | LR: 2.22e-05 | Alpha: 0.362\n",
      "  Batch 640/800 | Loss: 1.5641 | LR: 2.20e-05 | Alpha: 0.362\n",
      "  Batch 660/800 | Loss: 1.5455 | LR: 2.19e-05 | Alpha: 0.362\n",
      "  Batch 680/800 | Loss: 1.5356 | LR: 2.17e-05 | Alpha: 0.362\n",
      "  Batch 700/800 | Loss: 1.5275 | LR: 2.15e-05 | Alpha: 0.362\n",
      "  Batch 720/800 | Loss: 1.5221 | LR: 2.13e-05 | Alpha: 0.362\n",
      "  Batch 740/800 | Loss: 1.5200 | LR: 2.12e-05 | Alpha: 0.362\n",
      "  Batch 760/800 | Loss: 1.5150 | LR: 2.10e-05 | Alpha: 0.362\n",
      "  Batch 780/800 | Loss: 1.5012 | LR: 2.08e-05 | Alpha: 0.362\n",
      "  Batch 800/800 | Loss: 1.4967 | LR: 2.07e-05 | Alpha: 0.362\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.4967\n",
      "  Val Loss: 1.8220\n",
      "  Sentiment Accuracy: 0.4500\n",
      "  Emotion Accuracy: 0.1150\n",
      "  Combined Score: 0.2825\n",
      "  Alpha: 0.362\n",
      "  Learning Rate: 2.07e-05\n",
      "\n",
      "📍 Epoch 2/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.9923 | LR: 2.05e-05 | Alpha: 0.362\n",
      "  Batch 40/800 | Loss: 1.0758 | LR: 2.03e-05 | Alpha: 0.362\n",
      "  Batch 60/800 | Loss: 1.0747 | LR: 2.01e-05 | Alpha: 0.362\n",
      "  Batch 80/800 | Loss: 1.1133 | LR: 2.00e-05 | Alpha: 0.362\n",
      "  Batch 100/800 | Loss: 1.1151 | LR: 1.98e-05 | Alpha: 0.362\n",
      "  Batch 120/800 | Loss: 1.1125 | LR: 1.96e-05 | Alpha: 0.362\n",
      "  Batch 140/800 | Loss: 1.0888 | LR: 1.95e-05 | Alpha: 0.362\n",
      "  Batch 160/800 | Loss: 1.0902 | LR: 1.93e-05 | Alpha: 0.362\n",
      "  Batch 180/800 | Loss: 1.0753 | LR: 1.91e-05 | Alpha: 0.362\n",
      "  Batch 200/800 | Loss: 1.0830 | LR: 1.89e-05 | Alpha: 0.362\n",
      "  Batch 220/800 | Loss: 1.0676 | LR: 1.88e-05 | Alpha: 0.362\n",
      "  Batch 240/800 | Loss: 1.0772 | LR: 1.86e-05 | Alpha: 0.362\n",
      "  Batch 260/800 | Loss: 1.0647 | LR: 1.84e-05 | Alpha: 0.362\n",
      "  Batch 280/800 | Loss: 1.0645 | LR: 1.82e-05 | Alpha: 0.362\n",
      "  Batch 300/800 | Loss: 1.0683 | LR: 1.81e-05 | Alpha: 0.362\n",
      "  Batch 320/800 | Loss: 1.0781 | LR: 1.79e-05 | Alpha: 0.362\n",
      "  Batch 340/800 | Loss: 1.0734 | LR: 1.77e-05 | Alpha: 0.362\n",
      "  Batch 360/800 | Loss: 1.0723 | LR: 1.76e-05 | Alpha: 0.362\n",
      "  Batch 380/800 | Loss: 1.0784 | LR: 1.74e-05 | Alpha: 0.362\n",
      "  Batch 400/800 | Loss: 1.0724 | LR: 1.72e-05 | Alpha: 0.362\n",
      "  Batch 420/800 | Loss: 1.0535 | LR: 1.70e-05 | Alpha: 0.362\n",
      "  Batch 440/800 | Loss: 1.0439 | LR: 1.69e-05 | Alpha: 0.362\n",
      "  Batch 460/800 | Loss: 1.0311 | LR: 1.67e-05 | Alpha: 0.362\n",
      "  Batch 480/800 | Loss: 1.0267 | LR: 1.65e-05 | Alpha: 0.362\n",
      "  Batch 500/800 | Loss: 1.0367 | LR: 1.64e-05 | Alpha: 0.362\n",
      "  Batch 520/800 | Loss: 1.0351 | LR: 1.62e-05 | Alpha: 0.362\n",
      "  Batch 540/800 | Loss: 1.0278 | LR: 1.60e-05 | Alpha: 0.362\n",
      "  Batch 560/800 | Loss: 1.0126 | LR: 1.58e-05 | Alpha: 0.362\n",
      "  Batch 580/800 | Loss: 1.0052 | LR: 1.57e-05 | Alpha: 0.362\n",
      "  Batch 600/800 | Loss: 1.0032 | LR: 1.55e-05 | Alpha: 0.362\n",
      "  Batch 620/800 | Loss: 1.0001 | LR: 1.53e-05 | Alpha: 0.362\n",
      "  Batch 640/800 | Loss: 0.9933 | LR: 1.51e-05 | Alpha: 0.362\n",
      "  Batch 660/800 | Loss: 0.9966 | LR: 1.50e-05 | Alpha: 0.362\n",
      "  Batch 680/800 | Loss: 0.9845 | LR: 1.48e-05 | Alpha: 0.362\n",
      "  Batch 700/800 | Loss: 0.9887 | LR: 1.46e-05 | Alpha: 0.362\n",
      "  Batch 720/800 | Loss: 0.9902 | LR: 1.45e-05 | Alpha: 0.362\n",
      "  Batch 740/800 | Loss: 0.9825 | LR: 1.43e-05 | Alpha: 0.362\n",
      "  Batch 760/800 | Loss: 0.9703 | LR: 1.41e-05 | Alpha: 0.362\n",
      "  Batch 780/800 | Loss: 0.9677 | LR: 1.39e-05 | Alpha: 0.362\n",
      "  Batch 800/800 | Loss: 0.9594 | LR: 1.38e-05 | Alpha: 0.362\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 0.9594\n",
      "  Val Loss: 1.9567\n",
      "  Sentiment Accuracy: 0.7500\n",
      "  Emotion Accuracy: 0.1725\n",
      "  Combined Score: 0.4612\n",
      "  Alpha: 0.362\n",
      "  Learning Rate: 1.38e-05\n",
      "\n",
      "📍 Epoch 3/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.6003 | LR: 1.36e-05 | Alpha: 0.362\n",
      "  Batch 40/800 | Loss: 0.6654 | LR: 1.34e-05 | Alpha: 0.362\n",
      "  Batch 60/800 | Loss: 0.7170 | LR: 1.33e-05 | Alpha: 0.362\n",
      "  Batch 80/800 | Loss: 0.6908 | LR: 1.31e-05 | Alpha: 0.362\n",
      "  Batch 100/800 | Loss: 0.6973 | LR: 1.29e-05 | Alpha: 0.362\n",
      "  Batch 120/800 | Loss: 0.6336 | LR: 1.27e-05 | Alpha: 0.362\n",
      "  Batch 140/800 | Loss: 0.6240 | LR: 1.26e-05 | Alpha: 0.362\n",
      "  Batch 160/800 | Loss: 0.6335 | LR: 1.24e-05 | Alpha: 0.362\n",
      "  Batch 180/800 | Loss: 0.6208 | LR: 1.22e-05 | Alpha: 0.362\n",
      "  Batch 200/800 | Loss: 0.6270 | LR: 1.21e-05 | Alpha: 0.362\n",
      "  Batch 220/800 | Loss: 0.5968 | LR: 1.19e-05 | Alpha: 0.362\n",
      "  Batch 240/800 | Loss: 0.6075 | LR: 1.17e-05 | Alpha: 0.362\n",
      "  Batch 260/800 | Loss: 0.6446 | LR: 1.15e-05 | Alpha: 0.362\n",
      "  Batch 280/800 | Loss: 0.6581 | LR: 1.14e-05 | Alpha: 0.362\n",
      "  Batch 300/800 | Loss: 0.6545 | LR: 1.12e-05 | Alpha: 0.362\n",
      "  Batch 320/800 | Loss: 0.6715 | LR: 1.10e-05 | Alpha: 0.362\n",
      "  Batch 340/800 | Loss: 0.6681 | LR: 1.08e-05 | Alpha: 0.362\n",
      "  Batch 360/800 | Loss: 0.6610 | LR: 1.07e-05 | Alpha: 0.362\n",
      "  Batch 380/800 | Loss: 0.6552 | LR: 1.05e-05 | Alpha: 0.362\n",
      "  Batch 400/800 | Loss: 0.6734 | LR: 1.03e-05 | Alpha: 0.362\n",
      "  Batch 420/800 | Loss: 0.6654 | LR: 1.02e-05 | Alpha: 0.362\n",
      "  Batch 440/800 | Loss: 0.6501 | LR: 9.99e-06 | Alpha: 0.362\n",
      "  Batch 460/800 | Loss: 0.6377 | LR: 9.81e-06 | Alpha: 0.362\n",
      "  Batch 480/800 | Loss: 0.6344 | LR: 9.64e-06 | Alpha: 0.362\n",
      "  Batch 500/800 | Loss: 0.6274 | LR: 9.47e-06 | Alpha: 0.362\n",
      "  Batch 520/800 | Loss: 0.6314 | LR: 9.30e-06 | Alpha: 0.362\n",
      "  Batch 540/800 | Loss: 0.6145 | LR: 9.12e-06 | Alpha: 0.362\n",
      "  Batch 560/800 | Loss: 0.6105 | LR: 8.95e-06 | Alpha: 0.362\n",
      "  Batch 580/800 | Loss: 0.6029 | LR: 8.78e-06 | Alpha: 0.362\n",
      "  Batch 600/800 | Loss: 0.6095 | LR: 8.61e-06 | Alpha: 0.362\n",
      "  Batch 620/800 | Loss: 0.6020 | LR: 8.44e-06 | Alpha: 0.362\n",
      "  Batch 640/800 | Loss: 0.5933 | LR: 8.26e-06 | Alpha: 0.362\n",
      "  Batch 660/800 | Loss: 0.5845 | LR: 8.09e-06 | Alpha: 0.362\n",
      "  Batch 680/800 | Loss: 0.5737 | LR: 7.92e-06 | Alpha: 0.362\n",
      "  Batch 700/800 | Loss: 0.5758 | LR: 7.75e-06 | Alpha: 0.362\n",
      "  Batch 720/800 | Loss: 0.5739 | LR: 7.57e-06 | Alpha: 0.362\n",
      "  Batch 740/800 | Loss: 0.5658 | LR: 7.40e-06 | Alpha: 0.362\n",
      "  Batch 760/800 | Loss: 0.5663 | LR: 7.23e-06 | Alpha: 0.362\n",
      "  Batch 780/800 | Loss: 0.5646 | LR: 7.06e-06 | Alpha: 0.362\n",
      "  Batch 800/800 | Loss: 0.5601 | LR: 6.89e-06 | Alpha: 0.362\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.5601\n",
      "  Val Loss: 2.5421\n",
      "  Sentiment Accuracy: 0.8125\n",
      "  Emotion Accuracy: 0.1675\n",
      "  Combined Score: 0.4900\n",
      "  Alpha: 0.362\n",
      "  Learning Rate: 6.89e-06\n",
      "\n",
      "📍 Epoch 4/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.5345 | LR: 6.71e-06 | Alpha: 0.362\n",
      "  Batch 40/800 | Loss: 0.4655 | LR: 6.54e-06 | Alpha: 0.362\n",
      "  Batch 60/800 | Loss: 0.3904 | LR: 6.37e-06 | Alpha: 0.362\n",
      "  Batch 80/800 | Loss: 0.4453 | LR: 6.20e-06 | Alpha: 0.362\n",
      "  Batch 100/800 | Loss: 0.4372 | LR: 6.03e-06 | Alpha: 0.362\n",
      "  Batch 120/800 | Loss: 0.4412 | LR: 5.85e-06 | Alpha: 0.362\n",
      "  Batch 140/800 | Loss: 0.4241 | LR: 5.68e-06 | Alpha: 0.362\n",
      "  Batch 160/800 | Loss: 0.4220 | LR: 5.51e-06 | Alpha: 0.362\n",
      "  Batch 180/800 | Loss: 0.4165 | LR: 5.34e-06 | Alpha: 0.362\n",
      "  Batch 200/800 | Loss: 0.4182 | LR: 5.16e-06 | Alpha: 0.362\n",
      "  Batch 220/800 | Loss: 0.4487 | LR: 4.99e-06 | Alpha: 0.362\n",
      "  Batch 240/800 | Loss: 0.4389 | LR: 4.82e-06 | Alpha: 0.362\n",
      "  Batch 260/800 | Loss: 0.4238 | LR: 4.65e-06 | Alpha: 0.362\n",
      "  Batch 280/800 | Loss: 0.4313 | LR: 4.48e-06 | Alpha: 0.362\n",
      "  Batch 300/800 | Loss: 0.4151 | LR: 4.30e-06 | Alpha: 0.362\n",
      "  Batch 320/800 | Loss: 0.4059 | LR: 4.13e-06 | Alpha: 0.362\n",
      "  Batch 340/800 | Loss: 0.3885 | LR: 3.96e-06 | Alpha: 0.362\n",
      "  Batch 360/800 | Loss: 0.3752 | LR: 3.79e-06 | Alpha: 0.362\n",
      "  Batch 380/800 | Loss: 0.3886 | LR: 3.62e-06 | Alpha: 0.362\n",
      "  Batch 400/800 | Loss: 0.3834 | LR: 3.44e-06 | Alpha: 0.362\n",
      "  Batch 420/800 | Loss: 0.3866 | LR: 3.27e-06 | Alpha: 0.362\n",
      "  Batch 440/800 | Loss: 0.3883 | LR: 3.10e-06 | Alpha: 0.362\n",
      "  Batch 460/800 | Loss: 0.3912 | LR: 2.93e-06 | Alpha: 0.362\n",
      "  Batch 480/800 | Loss: 0.4003 | LR: 2.75e-06 | Alpha: 0.362\n",
      "  Batch 500/800 | Loss: 0.3986 | LR: 2.58e-06 | Alpha: 0.362\n",
      "  Batch 520/800 | Loss: 0.3961 | LR: 2.41e-06 | Alpha: 0.362\n",
      "  Batch 540/800 | Loss: 0.3956 | LR: 2.24e-06 | Alpha: 0.362\n",
      "  Batch 560/800 | Loss: 0.4017 | LR: 2.07e-06 | Alpha: 0.362\n",
      "  Batch 580/800 | Loss: 0.4009 | LR: 1.89e-06 | Alpha: 0.362\n",
      "  Batch 600/800 | Loss: 0.4015 | LR: 1.72e-06 | Alpha: 0.362\n",
      "  Batch 620/800 | Loss: 0.3980 | LR: 1.55e-06 | Alpha: 0.362\n",
      "  Batch 640/800 | Loss: 0.3956 | LR: 1.38e-06 | Alpha: 0.362\n",
      "  Batch 660/800 | Loss: 0.4043 | LR: 1.21e-06 | Alpha: 0.362\n",
      "  Batch 680/800 | Loss: 0.3967 | LR: 1.03e-06 | Alpha: 0.362\n",
      "  Batch 700/800 | Loss: 0.3962 | LR: 8.61e-07 | Alpha: 0.362\n",
      "  Batch 720/800 | Loss: 0.3890 | LR: 6.89e-07 | Alpha: 0.362\n",
      "  Batch 740/800 | Loss: 0.3930 | LR: 5.16e-07 | Alpha: 0.362\n",
      "  Batch 760/800 | Loss: 0.3947 | LR: 3.44e-07 | Alpha: 0.362\n",
      "  Batch 780/800 | Loss: 0.3958 | LR: 1.72e-07 | Alpha: 0.362\n",
      "  Batch 800/800 | Loss: 0.3911 | LR: 0.00e+00 | Alpha: 0.362\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.3911\n",
      "  Val Loss: 2.8310\n",
      "  Sentiment Accuracy: 0.8125\n",
      "  Emotion Accuracy: 0.2000\n",
      "  Combined Score: 0.5062\n",
      "  Alpha: 0.362\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.5062\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.61 GB\n",
      "  Cached: 6.22 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 22:44:17,935] Trial 0 finished with value: 0.3368421052631579 and parameters: {'learning_rate': 2.368863950364079e-05, 'batch_size': 2, 'alpha': 0.36240745617697456, 'hidden_dropout_prob': 0.08899863008405066, 'classifier_dropout': 0.11742508365045984, 'weight_decay': 0.08675143843171859, 'warmup_ratio': 0.14016725176148134, 'num_epochs': 4, 'max_length': 256}. Best is trial 0 with value: 0.3368421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: Combined Score = 0.3368 (Sentiment: 0.4421, Emotion: 0.2316)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 1600\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 2.4552 | LR: 6.02e-06 | Alpha: 0.422\n",
      "  Batch 40/800 | Loss: 2.2762 | LR: 1.20e-05 | Alpha: 0.422\n",
      "  Batch 60/800 | Loss: 2.0872 | LR: 1.81e-05 | Alpha: 0.422\n",
      "  Batch 80/800 | Loss: 2.0518 | LR: 2.41e-05 | Alpha: 0.422\n",
      "  Batch 100/800 | Loss: 2.0283 | LR: 3.01e-05 | Alpha: 0.422\n",
      "  Batch 120/800 | Loss: 1.9495 | LR: 3.61e-05 | Alpha: 0.422\n",
      "  Batch 140/800 | Loss: 1.9547 | LR: 4.21e-05 | Alpha: 0.422\n",
      "  Batch 160/800 | Loss: 1.9497 | LR: 4.81e-05 | Alpha: 0.422\n",
      "  Batch 180/800 | Loss: 1.9626 | LR: 5.42e-05 | Alpha: 0.422\n",
      "  Batch 200/800 | Loss: 1.9616 | LR: 6.02e-05 | Alpha: 0.422\n",
      "  Batch 220/800 | Loss: 1.9460 | LR: 6.62e-05 | Alpha: 0.422\n",
      "  Batch 240/800 | Loss: 1.9725 | LR: 6.73e-05 | Alpha: 0.422\n",
      "  Batch 260/800 | Loss: 2.0031 | LR: 6.63e-05 | Alpha: 0.422\n",
      "  Batch 280/800 | Loss: 2.0237 | LR: 6.53e-05 | Alpha: 0.422\n",
      "  Batch 300/800 | Loss: 2.0302 | LR: 6.43e-05 | Alpha: 0.422\n",
      "  Batch 320/800 | Loss: 2.0269 | LR: 6.33e-05 | Alpha: 0.422\n",
      "  Batch 340/800 | Loss: 2.0276 | LR: 6.23e-05 | Alpha: 0.422\n",
      "  Batch 360/800 | Loss: 2.0305 | LR: 6.14e-05 | Alpha: 0.422\n",
      "  Batch 380/800 | Loss: 2.0262 | LR: 6.04e-05 | Alpha: 0.422\n",
      "  Batch 400/800 | Loss: 2.0160 | LR: 5.94e-05 | Alpha: 0.422\n",
      "  Batch 420/800 | Loss: 2.0095 | LR: 5.84e-05 | Alpha: 0.422\n",
      "  Batch 440/800 | Loss: 1.9950 | LR: 5.74e-05 | Alpha: 0.422\n",
      "  Batch 460/800 | Loss: 1.9918 | LR: 5.64e-05 | Alpha: 0.422\n",
      "  Batch 480/800 | Loss: 1.9864 | LR: 5.54e-05 | Alpha: 0.422\n",
      "  Batch 500/800 | Loss: 1.9776 | LR: 5.44e-05 | Alpha: 0.422\n",
      "  Batch 520/800 | Loss: 1.9713 | LR: 5.34e-05 | Alpha: 0.422\n",
      "  Batch 540/800 | Loss: 1.9648 | LR: 5.25e-05 | Alpha: 0.422\n",
      "  Batch 560/800 | Loss: 1.9583 | LR: 5.15e-05 | Alpha: 0.422\n",
      "  Batch 580/800 | Loss: 1.9485 | LR: 5.05e-05 | Alpha: 0.422\n",
      "  Batch 600/800 | Loss: 1.9446 | LR: 4.95e-05 | Alpha: 0.422\n",
      "  Batch 620/800 | Loss: 1.9400 | LR: 4.85e-05 | Alpha: 0.422\n",
      "  Batch 640/800 | Loss: 1.9357 | LR: 4.75e-05 | Alpha: 0.422\n",
      "  Batch 660/800 | Loss: 1.9297 | LR: 4.65e-05 | Alpha: 0.422\n",
      "  Batch 680/800 | Loss: 1.9274 | LR: 4.55e-05 | Alpha: 0.422\n",
      "  Batch 700/800 | Loss: 1.9163 | LR: 4.45e-05 | Alpha: 0.422\n",
      "  Batch 720/800 | Loss: 1.9130 | LR: 4.35e-05 | Alpha: 0.422\n",
      "  Batch 740/800 | Loss: 1.9104 | LR: 4.26e-05 | Alpha: 0.422\n",
      "  Batch 760/800 | Loss: 1.9007 | LR: 4.16e-05 | Alpha: 0.422\n",
      "  Batch 780/800 | Loss: 1.9000 | LR: 4.06e-05 | Alpha: 0.422\n",
      "  Batch 800/800 | Loss: 1.8957 | LR: 3.96e-05 | Alpha: 0.422\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.8957\n",
      "  Val Loss: 1.5203\n",
      "  Sentiment Accuracy: 0.5150\n",
      "  Emotion Accuracy: 0.1475\n",
      "  Combined Score: 0.3312\n",
      "  Alpha: 0.422\n",
      "  Learning Rate: 3.96e-05\n",
      "\n",
      "📍 Epoch 2/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.6994 | LR: 3.86e-05 | Alpha: 0.422\n",
      "  Batch 40/800 | Loss: 1.6357 | LR: 3.76e-05 | Alpha: 0.422\n",
      "  Batch 60/800 | Loss: 1.6227 | LR: 3.66e-05 | Alpha: 0.422\n",
      "  Batch 80/800 | Loss: 1.6475 | LR: 3.56e-05 | Alpha: 0.422\n",
      "  Batch 100/800 | Loss: 1.6522 | LR: 3.46e-05 | Alpha: 0.422\n",
      "  Batch 120/800 | Loss: 1.6543 | LR: 3.36e-05 | Alpha: 0.422\n",
      "  Batch 140/800 | Loss: 1.6596 | LR: 3.27e-05 | Alpha: 0.422\n",
      "  Batch 160/800 | Loss: 1.6710 | LR: 3.17e-05 | Alpha: 0.422\n",
      "  Batch 180/800 | Loss: 1.6432 | LR: 3.07e-05 | Alpha: 0.422\n",
      "  Batch 200/800 | Loss: 1.6362 | LR: 2.97e-05 | Alpha: 0.422\n",
      "  Batch 220/800 | Loss: 1.6414 | LR: 2.87e-05 | Alpha: 0.422\n",
      "  Batch 240/800 | Loss: 1.6335 | LR: 2.77e-05 | Alpha: 0.422\n",
      "  Batch 260/800 | Loss: 1.6398 | LR: 2.67e-05 | Alpha: 0.422\n",
      "  Batch 280/800 | Loss: 1.6380 | LR: 2.57e-05 | Alpha: 0.422\n",
      "  Batch 300/800 | Loss: 1.6339 | LR: 2.47e-05 | Alpha: 0.422\n",
      "  Batch 320/800 | Loss: 1.6523 | LR: 2.38e-05 | Alpha: 0.422\n",
      "  Batch 340/800 | Loss: 1.6535 | LR: 2.28e-05 | Alpha: 0.422\n",
      "  Batch 360/800 | Loss: 1.6542 | LR: 2.18e-05 | Alpha: 0.422\n",
      "  Batch 380/800 | Loss: 1.6471 | LR: 2.08e-05 | Alpha: 0.422\n",
      "  Batch 400/800 | Loss: 1.6494 | LR: 1.98e-05 | Alpha: 0.422\n",
      "  Batch 420/800 | Loss: 1.6448 | LR: 1.88e-05 | Alpha: 0.422\n",
      "  Batch 440/800 | Loss: 1.6414 | LR: 1.78e-05 | Alpha: 0.422\n",
      "  Batch 460/800 | Loss: 1.6391 | LR: 1.68e-05 | Alpha: 0.422\n",
      "  Batch 480/800 | Loss: 1.6389 | LR: 1.58e-05 | Alpha: 0.422\n",
      "  Batch 500/800 | Loss: 1.6399 | LR: 1.48e-05 | Alpha: 0.422\n",
      "  Batch 520/800 | Loss: 1.6399 | LR: 1.39e-05 | Alpha: 0.422\n",
      "  Batch 540/800 | Loss: 1.6409 | LR: 1.29e-05 | Alpha: 0.422\n",
      "  Batch 560/800 | Loss: 1.6377 | LR: 1.19e-05 | Alpha: 0.422\n",
      "  Batch 580/800 | Loss: 1.6352 | LR: 1.09e-05 | Alpha: 0.422\n",
      "  Batch 600/800 | Loss: 1.6308 | LR: 9.90e-06 | Alpha: 0.422\n",
      "  Batch 620/800 | Loss: 1.6286 | LR: 8.91e-06 | Alpha: 0.422\n",
      "  Batch 640/800 | Loss: 1.6268 | LR: 7.92e-06 | Alpha: 0.422\n",
      "  Batch 660/800 | Loss: 1.6255 | LR: 6.93e-06 | Alpha: 0.422\n",
      "  Batch 680/800 | Loss: 1.6218 | LR: 5.94e-06 | Alpha: 0.422\n",
      "  Batch 700/800 | Loss: 1.6217 | LR: 4.95e-06 | Alpha: 0.422\n",
      "  Batch 720/800 | Loss: 1.6197 | LR: 3.96e-06 | Alpha: 0.422\n",
      "  Batch 740/800 | Loss: 1.6163 | LR: 2.97e-06 | Alpha: 0.422\n",
      "  Batch 760/800 | Loss: 1.6178 | LR: 1.98e-06 | Alpha: 0.422\n",
      "  Batch 780/800 | Loss: 1.6176 | LR: 9.90e-07 | Alpha: 0.422\n",
      "  Batch 800/800 | Loss: 1.6137 | LR: 0.00e+00 | Alpha: 0.422\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.6137\n",
      "  Val Loss: 1.6916\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.0850\n",
      "  Combined Score: 0.0625\n",
      "  Alpha: 0.422\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.3312\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.61 GB\n",
      "  Cached: 6.14 GB\n",
      "Trial 1: Combined Score = 0.1737 (Sentiment: 0.2211, Emotion: 0.1263)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 22:56:12,242] Trial 1 finished with value: 0.17368421052631577 and parameters: {'learning_rate': 6.798962421591133e-05, 'batch_size': 2, 'alpha': 0.4216968971838151, 'hidden_dropout_prob': 0.18118910790805948, 'classifier_dropout': 0.22958350559263474, 'weight_decay': 0.029831684879606152, 'warmup_ratio': 0.14177793420835694, 'num_epochs': 2, 'max_length': 256}. Best is trial 0 with value: 0.3368421052631579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 4000\n",
      "🚀 Starting training for 5 epochs...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 2.0603 | LR: 2.39e-06 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 1.9639 | LR: 4.78e-06 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 1.9266 | LR: 7.18e-06 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 1.9171 | LR: 9.57e-06 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 1.8591 | LR: 1.20e-05 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 1.8642 | LR: 1.44e-05 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 1.8883 | LR: 1.67e-05 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 1.8616 | LR: 1.91e-05 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 1.8507 | LR: 2.15e-05 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 1.8490 | LR: 2.39e-05 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 1.8083 | LR: 2.63e-05 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 1.7801 | LR: 2.86e-05 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 1.7463 | LR: 2.84e-05 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 1.7279 | LR: 2.83e-05 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 1.7372 | LR: 2.81e-05 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 1.7163 | LR: 2.80e-05 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 1.7082 | LR: 2.78e-05 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 1.7091 | LR: 2.77e-05 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 1.6993 | LR: 2.75e-05 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 1.6926 | LR: 2.74e-05 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 1.6681 | LR: 2.72e-05 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 1.6620 | LR: 2.71e-05 | Alpha: 0.537\n",
      "  Batch 460/800 | Loss: 1.6650 | LR: 2.69e-05 | Alpha: 0.537\n",
      "  Batch 480/800 | Loss: 1.6607 | LR: 2.67e-05 | Alpha: 0.537\n",
      "  Batch 500/800 | Loss: 1.6391 | LR: 2.66e-05 | Alpha: 0.537\n",
      "  Batch 520/800 | Loss: 1.6317 | LR: 2.64e-05 | Alpha: 0.537\n",
      "  Batch 540/800 | Loss: 1.6257 | LR: 2.63e-05 | Alpha: 0.537\n",
      "  Batch 560/800 | Loss: 1.6158 | LR: 2.61e-05 | Alpha: 0.537\n",
      "  Batch 580/800 | Loss: 1.6121 | LR: 2.60e-05 | Alpha: 0.537\n",
      "  Batch 600/800 | Loss: 1.6047 | LR: 2.58e-05 | Alpha: 0.537\n",
      "  Batch 620/800 | Loss: 1.5983 | LR: 2.57e-05 | Alpha: 0.537\n",
      "  Batch 640/800 | Loss: 1.5949 | LR: 2.55e-05 | Alpha: 0.537\n",
      "  Batch 660/800 | Loss: 1.5829 | LR: 2.54e-05 | Alpha: 0.537\n",
      "  Batch 680/800 | Loss: 1.5714 | LR: 2.52e-05 | Alpha: 0.537\n",
      "  Batch 700/800 | Loss: 1.5591 | LR: 2.51e-05 | Alpha: 0.537\n",
      "  Batch 720/800 | Loss: 1.5481 | LR: 2.49e-05 | Alpha: 0.537\n",
      "  Batch 740/800 | Loss: 1.5372 | LR: 2.48e-05 | Alpha: 0.537\n",
      "  Batch 760/800 | Loss: 1.5292 | LR: 2.46e-05 | Alpha: 0.537\n",
      "  Batch 780/800 | Loss: 1.5147 | LR: 2.45e-05 | Alpha: 0.537\n",
      "  Batch 800/800 | Loss: 1.5094 | LR: 2.43e-05 | Alpha: 0.537\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.5094\n",
      "  Val Loss: 1.3951\n",
      "  Sentiment Accuracy: 0.7150\n",
      "  Emotion Accuracy: 0.1225\n",
      "  Combined Score: 0.4187\n",
      "  Alpha: 0.537\n",
      "  Learning Rate: 2.43e-05\n",
      "\n",
      "📍 Epoch 2/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.3133 | LR: 2.42e-05 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 1.2924 | LR: 2.40e-05 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 1.2575 | LR: 2.39e-05 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 1.2713 | LR: 2.37e-05 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 1.2050 | LR: 2.36e-05 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 1.1842 | LR: 2.34e-05 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 1.1503 | LR: 2.33e-05 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 1.1496 | LR: 2.31e-05 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 1.1858 | LR: 2.29e-05 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 1.1810 | LR: 2.28e-05 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 1.1623 | LR: 2.26e-05 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 1.1474 | LR: 2.25e-05 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 1.1412 | LR: 2.23e-05 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 1.1226 | LR: 2.22e-05 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 1.1118 | LR: 2.20e-05 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 1.1240 | LR: 2.19e-05 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 1.1276 | LR: 2.17e-05 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 1.1126 | LR: 2.16e-05 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 1.1026 | LR: 2.14e-05 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 1.0917 | LR: 2.13e-05 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 1.0888 | LR: 2.11e-05 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 1.0629 | LR: 2.10e-05 | Alpha: 0.537\n",
      "  Batch 460/800 | Loss: 1.0507 | LR: 2.08e-05 | Alpha: 0.537\n",
      "  Batch 480/800 | Loss: 1.0448 | LR: 2.07e-05 | Alpha: 0.537\n",
      "  Batch 500/800 | Loss: 1.0290 | LR: 2.05e-05 | Alpha: 0.537\n",
      "  Batch 520/800 | Loss: 1.0181 | LR: 2.04e-05 | Alpha: 0.537\n",
      "  Batch 540/800 | Loss: 1.0149 | LR: 2.02e-05 | Alpha: 0.537\n",
      "  Batch 560/800 | Loss: 1.0089 | LR: 2.01e-05 | Alpha: 0.537\n",
      "  Batch 580/800 | Loss: 1.0021 | LR: 1.99e-05 | Alpha: 0.537\n",
      "  Batch 600/800 | Loss: 0.9998 | LR: 1.98e-05 | Alpha: 0.537\n",
      "  Batch 620/800 | Loss: 0.9878 | LR: 1.96e-05 | Alpha: 0.537\n",
      "  Batch 640/800 | Loss: 0.9785 | LR: 1.95e-05 | Alpha: 0.537\n",
      "  Batch 660/800 | Loss: 0.9736 | LR: 1.93e-05 | Alpha: 0.537\n",
      "  Batch 680/800 | Loss: 0.9714 | LR: 1.91e-05 | Alpha: 0.537\n",
      "  Batch 700/800 | Loss: 0.9637 | LR: 1.90e-05 | Alpha: 0.537\n",
      "  Batch 720/800 | Loss: 0.9620 | LR: 1.88e-05 | Alpha: 0.537\n",
      "  Batch 740/800 | Loss: 0.9583 | LR: 1.87e-05 | Alpha: 0.537\n",
      "  Batch 760/800 | Loss: 0.9531 | LR: 1.85e-05 | Alpha: 0.537\n",
      "  Batch 780/800 | Loss: 0.9426 | LR: 1.84e-05 | Alpha: 0.537\n",
      "  Batch 800/800 | Loss: 0.9350 | LR: 1.82e-05 | Alpha: 0.537\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 0.9350\n",
      "  Val Loss: 1.5950\n",
      "  Sentiment Accuracy: 0.7975\n",
      "  Emotion Accuracy: 0.1700\n",
      "  Combined Score: 0.4838\n",
      "  Alpha: 0.537\n",
      "  Learning Rate: 1.82e-05\n",
      "\n",
      "📍 Epoch 3/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.6465 | LR: 1.81e-05 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 0.6349 | LR: 1.79e-05 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 0.7369 | LR: 1.78e-05 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 0.6784 | LR: 1.76e-05 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 0.6970 | LR: 1.75e-05 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 0.6863 | LR: 1.73e-05 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 0.7015 | LR: 1.72e-05 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 0.7042 | LR: 1.70e-05 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 0.6871 | LR: 1.69e-05 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 0.6833 | LR: 1.67e-05 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 0.6812 | LR: 1.66e-05 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 0.6817 | LR: 1.64e-05 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 0.7028 | LR: 1.63e-05 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 0.6932 | LR: 1.61e-05 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 0.6945 | LR: 1.60e-05 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 0.6916 | LR: 1.58e-05 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 0.6910 | LR: 1.57e-05 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 0.6900 | LR: 1.55e-05 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 0.6855 | LR: 1.54e-05 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 0.6904 | LR: 1.52e-05 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 0.6810 | LR: 1.50e-05 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 0.6687 | LR: 1.49e-05 | Alpha: 0.537\n",
      "  Batch 460/800 | Loss: 0.6792 | LR: 1.47e-05 | Alpha: 0.537\n",
      "  Batch 480/800 | Loss: 0.6819 | LR: 1.46e-05 | Alpha: 0.537\n",
      "  Batch 500/800 | Loss: 0.6856 | LR: 1.44e-05 | Alpha: 0.537\n",
      "  Batch 520/800 | Loss: 0.6748 | LR: 1.43e-05 | Alpha: 0.537\n",
      "  Batch 540/800 | Loss: 0.6773 | LR: 1.41e-05 | Alpha: 0.537\n",
      "  Batch 560/800 | Loss: 0.6700 | LR: 1.40e-05 | Alpha: 0.537\n",
      "  Batch 580/800 | Loss: 0.6658 | LR: 1.38e-05 | Alpha: 0.537\n",
      "  Batch 600/800 | Loss: 0.6602 | LR: 1.37e-05 | Alpha: 0.537\n",
      "  Batch 620/800 | Loss: 0.6559 | LR: 1.35e-05 | Alpha: 0.537\n",
      "  Batch 640/800 | Loss: 0.6485 | LR: 1.34e-05 | Alpha: 0.537\n",
      "  Batch 660/800 | Loss: 0.6463 | LR: 1.32e-05 | Alpha: 0.537\n",
      "  Batch 680/800 | Loss: 0.6352 | LR: 1.31e-05 | Alpha: 0.537\n",
      "  Batch 700/800 | Loss: 0.6369 | LR: 1.29e-05 | Alpha: 0.537\n",
      "  Batch 720/800 | Loss: 0.6348 | LR: 1.28e-05 | Alpha: 0.537\n",
      "  Batch 740/800 | Loss: 0.6437 | LR: 1.26e-05 | Alpha: 0.537\n",
      "  Batch 760/800 | Loss: 0.6425 | LR: 1.25e-05 | Alpha: 0.537\n",
      "  Batch 780/800 | Loss: 0.6340 | LR: 1.23e-05 | Alpha: 0.537\n",
      "  Batch 800/800 | Loss: 0.6294 | LR: 1.22e-05 | Alpha: 0.537\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.6294\n",
      "  Val Loss: 1.9444\n",
      "  Sentiment Accuracy: 0.8175\n",
      "  Emotion Accuracy: 0.1550\n",
      "  Combined Score: 0.4863\n",
      "  Alpha: 0.537\n",
      "  Learning Rate: 1.22e-05\n",
      "\n",
      "📍 Epoch 4/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.8231 | LR: 1.20e-05 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 0.6539 | LR: 1.19e-05 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 0.6202 | LR: 1.17e-05 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 0.5583 | LR: 1.16e-05 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 0.5479 | LR: 1.14e-05 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 0.5774 | LR: 1.12e-05 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 0.5219 | LR: 1.11e-05 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 0.5170 | LR: 1.09e-05 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 0.5033 | LR: 1.08e-05 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 0.5091 | LR: 1.06e-05 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 0.4927 | LR: 1.05e-05 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 0.4974 | LR: 1.03e-05 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 0.5159 | LR: 1.02e-05 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 0.5345 | LR: 1.00e-05 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 0.5237 | LR: 9.88e-06 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 0.5162 | LR: 9.73e-06 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 0.5080 | LR: 9.57e-06 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 0.5186 | LR: 9.42e-06 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 0.5202 | LR: 9.27e-06 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 0.5142 | LR: 9.12e-06 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 0.5242 | LR: 8.97e-06 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 0.5207 | LR: 8.82e-06 | Alpha: 0.537\n",
      "  Batch 460/800 | Loss: 0.5230 | LR: 8.66e-06 | Alpha: 0.537\n",
      "  Batch 480/800 | Loss: 0.5133 | LR: 8.51e-06 | Alpha: 0.537\n",
      "  Batch 500/800 | Loss: 0.5013 | LR: 8.36e-06 | Alpha: 0.537\n",
      "  Batch 520/800 | Loss: 0.4935 | LR: 8.21e-06 | Alpha: 0.537\n",
      "  Batch 540/800 | Loss: 0.4927 | LR: 8.06e-06 | Alpha: 0.537\n",
      "  Batch 560/800 | Loss: 0.4952 | LR: 7.90e-06 | Alpha: 0.537\n",
      "  Batch 580/800 | Loss: 0.4894 | LR: 7.75e-06 | Alpha: 0.537\n",
      "  Batch 600/800 | Loss: 0.4817 | LR: 7.60e-06 | Alpha: 0.537\n",
      "  Batch 620/800 | Loss: 0.4719 | LR: 7.45e-06 | Alpha: 0.537\n",
      "  Batch 640/800 | Loss: 0.4867 | LR: 7.30e-06 | Alpha: 0.537\n",
      "  Batch 660/800 | Loss: 0.4816 | LR: 7.14e-06 | Alpha: 0.537\n",
      "  Batch 680/800 | Loss: 0.4787 | LR: 6.99e-06 | Alpha: 0.537\n",
      "  Batch 700/800 | Loss: 0.4767 | LR: 6.84e-06 | Alpha: 0.537\n",
      "  Batch 720/800 | Loss: 0.4689 | LR: 6.69e-06 | Alpha: 0.537\n",
      "  Batch 740/800 | Loss: 0.4676 | LR: 6.54e-06 | Alpha: 0.537\n",
      "  Batch 760/800 | Loss: 0.4604 | LR: 6.38e-06 | Alpha: 0.537\n",
      "  Batch 780/800 | Loss: 0.4539 | LR: 6.23e-06 | Alpha: 0.537\n",
      "  Batch 800/800 | Loss: 0.4508 | LR: 6.08e-06 | Alpha: 0.537\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.4508\n",
      "  Val Loss: 2.3797\n",
      "  Sentiment Accuracy: 0.8100\n",
      "  Emotion Accuracy: 0.2300\n",
      "  Combined Score: 0.5200\n",
      "  Alpha: 0.537\n",
      "  Learning Rate: 6.08e-06\n",
      "\n",
      "📍 Epoch 5/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.2073 | LR: 5.93e-06 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 0.3529 | LR: 5.78e-06 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 0.3041 | LR: 5.62e-06 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 0.3218 | LR: 5.47e-06 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 0.3149 | LR: 5.32e-06 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 0.3650 | LR: 5.17e-06 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 0.3505 | LR: 5.02e-06 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 0.3282 | LR: 4.86e-06 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 0.3477 | LR: 4.71e-06 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 0.3572 | LR: 4.56e-06 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 0.3738 | LR: 4.41e-06 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 0.3559 | LR: 4.26e-06 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 0.3482 | LR: 4.10e-06 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 0.3329 | LR: 3.95e-06 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 0.3311 | LR: 3.80e-06 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 0.3368 | LR: 3.65e-06 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 0.3328 | LR: 3.50e-06 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 0.3343 | LR: 3.34e-06 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 0.3390 | LR: 3.19e-06 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 0.3277 | LR: 3.04e-06 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 0.3297 | LR: 2.89e-06 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 0.3204 | LR: 2.74e-06 | Alpha: 0.537\n",
      "  Batch 460/800 | Loss: 0.3278 | LR: 2.58e-06 | Alpha: 0.537\n",
      "  Batch 480/800 | Loss: 0.3230 | LR: 2.43e-06 | Alpha: 0.537\n",
      "  Batch 500/800 | Loss: 0.3297 | LR: 2.28e-06 | Alpha: 0.537\n",
      "  Batch 520/800 | Loss: 0.3285 | LR: 2.13e-06 | Alpha: 0.537\n",
      "  Batch 540/800 | Loss: 0.3257 | LR: 1.98e-06 | Alpha: 0.537\n",
      "  Batch 560/800 | Loss: 0.3204 | LR: 1.82e-06 | Alpha: 0.537\n",
      "  Batch 580/800 | Loss: 0.3121 | LR: 1.67e-06 | Alpha: 0.537\n",
      "  Batch 600/800 | Loss: 0.3098 | LR: 1.52e-06 | Alpha: 0.537\n",
      "  Batch 620/800 | Loss: 0.3131 | LR: 1.37e-06 | Alpha: 0.537\n",
      "  Batch 640/800 | Loss: 0.3084 | LR: 1.22e-06 | Alpha: 0.537\n",
      "  Batch 660/800 | Loss: 0.3179 | LR: 1.06e-06 | Alpha: 0.537\n",
      "  Batch 680/800 | Loss: 0.3169 | LR: 9.12e-07 | Alpha: 0.537\n",
      "  Batch 700/800 | Loss: 0.3149 | LR: 7.60e-07 | Alpha: 0.537\n",
      "  Batch 720/800 | Loss: 0.3137 | LR: 6.08e-07 | Alpha: 0.537\n",
      "  Batch 740/800 | Loss: 0.3130 | LR: 4.56e-07 | Alpha: 0.537\n",
      "  Batch 760/800 | Loss: 0.3153 | LR: 3.04e-07 | Alpha: 0.537\n",
      "  Batch 780/800 | Loss: 0.3140 | LR: 1.52e-07 | Alpha: 0.537\n",
      "  Batch 800/800 | Loss: 0.3168 | LR: 0.00e+00 | Alpha: 0.537\n",
      "📊 Epoch 5 Results:\n",
      "  Train Loss: 0.3168\n",
      "  Val Loss: 2.6795\n",
      "  Sentiment Accuracy: 0.8075\n",
      "  Emotion Accuracy: 0.2225\n",
      "  Combined Score: 0.5150\n",
      "  Alpha: 0.537\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.5200\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.59 GB\n",
      "  Cached: 6.09 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 23:10:52,839] Trial 2 finished with value: 0.4263157894736842 and parameters: {'learning_rate': 2.858051065806938e-05, 'batch_size': 2, 'alpha': 0.5369658275448169, 'hidden_dropout_prob': 0.061612603179999434, 'classifier_dropout': 0.28226345557043153, 'weight_decay': 0.017881888245041864, 'warmup_ratio': 0.05975773894779193, 'num_epochs': 5, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2: Combined Score = 0.4263 (Sentiment: 0.5789, Emotion: 0.2737)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 1600\n",
      "🚀 Starting training for 4 epochs...\n",
      "\n",
      "📍 Epoch 1/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.8216 | LR: 2.84e-06 | Alpha: 0.349\n",
      "  Batch 40/400 | Loss: 1.7900 | LR: 5.68e-06 | Alpha: 0.349\n",
      "  Batch 60/400 | Loss: 1.7716 | LR: 8.52e-06 | Alpha: 0.349\n",
      "  Batch 80/400 | Loss: 1.7328 | LR: 1.14e-05 | Alpha: 0.349\n",
      "  Batch 100/400 | Loss: 1.7410 | LR: 1.42e-05 | Alpha: 0.349\n",
      "  Batch 120/400 | Loss: 1.7334 | LR: 1.70e-05 | Alpha: 0.349\n",
      "  Batch 140/400 | Loss: 1.7176 | LR: 1.99e-05 | Alpha: 0.349\n",
      "  Batch 160/400 | Loss: 1.6998 | LR: 1.99e-05 | Alpha: 0.349\n",
      "  Batch 180/400 | Loss: 1.6841 | LR: 1.96e-05 | Alpha: 0.349\n",
      "  Batch 200/400 | Loss: 1.6659 | LR: 1.94e-05 | Alpha: 0.349\n",
      "  Batch 220/400 | Loss: 1.6513 | LR: 1.91e-05 | Alpha: 0.349\n",
      "  Batch 240/400 | Loss: 1.6374 | LR: 1.88e-05 | Alpha: 0.349\n",
      "  Batch 260/400 | Loss: 1.6275 | LR: 1.85e-05 | Alpha: 0.349\n",
      "  Batch 280/400 | Loss: 1.6009 | LR: 1.83e-05 | Alpha: 0.349\n",
      "  Batch 300/400 | Loss: 1.5928 | LR: 1.80e-05 | Alpha: 0.349\n",
      "  Batch 320/400 | Loss: 1.5827 | LR: 1.77e-05 | Alpha: 0.349\n",
      "  Batch 340/400 | Loss: 1.5726 | LR: 1.74e-05 | Alpha: 0.349\n",
      "  Batch 360/400 | Loss: 1.5671 | LR: 1.72e-05 | Alpha: 0.349\n",
      "  Batch 380/400 | Loss: 1.5591 | LR: 1.69e-05 | Alpha: 0.349\n",
      "  Batch 400/400 | Loss: 1.5489 | LR: 1.66e-05 | Alpha: 0.349\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.5489\n",
      "  Val Loss: 1.8151\n",
      "  Sentiment Accuracy: 0.4000\n",
      "  Emotion Accuracy: 0.1125\n",
      "  Combined Score: 0.2563\n",
      "  Alpha: 0.349\n",
      "  Learning Rate: 1.66e-05\n",
      "\n",
      "📍 Epoch 2/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.3007 | LR: 1.63e-05 | Alpha: 0.349\n",
      "  Batch 40/400 | Loss: 1.2943 | LR: 1.60e-05 | Alpha: 0.349\n",
      "  Batch 60/400 | Loss: 1.2721 | LR: 1.58e-05 | Alpha: 0.349\n",
      "  Batch 80/400 | Loss: 1.2542 | LR: 1.55e-05 | Alpha: 0.349\n",
      "  Batch 100/400 | Loss: 1.2616 | LR: 1.52e-05 | Alpha: 0.349\n",
      "  Batch 120/400 | Loss: 1.2535 | LR: 1.49e-05 | Alpha: 0.349\n",
      "  Batch 140/400 | Loss: 1.2201 | LR: 1.47e-05 | Alpha: 0.349\n",
      "  Batch 160/400 | Loss: 1.1931 | LR: 1.44e-05 | Alpha: 0.349\n",
      "  Batch 180/400 | Loss: 1.1694 | LR: 1.41e-05 | Alpha: 0.349\n",
      "  Batch 200/400 | Loss: 1.1689 | LR: 1.38e-05 | Alpha: 0.349\n",
      "  Batch 220/400 | Loss: 1.1552 | LR: 1.36e-05 | Alpha: 0.349\n",
      "  Batch 240/400 | Loss: 1.1507 | LR: 1.33e-05 | Alpha: 0.349\n",
      "  Batch 260/400 | Loss: 1.1422 | LR: 1.30e-05 | Alpha: 0.349\n",
      "  Batch 280/400 | Loss: 1.1274 | LR: 1.27e-05 | Alpha: 0.349\n",
      "  Batch 300/400 | Loss: 1.1224 | LR: 1.24e-05 | Alpha: 0.349\n",
      "  Batch 320/400 | Loss: 1.1134 | LR: 1.22e-05 | Alpha: 0.349\n",
      "  Batch 340/400 | Loss: 1.1081 | LR: 1.19e-05 | Alpha: 0.349\n",
      "  Batch 360/400 | Loss: 1.1023 | LR: 1.16e-05 | Alpha: 0.349\n",
      "  Batch 380/400 | Loss: 1.0976 | LR: 1.13e-05 | Alpha: 0.349\n",
      "  Batch 400/400 | Loss: 1.0816 | LR: 1.11e-05 | Alpha: 0.349\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.0816\n",
      "  Val Loss: 1.9378\n",
      "  Sentiment Accuracy: 0.4850\n",
      "  Emotion Accuracy: 0.1325\n",
      "  Combined Score: 0.3087\n",
      "  Alpha: 0.349\n",
      "  Learning Rate: 1.11e-05\n",
      "\n",
      "📍 Epoch 3/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 0.8791 | LR: 1.08e-05 | Alpha: 0.349\n",
      "  Batch 40/400 | Loss: 0.8262 | LR: 1.05e-05 | Alpha: 0.349\n",
      "  Batch 60/400 | Loss: 0.8346 | LR: 1.02e-05 | Alpha: 0.349\n",
      "  Batch 80/400 | Loss: 0.8415 | LR: 9.96e-06 | Alpha: 0.349\n",
      "  Batch 100/400 | Loss: 0.8466 | LR: 9.68e-06 | Alpha: 0.349\n",
      "  Batch 120/400 | Loss: 0.8655 | LR: 9.41e-06 | Alpha: 0.349\n",
      "  Batch 140/400 | Loss: 0.8563 | LR: 9.13e-06 | Alpha: 0.349\n",
      "  Batch 160/400 | Loss: 0.8630 | LR: 8.85e-06 | Alpha: 0.349\n",
      "  Batch 180/400 | Loss: 0.8732 | LR: 8.58e-06 | Alpha: 0.349\n",
      "  Batch 200/400 | Loss: 0.8775 | LR: 8.30e-06 | Alpha: 0.349\n",
      "  Batch 220/400 | Loss: 0.8749 | LR: 8.02e-06 | Alpha: 0.349\n",
      "  Batch 240/400 | Loss: 0.8630 | LR: 7.75e-06 | Alpha: 0.349\n",
      "  Batch 260/400 | Loss: 0.8527 | LR: 7.47e-06 | Alpha: 0.349\n",
      "  Batch 280/400 | Loss: 0.8481 | LR: 7.19e-06 | Alpha: 0.349\n",
      "  Batch 300/400 | Loss: 0.8433 | LR: 6.92e-06 | Alpha: 0.349\n",
      "  Batch 320/400 | Loss: 0.8338 | LR: 6.64e-06 | Alpha: 0.349\n",
      "  Batch 340/400 | Loss: 0.8163 | LR: 6.36e-06 | Alpha: 0.349\n",
      "  Batch 360/400 | Loss: 0.8047 | LR: 6.09e-06 | Alpha: 0.349\n",
      "  Batch 380/400 | Loss: 0.7908 | LR: 5.81e-06 | Alpha: 0.349\n",
      "  Batch 400/400 | Loss: 0.7845 | LR: 5.53e-06 | Alpha: 0.349\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.7845\n",
      "  Val Loss: 1.9430\n",
      "  Sentiment Accuracy: 0.6300\n",
      "  Emotion Accuracy: 0.1500\n",
      "  Combined Score: 0.3900\n",
      "  Alpha: 0.349\n",
      "  Learning Rate: 5.53e-06\n",
      "\n",
      "📍 Epoch 4/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 0.5906 | LR: 5.26e-06 | Alpha: 0.349\n",
      "  Batch 40/400 | Loss: 0.6041 | LR: 4.98e-06 | Alpha: 0.349\n",
      "  Batch 60/400 | Loss: 0.6431 | LR: 4.70e-06 | Alpha: 0.349\n",
      "  Batch 80/400 | Loss: 0.6479 | LR: 4.43e-06 | Alpha: 0.349\n",
      "  Batch 100/400 | Loss: 0.6358 | LR: 4.15e-06 | Alpha: 0.349\n",
      "  Batch 120/400 | Loss: 0.5988 | LR: 3.87e-06 | Alpha: 0.349\n",
      "  Batch 140/400 | Loss: 0.6556 | LR: 3.60e-06 | Alpha: 0.349\n",
      "  Batch 160/400 | Loss: 0.6672 | LR: 3.32e-06 | Alpha: 0.349\n",
      "  Batch 180/400 | Loss: 0.6883 | LR: 3.04e-06 | Alpha: 0.349\n",
      "  Batch 200/400 | Loss: 0.6784 | LR: 2.77e-06 | Alpha: 0.349\n",
      "  Batch 220/400 | Loss: 0.6631 | LR: 2.49e-06 | Alpha: 0.349\n",
      "  Batch 240/400 | Loss: 0.6514 | LR: 2.21e-06 | Alpha: 0.349\n",
      "  Batch 260/400 | Loss: 0.6460 | LR: 1.94e-06 | Alpha: 0.349\n",
      "  Batch 280/400 | Loss: 0.6388 | LR: 1.66e-06 | Alpha: 0.349\n",
      "  Batch 300/400 | Loss: 0.6364 | LR: 1.38e-06 | Alpha: 0.349\n",
      "  Batch 320/400 | Loss: 0.6476 | LR: 1.11e-06 | Alpha: 0.349\n",
      "  Batch 340/400 | Loss: 0.6433 | LR: 8.30e-07 | Alpha: 0.349\n",
      "  Batch 360/400 | Loss: 0.6456 | LR: 5.53e-07 | Alpha: 0.349\n",
      "  Batch 380/400 | Loss: 0.6367 | LR: 2.77e-07 | Alpha: 0.349\n",
      "  Batch 400/400 | Loss: 0.6328 | LR: 0.00e+00 | Alpha: 0.349\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.6328\n",
      "  Val Loss: 2.0622\n",
      "  Sentiment Accuracy: 0.6875\n",
      "  Emotion Accuracy: 0.1300\n",
      "  Combined Score: 0.4088\n",
      "  Alpha: 0.349\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4088\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.62 GB\n",
      "  Cached: 6.31 GB\n",
      "Trial 3: Combined Score = 0.3684 (Sentiment: 0.6105, Emotion: 0.1263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 00:12:16,150] Trial 3 finished with value: 0.368421052631579 and parameters: {'learning_rate': 2.0165721691808572e-05, 'batch_size': 4, 'alpha': 0.3488152939379115, 'hidden_dropout_prob': 0.17379422752781754, 'classifier_dropout': 0.11031655633456552, 'weight_decay': 0.09102271980579943, 'warmup_ratio': 0.08881699724000255, 'num_epochs': 4, 'max_length': 256}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 800\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.6111 | LR: 1.41e-05 | Alpha: 0.676\n",
      "  Batch 40/400 | Loss: 1.5572 | LR: 2.82e-05 | Alpha: 0.676\n",
      "  Batch 60/400 | Loss: 1.4706 | LR: 3.47e-05 | Alpha: 0.676\n",
      "  Batch 80/400 | Loss: 1.4811 | LR: 3.38e-05 | Alpha: 0.676\n",
      "  Batch 100/400 | Loss: 1.5272 | LR: 3.29e-05 | Alpha: 0.676\n",
      "  Batch 120/400 | Loss: 1.5479 | LR: 3.19e-05 | Alpha: 0.676\n",
      "  Batch 140/400 | Loss: 1.5466 | LR: 3.10e-05 | Alpha: 0.676\n",
      "  Batch 160/400 | Loss: 1.5442 | LR: 3.00e-05 | Alpha: 0.676\n",
      "  Batch 180/400 | Loss: 1.5515 | LR: 2.91e-05 | Alpha: 0.676\n",
      "  Batch 200/400 | Loss: 1.5606 | LR: 2.82e-05 | Alpha: 0.676\n",
      "  Batch 220/400 | Loss: 1.5458 | LR: 2.72e-05 | Alpha: 0.676\n",
      "  Batch 240/400 | Loss: 1.5178 | LR: 2.63e-05 | Alpha: 0.676\n",
      "  Batch 260/400 | Loss: 1.5081 | LR: 2.54e-05 | Alpha: 0.676\n",
      "  Batch 280/400 | Loss: 1.5126 | LR: 2.44e-05 | Alpha: 0.676\n",
      "  Batch 300/400 | Loss: 1.5111 | LR: 2.35e-05 | Alpha: 0.676\n",
      "  Batch 320/400 | Loss: 1.5005 | LR: 2.25e-05 | Alpha: 0.676\n",
      "  Batch 340/400 | Loss: 1.4953 | LR: 2.16e-05 | Alpha: 0.676\n",
      "  Batch 360/400 | Loss: 1.4986 | LR: 2.07e-05 | Alpha: 0.676\n",
      "  Batch 380/400 | Loss: 1.4912 | LR: 1.97e-05 | Alpha: 0.676\n",
      "  Batch 400/400 | Loss: 1.4805 | LR: 1.88e-05 | Alpha: 0.676\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.4805\n",
      "  Val Loss: 1.9078\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.0775\n",
      "  Combined Score: 0.0587\n",
      "  Alpha: 0.676\n",
      "  Learning Rate: 1.88e-05\n",
      "\n",
      "📍 Epoch 2/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.4564 | LR: 1.78e-05 | Alpha: 0.676\n",
      "  Batch 40/400 | Loss: 1.3696 | LR: 1.69e-05 | Alpha: 0.676\n",
      "  Batch 60/400 | Loss: 1.3547 | LR: 1.60e-05 | Alpha: 0.676\n",
      "  Batch 80/400 | Loss: 1.3660 | LR: 1.50e-05 | Alpha: 0.676\n",
      "  Batch 100/400 | Loss: 1.3245 | LR: 1.41e-05 | Alpha: 0.676\n",
      "  Batch 120/400 | Loss: 1.3759 | LR: 1.31e-05 | Alpha: 0.676\n",
      "  Batch 140/400 | Loss: 1.3826 | LR: 1.22e-05 | Alpha: 0.676\n",
      "  Batch 160/400 | Loss: 1.3755 | LR: 1.13e-05 | Alpha: 0.676\n",
      "  Batch 180/400 | Loss: 1.3495 | LR: 1.03e-05 | Alpha: 0.676\n",
      "  Batch 200/400 | Loss: 1.3409 | LR: 9.39e-06 | Alpha: 0.676\n",
      "  Batch 220/400 | Loss: 1.3458 | LR: 8.45e-06 | Alpha: 0.676\n",
      "  Batch 240/400 | Loss: 1.3456 | LR: 7.51e-06 | Alpha: 0.676\n",
      "  Batch 260/400 | Loss: 1.3543 | LR: 6.57e-06 | Alpha: 0.676\n",
      "  Batch 280/400 | Loss: 1.3526 | LR: 5.63e-06 | Alpha: 0.676\n",
      "  Batch 300/400 | Loss: 1.3500 | LR: 4.70e-06 | Alpha: 0.676\n",
      "  Batch 320/400 | Loss: 1.3437 | LR: 3.76e-06 | Alpha: 0.676\n",
      "  Batch 340/400 | Loss: 1.3405 | LR: 2.82e-06 | Alpha: 0.676\n",
      "  Batch 360/400 | Loss: 1.3308 | LR: 1.88e-06 | Alpha: 0.676\n",
      "  Batch 380/400 | Loss: 1.3316 | LR: 9.39e-07 | Alpha: 0.676\n",
      "  Batch 400/400 | Loss: 1.3285 | LR: 0.00e+00 | Alpha: 0.676\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.3285\n",
      "  Val Loss: 1.8720\n",
      "  Sentiment Accuracy: 0.1400\n",
      "  Emotion Accuracy: 0.0850\n",
      "  Combined Score: 0.1125\n",
      "  Alpha: 0.676\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.1125\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.62 GB\n",
      "  Cached: 6.32 GB\n",
      "Trial 4: Combined Score = 0.1895 (Sentiment: 0.2211, Emotion: 0.1579)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 00:44:35,625] Trial 4 finished with value: 0.1894736842105263 and parameters: {'learning_rate': 3.521358805467871e-05, 'batch_size': 4, 'alpha': 0.6757995766256756, 'hidden_dropout_prob': 0.2737068376069122, 'classifier_dropout': 0.2793699936433256, 'weight_decay': 0.09226554926728857, 'warmup_ratio': 0.06327387530778793, 'num_epochs': 2, 'max_length': 256}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 2000\n",
      "🚀 Starting training for 5 epochs...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 2.1181 | LR: 4.01e-06 | Alpha: 0.412\n",
      "  Batch 40/400 | Loss: 1.9063 | LR: 8.02e-06 | Alpha: 0.412\n",
      "  Batch 60/400 | Loss: 1.7944 | LR: 1.20e-05 | Alpha: 0.412\n",
      "  Batch 80/400 | Loss: 1.7587 | LR: 1.60e-05 | Alpha: 0.412\n",
      "  Batch 100/400 | Loss: 1.7623 | LR: 2.01e-05 | Alpha: 0.412\n",
      "  Batch 120/400 | Loss: 1.7479 | LR: 2.41e-05 | Alpha: 0.412\n",
      "  Batch 140/400 | Loss: 1.7126 | LR: 2.42e-05 | Alpha: 0.412\n",
      "  Batch 160/400 | Loss: 1.7053 | LR: 2.40e-05 | Alpha: 0.412\n",
      "  Batch 180/400 | Loss: 1.6797 | LR: 2.37e-05 | Alpha: 0.412\n",
      "  Batch 200/400 | Loss: 1.6813 | LR: 2.35e-05 | Alpha: 0.412\n",
      "  Batch 220/400 | Loss: 1.6654 | LR: 2.32e-05 | Alpha: 0.412\n",
      "  Batch 240/400 | Loss: 1.6516 | LR: 2.29e-05 | Alpha: 0.412\n",
      "  Batch 260/400 | Loss: 1.6319 | LR: 2.27e-05 | Alpha: 0.412\n",
      "  Batch 280/400 | Loss: 1.6108 | LR: 2.24e-05 | Alpha: 0.412\n",
      "  Batch 300/400 | Loss: 1.6002 | LR: 2.22e-05 | Alpha: 0.412\n",
      "  Batch 320/400 | Loss: 1.5833 | LR: 2.19e-05 | Alpha: 0.412\n",
      "  Batch 340/400 | Loss: 1.5549 | LR: 2.16e-05 | Alpha: 0.412\n",
      "  Batch 360/400 | Loss: 1.5446 | LR: 2.14e-05 | Alpha: 0.412\n",
      "  Batch 380/400 | Loss: 1.5346 | LR: 2.11e-05 | Alpha: 0.412\n",
      "  Batch 400/400 | Loss: 1.5169 | LR: 2.08e-05 | Alpha: 0.412\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.5169\n",
      "  Val Loss: 1.7972\n",
      "  Sentiment Accuracy: 0.3675\n",
      "  Emotion Accuracy: 0.1250\n",
      "  Combined Score: 0.2462\n",
      "  Alpha: 0.412\n",
      "  Learning Rate: 2.08e-05\n",
      "\n",
      "📍 Epoch 2/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.1812 | LR: 2.06e-05 | Alpha: 0.412\n",
      "  Batch 40/400 | Loss: 1.2690 | LR: 2.03e-05 | Alpha: 0.412\n",
      "  Batch 60/400 | Loss: 1.2525 | LR: 2.01e-05 | Alpha: 0.412\n",
      "  Batch 80/400 | Loss: 1.2422 | LR: 1.98e-05 | Alpha: 0.412\n",
      "  Batch 100/400 | Loss: 1.2589 | LR: 1.95e-05 | Alpha: 0.412\n",
      "  Batch 120/400 | Loss: 1.2216 | LR: 1.93e-05 | Alpha: 0.412\n",
      "  Batch 140/400 | Loss: 1.2211 | LR: 1.90e-05 | Alpha: 0.412\n",
      "  Batch 160/400 | Loss: 1.2139 | LR: 1.88e-05 | Alpha: 0.412\n",
      "  Batch 180/400 | Loss: 1.2128 | LR: 1.85e-05 | Alpha: 0.412\n",
      "  Batch 200/400 | Loss: 1.1902 | LR: 1.82e-05 | Alpha: 0.412\n",
      "  Batch 220/400 | Loss: 1.1844 | LR: 1.80e-05 | Alpha: 0.412\n",
      "  Batch 240/400 | Loss: 1.1834 | LR: 1.77e-05 | Alpha: 0.412\n",
      "  Batch 260/400 | Loss: 1.1723 | LR: 1.75e-05 | Alpha: 0.412\n",
      "  Batch 280/400 | Loss: 1.1494 | LR: 1.72e-05 | Alpha: 0.412\n",
      "  Batch 300/400 | Loss: 1.1461 | LR: 1.69e-05 | Alpha: 0.412\n",
      "  Batch 320/400 | Loss: 1.1393 | LR: 1.67e-05 | Alpha: 0.412\n",
      "  Batch 340/400 | Loss: 1.1310 | LR: 1.64e-05 | Alpha: 0.412\n",
      "  Batch 360/400 | Loss: 1.1180 | LR: 1.62e-05 | Alpha: 0.412\n",
      "  Batch 380/400 | Loss: 1.1172 | LR: 1.59e-05 | Alpha: 0.412\n",
      "  Batch 400/400 | Loss: 1.1118 | LR: 1.56e-05 | Alpha: 0.412\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.1118\n",
      "  Val Loss: 1.6574\n",
      "  Sentiment Accuracy: 0.6400\n",
      "  Emotion Accuracy: 0.1125\n",
      "  Combined Score: 0.3763\n",
      "  Alpha: 0.412\n",
      "  Learning Rate: 1.56e-05\n",
      "\n",
      "📍 Epoch 3/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 0.8081 | LR: 1.54e-05 | Alpha: 0.412\n",
      "  Batch 40/400 | Loss: 0.8734 | LR: 1.51e-05 | Alpha: 0.412\n",
      "  Batch 60/400 | Loss: 0.9086 | LR: 1.49e-05 | Alpha: 0.412\n",
      "  Batch 80/400 | Loss: 0.9128 | LR: 1.46e-05 | Alpha: 0.412\n",
      "  Batch 100/400 | Loss: 0.9104 | LR: 1.43e-05 | Alpha: 0.412\n",
      "  Batch 120/400 | Loss: 0.8814 | LR: 1.41e-05 | Alpha: 0.412\n",
      "  Batch 140/400 | Loss: 0.8749 | LR: 1.38e-05 | Alpha: 0.412\n",
      "  Batch 160/400 | Loss: 0.8792 | LR: 1.36e-05 | Alpha: 0.412\n",
      "  Batch 180/400 | Loss: 0.8713 | LR: 1.33e-05 | Alpha: 0.412\n",
      "  Batch 200/400 | Loss: 0.8689 | LR: 1.30e-05 | Alpha: 0.412\n",
      "  Batch 220/400 | Loss: 0.8653 | LR: 1.28e-05 | Alpha: 0.412\n",
      "  Batch 240/400 | Loss: 0.8654 | LR: 1.25e-05 | Alpha: 0.412\n",
      "  Batch 260/400 | Loss: 0.8567 | LR: 1.22e-05 | Alpha: 0.412\n",
      "  Batch 280/400 | Loss: 0.8660 | LR: 1.20e-05 | Alpha: 0.412\n",
      "  Batch 300/400 | Loss: 0.8637 | LR: 1.17e-05 | Alpha: 0.412\n",
      "  Batch 320/400 | Loss: 0.8555 | LR: 1.15e-05 | Alpha: 0.412\n",
      "  Batch 340/400 | Loss: 0.8453 | LR: 1.12e-05 | Alpha: 0.412\n",
      "  Batch 360/400 | Loss: 0.8323 | LR: 1.09e-05 | Alpha: 0.412\n",
      "  Batch 380/400 | Loss: 0.8239 | LR: 1.07e-05 | Alpha: 0.412\n",
      "  Batch 400/400 | Loss: 0.8156 | LR: 1.04e-05 | Alpha: 0.412\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.8156\n",
      "  Val Loss: 1.7736\n",
      "  Sentiment Accuracy: 0.7550\n",
      "  Emotion Accuracy: 0.1950\n",
      "  Combined Score: 0.4750\n",
      "  Alpha: 0.412\n",
      "  Learning Rate: 1.04e-05\n",
      "\n",
      "📍 Epoch 4/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 0.7686 | LR: 1.02e-05 | Alpha: 0.412\n",
      "  Batch 40/400 | Loss: 0.7562 | LR: 9.90e-06 | Alpha: 0.412\n",
      "  Batch 60/400 | Loss: 0.7229 | LR: 9.64e-06 | Alpha: 0.412\n",
      "  Batch 80/400 | Loss: 0.6882 | LR: 9.38e-06 | Alpha: 0.412\n",
      "  Batch 100/400 | Loss: 0.6531 | LR: 9.12e-06 | Alpha: 0.412\n",
      "  Batch 120/400 | Loss: 0.6468 | LR: 8.86e-06 | Alpha: 0.412\n",
      "  Batch 140/400 | Loss: 0.6398 | LR: 8.60e-06 | Alpha: 0.412\n",
      "  Batch 160/400 | Loss: 0.6450 | LR: 8.34e-06 | Alpha: 0.412\n",
      "  Batch 180/400 | Loss: 0.6671 | LR: 8.08e-06 | Alpha: 0.412\n",
      "  Batch 200/400 | Loss: 0.6577 | LR: 7.82e-06 | Alpha: 0.412\n",
      "  Batch 220/400 | Loss: 0.6479 | LR: 7.56e-06 | Alpha: 0.412\n",
      "  Batch 240/400 | Loss: 0.6445 | LR: 7.30e-06 | Alpha: 0.412\n",
      "  Batch 260/400 | Loss: 0.6352 | LR: 7.04e-06 | Alpha: 0.412\n",
      "  Batch 280/400 | Loss: 0.6351 | LR: 6.78e-06 | Alpha: 0.412\n",
      "  Batch 300/400 | Loss: 0.6313 | LR: 6.52e-06 | Alpha: 0.412\n",
      "  Batch 320/400 | Loss: 0.6208 | LR: 6.25e-06 | Alpha: 0.412\n",
      "  Batch 340/400 | Loss: 0.6168 | LR: 5.99e-06 | Alpha: 0.412\n",
      "  Batch 360/400 | Loss: 0.6138 | LR: 5.73e-06 | Alpha: 0.412\n",
      "  Batch 380/400 | Loss: 0.6089 | LR: 5.47e-06 | Alpha: 0.412\n",
      "  Batch 400/400 | Loss: 0.6020 | LR: 5.21e-06 | Alpha: 0.412\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.6020\n",
      "  Val Loss: 2.0721\n",
      "  Sentiment Accuracy: 0.7375\n",
      "  Emotion Accuracy: 0.1500\n",
      "  Combined Score: 0.4438\n",
      "  Alpha: 0.412\n",
      "  Learning Rate: 5.21e-06\n",
      "\n",
      "📍 Epoch 5/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 0.4896 | LR: 4.95e-06 | Alpha: 0.412\n",
      "  Batch 40/400 | Loss: 0.4877 | LR: 4.69e-06 | Alpha: 0.412\n",
      "  Batch 60/400 | Loss: 0.5063 | LR: 4.43e-06 | Alpha: 0.412\n",
      "  Batch 80/400 | Loss: 0.5057 | LR: 4.17e-06 | Alpha: 0.412\n",
      "  Batch 100/400 | Loss: 0.5177 | LR: 3.91e-06 | Alpha: 0.412\n",
      "  Batch 120/400 | Loss: 0.5150 | LR: 3.65e-06 | Alpha: 0.412\n",
      "  Batch 140/400 | Loss: 0.5177 | LR: 3.39e-06 | Alpha: 0.412\n",
      "  Batch 160/400 | Loss: 0.5080 | LR: 3.13e-06 | Alpha: 0.412\n",
      "  Batch 180/400 | Loss: 0.5096 | LR: 2.87e-06 | Alpha: 0.412\n",
      "  Batch 200/400 | Loss: 0.5214 | LR: 2.61e-06 | Alpha: 0.412\n",
      "  Batch 220/400 | Loss: 0.5345 | LR: 2.35e-06 | Alpha: 0.412\n",
      "  Batch 240/400 | Loss: 0.5301 | LR: 2.08e-06 | Alpha: 0.412\n",
      "  Batch 260/400 | Loss: 0.5287 | LR: 1.82e-06 | Alpha: 0.412\n",
      "  Batch 280/400 | Loss: 0.5218 | LR: 1.56e-06 | Alpha: 0.412\n",
      "  Batch 300/400 | Loss: 0.5155 | LR: 1.30e-06 | Alpha: 0.412\n",
      "  Batch 320/400 | Loss: 0.5117 | LR: 1.04e-06 | Alpha: 0.412\n",
      "  Batch 340/400 | Loss: 0.5101 | LR: 7.82e-07 | Alpha: 0.412\n",
      "  Batch 360/400 | Loss: 0.5027 | LR: 5.21e-07 | Alpha: 0.412\n",
      "  Batch 380/400 | Loss: 0.5079 | LR: 2.61e-07 | Alpha: 0.412\n",
      "  Batch 400/400 | Loss: 0.5049 | LR: 0.00e+00 | Alpha: 0.412\n",
      "📊 Epoch 5 Results:\n",
      "  Train Loss: 0.5049\n",
      "  Val Loss: 2.0898\n",
      "  Sentiment Accuracy: 0.7550\n",
      "  Emotion Accuracy: 0.1600\n",
      "  Combined Score: 0.4575\n",
      "  Alpha: 0.412\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4750\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.59 GB\n",
      "  Cached: 6.13 GB\n",
      "Trial 5: Combined Score = 0.3947 (Sentiment: 0.6000, Emotion: 0.1895)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 00:53:09,460] Trial 5 finished with value: 0.39473684210526316 and parameters: {'learning_rate': 2.4472440973990114e-05, 'batch_size': 4, 'alpha': 0.41237380387495226, 'hidden_dropout_prob': 0.18567402078956213, 'classifier_dropout': 0.1422772674924288, 'weight_decay': 0.08041750109464993, 'warmup_ratio': 0.06118259655196563, 'num_epochs': 5, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 3200\n",
      "🚀 Starting training for 4 epochs...\n",
      "\n",
      "📍 Epoch 1/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 2.1773 | LR: 3.53e-07 | Alpha: 0.609\n",
      "  Batch 40/800 | Loss: 2.2202 | LR: 7.06e-07 | Alpha: 0.609\n",
      "  Batch 60/800 | Loss: 2.1256 | LR: 1.06e-06 | Alpha: 0.609\n",
      "  Batch 80/800 | Loss: 2.0827 | LR: 1.41e-06 | Alpha: 0.609\n",
      "  Batch 100/800 | Loss: 2.0332 | LR: 1.76e-06 | Alpha: 0.609\n",
      "  Batch 120/800 | Loss: 1.9595 | LR: 2.12e-06 | Alpha: 0.609\n",
      "  Batch 140/800 | Loss: 1.9284 | LR: 2.47e-06 | Alpha: 0.609\n",
      "  Batch 160/800 | Loss: 1.9076 | LR: 2.82e-06 | Alpha: 0.609\n",
      "  Batch 180/800 | Loss: 1.8841 | LR: 3.18e-06 | Alpha: 0.609\n",
      "  Batch 200/800 | Loss: 1.8678 | LR: 3.53e-06 | Alpha: 0.609\n",
      "  Batch 220/800 | Loss: 1.8517 | LR: 3.88e-06 | Alpha: 0.609\n",
      "  Batch 240/800 | Loss: 1.8224 | LR: 4.23e-06 | Alpha: 0.609\n",
      "  Batch 260/800 | Loss: 1.7904 | LR: 4.59e-06 | Alpha: 0.609\n",
      "  Batch 280/800 | Loss: 1.7791 | LR: 4.94e-06 | Alpha: 0.609\n",
      "  Batch 300/800 | Loss: 1.7662 | LR: 5.29e-06 | Alpha: 0.609\n",
      "  Batch 320/800 | Loss: 1.7501 | LR: 5.65e-06 | Alpha: 0.609\n",
      "  Batch 340/800 | Loss: 1.7359 | LR: 6.00e-06 | Alpha: 0.609\n",
      "  Batch 360/800 | Loss: 1.7177 | LR: 6.35e-06 | Alpha: 0.609\n",
      "  Batch 380/800 | Loss: 1.6953 | LR: 6.70e-06 | Alpha: 0.609\n",
      "  Batch 400/800 | Loss: 1.6773 | LR: 7.06e-06 | Alpha: 0.609\n",
      "  Batch 420/800 | Loss: 1.6717 | LR: 7.41e-06 | Alpha: 0.609\n",
      "  Batch 440/800 | Loss: 1.6595 | LR: 7.76e-06 | Alpha: 0.609\n",
      "  Batch 460/800 | Loss: 1.6516 | LR: 8.12e-06 | Alpha: 0.609\n",
      "  Batch 480/800 | Loss: 1.6405 | LR: 8.47e-06 | Alpha: 0.609\n",
      "  Batch 500/800 | Loss: 1.6271 | LR: 8.82e-06 | Alpha: 0.609\n",
      "  Batch 520/800 | Loss: 1.6170 | LR: 9.18e-06 | Alpha: 0.609\n",
      "  Batch 540/800 | Loss: 1.6084 | LR: 9.53e-06 | Alpha: 0.609\n",
      "  Batch 560/800 | Loss: 1.6013 | LR: 9.88e-06 | Alpha: 0.609\n",
      "  Batch 580/800 | Loss: 1.5872 | LR: 1.01e-05 | Alpha: 0.609\n",
      "  Batch 600/800 | Loss: 1.5854 | LR: 1.00e-05 | Alpha: 0.609\n",
      "  Batch 620/800 | Loss: 1.5820 | LR: 9.95e-06 | Alpha: 0.609\n",
      "  Batch 640/800 | Loss: 1.5748 | LR: 9.87e-06 | Alpha: 0.609\n",
      "  Batch 660/800 | Loss: 1.5621 | LR: 9.80e-06 | Alpha: 0.609\n",
      "  Batch 680/800 | Loss: 1.5531 | LR: 9.72e-06 | Alpha: 0.609\n",
      "  Batch 700/800 | Loss: 1.5380 | LR: 9.64e-06 | Alpha: 0.609\n",
      "  Batch 720/800 | Loss: 1.5278 | LR: 9.56e-06 | Alpha: 0.609\n",
      "  Batch 740/800 | Loss: 1.5161 | LR: 9.49e-06 | Alpha: 0.609\n",
      "  Batch 760/800 | Loss: 1.5071 | LR: 9.41e-06 | Alpha: 0.609\n",
      "  Batch 780/800 | Loss: 1.4967 | LR: 9.33e-06 | Alpha: 0.609\n",
      "  Batch 800/800 | Loss: 1.4847 | LR: 9.26e-06 | Alpha: 0.609\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.4847\n",
      "  Val Loss: 1.3332\n",
      "  Sentiment Accuracy: 0.6150\n",
      "  Emotion Accuracy: 0.1175\n",
      "  Combined Score: 0.3662\n",
      "  Alpha: 0.609\n",
      "  Learning Rate: 9.26e-06\n",
      "\n",
      "📍 Epoch 2/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.0676 | LR: 9.18e-06 | Alpha: 0.609\n",
      "  Batch 40/800 | Loss: 1.1165 | LR: 9.10e-06 | Alpha: 0.609\n",
      "  Batch 60/800 | Loss: 1.0750 | LR: 9.02e-06 | Alpha: 0.609\n",
      "  Batch 80/800 | Loss: 1.0323 | LR: 8.95e-06 | Alpha: 0.609\n",
      "  Batch 100/800 | Loss: 1.0861 | LR: 8.87e-06 | Alpha: 0.609\n",
      "  Batch 120/800 | Loss: 1.0750 | LR: 8.79e-06 | Alpha: 0.609\n",
      "  Batch 140/800 | Loss: 1.0905 | LR: 8.72e-06 | Alpha: 0.609\n",
      "  Batch 160/800 | Loss: 1.0618 | LR: 8.64e-06 | Alpha: 0.609\n",
      "  Batch 180/800 | Loss: 1.0493 | LR: 8.56e-06 | Alpha: 0.609\n",
      "  Batch 200/800 | Loss: 1.0412 | LR: 8.48e-06 | Alpha: 0.609\n",
      "  Batch 220/800 | Loss: 1.0214 | LR: 8.41e-06 | Alpha: 0.609\n",
      "  Batch 240/800 | Loss: 1.0177 | LR: 8.33e-06 | Alpha: 0.609\n",
      "  Batch 260/800 | Loss: 1.0304 | LR: 8.25e-06 | Alpha: 0.609\n",
      "  Batch 280/800 | Loss: 1.0047 | LR: 8.18e-06 | Alpha: 0.609\n",
      "  Batch 300/800 | Loss: 1.0024 | LR: 8.10e-06 | Alpha: 0.609\n",
      "  Batch 320/800 | Loss: 0.9984 | LR: 8.02e-06 | Alpha: 0.609\n",
      "  Batch 340/800 | Loss: 0.9848 | LR: 7.95e-06 | Alpha: 0.609\n",
      "  Batch 360/800 | Loss: 0.9814 | LR: 7.87e-06 | Alpha: 0.609\n",
      "  Batch 380/800 | Loss: 0.9755 | LR: 7.79e-06 | Alpha: 0.609\n",
      "  Batch 400/800 | Loss: 0.9640 | LR: 7.71e-06 | Alpha: 0.609\n",
      "  Batch 420/800 | Loss: 0.9594 | LR: 7.64e-06 | Alpha: 0.609\n",
      "  Batch 440/800 | Loss: 0.9498 | LR: 7.56e-06 | Alpha: 0.609\n",
      "  Batch 460/800 | Loss: 0.9412 | LR: 7.48e-06 | Alpha: 0.609\n",
      "  Batch 480/800 | Loss: 0.9377 | LR: 7.41e-06 | Alpha: 0.609\n",
      "  Batch 500/800 | Loss: 0.9318 | LR: 7.33e-06 | Alpha: 0.609\n",
      "  Batch 520/800 | Loss: 0.9242 | LR: 7.25e-06 | Alpha: 0.609\n",
      "  Batch 540/800 | Loss: 0.9194 | LR: 7.17e-06 | Alpha: 0.609\n",
      "  Batch 560/800 | Loss: 0.9126 | LR: 7.10e-06 | Alpha: 0.609\n",
      "  Batch 580/800 | Loss: 0.9047 | LR: 7.02e-06 | Alpha: 0.609\n",
      "  Batch 600/800 | Loss: 0.8978 | LR: 6.94e-06 | Alpha: 0.609\n",
      "  Batch 620/800 | Loss: 0.8845 | LR: 6.87e-06 | Alpha: 0.609\n",
      "  Batch 640/800 | Loss: 0.8805 | LR: 6.79e-06 | Alpha: 0.609\n",
      "  Batch 660/800 | Loss: 0.8686 | LR: 6.71e-06 | Alpha: 0.609\n",
      "  Batch 680/800 | Loss: 0.8659 | LR: 6.63e-06 | Alpha: 0.609\n",
      "  Batch 700/800 | Loss: 0.8559 | LR: 6.56e-06 | Alpha: 0.609\n",
      "  Batch 720/800 | Loss: 0.8458 | LR: 6.48e-06 | Alpha: 0.609\n",
      "  Batch 740/800 | Loss: 0.8342 | LR: 6.40e-06 | Alpha: 0.609\n",
      "  Batch 760/800 | Loss: 0.8340 | LR: 6.33e-06 | Alpha: 0.609\n",
      "  Batch 780/800 | Loss: 0.8362 | LR: 6.25e-06 | Alpha: 0.609\n",
      "  Batch 800/800 | Loss: 0.8320 | LR: 6.17e-06 | Alpha: 0.609\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 0.8320\n",
      "  Val Loss: 1.3136\n",
      "  Sentiment Accuracy: 0.8200\n",
      "  Emotion Accuracy: 0.1650\n",
      "  Combined Score: 0.4925\n",
      "  Alpha: 0.609\n",
      "  Learning Rate: 6.17e-06\n",
      "\n",
      "📍 Epoch 3/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.7089 | LR: 6.09e-06 | Alpha: 0.609\n",
      "  Batch 40/800 | Loss: 0.8591 | LR: 6.02e-06 | Alpha: 0.609\n",
      "  Batch 60/800 | Loss: 0.7908 | LR: 5.94e-06 | Alpha: 0.609\n",
      "  Batch 80/800 | Loss: 0.7593 | LR: 5.86e-06 | Alpha: 0.609\n",
      "  Batch 100/800 | Loss: 0.7316 | LR: 5.79e-06 | Alpha: 0.609\n",
      "  Batch 120/800 | Loss: 0.7043 | LR: 5.71e-06 | Alpha: 0.609\n",
      "  Batch 140/800 | Loss: 0.6899 | LR: 5.63e-06 | Alpha: 0.609\n",
      "  Batch 160/800 | Loss: 0.6787 | LR: 5.55e-06 | Alpha: 0.609\n",
      "  Batch 180/800 | Loss: 0.6837 | LR: 5.48e-06 | Alpha: 0.609\n",
      "  Batch 200/800 | Loss: 0.6886 | LR: 5.40e-06 | Alpha: 0.609\n",
      "  Batch 220/800 | Loss: 0.6775 | LR: 5.32e-06 | Alpha: 0.609\n",
      "  Batch 240/800 | Loss: 0.6985 | LR: 5.25e-06 | Alpha: 0.609\n",
      "  Batch 260/800 | Loss: 0.6752 | LR: 5.17e-06 | Alpha: 0.609\n",
      "  Batch 280/800 | Loss: 0.6608 | LR: 5.09e-06 | Alpha: 0.609\n",
      "  Batch 300/800 | Loss: 0.6500 | LR: 5.01e-06 | Alpha: 0.609\n",
      "  Batch 320/800 | Loss: 0.6493 | LR: 4.94e-06 | Alpha: 0.609\n",
      "  Batch 340/800 | Loss: 0.6394 | LR: 4.86e-06 | Alpha: 0.609\n",
      "  Batch 360/800 | Loss: 0.6275 | LR: 4.78e-06 | Alpha: 0.609\n",
      "  Batch 380/800 | Loss: 0.6397 | LR: 4.71e-06 | Alpha: 0.609\n",
      "  Batch 400/800 | Loss: 0.6316 | LR: 4.63e-06 | Alpha: 0.609\n",
      "  Batch 420/800 | Loss: 0.6278 | LR: 4.55e-06 | Alpha: 0.609\n",
      "  Batch 440/800 | Loss: 0.6348 | LR: 4.47e-06 | Alpha: 0.609\n",
      "  Batch 460/800 | Loss: 0.6273 | LR: 4.40e-06 | Alpha: 0.609\n",
      "  Batch 480/800 | Loss: 0.6176 | LR: 4.32e-06 | Alpha: 0.609\n",
      "  Batch 500/800 | Loss: 0.6194 | LR: 4.24e-06 | Alpha: 0.609\n",
      "  Batch 520/800 | Loss: 0.6056 | LR: 4.17e-06 | Alpha: 0.609\n",
      "  Batch 540/800 | Loss: 0.5982 | LR: 4.09e-06 | Alpha: 0.609\n",
      "  Batch 560/800 | Loss: 0.5976 | LR: 4.01e-06 | Alpha: 0.609\n",
      "  Batch 580/800 | Loss: 0.5909 | LR: 3.93e-06 | Alpha: 0.609\n",
      "  Batch 600/800 | Loss: 0.5899 | LR: 3.86e-06 | Alpha: 0.609\n",
      "  Batch 620/800 | Loss: 0.5888 | LR: 3.78e-06 | Alpha: 0.609\n",
      "  Batch 640/800 | Loss: 0.5868 | LR: 3.70e-06 | Alpha: 0.609\n",
      "  Batch 660/800 | Loss: 0.5822 | LR: 3.63e-06 | Alpha: 0.609\n",
      "  Batch 680/800 | Loss: 0.5789 | LR: 3.55e-06 | Alpha: 0.609\n",
      "  Batch 700/800 | Loss: 0.5775 | LR: 3.47e-06 | Alpha: 0.609\n",
      "  Batch 720/800 | Loss: 0.5734 | LR: 3.39e-06 | Alpha: 0.609\n",
      "  Batch 740/800 | Loss: 0.5672 | LR: 3.32e-06 | Alpha: 0.609\n",
      "  Batch 760/800 | Loss: 0.5627 | LR: 3.24e-06 | Alpha: 0.609\n",
      "  Batch 780/800 | Loss: 0.5603 | LR: 3.16e-06 | Alpha: 0.609\n",
      "  Batch 800/800 | Loss: 0.5581 | LR: 3.09e-06 | Alpha: 0.609\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.5581\n",
      "  Val Loss: 1.5954\n",
      "  Sentiment Accuracy: 0.8125\n",
      "  Emotion Accuracy: 0.1500\n",
      "  Combined Score: 0.4813\n",
      "  Alpha: 0.609\n",
      "  Learning Rate: 3.09e-06\n",
      "\n",
      "📍 Epoch 4/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.4952 | LR: 3.01e-06 | Alpha: 0.609\n",
      "  Batch 40/800 | Loss: 0.5241 | LR: 2.93e-06 | Alpha: 0.609\n",
      "  Batch 60/800 | Loss: 0.5219 | LR: 2.85e-06 | Alpha: 0.609\n",
      "  Batch 80/800 | Loss: 0.5106 | LR: 2.78e-06 | Alpha: 0.609\n",
      "  Batch 100/800 | Loss: 0.4996 | LR: 2.70e-06 | Alpha: 0.609\n",
      "  Batch 120/800 | Loss: 0.4953 | LR: 2.62e-06 | Alpha: 0.609\n",
      "  Batch 140/800 | Loss: 0.4780 | LR: 2.55e-06 | Alpha: 0.609\n",
      "  Batch 160/800 | Loss: 0.4562 | LR: 2.47e-06 | Alpha: 0.609\n",
      "  Batch 180/800 | Loss: 0.4715 | LR: 2.39e-06 | Alpha: 0.609\n",
      "  Batch 200/800 | Loss: 0.4754 | LR: 2.31e-06 | Alpha: 0.609\n",
      "  Batch 220/800 | Loss: 0.4736 | LR: 2.24e-06 | Alpha: 0.609\n",
      "  Batch 240/800 | Loss: 0.4677 | LR: 2.16e-06 | Alpha: 0.609\n",
      "  Batch 260/800 | Loss: 0.4662 | LR: 2.08e-06 | Alpha: 0.609\n",
      "  Batch 280/800 | Loss: 0.4508 | LR: 2.01e-06 | Alpha: 0.609\n",
      "  Batch 300/800 | Loss: 0.4499 | LR: 1.93e-06 | Alpha: 0.609\n",
      "  Batch 320/800 | Loss: 0.4508 | LR: 1.85e-06 | Alpha: 0.609\n",
      "  Batch 340/800 | Loss: 0.4486 | LR: 1.77e-06 | Alpha: 0.609\n",
      "  Batch 360/800 | Loss: 0.4532 | LR: 1.70e-06 | Alpha: 0.609\n",
      "  Batch 380/800 | Loss: 0.4508 | LR: 1.62e-06 | Alpha: 0.609\n",
      "  Batch 400/800 | Loss: 0.4540 | LR: 1.54e-06 | Alpha: 0.609\n",
      "  Batch 420/800 | Loss: 0.4529 | LR: 1.47e-06 | Alpha: 0.609\n",
      "  Batch 440/800 | Loss: 0.4503 | LR: 1.39e-06 | Alpha: 0.609\n",
      "  Batch 460/800 | Loss: 0.4486 | LR: 1.31e-06 | Alpha: 0.609\n",
      "  Batch 480/800 | Loss: 0.4493 | LR: 1.23e-06 | Alpha: 0.609\n",
      "  Batch 500/800 | Loss: 0.4538 | LR: 1.16e-06 | Alpha: 0.609\n",
      "  Batch 520/800 | Loss: 0.4524 | LR: 1.08e-06 | Alpha: 0.609\n",
      "  Batch 540/800 | Loss: 0.4491 | LR: 1.00e-06 | Alpha: 0.609\n",
      "  Batch 560/800 | Loss: 0.4504 | LR: 9.26e-07 | Alpha: 0.609\n",
      "  Batch 580/800 | Loss: 0.4483 | LR: 8.48e-07 | Alpha: 0.609\n",
      "  Batch 600/800 | Loss: 0.4521 | LR: 7.71e-07 | Alpha: 0.609\n",
      "  Batch 620/800 | Loss: 0.4465 | LR: 6.94e-07 | Alpha: 0.609\n",
      "  Batch 640/800 | Loss: 0.4471 | LR: 6.17e-07 | Alpha: 0.609\n",
      "  Batch 660/800 | Loss: 0.4512 | LR: 5.40e-07 | Alpha: 0.609\n",
      "  Batch 680/800 | Loss: 0.4558 | LR: 4.63e-07 | Alpha: 0.609\n",
      "  Batch 700/800 | Loss: 0.4559 | LR: 3.86e-07 | Alpha: 0.609\n",
      "  Batch 720/800 | Loss: 0.4544 | LR: 3.09e-07 | Alpha: 0.609\n",
      "  Batch 740/800 | Loss: 0.4512 | LR: 2.31e-07 | Alpha: 0.609\n",
      "  Batch 760/800 | Loss: 0.4509 | LR: 1.54e-07 | Alpha: 0.609\n",
      "  Batch 780/800 | Loss: 0.4511 | LR: 7.71e-08 | Alpha: 0.609\n",
      "  Batch 800/800 | Loss: 0.4470 | LR: 0.00e+00 | Alpha: 0.609\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.4470\n",
      "  Val Loss: 1.6354\n",
      "  Sentiment Accuracy: 0.8100\n",
      "  Emotion Accuracy: 0.1425\n",
      "  Combined Score: 0.4763\n",
      "  Alpha: 0.609\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4925\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.59 GB\n",
      "  Cached: 6.08 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 01:07:13,178] Trial 6 finished with value: 0.3473684210526316 and parameters: {'learning_rate': 1.012796325733148e-05, 'batch_size': 2, 'alpha': 0.6085081386743783, 'hidden_dropout_prob': 0.06851116293352259, 'classifier_dropout': 0.2075397185632818, 'weight_decay': 0.012471036892987841, 'warmup_ratio': 0.17946551388133902, 'num_epochs': 4, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6: Combined Score = 0.3474 (Sentiment: 0.5789, Emotion: 0.1158)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 1600\n",
      "🚀 Starting training for 4 epochs...\n",
      "\n",
      "📍 Epoch 1/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.7603 | LR: 1.56e-06 | Alpha: 0.655\n",
      "  Batch 40/400 | Loss: 1.6183 | LR: 3.12e-06 | Alpha: 0.655\n",
      "  Batch 60/400 | Loss: 1.5385 | LR: 4.69e-06 | Alpha: 0.655\n",
      "  Batch 80/400 | Loss: 1.4815 | LR: 6.25e-06 | Alpha: 0.655\n",
      "  Batch 100/400 | Loss: 1.4667 | LR: 7.81e-06 | Alpha: 0.655\n",
      "  Batch 120/400 | Loss: 1.4496 | LR: 9.37e-06 | Alpha: 0.655\n",
      "  Batch 140/400 | Loss: 1.4191 | LR: 1.09e-05 | Alpha: 0.655\n",
      "  Batch 160/400 | Loss: 1.4158 | LR: 1.25e-05 | Alpha: 0.655\n",
      "  Batch 180/400 | Loss: 1.3917 | LR: 1.41e-05 | Alpha: 0.655\n",
      "  Batch 200/400 | Loss: 1.3829 | LR: 1.56e-05 | Alpha: 0.655\n",
      "  Batch 220/400 | Loss: 1.3791 | LR: 1.72e-05 | Alpha: 0.655\n",
      "  Batch 240/400 | Loss: 1.3670 | LR: 1.87e-05 | Alpha: 0.655\n",
      "  Batch 260/400 | Loss: 1.3717 | LR: 2.03e-05 | Alpha: 0.655\n",
      "  Batch 280/400 | Loss: 1.3632 | LR: 2.02e-05 | Alpha: 0.655\n",
      "  Batch 300/400 | Loss: 1.3527 | LR: 1.99e-05 | Alpha: 0.655\n",
      "  Batch 320/400 | Loss: 1.3405 | LR: 1.96e-05 | Alpha: 0.655\n",
      "  Batch 340/400 | Loss: 1.3226 | LR: 1.93e-05 | Alpha: 0.655\n",
      "  Batch 360/400 | Loss: 1.3170 | LR: 1.90e-05 | Alpha: 0.655\n",
      "  Batch 380/400 | Loss: 1.3019 | LR: 1.87e-05 | Alpha: 0.655\n",
      "  Batch 400/400 | Loss: 1.2905 | LR: 1.84e-05 | Alpha: 0.655\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.2905\n",
      "  Val Loss: 1.6355\n",
      "  Sentiment Accuracy: 0.4300\n",
      "  Emotion Accuracy: 0.1175\n",
      "  Combined Score: 0.2737\n",
      "  Alpha: 0.655\n",
      "  Learning Rate: 1.84e-05\n",
      "\n",
      "📍 Epoch 2/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.1008 | LR: 1.80e-05 | Alpha: 0.655\n",
      "  Batch 40/400 | Loss: 1.0844 | LR: 1.77e-05 | Alpha: 0.655\n",
      "  Batch 60/400 | Loss: 1.0150 | LR: 1.74e-05 | Alpha: 0.655\n",
      "  Batch 80/400 | Loss: 0.9735 | LR: 1.71e-05 | Alpha: 0.655\n",
      "  Batch 100/400 | Loss: 0.9921 | LR: 1.68e-05 | Alpha: 0.655\n",
      "  Batch 120/400 | Loss: 0.9690 | LR: 1.65e-05 | Alpha: 0.655\n",
      "  Batch 140/400 | Loss: 1.0197 | LR: 1.62e-05 | Alpha: 0.655\n",
      "  Batch 160/400 | Loss: 1.0105 | LR: 1.59e-05 | Alpha: 0.655\n",
      "  Batch 180/400 | Loss: 1.0051 | LR: 1.56e-05 | Alpha: 0.655\n",
      "  Batch 200/400 | Loss: 0.9975 | LR: 1.53e-05 | Alpha: 0.655\n",
      "  Batch 220/400 | Loss: 0.9815 | LR: 1.50e-05 | Alpha: 0.655\n",
      "  Batch 240/400 | Loss: 0.9596 | LR: 1.47e-05 | Alpha: 0.655\n",
      "  Batch 260/400 | Loss: 0.9442 | LR: 1.44e-05 | Alpha: 0.655\n",
      "  Batch 280/400 | Loss: 0.9393 | LR: 1.41e-05 | Alpha: 0.655\n",
      "  Batch 300/400 | Loss: 0.9406 | LR: 1.38e-05 | Alpha: 0.655\n",
      "  Batch 320/400 | Loss: 0.9325 | LR: 1.35e-05 | Alpha: 0.655\n",
      "  Batch 340/400 | Loss: 0.9266 | LR: 1.32e-05 | Alpha: 0.655\n",
      "  Batch 360/400 | Loss: 0.9171 | LR: 1.28e-05 | Alpha: 0.655\n",
      "  Batch 380/400 | Loss: 0.9189 | LR: 1.25e-05 | Alpha: 0.655\n",
      "  Batch 400/400 | Loss: 0.9054 | LR: 1.22e-05 | Alpha: 0.655\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 0.9054\n",
      "  Val Loss: 1.2936\n",
      "  Sentiment Accuracy: 0.7075\n",
      "  Emotion Accuracy: 0.1125\n",
      "  Combined Score: 0.4100\n",
      "  Alpha: 0.655\n",
      "  Learning Rate: 1.22e-05\n",
      "\n",
      "📍 Epoch 3/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 0.7592 | LR: 1.19e-05 | Alpha: 0.655\n",
      "  Batch 40/400 | Loss: 0.6334 | LR: 1.16e-05 | Alpha: 0.655\n",
      "  Batch 60/400 | Loss: 0.6660 | LR: 1.13e-05 | Alpha: 0.655\n",
      "  Batch 80/400 | Loss: 0.6540 | LR: 1.10e-05 | Alpha: 0.655\n",
      "  Batch 100/400 | Loss: 0.6758 | LR: 1.07e-05 | Alpha: 0.655\n",
      "  Batch 120/400 | Loss: 0.6797 | LR: 1.04e-05 | Alpha: 0.655\n",
      "  Batch 140/400 | Loss: 0.6805 | LR: 1.01e-05 | Alpha: 0.655\n",
      "  Batch 160/400 | Loss: 0.6800 | LR: 9.79e-06 | Alpha: 0.655\n",
      "  Batch 180/400 | Loss: 0.6734 | LR: 9.48e-06 | Alpha: 0.655\n",
      "  Batch 200/400 | Loss: 0.6769 | LR: 9.18e-06 | Alpha: 0.655\n",
      "  Batch 220/400 | Loss: 0.6623 | LR: 8.87e-06 | Alpha: 0.655\n",
      "  Batch 240/400 | Loss: 0.6554 | LR: 8.56e-06 | Alpha: 0.655\n",
      "  Batch 260/400 | Loss: 0.6497 | LR: 8.26e-06 | Alpha: 0.655\n",
      "  Batch 280/400 | Loss: 0.6491 | LR: 7.95e-06 | Alpha: 0.655\n",
      "  Batch 300/400 | Loss: 0.6390 | LR: 7.65e-06 | Alpha: 0.655\n",
      "  Batch 320/400 | Loss: 0.6291 | LR: 7.34e-06 | Alpha: 0.655\n",
      "  Batch 340/400 | Loss: 0.6267 | LR: 7.04e-06 | Alpha: 0.655\n",
      "  Batch 360/400 | Loss: 0.6150 | LR: 6.73e-06 | Alpha: 0.655\n",
      "  Batch 380/400 | Loss: 0.6098 | LR: 6.42e-06 | Alpha: 0.655\n",
      "  Batch 400/400 | Loss: 0.6076 | LR: 6.12e-06 | Alpha: 0.655\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.6076\n",
      "  Val Loss: 1.2201\n",
      "  Sentiment Accuracy: 0.7675\n",
      "  Emotion Accuracy: 0.1425\n",
      "  Combined Score: 0.4550\n",
      "  Alpha: 0.655\n",
      "  Learning Rate: 6.12e-06\n",
      "\n",
      "📍 Epoch 4/4\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 0.5152 | LR: 5.81e-06 | Alpha: 0.655\n",
      "  Batch 40/400 | Loss: 0.4925 | LR: 5.51e-06 | Alpha: 0.655\n",
      "  Batch 60/400 | Loss: 0.5060 | LR: 5.20e-06 | Alpha: 0.655\n",
      "  Batch 80/400 | Loss: 0.4862 | LR: 4.89e-06 | Alpha: 0.655\n",
      "  Batch 100/400 | Loss: 0.5049 | LR: 4.59e-06 | Alpha: 0.655\n",
      "  Batch 120/400 | Loss: 0.4939 | LR: 4.28e-06 | Alpha: 0.655\n",
      "  Batch 140/400 | Loss: 0.4956 | LR: 3.98e-06 | Alpha: 0.655\n",
      "  Batch 160/400 | Loss: 0.4840 | LR: 3.67e-06 | Alpha: 0.655\n",
      "  Batch 180/400 | Loss: 0.4880 | LR: 3.36e-06 | Alpha: 0.655\n",
      "  Batch 200/400 | Loss: 0.4945 | LR: 3.06e-06 | Alpha: 0.655\n",
      "  Batch 220/400 | Loss: 0.4970 | LR: 2.75e-06 | Alpha: 0.655\n",
      "  Batch 240/400 | Loss: 0.4838 | LR: 2.45e-06 | Alpha: 0.655\n",
      "  Batch 260/400 | Loss: 0.4838 | LR: 2.14e-06 | Alpha: 0.655\n",
      "  Batch 280/400 | Loss: 0.4830 | LR: 1.84e-06 | Alpha: 0.655\n",
      "  Batch 300/400 | Loss: 0.4805 | LR: 1.53e-06 | Alpha: 0.655\n",
      "  Batch 320/400 | Loss: 0.4892 | LR: 1.22e-06 | Alpha: 0.655\n",
      "  Batch 340/400 | Loss: 0.4866 | LR: 9.18e-07 | Alpha: 0.655\n",
      "  Batch 360/400 | Loss: 0.4906 | LR: 6.12e-07 | Alpha: 0.655\n",
      "  Batch 380/400 | Loss: 0.4960 | LR: 3.06e-07 | Alpha: 0.655\n",
      "  Batch 400/400 | Loss: 0.4922 | LR: 0.00e+00 | Alpha: 0.655\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.4922\n",
      "  Val Loss: 1.3028\n",
      "  Sentiment Accuracy: 0.7800\n",
      "  Emotion Accuracy: 0.1525\n",
      "  Combined Score: 0.4662\n",
      "  Alpha: 0.655\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4662\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.60 GB\n",
      "  Cached: 6.15 GB\n",
      "Trial 7: Combined Score = 0.3737 (Sentiment: 0.5789, Emotion: 0.1684)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 01:16:39,068] Trial 7 finished with value: 0.3736842105263158 and parameters: {'learning_rate': 2.0463613363481594e-05, 'batch_size': 4, 'alpha': 0.6548850970305305, 'hidden_dropout_prob': 0.16805373129048734, 'classifier_dropout': 0.13587827378149053, 'weight_decay': 0.07161123393507651, 'warmup_ratio': 0.16411775729253464, 'num_epochs': 4, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 1600\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 2.2357 | LR: 2.24e-06 | Alpha: 0.313\n",
      "  Batch 40/800 | Loss: 2.2114 | LR: 4.49e-06 | Alpha: 0.313\n",
      "  Batch 60/800 | Loss: 2.1841 | LR: 6.73e-06 | Alpha: 0.313\n",
      "  Batch 80/800 | Loss: 2.1039 | LR: 8.98e-06 | Alpha: 0.313\n",
      "  Batch 100/800 | Loss: 2.0801 | LR: 1.12e-05 | Alpha: 0.313\n",
      "  Batch 120/800 | Loss: 2.0184 | LR: 1.35e-05 | Alpha: 0.313\n",
      "  Batch 140/800 | Loss: 1.9915 | LR: 1.57e-05 | Alpha: 0.313\n",
      "  Batch 160/800 | Loss: 1.9786 | LR: 1.80e-05 | Alpha: 0.313\n",
      "  Batch 180/800 | Loss: 1.9980 | LR: 2.02e-05 | Alpha: 0.313\n",
      "  Batch 200/800 | Loss: 1.9838 | LR: 2.24e-05 | Alpha: 0.313\n",
      "  Batch 220/800 | Loss: 1.9722 | LR: 2.47e-05 | Alpha: 0.313\n",
      "  Batch 240/800 | Loss: 1.9843 | LR: 2.69e-05 | Alpha: 0.313\n",
      "  Batch 260/800 | Loss: 1.9952 | LR: 2.92e-05 | Alpha: 0.313\n",
      "  Batch 280/800 | Loss: 1.9959 | LR: 3.14e-05 | Alpha: 0.313\n",
      "  Batch 300/800 | Loss: 2.0064 | LR: 3.32e-05 | Alpha: 0.313\n",
      "  Batch 320/800 | Loss: 2.0164 | LR: 3.27e-05 | Alpha: 0.313\n",
      "  Batch 340/800 | Loss: 2.0130 | LR: 3.22e-05 | Alpha: 0.313\n",
      "  Batch 360/800 | Loss: 2.0008 | LR: 3.17e-05 | Alpha: 0.313\n",
      "  Batch 380/800 | Loss: 1.9973 | LR: 3.12e-05 | Alpha: 0.313\n",
      "  Batch 400/800 | Loss: 1.9882 | LR: 3.07e-05 | Alpha: 0.313\n",
      "  Batch 420/800 | Loss: 1.9825 | LR: 3.02e-05 | Alpha: 0.313\n",
      "  Batch 440/800 | Loss: 1.9668 | LR: 2.97e-05 | Alpha: 0.313\n",
      "  Batch 460/800 | Loss: 1.9802 | LR: 2.92e-05 | Alpha: 0.313\n",
      "  Batch 480/800 | Loss: 1.9696 | LR: 2.86e-05 | Alpha: 0.313\n",
      "  Batch 500/800 | Loss: 1.9681 | LR: 2.81e-05 | Alpha: 0.313\n",
      "  Batch 520/800 | Loss: 1.9611 | LR: 2.76e-05 | Alpha: 0.313\n",
      "  Batch 540/800 | Loss: 1.9564 | LR: 2.71e-05 | Alpha: 0.313\n",
      "  Batch 560/800 | Loss: 1.9522 | LR: 2.66e-05 | Alpha: 0.313\n",
      "  Batch 580/800 | Loss: 1.9438 | LR: 2.61e-05 | Alpha: 0.313\n",
      "  Batch 600/800 | Loss: 1.9457 | LR: 2.56e-05 | Alpha: 0.313\n",
      "  Batch 620/800 | Loss: 1.9481 | LR: 2.51e-05 | Alpha: 0.313\n",
      "  Batch 640/800 | Loss: 1.9406 | LR: 2.46e-05 | Alpha: 0.313\n",
      "  Batch 660/800 | Loss: 1.9376 | LR: 2.40e-05 | Alpha: 0.313\n",
      "  Batch 680/800 | Loss: 1.9355 | LR: 2.35e-05 | Alpha: 0.313\n",
      "  Batch 700/800 | Loss: 1.9300 | LR: 2.30e-05 | Alpha: 0.313\n",
      "  Batch 720/800 | Loss: 1.9241 | LR: 2.25e-05 | Alpha: 0.313\n",
      "  Batch 740/800 | Loss: 1.9197 | LR: 2.20e-05 | Alpha: 0.313\n",
      "  Batch 760/800 | Loss: 1.9151 | LR: 2.15e-05 | Alpha: 0.313\n",
      "  Batch 780/800 | Loss: 1.9146 | LR: 2.10e-05 | Alpha: 0.313\n",
      "  Batch 800/800 | Loss: 1.9132 | LR: 2.05e-05 | Alpha: 0.313\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.9132\n",
      "  Val Loss: 1.7233\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.1475\n",
      "  Combined Score: 0.0938\n",
      "  Alpha: 0.313\n",
      "  Learning Rate: 2.05e-05\n",
      "\n",
      "📍 Epoch 2/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.9073 | LR: 1.99e-05 | Alpha: 0.313\n",
      "  Batch 40/800 | Loss: 1.7998 | LR: 1.94e-05 | Alpha: 0.313\n",
      "  Batch 60/800 | Loss: 1.7579 | LR: 1.89e-05 | Alpha: 0.313\n",
      "  Batch 80/800 | Loss: 1.8043 | LR: 1.84e-05 | Alpha: 0.313\n",
      "  Batch 100/800 | Loss: 1.7884 | LR: 1.79e-05 | Alpha: 0.313\n",
      "  Batch 120/800 | Loss: 1.7907 | LR: 1.74e-05 | Alpha: 0.313\n",
      "  Batch 140/800 | Loss: 1.7874 | LR: 1.69e-05 | Alpha: 0.313\n",
      "  Batch 160/800 | Loss: 1.7903 | LR: 1.64e-05 | Alpha: 0.313\n",
      "  Batch 180/800 | Loss: 1.7839 | LR: 1.59e-05 | Alpha: 0.313\n",
      "  Batch 200/800 | Loss: 1.7835 | LR: 1.53e-05 | Alpha: 0.313\n",
      "  Batch 220/800 | Loss: 1.7965 | LR: 1.48e-05 | Alpha: 0.313\n",
      "  Batch 240/800 | Loss: 1.7970 | LR: 1.43e-05 | Alpha: 0.313\n",
      "  Batch 260/800 | Loss: 1.7966 | LR: 1.38e-05 | Alpha: 0.313\n",
      "  Batch 280/800 | Loss: 1.7925 | LR: 1.33e-05 | Alpha: 0.313\n",
      "  Batch 300/800 | Loss: 1.7811 | LR: 1.28e-05 | Alpha: 0.313\n",
      "  Batch 320/800 | Loss: 1.7774 | LR: 1.23e-05 | Alpha: 0.313\n",
      "  Batch 340/800 | Loss: 1.7654 | LR: 1.18e-05 | Alpha: 0.313\n",
      "  Batch 360/800 | Loss: 1.7638 | LR: 1.13e-05 | Alpha: 0.313\n",
      "  Batch 380/800 | Loss: 1.7659 | LR: 1.07e-05 | Alpha: 0.313\n",
      "  Batch 400/800 | Loss: 1.7675 | LR: 1.02e-05 | Alpha: 0.313\n",
      "  Batch 420/800 | Loss: 1.7663 | LR: 9.72e-06 | Alpha: 0.313\n",
      "  Batch 440/800 | Loss: 1.7622 | LR: 9.21e-06 | Alpha: 0.313\n",
      "  Batch 460/800 | Loss: 1.7570 | LR: 8.69e-06 | Alpha: 0.313\n",
      "  Batch 480/800 | Loss: 1.7575 | LR: 8.18e-06 | Alpha: 0.313\n",
      "  Batch 500/800 | Loss: 1.7523 | LR: 7.67e-06 | Alpha: 0.313\n",
      "  Batch 520/800 | Loss: 1.7528 | LR: 7.16e-06 | Alpha: 0.313\n",
      "  Batch 540/800 | Loss: 1.7518 | LR: 6.65e-06 | Alpha: 0.313\n",
      "  Batch 560/800 | Loss: 1.7455 | LR: 6.14e-06 | Alpha: 0.313\n",
      "  Batch 580/800 | Loss: 1.7438 | LR: 5.63e-06 | Alpha: 0.313\n",
      "  Batch 600/800 | Loss: 1.7393 | LR: 5.11e-06 | Alpha: 0.313\n",
      "  Batch 620/800 | Loss: 1.7383 | LR: 4.60e-06 | Alpha: 0.313\n",
      "  Batch 640/800 | Loss: 1.7383 | LR: 4.09e-06 | Alpha: 0.313\n",
      "  Batch 660/800 | Loss: 1.7354 | LR: 3.58e-06 | Alpha: 0.313\n",
      "  Batch 680/800 | Loss: 1.7330 | LR: 3.07e-06 | Alpha: 0.313\n",
      "  Batch 700/800 | Loss: 1.7268 | LR: 2.56e-06 | Alpha: 0.313\n",
      "  Batch 720/800 | Loss: 1.7262 | LR: 2.05e-06 | Alpha: 0.313\n",
      "  Batch 740/800 | Loss: 1.7223 | LR: 1.53e-06 | Alpha: 0.313\n",
      "  Batch 760/800 | Loss: 1.7215 | LR: 1.02e-06 | Alpha: 0.313\n",
      "  Batch 780/800 | Loss: 1.7150 | LR: 5.11e-07 | Alpha: 0.313\n",
      "  Batch 800/800 | Loss: 1.7146 | LR: 0.00e+00 | Alpha: 0.313\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.7146\n",
      "  Val Loss: 1.6977\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.0825\n",
      "  Combined Score: 0.0612\n",
      "  Alpha: 0.313\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.0938\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.61 GB\n",
      "  Cached: 6.14 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 01:23:02,826] Trial 8 finished with value: 0.16842105263157894 and parameters: {'learning_rate': 3.332213575546236e-05, 'batch_size': 2, 'alpha': 0.3125716742746937, 'hidden_dropout_prob': 0.20910260281594512, 'classifier_dropout': 0.19430679432289802, 'weight_decay': 0.051348498425305575, 'warmup_ratio': 0.18613497108891397, 'num_epochs': 2, 'max_length': 256}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8: Combined Score = 0.1684 (Sentiment: 0.2211, Emotion: 0.1158)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 800\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.9237 | LR: 2.49e-06 | Alpha: 0.672\n",
      "  Batch 40/400 | Loss: 1.8148 | LR: 4.98e-06 | Alpha: 0.672\n",
      "  Batch 60/400 | Loss: 1.7651 | LR: 7.47e-06 | Alpha: 0.672\n",
      "  Batch 80/400 | Loss: 1.7332 | LR: 9.96e-06 | Alpha: 0.672\n",
      "  Batch 100/400 | Loss: 1.7455 | LR: 1.25e-05 | Alpha: 0.672\n",
      "  Batch 120/400 | Loss: 1.7221 | LR: 1.49e-05 | Alpha: 0.672\n",
      "  Batch 140/400 | Loss: 1.7057 | LR: 1.68e-05 | Alpha: 0.672\n",
      "  Batch 160/400 | Loss: 1.6521 | LR: 1.63e-05 | Alpha: 0.672\n",
      "  Batch 180/400 | Loss: 1.6408 | LR: 1.58e-05 | Alpha: 0.672\n",
      "  Batch 200/400 | Loss: 1.6269 | LR: 1.53e-05 | Alpha: 0.672\n",
      "  Batch 220/400 | Loss: 1.6379 | LR: 1.48e-05 | Alpha: 0.672\n",
      "  Batch 240/400 | Loss: 1.6000 | LR: 1.43e-05 | Alpha: 0.672\n",
      "  Batch 260/400 | Loss: 1.5905 | LR: 1.38e-05 | Alpha: 0.672\n",
      "  Batch 280/400 | Loss: 1.5901 | LR: 1.33e-05 | Alpha: 0.672\n",
      "  Batch 300/400 | Loss: 1.5908 | LR: 1.28e-05 | Alpha: 0.672\n",
      "  Batch 320/400 | Loss: 1.5751 | LR: 1.22e-05 | Alpha: 0.672\n",
      "  Batch 340/400 | Loss: 1.5679 | LR: 1.17e-05 | Alpha: 0.672\n",
      "  Batch 360/400 | Loss: 1.5577 | LR: 1.12e-05 | Alpha: 0.672\n",
      "  Batch 380/400 | Loss: 1.5397 | LR: 1.07e-05 | Alpha: 0.672\n",
      "  Batch 400/400 | Loss: 1.5278 | LR: 1.02e-05 | Alpha: 0.672\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.5278\n",
      "  Val Loss: 1.7227\n",
      "  Sentiment Accuracy: 0.2900\n",
      "  Emotion Accuracy: 0.0725\n",
      "  Combined Score: 0.1812\n",
      "  Alpha: 0.672\n",
      "  Learning Rate: 1.02e-05\n",
      "\n",
      "📍 Epoch 2/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.1920 | LR: 9.69e-06 | Alpha: 0.672\n",
      "  Batch 40/400 | Loss: 1.2289 | LR: 9.18e-06 | Alpha: 0.672\n",
      "  Batch 60/400 | Loss: 1.2365 | LR: 8.67e-06 | Alpha: 0.672\n",
      "  Batch 80/400 | Loss: 1.2395 | LR: 8.16e-06 | Alpha: 0.672\n",
      "  Batch 100/400 | Loss: 1.1946 | LR: 7.65e-06 | Alpha: 0.672\n",
      "  Batch 120/400 | Loss: 1.1711 | LR: 7.14e-06 | Alpha: 0.672\n",
      "  Batch 140/400 | Loss: 1.1743 | LR: 6.63e-06 | Alpha: 0.672\n",
      "  Batch 160/400 | Loss: 1.1615 | LR: 6.12e-06 | Alpha: 0.672\n",
      "  Batch 180/400 | Loss: 1.1514 | LR: 5.61e-06 | Alpha: 0.672\n",
      "  Batch 200/400 | Loss: 1.1579 | LR: 5.10e-06 | Alpha: 0.672\n",
      "  Batch 220/400 | Loss: 1.1487 | LR: 4.59e-06 | Alpha: 0.672\n",
      "  Batch 240/400 | Loss: 1.1535 | LR: 4.08e-06 | Alpha: 0.672\n",
      "  Batch 260/400 | Loss: 1.1421 | LR: 3.57e-06 | Alpha: 0.672\n",
      "  Batch 280/400 | Loss: 1.1339 | LR: 3.06e-06 | Alpha: 0.672\n",
      "  Batch 300/400 | Loss: 1.1350 | LR: 2.55e-06 | Alpha: 0.672\n",
      "  Batch 320/400 | Loss: 1.1293 | LR: 2.04e-06 | Alpha: 0.672\n",
      "  Batch 340/400 | Loss: 1.1159 | LR: 1.53e-06 | Alpha: 0.672\n",
      "  Batch 360/400 | Loss: 1.1255 | LR: 1.02e-06 | Alpha: 0.672\n",
      "  Batch 380/400 | Loss: 1.1215 | LR: 5.10e-07 | Alpha: 0.672\n",
      "  Batch 400/400 | Loss: 1.1245 | LR: 0.00e+00 | Alpha: 0.672\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.1245\n",
      "  Val Loss: 1.5430\n",
      "  Sentiment Accuracy: 0.4250\n",
      "  Emotion Accuracy: 0.0875\n",
      "  Combined Score: 0.2562\n",
      "  Alpha: 0.672\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.2562\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.60 GB\n",
      "  Cached: 6.19 GB\n",
      "Trial 9: Combined Score = 0.3211 (Sentiment: 0.5263, Emotion: 0.1158)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 01:26:49,271] Trial 9 finished with value: 0.32105263157894737 and parameters: {'learning_rate': 1.693550554929792e-05, 'batch_size': 4, 'alpha': 0.6718790609370292, 'hidden_dropout_prob': 0.25203009489110423, 'classifier_dropout': 0.2900211269531271, 'weight_decay': 0.08727459842858405, 'warmup_ratio': 0.17055081153486717, 'num_epochs': 2, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 200\n",
      "  Total training steps: 1000\n",
      "🚀 Starting training for 5 epochs...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 2.8077 | LR: 1.15e-05 | Alpha: 0.528\n",
      "  Batch 40/200 | Loss: 2.3659 | LR: 2.30e-05 | Alpha: 0.528\n",
      "  Batch 60/200 | Loss: 2.1232 | LR: 3.45e-05 | Alpha: 0.528\n",
      "  Batch 80/200 | Loss: 2.0062 | LR: 4.61e-05 | Alpha: 0.528\n",
      "  Batch 100/200 | Loss: 1.9342 | LR: 5.76e-05 | Alpha: 0.528\n",
      "  Batch 120/200 | Loss: 1.8811 | LR: 5.88e-05 | Alpha: 0.528\n",
      "  Batch 140/200 | Loss: 1.8708 | LR: 5.75e-05 | Alpha: 0.528\n",
      "  Batch 160/200 | Loss: 1.8535 | LR: 5.61e-05 | Alpha: 0.528\n",
      "  Batch 180/200 | Loss: 1.8445 | LR: 5.48e-05 | Alpha: 0.528\n",
      "  Batch 200/200 | Loss: 1.8310 | LR: 5.35e-05 | Alpha: 0.528\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.8310\n",
      "  Val Loss: 2.9437\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.1525\n",
      "  Combined Score: 0.0963\n",
      "  Alpha: 0.528\n",
      "  Learning Rate: 5.35e-05\n",
      "\n",
      "📍 Epoch 2/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 1.6456 | LR: 5.21e-05 | Alpha: 0.528\n",
      "  Batch 40/200 | Loss: 1.6097 | LR: 5.08e-05 | Alpha: 0.528\n",
      "  Batch 60/200 | Loss: 1.6428 | LR: 4.94e-05 | Alpha: 0.528\n",
      "  Batch 80/200 | Loss: 1.6279 | LR: 4.81e-05 | Alpha: 0.528\n",
      "  Batch 100/200 | Loss: 1.6075 | LR: 4.68e-05 | Alpha: 0.528\n",
      "  Batch 120/200 | Loss: 1.5787 | LR: 4.54e-05 | Alpha: 0.528\n",
      "  Batch 140/200 | Loss: 1.5685 | LR: 4.41e-05 | Alpha: 0.528\n",
      "  Batch 160/200 | Loss: 1.5580 | LR: 4.28e-05 | Alpha: 0.528\n",
      "  Batch 180/200 | Loss: 1.5375 | LR: 4.14e-05 | Alpha: 0.528\n",
      "  Batch 200/200 | Loss: 1.5303 | LR: 4.01e-05 | Alpha: 0.528\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.5303\n",
      "  Val Loss: 1.8031\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.2250\n",
      "  Combined Score: 0.1325\n",
      "  Alpha: 0.528\n",
      "  Learning Rate: 4.01e-05\n",
      "\n",
      "📍 Epoch 3/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 1.5254 | LR: 3.88e-05 | Alpha: 0.528\n",
      "  Batch 40/200 | Loss: 1.4263 | LR: 3.74e-05 | Alpha: 0.528\n",
      "  Batch 60/200 | Loss: 1.4198 | LR: 3.61e-05 | Alpha: 0.528\n",
      "  Batch 80/200 | Loss: 1.4164 | LR: 3.47e-05 | Alpha: 0.528\n",
      "  Batch 100/200 | Loss: 1.4117 | LR: 3.34e-05 | Alpha: 0.528\n",
      "  Batch 120/200 | Loss: 1.4134 | LR: 3.21e-05 | Alpha: 0.528\n",
      "  Batch 140/200 | Loss: 1.4119 | LR: 3.07e-05 | Alpha: 0.528\n",
      "  Batch 160/200 | Loss: 1.3921 | LR: 2.94e-05 | Alpha: 0.528\n",
      "  Batch 180/200 | Loss: 1.3826 | LR: 2.81e-05 | Alpha: 0.528\n",
      "  Batch 200/200 | Loss: 1.3726 | LR: 2.67e-05 | Alpha: 0.528\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 1.3726\n",
      "  Val Loss: 1.7686\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.0850\n",
      "  Combined Score: 0.0625\n",
      "  Alpha: 0.528\n",
      "  Learning Rate: 2.67e-05\n",
      "\n",
      "📍 Epoch 4/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 1.4049 | LR: 2.54e-05 | Alpha: 0.528\n",
      "  Batch 40/200 | Loss: 1.3642 | LR: 2.41e-05 | Alpha: 0.528\n",
      "  Batch 60/200 | Loss: 1.3343 | LR: 2.27e-05 | Alpha: 0.528\n",
      "  Batch 80/200 | Loss: 1.3552 | LR: 2.14e-05 | Alpha: 0.528\n",
      "  Batch 100/200 | Loss: 1.3253 | LR: 2.00e-05 | Alpha: 0.528\n",
      "  Batch 120/200 | Loss: 1.3145 | LR: 1.87e-05 | Alpha: 0.528\n",
      "  Batch 140/200 | Loss: 1.3098 | LR: 1.74e-05 | Alpha: 0.528\n",
      "  Batch 160/200 | Loss: 1.3085 | LR: 1.60e-05 | Alpha: 0.528\n",
      "  Batch 180/200 | Loss: 1.3039 | LR: 1.47e-05 | Alpha: 0.528\n",
      "  Batch 200/200 | Loss: 1.2914 | LR: 1.34e-05 | Alpha: 0.528\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 1.2914\n",
      "  Val Loss: 1.7695\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.1475\n",
      "  Combined Score: 0.0938\n",
      "  Alpha: 0.528\n",
      "  Learning Rate: 1.34e-05\n",
      "\n",
      "📍 Epoch 5/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 1.3029 | LR: 1.20e-05 | Alpha: 0.528\n",
      "  Batch 40/200 | Loss: 1.2808 | LR: 1.07e-05 | Alpha: 0.528\n",
      "  Batch 60/200 | Loss: 1.2781 | LR: 9.36e-06 | Alpha: 0.528\n",
      "  Batch 80/200 | Loss: 1.2630 | LR: 8.02e-06 | Alpha: 0.528\n",
      "  Batch 100/200 | Loss: 1.2583 | LR: 6.68e-06 | Alpha: 0.528\n",
      "  Batch 120/200 | Loss: 1.2329 | LR: 5.35e-06 | Alpha: 0.528\n",
      "  Batch 140/200 | Loss: 1.2283 | LR: 4.01e-06 | Alpha: 0.528\n",
      "  Batch 160/200 | Loss: 1.2224 | LR: 2.67e-06 | Alpha: 0.528\n",
      "  Batch 180/200 | Loss: 1.2242 | LR: 1.34e-06 | Alpha: 0.528\n",
      "  Batch 200/200 | Loss: 1.2288 | LR: 0.00e+00 | Alpha: 0.528\n",
      "📊 Epoch 5 Results:\n",
      "  Train Loss: 1.2288\n",
      "  Val Loss: 1.7273\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.0850\n",
      "  Combined Score: 0.0625\n",
      "  Alpha: 0.528\n",
      "  Learning Rate: 0.00e+00\n",
      "⏹️ Early stopping triggered at epoch 5\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.1325\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.61 GB\n",
      "  Cached: 6.29 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 01:47:06,341] Trial 10 finished with value: 0.17368421052631577 and parameters: {'learning_rate': 5.987384184453176e-05, 'batch_size': 8, 'alpha': 0.527827263346871, 'hidden_dropout_prob': 0.11240633271029743, 'classifier_dropout': 0.38923847118213384, 'weight_decay': 0.004833794796157115, 'warmup_ratio': 0.1041967994662714, 'num_epochs': 5, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10: Combined Score = 0.1737 (Sentiment: 0.2211, Emotion: 0.1263)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 200\n",
      "  Total training steps: 1000\n",
      "🚀 Starting training for 5 epochs...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 2.2936 | LR: 1.66e-05 | Alpha: 0.476\n",
      "  Batch 40/200 | Loss: 1.9937 | LR: 3.32e-05 | Alpha: 0.476\n",
      "  Batch 60/200 | Loss: 1.8716 | LR: 4.37e-05 | Alpha: 0.476\n",
      "  Batch 80/200 | Loss: 1.8119 | LR: 4.28e-05 | Alpha: 0.476\n",
      "  Batch 100/200 | Loss: 1.8144 | LR: 4.18e-05 | Alpha: 0.476\n",
      "  Batch 120/200 | Loss: 1.7658 | LR: 4.09e-05 | Alpha: 0.476\n",
      "  Batch 140/200 | Loss: 1.7280 | LR: 4.00e-05 | Alpha: 0.476\n",
      "  Batch 160/200 | Loss: 1.6971 | LR: 3.91e-05 | Alpha: 0.476\n",
      "  Batch 180/200 | Loss: 1.6687 | LR: 3.81e-05 | Alpha: 0.476\n",
      "  Batch 200/200 | Loss: 1.6613 | LR: 3.72e-05 | Alpha: 0.476\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.6613\n",
      "  Val Loss: 1.9863\n",
      "  Sentiment Accuracy: 0.1025\n",
      "  Emotion Accuracy: 0.1275\n",
      "  Combined Score: 0.1150\n",
      "  Alpha: 0.476\n",
      "  Learning Rate: 3.72e-05\n",
      "\n",
      "📍 Epoch 2/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 1.3047 | LR: 3.63e-05 | Alpha: 0.476\n",
      "  Batch 40/200 | Loss: 1.2714 | LR: 3.53e-05 | Alpha: 0.476\n",
      "  Batch 60/200 | Loss: 1.3068 | LR: 3.44e-05 | Alpha: 0.476\n",
      "  Batch 80/200 | Loss: 1.2759 | LR: 3.35e-05 | Alpha: 0.476\n",
      "  Batch 100/200 | Loss: 1.2617 | LR: 3.25e-05 | Alpha: 0.476\n",
      "  Batch 120/200 | Loss: 1.2509 | LR: 3.16e-05 | Alpha: 0.476\n",
      "  Batch 140/200 | Loss: 1.2264 | LR: 3.07e-05 | Alpha: 0.476\n",
      "  Batch 160/200 | Loss: 1.2129 | LR: 2.98e-05 | Alpha: 0.476\n",
      "  Batch 180/200 | Loss: 1.1901 | LR: 2.88e-05 | Alpha: 0.476\n",
      "  Batch 200/200 | Loss: 1.1715 | LR: 2.79e-05 | Alpha: 0.476\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.1715\n",
      "  Val Loss: 1.4658\n",
      "  Sentiment Accuracy: 0.5850\n",
      "  Emotion Accuracy: 0.1525\n",
      "  Combined Score: 0.3687\n",
      "  Alpha: 0.476\n",
      "  Learning Rate: 2.79e-05\n",
      "\n",
      "📍 Epoch 3/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 0.8742 | LR: 2.70e-05 | Alpha: 0.476\n",
      "  Batch 40/200 | Loss: 0.9525 | LR: 2.60e-05 | Alpha: 0.476\n",
      "  Batch 60/200 | Loss: 0.9245 | LR: 2.51e-05 | Alpha: 0.476\n",
      "  Batch 80/200 | Loss: 0.9049 | LR: 2.42e-05 | Alpha: 0.476\n",
      "  Batch 100/200 | Loss: 0.9062 | LR: 2.32e-05 | Alpha: 0.476\n",
      "  Batch 120/200 | Loss: 0.9050 | LR: 2.23e-05 | Alpha: 0.476\n",
      "  Batch 140/200 | Loss: 0.8726 | LR: 2.14e-05 | Alpha: 0.476\n",
      "  Batch 160/200 | Loss: 0.8563 | LR: 2.05e-05 | Alpha: 0.476\n",
      "  Batch 180/200 | Loss: 0.8456 | LR: 1.95e-05 | Alpha: 0.476\n",
      "  Batch 200/200 | Loss: 0.8410 | LR: 1.86e-05 | Alpha: 0.476\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.8410\n",
      "  Val Loss: 1.5258\n",
      "  Sentiment Accuracy: 0.7650\n",
      "  Emotion Accuracy: 0.1375\n",
      "  Combined Score: 0.4513\n",
      "  Alpha: 0.476\n",
      "  Learning Rate: 1.86e-05\n",
      "\n",
      "📍 Epoch 4/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 0.6449 | LR: 1.77e-05 | Alpha: 0.476\n",
      "  Batch 40/200 | Loss: 0.6312 | LR: 1.67e-05 | Alpha: 0.476\n",
      "  Batch 60/200 | Loss: 0.6574 | LR: 1.58e-05 | Alpha: 0.476\n",
      "  Batch 80/200 | Loss: 0.6765 | LR: 1.49e-05 | Alpha: 0.476\n",
      "  Batch 100/200 | Loss: 0.6733 | LR: 1.39e-05 | Alpha: 0.476\n",
      "  Batch 120/200 | Loss: 0.6700 | LR: 1.30e-05 | Alpha: 0.476\n",
      "  Batch 140/200 | Loss: 0.6647 | LR: 1.21e-05 | Alpha: 0.476\n",
      "  Batch 160/200 | Loss: 0.6614 | LR: 1.12e-05 | Alpha: 0.476\n",
      "  Batch 180/200 | Loss: 0.6539 | LR: 1.02e-05 | Alpha: 0.476\n",
      "  Batch 200/200 | Loss: 0.6518 | LR: 9.30e-06 | Alpha: 0.476\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.6518\n",
      "  Val Loss: 1.6305\n",
      "  Sentiment Accuracy: 0.7725\n",
      "  Emotion Accuracy: 0.1525\n",
      "  Combined Score: 0.4625\n",
      "  Alpha: 0.476\n",
      "  Learning Rate: 9.30e-06\n",
      "\n",
      "📍 Epoch 5/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 0.5135 | LR: 8.37e-06 | Alpha: 0.476\n",
      "  Batch 40/200 | Loss: 0.5648 | LR: 7.44e-06 | Alpha: 0.476\n",
      "  Batch 60/200 | Loss: 0.5592 | LR: 6.51e-06 | Alpha: 0.476\n",
      "  Batch 80/200 | Loss: 0.5303 | LR: 5.58e-06 | Alpha: 0.476\n",
      "  Batch 100/200 | Loss: 0.5221 | LR: 4.65e-06 | Alpha: 0.476\n",
      "  Batch 120/200 | Loss: 0.5163 | LR: 3.72e-06 | Alpha: 0.476\n",
      "  Batch 140/200 | Loss: 0.5176 | LR: 2.79e-06 | Alpha: 0.476\n",
      "  Batch 160/200 | Loss: 0.5242 | LR: 1.86e-06 | Alpha: 0.476\n",
      "  Batch 180/200 | Loss: 0.5320 | LR: 9.30e-07 | Alpha: 0.476\n",
      "  Batch 200/200 | Loss: 0.5231 | LR: 0.00e+00 | Alpha: 0.476\n",
      "📊 Epoch 5 Results:\n",
      "  Train Loss: 0.5231\n",
      "  Val Loss: 1.7092\n",
      "  Sentiment Accuracy: 0.7300\n",
      "  Emotion Accuracy: 0.1550\n",
      "  Combined Score: 0.4425\n",
      "  Alpha: 0.476\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4625\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.61 GB\n",
      "  Cached: 6.29 GB\n",
      "Trial 11: Combined Score = 0.3474 (Sentiment: 0.5895, Emotion: 0.1053)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 02:14:35,402] Trial 11 finished with value: 0.34736842105263155 and parameters: {'learning_rate': 4.402635555643861e-05, 'batch_size': 8, 'alpha': 0.47597968181131295, 'hidden_dropout_prob': 0.12926204858999552, 'classifier_dropout': 0.3224490909495544, 'weight_decay': 0.048707736636669656, 'warmup_ratio': 0.053010363186629, 'num_epochs': 5, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 4000\n",
      "🚀 Starting training for 5 epochs...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 2.1519 | LR: 7.75e-07 | Alpha: 0.541\n",
      "  Batch 40/800 | Loss: 1.9872 | LR: 1.55e-06 | Alpha: 0.541\n",
      "  Batch 60/800 | Loss: 1.9896 | LR: 2.33e-06 | Alpha: 0.541\n",
      "  Batch 80/800 | Loss: 1.8751 | LR: 3.10e-06 | Alpha: 0.541\n",
      "  Batch 100/800 | Loss: 1.8404 | LR: 3.88e-06 | Alpha: 0.541\n",
      "  Batch 120/800 | Loss: 1.7883 | LR: 4.65e-06 | Alpha: 0.541\n",
      "  Batch 140/800 | Loss: 1.7871 | LR: 5.43e-06 | Alpha: 0.541\n",
      "  Batch 160/800 | Loss: 1.7767 | LR: 6.20e-06 | Alpha: 0.541\n",
      "  Batch 180/800 | Loss: 1.7853 | LR: 6.98e-06 | Alpha: 0.541\n",
      "  Batch 200/800 | Loss: 1.7541 | LR: 7.75e-06 | Alpha: 0.541\n",
      "  Batch 220/800 | Loss: 1.7388 | LR: 8.53e-06 | Alpha: 0.541\n",
      "  Batch 240/800 | Loss: 1.7423 | LR: 9.30e-06 | Alpha: 0.541\n",
      "  Batch 260/800 | Loss: 1.7298 | LR: 1.01e-05 | Alpha: 0.541\n",
      "  Batch 280/800 | Loss: 1.7360 | LR: 1.09e-05 | Alpha: 0.541\n",
      "  Batch 300/800 | Loss: 1.7228 | LR: 1.16e-05 | Alpha: 0.541\n",
      "  Batch 320/800 | Loss: 1.7130 | LR: 1.24e-05 | Alpha: 0.541\n",
      "  Batch 340/800 | Loss: 1.7022 | LR: 1.32e-05 | Alpha: 0.541\n",
      "  Batch 360/800 | Loss: 1.7046 | LR: 1.35e-05 | Alpha: 0.541\n",
      "  Batch 380/800 | Loss: 1.7007 | LR: 1.35e-05 | Alpha: 0.541\n",
      "  Batch 400/800 | Loss: 1.6849 | LR: 1.34e-05 | Alpha: 0.541\n",
      "  Batch 420/800 | Loss: 1.6769 | LR: 1.33e-05 | Alpha: 0.541\n",
      "  Batch 440/800 | Loss: 1.6722 | LR: 1.32e-05 | Alpha: 0.541\n",
      "  Batch 460/800 | Loss: 1.6646 | LR: 1.32e-05 | Alpha: 0.541\n",
      "  Batch 480/800 | Loss: 1.6600 | LR: 1.31e-05 | Alpha: 0.541\n",
      "  Batch 500/800 | Loss: 1.6487 | LR: 1.30e-05 | Alpha: 0.541\n",
      "  Batch 520/800 | Loss: 1.6470 | LR: 1.29e-05 | Alpha: 0.541\n",
      "  Batch 540/800 | Loss: 1.6367 | LR: 1.29e-05 | Alpha: 0.541\n",
      "  Batch 560/800 | Loss: 1.6310 | LR: 1.28e-05 | Alpha: 0.541\n",
      "  Batch 580/800 | Loss: 1.6245 | LR: 1.27e-05 | Alpha: 0.541\n",
      "  Batch 600/800 | Loss: 1.6188 | LR: 1.26e-05 | Alpha: 0.541\n",
      "  Batch 620/800 | Loss: 1.6108 | LR: 1.26e-05 | Alpha: 0.541\n",
      "  Batch 640/800 | Loss: 1.6061 | LR: 1.25e-05 | Alpha: 0.541\n",
      "  Batch 660/800 | Loss: 1.6020 | LR: 1.24e-05 | Alpha: 0.541\n",
      "  Batch 680/800 | Loss: 1.5949 | LR: 1.23e-05 | Alpha: 0.541\n",
      "  Batch 700/800 | Loss: 1.5915 | LR: 1.23e-05 | Alpha: 0.541\n",
      "  Batch 720/800 | Loss: 1.5868 | LR: 1.22e-05 | Alpha: 0.541\n",
      "  Batch 740/800 | Loss: 1.5824 | LR: 1.21e-05 | Alpha: 0.541\n",
      "  Batch 760/800 | Loss: 1.5745 | LR: 1.20e-05 | Alpha: 0.541\n",
      "  Batch 780/800 | Loss: 1.5657 | LR: 1.20e-05 | Alpha: 0.541\n",
      "  Batch 800/800 | Loss: 1.5642 | LR: 1.19e-05 | Alpha: 0.541\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.5642\n",
      "  Val Loss: 1.4459\n",
      "  Sentiment Accuracy: 0.6950\n",
      "  Emotion Accuracy: 0.1050\n",
      "  Combined Score: 0.4000\n",
      "  Alpha: 0.541\n",
      "  Learning Rate: 1.19e-05\n",
      "\n",
      "📍 Epoch 2/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.5236 | LR: 1.18e-05 | Alpha: 0.541\n",
      "  Batch 40/800 | Loss: 1.4452 | LR: 1.17e-05 | Alpha: 0.541\n",
      "  Batch 60/800 | Loss: 1.3531 | LR: 1.17e-05 | Alpha: 0.541\n",
      "  Batch 80/800 | Loss: 1.3472 | LR: 1.16e-05 | Alpha: 0.541\n",
      "  Batch 100/800 | Loss: 1.3214 | LR: 1.15e-05 | Alpha: 0.541\n",
      "  Batch 120/800 | Loss: 1.3201 | LR: 1.14e-05 | Alpha: 0.541\n",
      "  Batch 140/800 | Loss: 1.2882 | LR: 1.14e-05 | Alpha: 0.541\n",
      "  Batch 160/800 | Loss: 1.2566 | LR: 1.13e-05 | Alpha: 0.541\n",
      "  Batch 180/800 | Loss: 1.2848 | LR: 1.12e-05 | Alpha: 0.541\n",
      "  Batch 200/800 | Loss: 1.2617 | LR: 1.12e-05 | Alpha: 0.541\n",
      "  Batch 220/800 | Loss: 1.2564 | LR: 1.11e-05 | Alpha: 0.541\n",
      "  Batch 240/800 | Loss: 1.2473 | LR: 1.10e-05 | Alpha: 0.541\n",
      "  Batch 260/800 | Loss: 1.2534 | LR: 1.09e-05 | Alpha: 0.541\n",
      "  Batch 280/800 | Loss: 1.2467 | LR: 1.09e-05 | Alpha: 0.541\n",
      "  Batch 300/800 | Loss: 1.2397 | LR: 1.08e-05 | Alpha: 0.541\n",
      "  Batch 320/800 | Loss: 1.2430 | LR: 1.07e-05 | Alpha: 0.541\n",
      "  Batch 340/800 | Loss: 1.2241 | LR: 1.06e-05 | Alpha: 0.541\n",
      "  Batch 360/800 | Loss: 1.2153 | LR: 1.06e-05 | Alpha: 0.541\n",
      "  Batch 380/800 | Loss: 1.2060 | LR: 1.05e-05 | Alpha: 0.541\n",
      "  Batch 400/800 | Loss: 1.2021 | LR: 1.04e-05 | Alpha: 0.541\n",
      "  Batch 420/800 | Loss: 1.1944 | LR: 1.03e-05 | Alpha: 0.541\n",
      "  Batch 440/800 | Loss: 1.1889 | LR: 1.03e-05 | Alpha: 0.541\n",
      "  Batch 460/800 | Loss: 1.1772 | LR: 1.02e-05 | Alpha: 0.541\n",
      "  Batch 480/800 | Loss: 1.1674 | LR: 1.01e-05 | Alpha: 0.541\n",
      "  Batch 500/800 | Loss: 1.1652 | LR: 1.00e-05 | Alpha: 0.541\n",
      "  Batch 520/800 | Loss: 1.1686 | LR: 9.96e-06 | Alpha: 0.541\n",
      "  Batch 540/800 | Loss: 1.1670 | LR: 9.89e-06 | Alpha: 0.541\n",
      "  Batch 560/800 | Loss: 1.1678 | LR: 9.81e-06 | Alpha: 0.541\n",
      "  Batch 580/800 | Loss: 1.1625 | LR: 9.74e-06 | Alpha: 0.541\n",
      "  Batch 600/800 | Loss: 1.1563 | LR: 9.66e-06 | Alpha: 0.541\n",
      "  Batch 620/800 | Loss: 1.1564 | LR: 9.59e-06 | Alpha: 0.541\n",
      "  Batch 640/800 | Loss: 1.1505 | LR: 9.52e-06 | Alpha: 0.541\n",
      "  Batch 660/800 | Loss: 1.1438 | LR: 9.44e-06 | Alpha: 0.541\n",
      "  Batch 680/800 | Loss: 1.1468 | LR: 9.37e-06 | Alpha: 0.541\n",
      "  Batch 700/800 | Loss: 1.1432 | LR: 9.29e-06 | Alpha: 0.541\n",
      "  Batch 720/800 | Loss: 1.1418 | LR: 9.22e-06 | Alpha: 0.541\n",
      "  Batch 740/800 | Loss: 1.1368 | LR: 9.14e-06 | Alpha: 0.541\n",
      "  Batch 760/800 | Loss: 1.1341 | LR: 9.07e-06 | Alpha: 0.541\n",
      "  Batch 780/800 | Loss: 1.1341 | LR: 9.00e-06 | Alpha: 0.541\n",
      "  Batch 800/800 | Loss: 1.1401 | LR: 8.92e-06 | Alpha: 0.541\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.1401\n",
      "  Val Loss: 1.3975\n",
      "  Sentiment Accuracy: 0.7225\n",
      "  Emotion Accuracy: 0.1150\n",
      "  Combined Score: 0.4188\n",
      "  Alpha: 0.541\n",
      "  Learning Rate: 8.92e-06\n",
      "\n",
      "📍 Epoch 3/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.9754 | LR: 8.85e-06 | Alpha: 0.541\n",
      "  Batch 40/800 | Loss: 1.2446 | LR: 8.77e-06 | Alpha: 0.541\n",
      "  Batch 60/800 | Loss: 1.2093 | LR: 8.70e-06 | Alpha: 0.541\n",
      "  Batch 80/800 | Loss: 1.1243 | LR: 8.62e-06 | Alpha: 0.541\n",
      "  Batch 100/800 | Loss: 1.1039 | LR: 8.55e-06 | Alpha: 0.541\n",
      "  Batch 120/800 | Loss: 1.0741 | LR: 8.48e-06 | Alpha: 0.541\n",
      "  Batch 140/800 | Loss: 1.0528 | LR: 8.40e-06 | Alpha: 0.541\n",
      "  Batch 160/800 | Loss: 1.0096 | LR: 8.33e-06 | Alpha: 0.541\n",
      "  Batch 180/800 | Loss: 1.0156 | LR: 8.25e-06 | Alpha: 0.541\n",
      "  Batch 200/800 | Loss: 1.0289 | LR: 8.18e-06 | Alpha: 0.541\n",
      "  Batch 220/800 | Loss: 1.0159 | LR: 8.10e-06 | Alpha: 0.541\n",
      "  Batch 240/800 | Loss: 1.0185 | LR: 8.03e-06 | Alpha: 0.541\n",
      "  Batch 260/800 | Loss: 1.0211 | LR: 7.95e-06 | Alpha: 0.541\n",
      "  Batch 280/800 | Loss: 1.0038 | LR: 7.88e-06 | Alpha: 0.541\n",
      "  Batch 300/800 | Loss: 1.0142 | LR: 7.81e-06 | Alpha: 0.541\n",
      "  Batch 320/800 | Loss: 1.0030 | LR: 7.73e-06 | Alpha: 0.541\n",
      "  Batch 340/800 | Loss: 0.9994 | LR: 7.66e-06 | Alpha: 0.541\n",
      "  Batch 360/800 | Loss: 1.0004 | LR: 7.58e-06 | Alpha: 0.541\n",
      "  Batch 380/800 | Loss: 0.9919 | LR: 7.51e-06 | Alpha: 0.541\n",
      "  Batch 400/800 | Loss: 0.9891 | LR: 7.43e-06 | Alpha: 0.541\n",
      "  Batch 420/800 | Loss: 0.9821 | LR: 7.36e-06 | Alpha: 0.541\n",
      "  Batch 440/800 | Loss: 0.9780 | LR: 7.29e-06 | Alpha: 0.541\n",
      "  Batch 460/800 | Loss: 0.9777 | LR: 7.21e-06 | Alpha: 0.541\n",
      "  Batch 480/800 | Loss: 0.9814 | LR: 7.14e-06 | Alpha: 0.541\n",
      "  Batch 500/800 | Loss: 0.9753 | LR: 7.06e-06 | Alpha: 0.541\n",
      "  Batch 520/800 | Loss: 0.9738 | LR: 6.99e-06 | Alpha: 0.541\n",
      "  Batch 540/800 | Loss: 0.9798 | LR: 6.91e-06 | Alpha: 0.541\n",
      "  Batch 560/800 | Loss: 0.9757 | LR: 6.84e-06 | Alpha: 0.541\n",
      "  Batch 580/800 | Loss: 0.9732 | LR: 6.77e-06 | Alpha: 0.541\n",
      "  Batch 600/800 | Loss: 0.9715 | LR: 6.69e-06 | Alpha: 0.541\n",
      "  Batch 620/800 | Loss: 0.9618 | LR: 6.62e-06 | Alpha: 0.541\n",
      "  Batch 640/800 | Loss: 0.9659 | LR: 6.54e-06 | Alpha: 0.541\n",
      "  Batch 660/800 | Loss: 0.9661 | LR: 6.47e-06 | Alpha: 0.541\n",
      "  Batch 680/800 | Loss: 0.9656 | LR: 6.39e-06 | Alpha: 0.541\n",
      "  Batch 700/800 | Loss: 0.9653 | LR: 6.32e-06 | Alpha: 0.541\n",
      "  Batch 720/800 | Loss: 0.9678 | LR: 6.24e-06 | Alpha: 0.541\n",
      "  Batch 740/800 | Loss: 0.9692 | LR: 6.17e-06 | Alpha: 0.541\n",
      "  Batch 760/800 | Loss: 0.9672 | LR: 6.10e-06 | Alpha: 0.541\n",
      "  Batch 780/800 | Loss: 0.9740 | LR: 6.02e-06 | Alpha: 0.541\n",
      "  Batch 800/800 | Loss: 0.9771 | LR: 5.95e-06 | Alpha: 0.541\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.9771\n",
      "  Val Loss: 1.4041\n",
      "  Sentiment Accuracy: 0.7350\n",
      "  Emotion Accuracy: 0.1450\n",
      "  Combined Score: 0.4400\n",
      "  Alpha: 0.541\n",
      "  Learning Rate: 5.95e-06\n",
      "\n",
      "📍 Epoch 4/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.8464 | LR: 5.87e-06 | Alpha: 0.541\n",
      "  Batch 40/800 | Loss: 0.8579 | LR: 5.80e-06 | Alpha: 0.541\n",
      "  Batch 60/800 | Loss: 0.8673 | LR: 5.72e-06 | Alpha: 0.541\n",
      "  Batch 80/800 | Loss: 0.8690 | LR: 5.65e-06 | Alpha: 0.541\n",
      "  Batch 100/800 | Loss: 0.8942 | LR: 5.58e-06 | Alpha: 0.541\n",
      "  Batch 120/800 | Loss: 0.8817 | LR: 5.50e-06 | Alpha: 0.541\n",
      "  Batch 140/800 | Loss: 0.8775 | LR: 5.43e-06 | Alpha: 0.541\n",
      "  Batch 160/800 | Loss: 0.8623 | LR: 5.35e-06 | Alpha: 0.541\n",
      "  Batch 180/800 | Loss: 0.8644 | LR: 5.28e-06 | Alpha: 0.541\n",
      "  Batch 200/800 | Loss: 0.8713 | LR: 5.20e-06 | Alpha: 0.541\n",
      "  Batch 220/800 | Loss: 0.8760 | LR: 5.13e-06 | Alpha: 0.541\n",
      "  Batch 240/800 | Loss: 0.8611 | LR: 5.06e-06 | Alpha: 0.541\n",
      "  Batch 260/800 | Loss: 0.8584 | LR: 4.98e-06 | Alpha: 0.541\n",
      "  Batch 280/800 | Loss: 0.8516 | LR: 4.91e-06 | Alpha: 0.541\n",
      "  Batch 300/800 | Loss: 0.8536 | LR: 4.83e-06 | Alpha: 0.541\n",
      "  Batch 320/800 | Loss: 0.8618 | LR: 4.76e-06 | Alpha: 0.541\n",
      "  Batch 340/800 | Loss: 0.8686 | LR: 4.68e-06 | Alpha: 0.541\n",
      "  Batch 360/800 | Loss: 0.8722 | LR: 4.61e-06 | Alpha: 0.541\n",
      "  Batch 380/800 | Loss: 0.8708 | LR: 4.54e-06 | Alpha: 0.541\n",
      "  Batch 400/800 | Loss: 0.8786 | LR: 4.46e-06 | Alpha: 0.541\n",
      "  Batch 420/800 | Loss: 0.8700 | LR: 4.39e-06 | Alpha: 0.541\n",
      "  Batch 440/800 | Loss: 0.8713 | LR: 4.31e-06 | Alpha: 0.541\n",
      "  Batch 460/800 | Loss: 0.8630 | LR: 4.24e-06 | Alpha: 0.541\n",
      "  Batch 480/800 | Loss: 0.8586 | LR: 4.16e-06 | Alpha: 0.541\n",
      "  Batch 500/800 | Loss: 0.8492 | LR: 4.09e-06 | Alpha: 0.541\n",
      "  Batch 520/800 | Loss: 0.8552 | LR: 4.01e-06 | Alpha: 0.541\n",
      "  Batch 540/800 | Loss: 0.8521 | LR: 3.94e-06 | Alpha: 0.541\n",
      "  Batch 560/800 | Loss: 0.8527 | LR: 3.87e-06 | Alpha: 0.541\n",
      "  Batch 580/800 | Loss: 0.8478 | LR: 3.79e-06 | Alpha: 0.541\n",
      "  Batch 600/800 | Loss: 0.8428 | LR: 3.72e-06 | Alpha: 0.541\n",
      "  Batch 620/800 | Loss: 0.8366 | LR: 3.64e-06 | Alpha: 0.541\n",
      "  Batch 640/800 | Loss: 0.8286 | LR: 3.57e-06 | Alpha: 0.541\n",
      "  Batch 660/800 | Loss: 0.8266 | LR: 3.49e-06 | Alpha: 0.541\n",
      "  Batch 680/800 | Loss: 0.8187 | LR: 3.42e-06 | Alpha: 0.541\n",
      "  Batch 700/800 | Loss: 0.8169 | LR: 3.35e-06 | Alpha: 0.541\n",
      "  Batch 720/800 | Loss: 0.8231 | LR: 3.27e-06 | Alpha: 0.541\n",
      "  Batch 740/800 | Loss: 0.8228 | LR: 3.20e-06 | Alpha: 0.541\n",
      "  Batch 760/800 | Loss: 0.8187 | LR: 3.12e-06 | Alpha: 0.541\n",
      "  Batch 780/800 | Loss: 0.8166 | LR: 3.05e-06 | Alpha: 0.541\n",
      "  Batch 800/800 | Loss: 0.8179 | LR: 2.97e-06 | Alpha: 0.541\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.8179\n",
      "  Val Loss: 1.4087\n",
      "  Sentiment Accuracy: 0.7850\n",
      "  Emotion Accuracy: 0.1325\n",
      "  Combined Score: 0.4587\n",
      "  Alpha: 0.541\n",
      "  Learning Rate: 2.97e-06\n",
      "\n",
      "📍 Epoch 5/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 0.8444 | LR: 2.90e-06 | Alpha: 0.541\n",
      "  Batch 40/800 | Loss: 0.8107 | LR: 2.83e-06 | Alpha: 0.541\n",
      "  Batch 60/800 | Loss: 0.8174 | LR: 2.75e-06 | Alpha: 0.541\n",
      "  Batch 80/800 | Loss: 0.8845 | LR: 2.68e-06 | Alpha: 0.541\n",
      "  Batch 100/800 | Loss: 0.8600 | LR: 2.60e-06 | Alpha: 0.541\n",
      "  Batch 120/800 | Loss: 0.8459 | LR: 2.53e-06 | Alpha: 0.541\n",
      "  Batch 140/800 | Loss: 0.8403 | LR: 2.45e-06 | Alpha: 0.541\n",
      "  Batch 160/800 | Loss: 0.8520 | LR: 2.38e-06 | Alpha: 0.541\n",
      "  Batch 180/800 | Loss: 0.8269 | LR: 2.30e-06 | Alpha: 0.541\n",
      "  Batch 200/800 | Loss: 0.8324 | LR: 2.23e-06 | Alpha: 0.541\n",
      "  Batch 220/800 | Loss: 0.8286 | LR: 2.16e-06 | Alpha: 0.541\n",
      "  Batch 240/800 | Loss: 0.8239 | LR: 2.08e-06 | Alpha: 0.541\n",
      "  Batch 260/800 | Loss: 0.8191 | LR: 2.01e-06 | Alpha: 0.541\n",
      "  Batch 280/800 | Loss: 0.8184 | LR: 1.93e-06 | Alpha: 0.541\n",
      "  Batch 300/800 | Loss: 0.8041 | LR: 1.86e-06 | Alpha: 0.541\n",
      "  Batch 320/800 | Loss: 0.7947 | LR: 1.78e-06 | Alpha: 0.541\n",
      "  Batch 340/800 | Loss: 0.7942 | LR: 1.71e-06 | Alpha: 0.541\n",
      "  Batch 360/800 | Loss: 0.7920 | LR: 1.64e-06 | Alpha: 0.541\n",
      "  Batch 380/800 | Loss: 0.8020 | LR: 1.56e-06 | Alpha: 0.541\n",
      "  Batch 400/800 | Loss: 0.7988 | LR: 1.49e-06 | Alpha: 0.541\n",
      "  Batch 420/800 | Loss: 0.8034 | LR: 1.41e-06 | Alpha: 0.541\n",
      "  Batch 440/800 | Loss: 0.8006 | LR: 1.34e-06 | Alpha: 0.541\n",
      "  Batch 460/800 | Loss: 0.8011 | LR: 1.26e-06 | Alpha: 0.541\n",
      "  Batch 480/800 | Loss: 0.8018 | LR: 1.19e-06 | Alpha: 0.541\n",
      "  Batch 500/800 | Loss: 0.8031 | LR: 1.12e-06 | Alpha: 0.541\n",
      "  Batch 520/800 | Loss: 0.8119 | LR: 1.04e-06 | Alpha: 0.541\n",
      "  Batch 540/800 | Loss: 0.8174 | LR: 9.66e-07 | Alpha: 0.541\n",
      "  Batch 560/800 | Loss: 0.8120 | LR: 8.92e-07 | Alpha: 0.541\n",
      "  Batch 580/800 | Loss: 0.8144 | LR: 8.18e-07 | Alpha: 0.541\n",
      "  Batch 600/800 | Loss: 0.8130 | LR: 7.43e-07 | Alpha: 0.541\n",
      "  Batch 620/800 | Loss: 0.8118 | LR: 6.69e-07 | Alpha: 0.541\n",
      "  Batch 640/800 | Loss: 0.8092 | LR: 5.95e-07 | Alpha: 0.541\n",
      "  Batch 660/800 | Loss: 0.8051 | LR: 5.20e-07 | Alpha: 0.541\n",
      "  Batch 680/800 | Loss: 0.8050 | LR: 4.46e-07 | Alpha: 0.541\n",
      "  Batch 700/800 | Loss: 0.8071 | LR: 3.72e-07 | Alpha: 0.541\n",
      "  Batch 720/800 | Loss: 0.8028 | LR: 2.97e-07 | Alpha: 0.541\n",
      "  Batch 740/800 | Loss: 0.7986 | LR: 2.23e-07 | Alpha: 0.541\n",
      "  Batch 760/800 | Loss: 0.7995 | LR: 1.49e-07 | Alpha: 0.541\n",
      "  Batch 780/800 | Loss: 0.8009 | LR: 7.43e-08 | Alpha: 0.541\n",
      "  Batch 800/800 | Loss: 0.8072 | LR: 0.00e+00 | Alpha: 0.541\n",
      "📊 Epoch 5 Results:\n",
      "  Train Loss: 0.8072\n",
      "  Val Loss: 1.4565\n",
      "  Sentiment Accuracy: 0.7400\n",
      "  Emotion Accuracy: 0.1375\n",
      "  Combined Score: 0.4387\n",
      "  Alpha: 0.541\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4587\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.59 GB\n",
      "  Cached: 6.09 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 02:23:20,758] Trial 12 finished with value: 0.3368421052631579 and parameters: {'learning_rate': 1.3567901359939668e-05, 'batch_size': 2, 'alpha': 0.5414951563175963, 'hidden_dropout_prob': 0.22238415028016056, 'classifier_dropout': 0.17185882911732656, 'weight_decay': 0.058092639497543494, 'warmup_ratio': 0.08764864857717905, 'num_epochs': 5, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12: Combined Score = 0.3368 (Sentiment: 0.5263, Emotion: 0.1474)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 1200\n",
      "🚀 Starting training for 3 epochs...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 2.4159 | LR: 2.15e-05 | Alpha: 0.461\n",
      "  Batch 40/400 | Loss: 2.2261 | LR: 4.31e-05 | Alpha: 0.461\n",
      "  Batch 60/400 | Loss: 2.0970 | LR: 6.46e-05 | Alpha: 0.461\n",
      "  Batch 80/400 | Loss: 2.0756 | LR: 8.62e-05 | Alpha: 0.461\n",
      "  Batch 100/400 | Loss: 2.0575 | LR: 9.84e-05 | Alpha: 0.461\n",
      "  Batch 120/400 | Loss: 1.9941 | LR: 9.66e-05 | Alpha: 0.461\n",
      "  Batch 140/400 | Loss: 2.0086 | LR: 9.48e-05 | Alpha: 0.461\n",
      "  Batch 160/400 | Loss: 2.0283 | LR: 9.30e-05 | Alpha: 0.461\n",
      "  Batch 180/400 | Loss: 2.0306 | LR: 9.12e-05 | Alpha: 0.461\n",
      "  Batch 200/400 | Loss: 2.0068 | LR: 8.94e-05 | Alpha: 0.461\n",
      "  Batch 220/400 | Loss: 1.9731 | LR: 8.76e-05 | Alpha: 0.461\n",
      "  Batch 240/400 | Loss: 1.9700 | LR: 8.59e-05 | Alpha: 0.461\n",
      "  Batch 260/400 | Loss: 1.9411 | LR: 8.41e-05 | Alpha: 0.461\n",
      "  Batch 280/400 | Loss: 1.9314 | LR: 8.23e-05 | Alpha: 0.461\n",
      "  Batch 300/400 | Loss: 1.9111 | LR: 8.05e-05 | Alpha: 0.461\n",
      "  Batch 320/400 | Loss: 1.9094 | LR: 7.87e-05 | Alpha: 0.461\n",
      "  Batch 340/400 | Loss: 1.9053 | LR: 7.69e-05 | Alpha: 0.461\n",
      "  Batch 360/400 | Loss: 1.8838 | LR: 7.51e-05 | Alpha: 0.461\n",
      "  Batch 380/400 | Loss: 1.8786 | LR: 7.33e-05 | Alpha: 0.461\n",
      "  Batch 400/400 | Loss: 1.8661 | LR: 7.15e-05 | Alpha: 0.461\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.8661\n",
      "  Val Loss: 1.9673\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.1475\n",
      "  Combined Score: 0.0938\n",
      "  Alpha: 0.461\n",
      "  Learning Rate: 7.15e-05\n",
      "\n",
      "📍 Epoch 2/3\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.7742 | LR: 6.98e-05 | Alpha: 0.461\n",
      "  Batch 40/400 | Loss: 1.6995 | LR: 6.80e-05 | Alpha: 0.461\n",
      "  Batch 60/400 | Loss: 1.6786 | LR: 6.62e-05 | Alpha: 0.461\n",
      "  Batch 80/400 | Loss: 1.6673 | LR: 6.44e-05 | Alpha: 0.461\n",
      "  Batch 100/400 | Loss: 1.7015 | LR: 6.26e-05 | Alpha: 0.461\n",
      "  Batch 120/400 | Loss: 1.7042 | LR: 6.08e-05 | Alpha: 0.461\n",
      "  Batch 140/400 | Loss: 1.7285 | LR: 5.90e-05 | Alpha: 0.461\n",
      "  Batch 160/400 | Loss: 1.7123 | LR: 5.72e-05 | Alpha: 0.461\n",
      "  Batch 180/400 | Loss: 1.7420 | LR: 5.54e-05 | Alpha: 0.461\n",
      "  Batch 200/400 | Loss: 1.7226 | LR: 5.37e-05 | Alpha: 0.461\n",
      "  Batch 220/400 | Loss: 1.7197 | LR: 5.19e-05 | Alpha: 0.461\n",
      "  Batch 240/400 | Loss: 1.7040 | LR: 5.01e-05 | Alpha: 0.461\n",
      "  Batch 260/400 | Loss: 1.6943 | LR: 4.83e-05 | Alpha: 0.461\n",
      "  Batch 280/400 | Loss: 1.6842 | LR: 4.65e-05 | Alpha: 0.461\n",
      "  Batch 300/400 | Loss: 1.6683 | LR: 4.47e-05 | Alpha: 0.461\n",
      "  Batch 320/400 | Loss: 1.6609 | LR: 4.29e-05 | Alpha: 0.461\n",
      "  Batch 340/400 | Loss: 1.6630 | LR: 4.11e-05 | Alpha: 0.461\n",
      "  Batch 360/400 | Loss: 1.6585 | LR: 3.93e-05 | Alpha: 0.461\n",
      "  Batch 380/400 | Loss: 1.6483 | LR: 3.76e-05 | Alpha: 0.461\n",
      "  Batch 400/400 | Loss: 1.6459 | LR: 3.58e-05 | Alpha: 0.461\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.6459\n",
      "  Val Loss: 2.0620\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.0850\n",
      "  Combined Score: 0.0625\n",
      "  Alpha: 0.461\n",
      "  Learning Rate: 3.58e-05\n",
      "\n",
      "📍 Epoch 3/3\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.5758 | LR: 3.40e-05 | Alpha: 0.461\n",
      "  Batch 40/400 | Loss: 1.5493 | LR: 3.22e-05 | Alpha: 0.461\n",
      "  Batch 60/400 | Loss: 1.4951 | LR: 3.04e-05 | Alpha: 0.461\n",
      "  Batch 80/400 | Loss: 1.5035 | LR: 2.86e-05 | Alpha: 0.461\n",
      "  Batch 100/400 | Loss: 1.4751 | LR: 2.68e-05 | Alpha: 0.461\n",
      "  Batch 120/400 | Loss: 1.5103 | LR: 2.50e-05 | Alpha: 0.461\n",
      "  Batch 140/400 | Loss: 1.5019 | LR: 2.33e-05 | Alpha: 0.461\n",
      "  Batch 160/400 | Loss: 1.5027 | LR: 2.15e-05 | Alpha: 0.461\n",
      "  Batch 180/400 | Loss: 1.4879 | LR: 1.97e-05 | Alpha: 0.461\n",
      "  Batch 200/400 | Loss: 1.4934 | LR: 1.79e-05 | Alpha: 0.461\n",
      "  Batch 220/400 | Loss: 1.5012 | LR: 1.61e-05 | Alpha: 0.461\n",
      "  Batch 240/400 | Loss: 1.4875 | LR: 1.43e-05 | Alpha: 0.461\n",
      "  Batch 260/400 | Loss: 1.4827 | LR: 1.25e-05 | Alpha: 0.461\n",
      "  Batch 280/400 | Loss: 1.4749 | LR: 1.07e-05 | Alpha: 0.461\n",
      "  Batch 300/400 | Loss: 1.4716 | LR: 8.94e-06 | Alpha: 0.461\n",
      "  Batch 320/400 | Loss: 1.4724 | LR: 7.15e-06 | Alpha: 0.461\n",
      "  Batch 340/400 | Loss: 1.4660 | LR: 5.37e-06 | Alpha: 0.461\n",
      "  Batch 360/400 | Loss: 1.4588 | LR: 3.58e-06 | Alpha: 0.461\n",
      "  Batch 380/400 | Loss: 1.4624 | LR: 1.79e-06 | Alpha: 0.461\n",
      "  Batch 400/400 | Loss: 1.4643 | LR: 0.00e+00 | Alpha: 0.461\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 1.4643\n",
      "  Val Loss: 1.9339\n",
      "  Sentiment Accuracy: 0.0400\n",
      "  Emotion Accuracy: 0.0850\n",
      "  Combined Score: 0.0625\n",
      "  Alpha: 0.461\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.0938\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.60 GB\n",
      "  Cached: 6.15 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 02:26:42,675] Trial 13 finished with value: 0.17368421052631577 and parameters: {'learning_rate': 9.908696623769013e-05, 'batch_size': 4, 'alpha': 0.460532761225948, 'hidden_dropout_prob': 0.13596253859670623, 'classifier_dropout': 0.3588816179337121, 'weight_decay': 0.025765020727265504, 'warmup_ratio': 0.07705191423702486, 'num_epochs': 3, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13: Combined Score = 0.1737 (Sentiment: 0.2211, Emotion: 0.1263)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 200\n",
      "  Total training steps: 1000\n",
      "🚀 Starting training for 5 epochs...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 2.1462 | LR: 4.89e-06 | Alpha: 0.571\n",
      "  Batch 40/200 | Loss: 1.7950 | LR: 9.79e-06 | Alpha: 0.571\n",
      "  Batch 60/200 | Loss: 1.6766 | LR: 1.47e-05 | Alpha: 0.571\n",
      "  Batch 80/200 | Loss: 1.6119 | LR: 1.96e-05 | Alpha: 0.571\n",
      "  Batch 100/200 | Loss: 1.5773 | LR: 2.45e-05 | Alpha: 0.571\n",
      "  Batch 120/200 | Loss: 1.5420 | LR: 2.72e-05 | Alpha: 0.571\n",
      "  Batch 140/200 | Loss: 1.4893 | LR: 2.65e-05 | Alpha: 0.571\n",
      "  Batch 160/200 | Loss: 1.4625 | LR: 2.59e-05 | Alpha: 0.571\n",
      "  Batch 180/200 | Loss: 1.4416 | LR: 2.53e-05 | Alpha: 0.571\n",
      "  Batch 200/200 | Loss: 1.4019 | LR: 2.47e-05 | Alpha: 0.571\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.4019\n",
      "  Val Loss: 1.6923\n",
      "  Sentiment Accuracy: 0.4175\n",
      "  Emotion Accuracy: 0.1325\n",
      "  Combined Score: 0.2750\n",
      "  Alpha: 0.571\n",
      "  Learning Rate: 2.47e-05\n",
      "\n",
      "📍 Epoch 2/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 1.1023 | LR: 2.41e-05 | Alpha: 0.571\n",
      "  Batch 40/200 | Loss: 1.0186 | LR: 2.35e-05 | Alpha: 0.571\n",
      "  Batch 60/200 | Loss: 1.0128 | LR: 2.28e-05 | Alpha: 0.571\n",
      "  Batch 80/200 | Loss: 1.0104 | LR: 2.22e-05 | Alpha: 0.571\n",
      "  Batch 100/200 | Loss: 0.9988 | LR: 2.16e-05 | Alpha: 0.571\n",
      "  Batch 120/200 | Loss: 0.9710 | LR: 2.10e-05 | Alpha: 0.571\n",
      "  Batch 140/200 | Loss: 0.9506 | LR: 2.04e-05 | Alpha: 0.571\n",
      "  Batch 160/200 | Loss: 0.9231 | LR: 1.98e-05 | Alpha: 0.571\n",
      "  Batch 180/200 | Loss: 0.8876 | LR: 1.91e-05 | Alpha: 0.571\n",
      "  Batch 200/200 | Loss: 0.8598 | LR: 1.85e-05 | Alpha: 0.571\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 0.8598\n",
      "  Val Loss: 1.4019\n",
      "  Sentiment Accuracy: 0.7525\n",
      "  Emotion Accuracy: 0.1350\n",
      "  Combined Score: 0.4437\n",
      "  Alpha: 0.571\n",
      "  Learning Rate: 1.85e-05\n",
      "\n",
      "📍 Epoch 3/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 0.6761 | LR: 1.79e-05 | Alpha: 0.571\n",
      "  Batch 40/200 | Loss: 0.6348 | LR: 1.73e-05 | Alpha: 0.571\n",
      "  Batch 60/200 | Loss: 0.6374 | LR: 1.67e-05 | Alpha: 0.571\n",
      "  Batch 80/200 | Loss: 0.6669 | LR: 1.61e-05 | Alpha: 0.571\n",
      "  Batch 100/200 | Loss: 0.6492 | LR: 1.54e-05 | Alpha: 0.571\n",
      "  Batch 120/200 | Loss: 0.6382 | LR: 1.48e-05 | Alpha: 0.571\n",
      "  Batch 140/200 | Loss: 0.6166 | LR: 1.42e-05 | Alpha: 0.571\n",
      "  Batch 160/200 | Loss: 0.5964 | LR: 1.36e-05 | Alpha: 0.571\n",
      "  Batch 180/200 | Loss: 0.5881 | LR: 1.30e-05 | Alpha: 0.571\n",
      "  Batch 200/200 | Loss: 0.5880 | LR: 1.23e-05 | Alpha: 0.571\n",
      "📊 Epoch 3 Results:\n",
      "  Train Loss: 0.5880\n",
      "  Val Loss: 1.5170\n",
      "  Sentiment Accuracy: 0.8175\n",
      "  Emotion Accuracy: 0.1600\n",
      "  Combined Score: 0.4888\n",
      "  Alpha: 0.571\n",
      "  Learning Rate: 1.23e-05\n",
      "\n",
      "📍 Epoch 4/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 0.3684 | LR: 1.17e-05 | Alpha: 0.571\n",
      "  Batch 40/200 | Loss: 0.3847 | LR: 1.11e-05 | Alpha: 0.571\n",
      "  Batch 60/200 | Loss: 0.4003 | LR: 1.05e-05 | Alpha: 0.571\n",
      "  Batch 80/200 | Loss: 0.3880 | LR: 9.88e-06 | Alpha: 0.571\n",
      "  Batch 100/200 | Loss: 0.3802 | LR: 9.26e-06 | Alpha: 0.571\n",
      "  Batch 120/200 | Loss: 0.3819 | LR: 8.64e-06 | Alpha: 0.571\n",
      "  Batch 140/200 | Loss: 0.3849 | LR: 8.03e-06 | Alpha: 0.571\n",
      "  Batch 160/200 | Loss: 0.3677 | LR: 7.41e-06 | Alpha: 0.571\n",
      "  Batch 180/200 | Loss: 0.3609 | LR: 6.79e-06 | Alpha: 0.571\n",
      "  Batch 200/200 | Loss: 0.3512 | LR: 6.17e-06 | Alpha: 0.571\n",
      "📊 Epoch 4 Results:\n",
      "  Train Loss: 0.3512\n",
      "  Val Loss: 1.6436\n",
      "  Sentiment Accuracy: 0.8150\n",
      "  Emotion Accuracy: 0.1975\n",
      "  Combined Score: 0.5062\n",
      "  Alpha: 0.571\n",
      "  Learning Rate: 6.17e-06\n",
      "\n",
      "📍 Epoch 5/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/200 | Loss: 0.2895 | LR: 5.56e-06 | Alpha: 0.571\n",
      "  Batch 40/200 | Loss: 0.3304 | LR: 4.94e-06 | Alpha: 0.571\n",
      "  Batch 60/200 | Loss: 0.3418 | LR: 4.32e-06 | Alpha: 0.571\n",
      "  Batch 80/200 | Loss: 0.3153 | LR: 3.70e-06 | Alpha: 0.571\n",
      "  Batch 100/200 | Loss: 0.2920 | LR: 3.09e-06 | Alpha: 0.571\n",
      "  Batch 120/200 | Loss: 0.2813 | LR: 2.47e-06 | Alpha: 0.571\n",
      "  Batch 140/200 | Loss: 0.2758 | LR: 1.85e-06 | Alpha: 0.571\n",
      "  Batch 160/200 | Loss: 0.2712 | LR: 1.23e-06 | Alpha: 0.571\n",
      "  Batch 180/200 | Loss: 0.2627 | LR: 6.17e-07 | Alpha: 0.571\n",
      "  Batch 200/200 | Loss: 0.2580 | LR: 0.00e+00 | Alpha: 0.571\n",
      "📊 Epoch 5 Results:\n",
      "  Train Loss: 0.2580\n",
      "  Val Loss: 1.7574\n",
      "  Sentiment Accuracy: 0.8175\n",
      "  Emotion Accuracy: 0.1875\n",
      "  Combined Score: 0.5025\n",
      "  Alpha: 0.571\n",
      "  Learning Rate: 0.00e+00\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.5062\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.61 GB\n",
      "  Cached: 6.29 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 02:40:52,434] Trial 14 finished with value: 0.34210526315789475 and parameters: {'learning_rate': 2.7410638932887623e-05, 'batch_size': 8, 'alpha': 0.5710146980775144, 'hidden_dropout_prob': 0.05724867212399054, 'classifier_dropout': 0.25660486731910687, 'weight_decay': 0.06780121012459085, 'warmup_ratio': 0.11228099245449503, 'num_epochs': 5, 'max_length': 128}. Best is trial 2 with value: 0.4263157894736842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14: Combined Score = 0.3421 (Sentiment: 0.5684, Emotion: 0.1158)\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "\n",
      "🏆 Optimization completed!\n",
      "Best trial: 2\n",
      "Best combined score: 0.4263\n",
      "Best parameters:\n",
      "  learning_rate: 2.858051065806938e-05\n",
      "  batch_size: 2\n",
      "  alpha: 0.5369658275448169\n",
      "  hidden_dropout_prob: 0.061612603179999434\n",
      "  classifier_dropout: 0.28226345557043153\n",
      "  weight_decay: 0.017881888245041864\n",
      "  warmup_ratio: 0.05975773894779193\n",
      "  num_epochs: 5\n",
      "  max_length: 128\n",
      "\n",
      "🚀 Training final model with best hyperparameters...\n",
      "============================================================\n",
      "🚀 Starting Ultra-Lightweight Training\n",
      "==================================================\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "\n",
      "1️⃣ Loading minimal external datasets...\n",
      "Loading external datasets for training...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 val\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 val\n",
      "🔄 Preparing external datasets for multitask training...\n",
      "✅ External data prepared:\n",
      "  Train samples: 4000\n",
      "  Validation samples: 1000\n",
      "  Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "  Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "📈 Training set class distribution:\n",
      "  Sentiment 'Negative': 1807 samples\n",
      "  Sentiment 'Neutral': 216 samples\n",
      "  Sentiment 'Positive': 1977 samples\n",
      "  Emotion 'Anger': 1239 samples\n",
      "  Emotion 'Fear': 653 samples\n",
      "  Emotion 'Joy': 457 samples\n",
      "  Emotion 'No Emotion': 607 samples\n",
      "  Emotion 'Sadness': 767 samples\n",
      "  Emotion 'Surprise': 277 samples\n",
      "\n",
      "2️⃣ Loading Reddit data...\n",
      "🔄 Preparing Reddit data for evaluation...\n",
      "✅ Reddit evaluation data prepared: 95 samples\n",
      "\n",
      "3️⃣ Ultra-lightweight config:\n",
      "  Batch size: 1\n",
      "  Max length: 128\n",
      "  Training samples: 4000\n",
      "  Epochs: 2\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 3.36 GB\n",
      "  Cached: 3.66 GB\n",
      "\n",
      "4️⃣ Initializing trainer...\n",
      "\n",
      "5️⃣ Setting up with memory optimizations...\n",
      "✅ Setup complete!\n",
      "  Model: microsoft/deberta-base\n",
      "  Training samples: 4000\n",
      "  Validation samples: 1000\n",
      "  Training steps per epoch: 4000\n",
      "  Total training steps: 8000\n",
      "✅ Gradient checkpointing enabled\n",
      "\n",
      "6️⃣ Training with memory monitoring...\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/4000 | Loss: 2.1090 | LR: 1.00e-06 | Alpha: 0.500\n",
      "  Batch 40/4000 | Loss: 1.9405 | LR: 2.00e-06 | Alpha: 0.500\n",
      "  Batch 60/4000 | Loss: 1.9511 | LR: 3.00e-06 | Alpha: 0.500\n",
      "  Batch 80/4000 | Loss: 1.9031 | LR: 4.00e-06 | Alpha: 0.500\n",
      "  Batch 100/4000 | Loss: 1.8726 | LR: 5.00e-06 | Alpha: 0.500\n",
      "  Batch 120/4000 | Loss: 1.8277 | LR: 6.00e-06 | Alpha: 0.500\n",
      "  Batch 140/4000 | Loss: 1.7869 | LR: 7.00e-06 | Alpha: 0.500\n",
      "  Batch 160/4000 | Loss: 1.7909 | LR: 8.00e-06 | Alpha: 0.500\n",
      "  Batch 180/4000 | Loss: 1.7937 | LR: 9.00e-06 | Alpha: 0.500\n",
      "  Batch 200/4000 | Loss: 1.7760 | LR: 1.00e-05 | Alpha: 0.500\n",
      "  Batch 220/4000 | Loss: 1.7770 | LR: 1.10e-05 | Alpha: 0.500\n",
      "  Batch 240/4000 | Loss: 1.7645 | LR: 1.20e-05 | Alpha: 0.500\n",
      "  Batch 260/4000 | Loss: 1.7458 | LR: 1.30e-05 | Alpha: 0.500\n",
      "  Batch 280/4000 | Loss: 1.7368 | LR: 1.40e-05 | Alpha: 0.500\n",
      "  Batch 300/4000 | Loss: 1.7560 | LR: 1.50e-05 | Alpha: 0.500\n",
      "  Batch 320/4000 | Loss: 1.7610 | LR: 1.60e-05 | Alpha: 0.500\n",
      "  Batch 340/4000 | Loss: 1.7658 | LR: 1.70e-05 | Alpha: 0.500\n",
      "  Batch 360/4000 | Loss: 1.7645 | LR: 1.80e-05 | Alpha: 0.500\n",
      "  Batch 380/4000 | Loss: 1.7822 | LR: 1.90e-05 | Alpha: 0.500\n",
      "  Batch 400/4000 | Loss: 1.7900 | LR: 2.00e-05 | Alpha: 0.500\n",
      "  Batch 420/4000 | Loss: 1.7781 | LR: 1.99e-05 | Alpha: 0.500\n",
      "  Batch 440/4000 | Loss: 1.7697 | LR: 1.99e-05 | Alpha: 0.500\n",
      "  Batch 460/4000 | Loss: 1.7649 | LR: 1.98e-05 | Alpha: 0.500\n",
      "  Batch 480/4000 | Loss: 1.7624 | LR: 1.98e-05 | Alpha: 0.500\n",
      "  Batch 500/4000 | Loss: 1.7643 | LR: 1.97e-05 | Alpha: 0.500\n",
      "  Batch 520/4000 | Loss: 1.7611 | LR: 1.97e-05 | Alpha: 0.500\n",
      "  Batch 540/4000 | Loss: 1.7629 | LR: 1.96e-05 | Alpha: 0.500\n",
      "  Batch 560/4000 | Loss: 1.7562 | LR: 1.96e-05 | Alpha: 0.500\n",
      "  Batch 580/4000 | Loss: 1.7617 | LR: 1.95e-05 | Alpha: 0.500\n",
      "  Batch 600/4000 | Loss: 1.7541 | LR: 1.95e-05 | Alpha: 0.500\n",
      "  Batch 620/4000 | Loss: 1.7548 | LR: 1.94e-05 | Alpha: 0.500\n",
      "  Batch 640/4000 | Loss: 1.7560 | LR: 1.94e-05 | Alpha: 0.500\n",
      "  Batch 660/4000 | Loss: 1.7538 | LR: 1.93e-05 | Alpha: 0.500\n",
      "  Batch 680/4000 | Loss: 1.7527 | LR: 1.93e-05 | Alpha: 0.500\n",
      "  Batch 700/4000 | Loss: 1.7504 | LR: 1.92e-05 | Alpha: 0.500\n",
      "  Batch 720/4000 | Loss: 1.7415 | LR: 1.92e-05 | Alpha: 0.500\n",
      "  Batch 740/4000 | Loss: 1.7394 | LR: 1.91e-05 | Alpha: 0.500\n",
      "  Batch 760/4000 | Loss: 1.7419 | LR: 1.91e-05 | Alpha: 0.500\n",
      "  Batch 780/4000 | Loss: 1.7303 | LR: 1.90e-05 | Alpha: 0.500\n",
      "  Batch 800/4000 | Loss: 1.7274 | LR: 1.89e-05 | Alpha: 0.500\n",
      "  Batch 820/4000 | Loss: 1.7273 | LR: 1.89e-05 | Alpha: 0.500\n",
      "  Batch 840/4000 | Loss: 1.7263 | LR: 1.88e-05 | Alpha: 0.500\n",
      "  Batch 860/4000 | Loss: 1.7232 | LR: 1.88e-05 | Alpha: 0.500\n",
      "  Batch 880/4000 | Loss: 1.7234 | LR: 1.87e-05 | Alpha: 0.500\n",
      "  Batch 900/4000 | Loss: 1.7187 | LR: 1.87e-05 | Alpha: 0.500\n",
      "  Batch 920/4000 | Loss: 1.7233 | LR: 1.86e-05 | Alpha: 0.500\n",
      "  Batch 940/4000 | Loss: 1.7242 | LR: 1.86e-05 | Alpha: 0.500\n",
      "  Batch 960/4000 | Loss: 1.7219 | LR: 1.85e-05 | Alpha: 0.500\n",
      "  Batch 980/4000 | Loss: 1.7170 | LR: 1.85e-05 | Alpha: 0.500\n",
      "  Batch 1000/4000 | Loss: 1.7162 | LR: 1.84e-05 | Alpha: 0.500\n",
      "  Batch 1020/4000 | Loss: 1.7124 | LR: 1.84e-05 | Alpha: 0.500\n",
      "  Batch 1040/4000 | Loss: 1.7065 | LR: 1.83e-05 | Alpha: 0.500\n",
      "  Batch 1060/4000 | Loss: 1.7013 | LR: 1.83e-05 | Alpha: 0.500\n",
      "  Batch 1080/4000 | Loss: 1.6961 | LR: 1.82e-05 | Alpha: 0.500\n",
      "  Batch 1100/4000 | Loss: 1.6902 | LR: 1.82e-05 | Alpha: 0.500\n",
      "  Batch 1120/4000 | Loss: 1.6842 | LR: 1.81e-05 | Alpha: 0.500\n",
      "  Batch 1140/4000 | Loss: 1.6787 | LR: 1.81e-05 | Alpha: 0.500\n",
      "  Batch 1160/4000 | Loss: 1.6768 | LR: 1.80e-05 | Alpha: 0.500\n",
      "  Batch 1180/4000 | Loss: 1.6754 | LR: 1.79e-05 | Alpha: 0.500\n",
      "  Batch 1200/4000 | Loss: 1.6720 | LR: 1.79e-05 | Alpha: 0.500\n",
      "  Batch 1220/4000 | Loss: 1.6658 | LR: 1.78e-05 | Alpha: 0.500\n",
      "  Batch 1240/4000 | Loss: 1.6673 | LR: 1.78e-05 | Alpha: 0.500\n",
      "  Batch 1260/4000 | Loss: 1.6707 | LR: 1.77e-05 | Alpha: 0.500\n",
      "  Batch 1280/4000 | Loss: 1.6673 | LR: 1.77e-05 | Alpha: 0.500\n",
      "  Batch 1300/4000 | Loss: 1.6658 | LR: 1.76e-05 | Alpha: 0.500\n",
      "  Batch 1320/4000 | Loss: 1.6599 | LR: 1.76e-05 | Alpha: 0.500\n",
      "  Batch 1340/4000 | Loss: 1.6572 | LR: 1.75e-05 | Alpha: 0.500\n",
      "  Batch 1360/4000 | Loss: 1.6546 | LR: 1.75e-05 | Alpha: 0.500\n",
      "  Batch 1380/4000 | Loss: 1.6482 | LR: 1.74e-05 | Alpha: 0.500\n",
      "  Batch 1400/4000 | Loss: 1.6439 | LR: 1.74e-05 | Alpha: 0.500\n",
      "  Batch 1420/4000 | Loss: 1.6419 | LR: 1.73e-05 | Alpha: 0.500\n",
      "  Batch 1440/4000 | Loss: 1.6375 | LR: 1.73e-05 | Alpha: 0.500\n",
      "  Batch 1460/4000 | Loss: 1.6302 | LR: 1.72e-05 | Alpha: 0.500\n",
      "  Batch 1480/4000 | Loss: 1.6291 | LR: 1.72e-05 | Alpha: 0.500\n",
      "  Batch 1500/4000 | Loss: 1.6308 | LR: 1.71e-05 | Alpha: 0.500\n",
      "  Batch 1520/4000 | Loss: 1.6251 | LR: 1.71e-05 | Alpha: 0.500\n",
      "  Batch 1540/4000 | Loss: 1.6238 | LR: 1.70e-05 | Alpha: 0.500\n",
      "  Batch 1560/4000 | Loss: 1.6184 | LR: 1.69e-05 | Alpha: 0.500\n",
      "  Batch 1580/4000 | Loss: 1.6161 | LR: 1.69e-05 | Alpha: 0.500\n",
      "  Batch 1600/4000 | Loss: 1.6125 | LR: 1.68e-05 | Alpha: 0.500\n",
      "  Batch 1620/4000 | Loss: 1.6090 | LR: 1.68e-05 | Alpha: 0.500\n",
      "  Batch 1640/4000 | Loss: 1.6039 | LR: 1.67e-05 | Alpha: 0.500\n",
      "  Batch 1660/4000 | Loss: 1.6013 | LR: 1.67e-05 | Alpha: 0.500\n",
      "  Batch 1680/4000 | Loss: 1.6002 | LR: 1.66e-05 | Alpha: 0.500\n",
      "  Batch 1700/4000 | Loss: 1.5946 | LR: 1.66e-05 | Alpha: 0.500\n",
      "  Batch 1720/4000 | Loss: 1.5945 | LR: 1.65e-05 | Alpha: 0.500\n",
      "  Batch 1740/4000 | Loss: 1.5920 | LR: 1.65e-05 | Alpha: 0.500\n",
      "  Batch 1760/4000 | Loss: 1.5871 | LR: 1.64e-05 | Alpha: 0.500\n",
      "  Batch 1780/4000 | Loss: 1.5827 | LR: 1.64e-05 | Alpha: 0.500\n",
      "  Batch 1800/4000 | Loss: 1.5796 | LR: 1.63e-05 | Alpha: 0.500\n",
      "  Batch 1820/4000 | Loss: 1.5761 | LR: 1.63e-05 | Alpha: 0.500\n",
      "  Batch 1840/4000 | Loss: 1.5746 | LR: 1.62e-05 | Alpha: 0.500\n",
      "  Batch 1860/4000 | Loss: 1.5730 | LR: 1.62e-05 | Alpha: 0.500\n",
      "  Batch 1880/4000 | Loss: 1.5741 | LR: 1.61e-05 | Alpha: 0.500\n",
      "  Batch 1900/4000 | Loss: 1.5748 | LR: 1.61e-05 | Alpha: 0.500\n",
      "  Batch 1920/4000 | Loss: 1.5755 | LR: 1.60e-05 | Alpha: 0.500\n",
      "  Batch 1940/4000 | Loss: 1.5737 | LR: 1.59e-05 | Alpha: 0.500\n",
      "  Batch 1960/4000 | Loss: 1.5695 | LR: 1.59e-05 | Alpha: 0.500\n",
      "  Batch 1980/4000 | Loss: 1.5690 | LR: 1.58e-05 | Alpha: 0.500\n",
      "  Batch 2000/4000 | Loss: 1.5684 | LR: 1.58e-05 | Alpha: 0.500\n",
      "  Batch 2020/4000 | Loss: 1.5639 | LR: 1.57e-05 | Alpha: 0.500\n",
      "  Batch 2040/4000 | Loss: 1.5623 | LR: 1.57e-05 | Alpha: 0.500\n",
      "  Batch 2060/4000 | Loss: 1.5594 | LR: 1.56e-05 | Alpha: 0.500\n",
      "  Batch 2080/4000 | Loss: 1.5579 | LR: 1.56e-05 | Alpha: 0.500\n",
      "  Batch 2100/4000 | Loss: 1.5518 | LR: 1.55e-05 | Alpha: 0.500\n",
      "  Batch 2120/4000 | Loss: 1.5525 | LR: 1.55e-05 | Alpha: 0.500\n",
      "  Batch 2140/4000 | Loss: 1.5499 | LR: 1.54e-05 | Alpha: 0.500\n",
      "  Batch 2160/4000 | Loss: 1.5466 | LR: 1.54e-05 | Alpha: 0.500\n",
      "  Batch 2180/4000 | Loss: 1.5447 | LR: 1.53e-05 | Alpha: 0.500\n",
      "  Batch 2200/4000 | Loss: 1.5425 | LR: 1.53e-05 | Alpha: 0.500\n",
      "  Batch 2220/4000 | Loss: 1.5375 | LR: 1.52e-05 | Alpha: 0.500\n",
      "  Batch 2240/4000 | Loss: 1.5357 | LR: 1.52e-05 | Alpha: 0.500\n",
      "  Batch 2260/4000 | Loss: 1.5305 | LR: 1.51e-05 | Alpha: 0.500\n",
      "  Batch 2280/4000 | Loss: 1.5280 | LR: 1.51e-05 | Alpha: 0.500\n",
      "  Batch 2300/4000 | Loss: 1.5237 | LR: 1.50e-05 | Alpha: 0.500\n",
      "  Batch 2320/4000 | Loss: 1.5208 | LR: 1.49e-05 | Alpha: 0.500\n",
      "  Batch 2340/4000 | Loss: 1.5226 | LR: 1.49e-05 | Alpha: 0.500\n",
      "  Batch 2360/4000 | Loss: 1.5205 | LR: 1.48e-05 | Alpha: 0.500\n",
      "  Batch 2380/4000 | Loss: 1.5183 | LR: 1.48e-05 | Alpha: 0.500\n",
      "  Batch 2400/4000 | Loss: 1.5133 | LR: 1.47e-05 | Alpha: 0.500\n",
      "  Batch 2420/4000 | Loss: 1.5163 | LR: 1.47e-05 | Alpha: 0.500\n",
      "  Batch 2440/4000 | Loss: 1.5128 | LR: 1.46e-05 | Alpha: 0.500\n",
      "  Batch 2460/4000 | Loss: 1.5114 | LR: 1.46e-05 | Alpha: 0.500\n",
      "  Batch 2480/4000 | Loss: 1.5103 | LR: 1.45e-05 | Alpha: 0.500\n",
      "  Batch 2500/4000 | Loss: 1.5085 | LR: 1.45e-05 | Alpha: 0.500\n",
      "  Batch 2520/4000 | Loss: 1.5062 | LR: 1.44e-05 | Alpha: 0.500\n",
      "  Batch 2540/4000 | Loss: 1.5059 | LR: 1.44e-05 | Alpha: 0.500\n",
      "  Batch 2560/4000 | Loss: 1.5024 | LR: 1.43e-05 | Alpha: 0.500\n",
      "  Batch 2580/4000 | Loss: 1.4993 | LR: 1.43e-05 | Alpha: 0.500\n",
      "  Batch 2600/4000 | Loss: 1.4963 | LR: 1.42e-05 | Alpha: 0.500\n",
      "  Batch 2620/4000 | Loss: 1.4943 | LR: 1.42e-05 | Alpha: 0.500\n",
      "  Batch 2640/4000 | Loss: 1.4900 | LR: 1.41e-05 | Alpha: 0.500\n",
      "  Batch 2660/4000 | Loss: 1.4875 | LR: 1.41e-05 | Alpha: 0.500\n",
      "  Batch 2680/4000 | Loss: 1.4838 | LR: 1.40e-05 | Alpha: 0.500\n",
      "  Batch 2700/4000 | Loss: 1.4799 | LR: 1.39e-05 | Alpha: 0.500\n",
      "  Batch 2720/4000 | Loss: 1.4756 | LR: 1.39e-05 | Alpha: 0.500\n",
      "  Batch 2740/4000 | Loss: 1.4737 | LR: 1.38e-05 | Alpha: 0.500\n",
      "  Batch 2760/4000 | Loss: 1.4726 | LR: 1.38e-05 | Alpha: 0.500\n",
      "  Batch 2780/4000 | Loss: 1.4702 | LR: 1.37e-05 | Alpha: 0.500\n",
      "  Batch 2800/4000 | Loss: 1.4661 | LR: 1.37e-05 | Alpha: 0.500\n",
      "  Batch 2820/4000 | Loss: 1.4644 | LR: 1.36e-05 | Alpha: 0.500\n",
      "  Batch 2840/4000 | Loss: 1.4615 | LR: 1.36e-05 | Alpha: 0.500\n",
      "  Batch 2860/4000 | Loss: 1.4599 | LR: 1.35e-05 | Alpha: 0.500\n",
      "  Batch 2880/4000 | Loss: 1.4577 | LR: 1.35e-05 | Alpha: 0.500\n",
      "  Batch 2900/4000 | Loss: 1.4553 | LR: 1.34e-05 | Alpha: 0.500\n",
      "  Batch 2920/4000 | Loss: 1.4524 | LR: 1.34e-05 | Alpha: 0.500\n",
      "  Batch 2940/4000 | Loss: 1.4512 | LR: 1.33e-05 | Alpha: 0.500\n",
      "  Batch 2960/4000 | Loss: 1.4469 | LR: 1.33e-05 | Alpha: 0.500\n",
      "  Batch 2980/4000 | Loss: 1.4435 | LR: 1.32e-05 | Alpha: 0.500\n",
      "  Batch 3000/4000 | Loss: 1.4422 | LR: 1.32e-05 | Alpha: 0.500\n",
      "  Batch 3020/4000 | Loss: 1.4381 | LR: 1.31e-05 | Alpha: 0.500\n",
      "  Batch 3040/4000 | Loss: 1.4345 | LR: 1.31e-05 | Alpha: 0.500\n",
      "  Batch 3060/4000 | Loss: 1.4311 | LR: 1.30e-05 | Alpha: 0.500\n",
      "  Batch 3080/4000 | Loss: 1.4279 | LR: 1.29e-05 | Alpha: 0.500\n",
      "  Batch 3100/4000 | Loss: 1.4252 | LR: 1.29e-05 | Alpha: 0.500\n",
      "  Batch 3120/4000 | Loss: 1.4224 | LR: 1.28e-05 | Alpha: 0.500\n",
      "  Batch 3140/4000 | Loss: 1.4210 | LR: 1.28e-05 | Alpha: 0.500\n",
      "  Batch 3160/4000 | Loss: 1.4185 | LR: 1.27e-05 | Alpha: 0.500\n",
      "  Batch 3180/4000 | Loss: 1.4171 | LR: 1.27e-05 | Alpha: 0.500\n",
      "  Batch 3200/4000 | Loss: 1.4166 | LR: 1.26e-05 | Alpha: 0.500\n",
      "  Batch 3220/4000 | Loss: 1.4134 | LR: 1.26e-05 | Alpha: 0.500\n",
      "  Batch 3240/4000 | Loss: 1.4112 | LR: 1.25e-05 | Alpha: 0.500\n",
      "  Batch 3260/4000 | Loss: 1.4076 | LR: 1.25e-05 | Alpha: 0.500\n",
      "  Batch 3280/4000 | Loss: 1.4067 | LR: 1.24e-05 | Alpha: 0.500\n",
      "  Batch 3300/4000 | Loss: 1.4048 | LR: 1.24e-05 | Alpha: 0.500\n",
      "  Batch 3320/4000 | Loss: 1.4052 | LR: 1.23e-05 | Alpha: 0.500\n",
      "  Batch 3340/4000 | Loss: 1.4018 | LR: 1.23e-05 | Alpha: 0.500\n",
      "  Batch 3360/4000 | Loss: 1.4013 | LR: 1.22e-05 | Alpha: 0.500\n",
      "  Batch 3380/4000 | Loss: 1.3996 | LR: 1.22e-05 | Alpha: 0.500\n",
      "  Batch 3400/4000 | Loss: 1.3976 | LR: 1.21e-05 | Alpha: 0.500\n",
      "  Batch 3420/4000 | Loss: 1.3943 | LR: 1.21e-05 | Alpha: 0.500\n",
      "  Batch 3440/4000 | Loss: 1.3929 | LR: 1.20e-05 | Alpha: 0.500\n",
      "  Batch 3460/4000 | Loss: 1.3906 | LR: 1.19e-05 | Alpha: 0.500\n",
      "  Batch 3480/4000 | Loss: 1.3886 | LR: 1.19e-05 | Alpha: 0.500\n",
      "  Batch 3500/4000 | Loss: 1.3868 | LR: 1.18e-05 | Alpha: 0.500\n",
      "  Batch 3520/4000 | Loss: 1.3847 | LR: 1.18e-05 | Alpha: 0.500\n",
      "  Batch 3540/4000 | Loss: 1.3822 | LR: 1.17e-05 | Alpha: 0.500\n",
      "  Batch 3560/4000 | Loss: 1.3791 | LR: 1.17e-05 | Alpha: 0.500\n",
      "  Batch 3580/4000 | Loss: 1.3768 | LR: 1.16e-05 | Alpha: 0.500\n",
      "  Batch 3600/4000 | Loss: 1.3764 | LR: 1.16e-05 | Alpha: 0.500\n",
      "  Batch 3620/4000 | Loss: 1.3768 | LR: 1.15e-05 | Alpha: 0.500\n",
      "  Batch 3640/4000 | Loss: 1.3737 | LR: 1.15e-05 | Alpha: 0.500\n",
      "  Batch 3660/4000 | Loss: 1.3716 | LR: 1.14e-05 | Alpha: 0.500\n",
      "  Batch 3680/4000 | Loss: 1.3703 | LR: 1.14e-05 | Alpha: 0.500\n",
      "  Batch 3700/4000 | Loss: 1.3688 | LR: 1.13e-05 | Alpha: 0.500\n",
      "  Batch 3720/4000 | Loss: 1.3677 | LR: 1.13e-05 | Alpha: 0.500\n",
      "  Batch 3740/4000 | Loss: 1.3641 | LR: 1.12e-05 | Alpha: 0.500\n",
      "  Batch 3760/4000 | Loss: 1.3638 | LR: 1.12e-05 | Alpha: 0.500\n",
      "  Batch 3780/4000 | Loss: 1.3623 | LR: 1.11e-05 | Alpha: 0.500\n",
      "  Batch 3800/4000 | Loss: 1.3606 | LR: 1.11e-05 | Alpha: 0.500\n",
      "  Batch 3820/4000 | Loss: 1.3591 | LR: 1.10e-05 | Alpha: 0.500\n",
      "  Batch 3840/4000 | Loss: 1.3578 | LR: 1.09e-05 | Alpha: 0.500\n",
      "  Batch 3860/4000 | Loss: 1.3539 | LR: 1.09e-05 | Alpha: 0.500\n",
      "  Batch 3880/4000 | Loss: 1.3515 | LR: 1.08e-05 | Alpha: 0.500\n",
      "  Batch 3900/4000 | Loss: 1.3506 | LR: 1.08e-05 | Alpha: 0.500\n",
      "  Batch 3920/4000 | Loss: 1.3495 | LR: 1.07e-05 | Alpha: 0.500\n",
      "  Batch 3940/4000 | Loss: 1.3480 | LR: 1.07e-05 | Alpha: 0.500\n",
      "  Batch 3960/4000 | Loss: 1.3445 | LR: 1.06e-05 | Alpha: 0.500\n",
      "  Batch 3980/4000 | Loss: 1.3405 | LR: 1.06e-05 | Alpha: 0.500\n",
      "  Batch 4000/4000 | Loss: 1.3378 | LR: 1.05e-05 | Alpha: 0.500\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.3378\n",
      "  Val Loss: 1.8954\n",
      "  Sentiment Accuracy: 0.6340\n",
      "  Emotion Accuracy: 0.1920\n",
      "  Combined Score: 0.4130\n",
      "  Alpha: 0.500\n",
      "  Learning Rate: 1.05e-05\n",
      "Model saved to ./multitask_model_optimized\\checkpoint-epoch-1\n",
      "Model saved to ./multitask_model_optimized\\best_model\n",
      "\n",
      "📍 Epoch 2/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/4000 | Loss: 1.0246 | LR: 1.05e-05 | Alpha: 0.500\n",
      "  Batch 40/4000 | Loss: 0.9799 | LR: 1.04e-05 | Alpha: 0.500\n",
      "  Batch 60/4000 | Loss: 1.0509 | LR: 1.04e-05 | Alpha: 0.500\n",
      "  Batch 80/4000 | Loss: 1.1154 | LR: 1.03e-05 | Alpha: 0.500\n",
      "  Batch 100/4000 | Loss: 1.0546 | LR: 1.03e-05 | Alpha: 0.500\n",
      "  Batch 120/4000 | Loss: 1.0240 | LR: 1.02e-05 | Alpha: 0.500\n",
      "  Batch 140/4000 | Loss: 0.9810 | LR: 1.02e-05 | Alpha: 0.500\n",
      "  Batch 160/4000 | Loss: 1.0049 | LR: 1.01e-05 | Alpha: 0.500\n",
      "  Batch 180/4000 | Loss: 1.0320 | LR: 1.01e-05 | Alpha: 0.500\n",
      "  Batch 200/4000 | Loss: 1.0247 | LR: 1.00e-05 | Alpha: 0.500\n",
      "  Batch 220/4000 | Loss: 1.0324 | LR: 9.95e-06 | Alpha: 0.500\n",
      "  Batch 240/4000 | Loss: 1.0187 | LR: 9.89e-06 | Alpha: 0.500\n",
      "  Batch 260/4000 | Loss: 1.0026 | LR: 9.84e-06 | Alpha: 0.500\n",
      "  Batch 280/4000 | Loss: 1.0181 | LR: 9.79e-06 | Alpha: 0.500\n",
      "  Batch 300/4000 | Loss: 1.0216 | LR: 9.74e-06 | Alpha: 0.500\n",
      "  Batch 320/4000 | Loss: 0.9980 | LR: 9.68e-06 | Alpha: 0.500\n",
      "  Batch 340/4000 | Loss: 0.9818 | LR: 9.63e-06 | Alpha: 0.500\n",
      "  Batch 360/4000 | Loss: 1.0111 | LR: 9.58e-06 | Alpha: 0.500\n",
      "  Batch 380/4000 | Loss: 0.9992 | LR: 9.53e-06 | Alpha: 0.500\n",
      "  Batch 400/4000 | Loss: 1.0076 | LR: 9.47e-06 | Alpha: 0.500\n",
      "  Batch 420/4000 | Loss: 0.9905 | LR: 9.42e-06 | Alpha: 0.500\n",
      "  Batch 440/4000 | Loss: 1.0019 | LR: 9.37e-06 | Alpha: 0.500\n",
      "  Batch 460/4000 | Loss: 0.9965 | LR: 9.32e-06 | Alpha: 0.500\n",
      "  Batch 480/4000 | Loss: 0.9854 | LR: 9.26e-06 | Alpha: 0.500\n",
      "  Batch 500/4000 | Loss: 0.9844 | LR: 9.21e-06 | Alpha: 0.500\n",
      "  Batch 520/4000 | Loss: 0.9737 | LR: 9.16e-06 | Alpha: 0.500\n",
      "  Batch 540/4000 | Loss: 0.9675 | LR: 9.11e-06 | Alpha: 0.500\n",
      "  Batch 560/4000 | Loss: 0.9639 | LR: 9.05e-06 | Alpha: 0.500\n",
      "  Batch 580/4000 | Loss: 0.9784 | LR: 9.00e-06 | Alpha: 0.500\n",
      "  Batch 600/4000 | Loss: 0.9853 | LR: 8.95e-06 | Alpha: 0.500\n",
      "  Batch 620/4000 | Loss: 0.9856 | LR: 8.89e-06 | Alpha: 0.500\n",
      "  Batch 640/4000 | Loss: 1.0003 | LR: 8.84e-06 | Alpha: 0.500\n",
      "  Batch 660/4000 | Loss: 0.9989 | LR: 8.79e-06 | Alpha: 0.500\n",
      "  Batch 680/4000 | Loss: 0.9970 | LR: 8.74e-06 | Alpha: 0.500\n",
      "  Batch 700/4000 | Loss: 0.9862 | LR: 8.68e-06 | Alpha: 0.500\n",
      "  Batch 720/4000 | Loss: 0.9850 | LR: 8.63e-06 | Alpha: 0.500\n",
      "  Batch 740/4000 | Loss: 0.9872 | LR: 8.58e-06 | Alpha: 0.500\n",
      "  Batch 760/4000 | Loss: 0.9856 | LR: 8.53e-06 | Alpha: 0.500\n",
      "  Batch 780/4000 | Loss: 0.9831 | LR: 8.47e-06 | Alpha: 0.500\n",
      "  Batch 800/4000 | Loss: 0.9779 | LR: 8.42e-06 | Alpha: 0.500\n",
      "  Batch 820/4000 | Loss: 0.9856 | LR: 8.37e-06 | Alpha: 0.500\n",
      "  Batch 840/4000 | Loss: 0.9790 | LR: 8.32e-06 | Alpha: 0.500\n",
      "  Batch 860/4000 | Loss: 0.9764 | LR: 8.26e-06 | Alpha: 0.500\n",
      "  Batch 880/4000 | Loss: 0.9754 | LR: 8.21e-06 | Alpha: 0.500\n",
      "  Batch 900/4000 | Loss: 0.9746 | LR: 8.16e-06 | Alpha: 0.500\n",
      "  Batch 920/4000 | Loss: 0.9697 | LR: 8.11e-06 | Alpha: 0.500\n",
      "  Batch 940/4000 | Loss: 0.9653 | LR: 8.05e-06 | Alpha: 0.500\n",
      "  Batch 960/4000 | Loss: 0.9612 | LR: 8.00e-06 | Alpha: 0.500\n",
      "  Batch 980/4000 | Loss: 0.9562 | LR: 7.95e-06 | Alpha: 0.500\n",
      "  Batch 1000/4000 | Loss: 0.9574 | LR: 7.89e-06 | Alpha: 0.500\n",
      "  Batch 1020/4000 | Loss: 0.9538 | LR: 7.84e-06 | Alpha: 0.500\n",
      "  Batch 1040/4000 | Loss: 0.9564 | LR: 7.79e-06 | Alpha: 0.500\n",
      "  Batch 1060/4000 | Loss: 0.9600 | LR: 7.74e-06 | Alpha: 0.500\n",
      "  Batch 1080/4000 | Loss: 0.9579 | LR: 7.68e-06 | Alpha: 0.500\n",
      "  Batch 1100/4000 | Loss: 0.9518 | LR: 7.63e-06 | Alpha: 0.500\n",
      "  Batch 1120/4000 | Loss: 0.9476 | LR: 7.58e-06 | Alpha: 0.500\n",
      "  Batch 1140/4000 | Loss: 0.9464 | LR: 7.53e-06 | Alpha: 0.500\n",
      "  Batch 1160/4000 | Loss: 0.9452 | LR: 7.47e-06 | Alpha: 0.500\n",
      "  Batch 1180/4000 | Loss: 0.9400 | LR: 7.42e-06 | Alpha: 0.500\n",
      "  Batch 1200/4000 | Loss: 0.9370 | LR: 7.37e-06 | Alpha: 0.500\n",
      "  Batch 1220/4000 | Loss: 0.9416 | LR: 7.32e-06 | Alpha: 0.500\n",
      "  Batch 1240/4000 | Loss: 0.9415 | LR: 7.26e-06 | Alpha: 0.500\n",
      "  Batch 1260/4000 | Loss: 0.9462 | LR: 7.21e-06 | Alpha: 0.500\n",
      "  Batch 1280/4000 | Loss: 0.9420 | LR: 7.16e-06 | Alpha: 0.500\n",
      "  Batch 1300/4000 | Loss: 0.9423 | LR: 7.11e-06 | Alpha: 0.500\n",
      "  Batch 1320/4000 | Loss: 0.9407 | LR: 7.05e-06 | Alpha: 0.500\n",
      "  Batch 1340/4000 | Loss: 0.9372 | LR: 7.00e-06 | Alpha: 0.500\n",
      "  Batch 1360/4000 | Loss: 0.9401 | LR: 6.95e-06 | Alpha: 0.500\n",
      "  Batch 1380/4000 | Loss: 0.9392 | LR: 6.89e-06 | Alpha: 0.500\n",
      "  Batch 1400/4000 | Loss: 0.9351 | LR: 6.84e-06 | Alpha: 0.500\n",
      "  Batch 1420/4000 | Loss: 0.9350 | LR: 6.79e-06 | Alpha: 0.500\n",
      "  Batch 1440/4000 | Loss: 0.9343 | LR: 6.74e-06 | Alpha: 0.500\n",
      "  Batch 1460/4000 | Loss: 0.9324 | LR: 6.68e-06 | Alpha: 0.500\n",
      "  Batch 1480/4000 | Loss: 0.9291 | LR: 6.63e-06 | Alpha: 0.500\n",
      "  Batch 1500/4000 | Loss: 0.9253 | LR: 6.58e-06 | Alpha: 0.500\n",
      "  Batch 1520/4000 | Loss: 0.9341 | LR: 6.53e-06 | Alpha: 0.500\n",
      "  Batch 1540/4000 | Loss: 0.9321 | LR: 6.47e-06 | Alpha: 0.500\n",
      "  Batch 1560/4000 | Loss: 0.9292 | LR: 6.42e-06 | Alpha: 0.500\n",
      "  Batch 1580/4000 | Loss: 0.9248 | LR: 6.37e-06 | Alpha: 0.500\n",
      "  Batch 1600/4000 | Loss: 0.9219 | LR: 6.32e-06 | Alpha: 0.500\n",
      "  Batch 1620/4000 | Loss: 0.9179 | LR: 6.26e-06 | Alpha: 0.500\n",
      "  Batch 1640/4000 | Loss: 0.9136 | LR: 6.21e-06 | Alpha: 0.500\n",
      "  Batch 1660/4000 | Loss: 0.9112 | LR: 6.16e-06 | Alpha: 0.500\n",
      "  Batch 1680/4000 | Loss: 0.9107 | LR: 6.11e-06 | Alpha: 0.500\n",
      "  Batch 1700/4000 | Loss: 0.9075 | LR: 6.05e-06 | Alpha: 0.500\n",
      "  Batch 1720/4000 | Loss: 0.9076 | LR: 6.00e-06 | Alpha: 0.500\n",
      "  Batch 1740/4000 | Loss: 0.9097 | LR: 5.95e-06 | Alpha: 0.500\n",
      "  Batch 1760/4000 | Loss: 0.9042 | LR: 5.89e-06 | Alpha: 0.500\n",
      "  Batch 1780/4000 | Loss: 0.9108 | LR: 5.84e-06 | Alpha: 0.500\n",
      "  Batch 1800/4000 | Loss: 0.9111 | LR: 5.79e-06 | Alpha: 0.500\n",
      "  Batch 1820/4000 | Loss: 0.9088 | LR: 5.74e-06 | Alpha: 0.500\n",
      "  Batch 1840/4000 | Loss: 0.9136 | LR: 5.68e-06 | Alpha: 0.500\n",
      "  Batch 1860/4000 | Loss: 0.9138 | LR: 5.63e-06 | Alpha: 0.500\n",
      "  Batch 1880/4000 | Loss: 0.9144 | LR: 5.58e-06 | Alpha: 0.500\n",
      "  Batch 1900/4000 | Loss: 0.9135 | LR: 5.53e-06 | Alpha: 0.500\n",
      "  Batch 1920/4000 | Loss: 0.9093 | LR: 5.47e-06 | Alpha: 0.500\n",
      "  Batch 1940/4000 | Loss: 0.9067 | LR: 5.42e-06 | Alpha: 0.500\n",
      "  Batch 1960/4000 | Loss: 0.9089 | LR: 5.37e-06 | Alpha: 0.500\n",
      "  Batch 1980/4000 | Loss: 0.9102 | LR: 5.32e-06 | Alpha: 0.500\n",
      "  Batch 2000/4000 | Loss: 0.9068 | LR: 5.26e-06 | Alpha: 0.500\n",
      "  Batch 2020/4000 | Loss: 0.9054 | LR: 5.21e-06 | Alpha: 0.500\n",
      "  Batch 2040/4000 | Loss: 0.9017 | LR: 5.16e-06 | Alpha: 0.500\n",
      "  Batch 2060/4000 | Loss: 0.8994 | LR: 5.11e-06 | Alpha: 0.500\n",
      "  Batch 2080/4000 | Loss: 0.9025 | LR: 5.05e-06 | Alpha: 0.500\n",
      "  Batch 2100/4000 | Loss: 0.9021 | LR: 5.00e-06 | Alpha: 0.500\n",
      "  Batch 2120/4000 | Loss: 0.9044 | LR: 4.95e-06 | Alpha: 0.500\n",
      "  Batch 2140/4000 | Loss: 0.9023 | LR: 4.89e-06 | Alpha: 0.500\n",
      "  Batch 2160/4000 | Loss: 0.9045 | LR: 4.84e-06 | Alpha: 0.500\n",
      "  Batch 2180/4000 | Loss: 0.9022 | LR: 4.79e-06 | Alpha: 0.500\n",
      "  Batch 2200/4000 | Loss: 0.9044 | LR: 4.74e-06 | Alpha: 0.500\n",
      "  Batch 2220/4000 | Loss: 0.9042 | LR: 4.68e-06 | Alpha: 0.500\n",
      "  Batch 2240/4000 | Loss: 0.9003 | LR: 4.63e-06 | Alpha: 0.500\n",
      "  Batch 2260/4000 | Loss: 0.8971 | LR: 4.58e-06 | Alpha: 0.500\n",
      "  Batch 2280/4000 | Loss: 0.8984 | LR: 4.53e-06 | Alpha: 0.500\n",
      "  Batch 2300/4000 | Loss: 0.8969 | LR: 4.47e-06 | Alpha: 0.500\n",
      "  Batch 2320/4000 | Loss: 0.8941 | LR: 4.42e-06 | Alpha: 0.500\n",
      "  Batch 2340/4000 | Loss: 0.8932 | LR: 4.37e-06 | Alpha: 0.500\n",
      "  Batch 2360/4000 | Loss: 0.8912 | LR: 4.32e-06 | Alpha: 0.500\n",
      "  Batch 2380/4000 | Loss: 0.8884 | LR: 4.26e-06 | Alpha: 0.500\n",
      "  Batch 2400/4000 | Loss: 0.8888 | LR: 4.21e-06 | Alpha: 0.500\n",
      "  Batch 2420/4000 | Loss: 0.8874 | LR: 4.16e-06 | Alpha: 0.500\n",
      "  Batch 2440/4000 | Loss: 0.8885 | LR: 4.11e-06 | Alpha: 0.500\n",
      "  Batch 2460/4000 | Loss: 0.8892 | LR: 4.05e-06 | Alpha: 0.500\n",
      "  Batch 2480/4000 | Loss: 0.8892 | LR: 4.00e-06 | Alpha: 0.500\n",
      "  Batch 2500/4000 | Loss: 0.8878 | LR: 3.95e-06 | Alpha: 0.500\n",
      "  Batch 2520/4000 | Loss: 0.8855 | LR: 3.89e-06 | Alpha: 0.500\n",
      "  Batch 2540/4000 | Loss: 0.8854 | LR: 3.84e-06 | Alpha: 0.500\n",
      "  Batch 2560/4000 | Loss: 0.8833 | LR: 3.79e-06 | Alpha: 0.500\n",
      "  Batch 2580/4000 | Loss: 0.8836 | LR: 3.74e-06 | Alpha: 0.500\n",
      "  Batch 2600/4000 | Loss: 0.8832 | LR: 3.68e-06 | Alpha: 0.500\n",
      "  Batch 2620/4000 | Loss: 0.8819 | LR: 3.63e-06 | Alpha: 0.500\n",
      "  Batch 2640/4000 | Loss: 0.8815 | LR: 3.58e-06 | Alpha: 0.500\n",
      "  Batch 2660/4000 | Loss: 0.8829 | LR: 3.53e-06 | Alpha: 0.500\n",
      "  Batch 2680/4000 | Loss: 0.8841 | LR: 3.47e-06 | Alpha: 0.500\n",
      "  Batch 2700/4000 | Loss: 0.8828 | LR: 3.42e-06 | Alpha: 0.500\n",
      "  Batch 2720/4000 | Loss: 0.8821 | LR: 3.37e-06 | Alpha: 0.500\n",
      "  Batch 2740/4000 | Loss: 0.8818 | LR: 3.32e-06 | Alpha: 0.500\n",
      "  Batch 2760/4000 | Loss: 0.8775 | LR: 3.26e-06 | Alpha: 0.500\n",
      "  Batch 2780/4000 | Loss: 0.8778 | LR: 3.21e-06 | Alpha: 0.500\n",
      "  Batch 2800/4000 | Loss: 0.8772 | LR: 3.16e-06 | Alpha: 0.500\n",
      "  Batch 2820/4000 | Loss: 0.8781 | LR: 3.11e-06 | Alpha: 0.500\n",
      "  Batch 2840/4000 | Loss: 0.8771 | LR: 3.05e-06 | Alpha: 0.500\n",
      "  Batch 2860/4000 | Loss: 0.8785 | LR: 3.00e-06 | Alpha: 0.500\n",
      "  Batch 2880/4000 | Loss: 0.8799 | LR: 2.95e-06 | Alpha: 0.500\n",
      "  Batch 2900/4000 | Loss: 0.8799 | LR: 2.89e-06 | Alpha: 0.500\n",
      "  Batch 2920/4000 | Loss: 0.8787 | LR: 2.84e-06 | Alpha: 0.500\n",
      "  Batch 2940/4000 | Loss: 0.8781 | LR: 2.79e-06 | Alpha: 0.500\n",
      "  Batch 2960/4000 | Loss: 0.8757 | LR: 2.74e-06 | Alpha: 0.500\n",
      "  Batch 2980/4000 | Loss: 0.8742 | LR: 2.68e-06 | Alpha: 0.500\n",
      "  Batch 3000/4000 | Loss: 0.8714 | LR: 2.63e-06 | Alpha: 0.500\n",
      "  Batch 3020/4000 | Loss: 0.8690 | LR: 2.58e-06 | Alpha: 0.500\n",
      "  Batch 3040/4000 | Loss: 0.8692 | LR: 2.53e-06 | Alpha: 0.500\n",
      "  Batch 3060/4000 | Loss: 0.8671 | LR: 2.47e-06 | Alpha: 0.500\n",
      "  Batch 3080/4000 | Loss: 0.8673 | LR: 2.42e-06 | Alpha: 0.500\n",
      "  Batch 3100/4000 | Loss: 0.8646 | LR: 2.37e-06 | Alpha: 0.500\n",
      "  Batch 3120/4000 | Loss: 0.8664 | LR: 2.32e-06 | Alpha: 0.500\n",
      "  Batch 3140/4000 | Loss: 0.8651 | LR: 2.26e-06 | Alpha: 0.500\n",
      "  Batch 3160/4000 | Loss: 0.8638 | LR: 2.21e-06 | Alpha: 0.500\n",
      "  Batch 3180/4000 | Loss: 0.8639 | LR: 2.16e-06 | Alpha: 0.500\n",
      "  Batch 3200/4000 | Loss: 0.8638 | LR: 2.11e-06 | Alpha: 0.500\n",
      "  Batch 3220/4000 | Loss: 0.8632 | LR: 2.05e-06 | Alpha: 0.500\n",
      "  Batch 3240/4000 | Loss: 0.8612 | LR: 2.00e-06 | Alpha: 0.500\n",
      "  Batch 3260/4000 | Loss: 0.8603 | LR: 1.95e-06 | Alpha: 0.500\n",
      "  Batch 3280/4000 | Loss: 0.8609 | LR: 1.89e-06 | Alpha: 0.500\n",
      "  Batch 3300/4000 | Loss: 0.8587 | LR: 1.84e-06 | Alpha: 0.500\n",
      "  Batch 3320/4000 | Loss: 0.8557 | LR: 1.79e-06 | Alpha: 0.500\n",
      "  Batch 3340/4000 | Loss: 0.8529 | LR: 1.74e-06 | Alpha: 0.500\n",
      "  Batch 3360/4000 | Loss: 0.8519 | LR: 1.68e-06 | Alpha: 0.500\n",
      "  Batch 3380/4000 | Loss: 0.8504 | LR: 1.63e-06 | Alpha: 0.500\n",
      "  Batch 3400/4000 | Loss: 0.8517 | LR: 1.58e-06 | Alpha: 0.500\n",
      "  Batch 3420/4000 | Loss: 0.8495 | LR: 1.53e-06 | Alpha: 0.500\n",
      "  Batch 3440/4000 | Loss: 0.8486 | LR: 1.47e-06 | Alpha: 0.500\n",
      "  Batch 3460/4000 | Loss: 0.8470 | LR: 1.42e-06 | Alpha: 0.500\n",
      "  Batch 3480/4000 | Loss: 0.8461 | LR: 1.37e-06 | Alpha: 0.500\n",
      "  Batch 3500/4000 | Loss: 0.8453 | LR: 1.32e-06 | Alpha: 0.500\n",
      "  Batch 3520/4000 | Loss: 0.8441 | LR: 1.26e-06 | Alpha: 0.500\n",
      "  Batch 3540/4000 | Loss: 0.8439 | LR: 1.21e-06 | Alpha: 0.500\n",
      "  Batch 3560/4000 | Loss: 0.8418 | LR: 1.16e-06 | Alpha: 0.500\n",
      "  Batch 3580/4000 | Loss: 0.8426 | LR: 1.11e-06 | Alpha: 0.500\n",
      "  Batch 3600/4000 | Loss: 0.8393 | LR: 1.05e-06 | Alpha: 0.500\n",
      "  Batch 3620/4000 | Loss: 0.8383 | LR: 1.00e-06 | Alpha: 0.500\n",
      "  Batch 3640/4000 | Loss: 0.8389 | LR: 9.47e-07 | Alpha: 0.500\n",
      "  Batch 3660/4000 | Loss: 0.8391 | LR: 8.95e-07 | Alpha: 0.500\n",
      "  Batch 3680/4000 | Loss: 0.8396 | LR: 8.42e-07 | Alpha: 0.500\n",
      "  Batch 3700/4000 | Loss: 0.8376 | LR: 7.89e-07 | Alpha: 0.500\n",
      "  Batch 3720/4000 | Loss: 0.8364 | LR: 7.37e-07 | Alpha: 0.500\n",
      "  Batch 3740/4000 | Loss: 0.8356 | LR: 6.84e-07 | Alpha: 0.500\n",
      "  Batch 3760/4000 | Loss: 0.8351 | LR: 6.32e-07 | Alpha: 0.500\n",
      "  Batch 3780/4000 | Loss: 0.8333 | LR: 5.79e-07 | Alpha: 0.500\n",
      "  Batch 3800/4000 | Loss: 0.8321 | LR: 5.26e-07 | Alpha: 0.500\n",
      "  Batch 3820/4000 | Loss: 0.8314 | LR: 4.74e-07 | Alpha: 0.500\n",
      "  Batch 3840/4000 | Loss: 0.8303 | LR: 4.21e-07 | Alpha: 0.500\n",
      "  Batch 3860/4000 | Loss: 0.8294 | LR: 3.68e-07 | Alpha: 0.500\n",
      "  Batch 3880/4000 | Loss: 0.8286 | LR: 3.16e-07 | Alpha: 0.500\n",
      "  Batch 3900/4000 | Loss: 0.8303 | LR: 2.63e-07 | Alpha: 0.500\n",
      "  Batch 3920/4000 | Loss: 0.8281 | LR: 2.11e-07 | Alpha: 0.500\n",
      "  Batch 3940/4000 | Loss: 0.8262 | LR: 1.58e-07 | Alpha: 0.500\n",
      "  Batch 3960/4000 | Loss: 0.8268 | LR: 1.05e-07 | Alpha: 0.500\n",
      "  Batch 3980/4000 | Loss: 0.8272 | LR: 5.26e-08 | Alpha: 0.500\n",
      "  Batch 4000/4000 | Loss: 0.8261 | LR: 0.00e+00 | Alpha: 0.500\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 0.8261\n",
      "  Val Loss: 1.7036\n",
      "  Sentiment Accuracy: 0.7630\n",
      "  Emotion Accuracy: 0.1760\n",
      "  Combined Score: 0.4695\n",
      "  Alpha: 0.500\n",
      "  Learning Rate: 0.00e+00\n",
      "Model saved to ./multitask_model_optimized\\checkpoint-epoch-2\n",
      "Model saved to ./multitask_model_optimized\\best_model\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.4695\n",
      "✅ Training completed!\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.58 GB\n",
      "  Cached: 6.02 GB\n",
      "\n",
      "7️⃣ Evaluating...\n",
      "Model saved to ./multitask_model_optimized\\final_model\n",
      "✅ Model and encoders saved to: ./multitask_model_optimized\\final_model\n",
      "\n",
      "📈 Results:\n",
      "Sentiment Accuracy: 0.5895\n",
      "Emotion Accuracy: 0.1474\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 5.58 GB\n",
      "  Cached: 6.02 GB\n",
      "\n",
      "📋 Best hyperparameters found:\n",
      "  learning_rate: 2.858051065806938e-05\n",
      "  batch_size: 2\n",
      "  alpha: 0.5369658275448169\n",
      "  hidden_dropout_prob: 0.061612603179999434\n",
      "  classifier_dropout: 0.28226345557043153\n",
      "  weight_decay: 0.017881888245041864\n",
      "  warmup_ratio: 0.05975773894779193\n",
      "  num_epochs: 5\n",
      "  max_length: 128\n",
      "\n",
      "📈 Final optimized results:\n",
      "Sentiment Accuracy: 0.5895\n",
      "Emotion Accuracy: 0.1474\n",
      "\n",
      "Optimization Complete!\n",
      "==================================================\n",
      "📊 Before vs After Optimization:\n",
      "Original  - Sentiment: 0.5579, Emotion: 0.0842\n",
      "Optimized - Sentiment: 0.5895, Emotion: 0.1474\n"
     ]
    }
   ],
   "source": [
    "# Cell: Run Hyperparameter Tuning\n",
    "print(\"Starting Hyperparameter Optimization...\")\n",
    "\n",
    "# Run tuning (this will take some time - each trial trains a model)\n",
    "model_optimized, results_optimized, study = run_hyperparameter_tuning_fixed(\n",
    "    reddit_data_path=\"annotated_reddit_posts.csv\",\n",
    "    n_trials=15,  # Adjust based on how much time you want to spend\n",
    "    model_name=\"microsoft/deberta-base\"\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 Before vs After Optimization:\")\n",
    "print(f\"Original  - Sentiment: 0.5579, Emotion: 0.0842\")\n",
    "print(f\"Optimized - Sentiment: {results_optimized['sentiment']['accuracy']:.4f}, Emotion: {results_optimized['emotion']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87df374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Optimization completed!\n",
      "Best trial: 2\n",
      "Best combined score: 0.4263\n",
      "Best parameters:\n",
      "  learning_rate: 2.858051065806938e-05\n",
      "  batch_size: 2\n",
      "  alpha: 0.5369658275448169\n",
      "  hidden_dropout_prob: 0.061612603179999434\n",
      "  classifier_dropout: 0.28226345557043153\n",
      "  weight_decay: 0.017881888245041864\n",
      "  warmup_ratio: 0.05975773894779193\n",
      "  num_epochs: 5\n",
      "  max_length: 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n🏆 Optimization completed!\")\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best combined score: {study.best_value:.4f}\")\n",
    "print(f\"Best parameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c310f",
   "metadata": {},
   "source": [
    "#### EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d04bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed predictor class created!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Create Fixed Predictor Class\n",
    "class FixedMultiTaskPredictor:\n",
    "    \"\"\"\n",
    "    Fixed inference class for multitask model - handles token_type_ids issue\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        sentiment_encoder_path: str,\n",
    "        emotion_encoder_path: str,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = MultiTaskTransformer.from_pretrained(model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load label encoders\n",
    "        import joblib\n",
    "        self.sentiment_encoder = joblib.load(sentiment_encoder_path)\n",
    "        self.emotion_encoder = joblib.load(emotion_encoder_path)\n",
    "        \n",
    "        print(f\"✅ Fixed Model loaded successfully!\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Sentiment classes: {list(self.sentiment_encoder.classes_)}\")\n",
    "        print(f\"Emotion classes: {list(self.emotion_encoder.classes_)}\")\n",
    "    \n",
    "    def predict_batch(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        batch_size: int = 32,\n",
    "        return_probabilities: bool = False\n",
    "    ) -> List[Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        Predict sentiment and emotion for a batch of texts - FIXED VERSION\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            # FIXED: Only pass the required inputs, filter out token_type_ids\n",
    "            model_inputs = {\n",
    "                'input_ids': inputs['input_ids'].to(self.device),\n",
    "                'attention_mask': inputs['attention_mask'].to(self.device)\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**model_inputs)  # Only pass filtered inputs\n",
    "                \n",
    "                sentiment_logits = outputs['sentiment_logits']\n",
    "                emotion_logits = outputs['emotion_logits']\n",
    "                \n",
    "                sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "                emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "                emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                # Process each item in batch\n",
    "                for j in range(len(batch_texts)):\n",
    "                    sentiment_pred_id = sentiment_preds[j].item()\n",
    "                    emotion_pred_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    sentiment_label = self.sentiment_encoder.inverse_transform([sentiment_pred_id])[0]\n",
    "                    emotion_label = self.emotion_encoder.inverse_transform([emotion_pred_id])[0]\n",
    "                    \n",
    "                    result = {\n",
    "                        'text': batch_texts[j],\n",
    "                        'sentiment': {\n",
    "                            'label': sentiment_label,\n",
    "                            'confidence': sentiment_probs[j][sentiment_pred_id].item(),\n",
    "                            'class_id': sentiment_pred_id\n",
    "                        },\n",
    "                        'emotion': {\n",
    "                            'label': emotion_label,\n",
    "                            'confidence': emotion_probs[j][emotion_pred_id].item(),\n",
    "                            'class_id': emotion_pred_id\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    if return_probabilities:\n",
    "                        result['sentiment']['probabilities'] = {\n",
    "                            class_name: prob.item() for class_name, prob in \n",
    "                            zip(self.sentiment_encoder.classes_, sentiment_probs[j])\n",
    "                        }\n",
    "                        result['emotion']['probabilities'] = {\n",
    "                            class_name: prob.item() for class_name, prob in \n",
    "                            zip(self.emotion_encoder.classes_, emotion_probs[j])\n",
    "                        }\n",
    "                    \n",
    "                    results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"✅ Fixed predictor class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa1ea9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Run Evaluation with Fixed Predictor\n",
    "def evaluate_model_final():\n",
    "    \"\"\"\n",
    "    Final evaluation using the fixed predictor\n",
    "    \"\"\"\n",
    "    print(\"🚀 Final Evaluation with Fixed Predictor\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Model paths\n",
    "    model_path = \"multitask_model_optimized/final_model\"\n",
    "    sentiment_encoder_path = \"multitask_model_optimized/final_model/sentiment_encoder.pkl\"\n",
    "    emotion_encoder_path = \"multitask_model_optimized/final_model/emotion_encoder.pkl\"\n",
    "    data_path = \"annotated_reddit_posts.csv\"\n",
    "    \n",
    "    # Load the model with FIXED predictor\n",
    "    print(\"📥 Loading model with fixed predictor...\")\n",
    "    predictor = FixedMultiTaskPredictor(  # Using the new class!\n",
    "        model_path=model_path,\n",
    "        sentiment_encoder_path=sentiment_encoder_path,\n",
    "        emotion_encoder_path=emotion_encoder_path\n",
    "    )\n",
    "    \n",
    "    # Load the annotated data\n",
    "    print(\"📥 Loading annotated data...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = df['text_content'].tolist()\n",
    "    true_sentiments = df['sentiment'].tolist()\n",
    "    true_emotions = df['emotion'].tolist()\n",
    "    \n",
    "    print(f\"📊 Data loaded: {len(texts)} samples\")\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"\\n🔮 Running inference...\")\n",
    "    predictions = predictor.predict_batch(texts, batch_size=8)\n",
    "    \n",
    "    # Extract predictions\n",
    "    pred_sentiments = [pred['sentiment']['label'] for pred in predictions]\n",
    "    pred_emotions = [pred['emotion']['label'] for pred in predictions]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\n📊 Calculating metrics...\")\n",
    "    sentiment_accuracy = accuracy_score(true_sentiments, pred_sentiments)\n",
    "    sentiment_f1_macro = f1_score(true_sentiments, pred_sentiments, average='macro', zero_division=0)\n",
    "    \n",
    "    emotion_accuracy = accuracy_score(true_emotions, pred_emotions)\n",
    "    emotion_f1_macro = f1_score(true_emotions, pred_emotions, average='macro', zero_division=0)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n🎯 FINAL EVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📊 SENTIMENT CLASSIFICATION:\")\n",
    "    print(f\"   Accuracy:    {sentiment_accuracy:.4f} ({sentiment_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Macro F1:    {sentiment_f1_macro:.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 EMOTION CLASSIFICATION:\")\n",
    "    print(f\"   Accuracy:    {emotion_accuracy:.4f} ({emotion_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Macro F1:    {emotion_f1_macro:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🏆 COMBINED PERFORMANCE:\")\n",
    "    print(f\"   Average Accuracy: {(sentiment_accuracy + emotion_accuracy)/2:.4f}\")\n",
    "    print(f\"   Average Macro F1: {(sentiment_f1_macro + emotion_f1_macro)/2:.4f}\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(f\"\\n📝 EXAMPLE PREDICTIONS:\")\n",
    "    for i in range(min(5, len(predictions))):\n",
    "        correct_sent = \"✅\" if pred_sentiments[i] == true_sentiments[i] else \"❌\"\n",
    "        correct_emot = \"✅\" if pred_emotions[i] == true_emotions[i] else \"❌\"\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Text: {texts[i][:80]}...\")\n",
    "        print(f\"  True:  Sentiment={true_sentiments[i]}, Emotion={true_emotions[i]}\")\n",
    "        print(f\"  Pred:  Sentiment={pred_sentiments[i]} {correct_sent}, Emotion={pred_emotions[i]} {correct_emot}\")\n",
    "        print(f\"  Conf:  Sentiment={predictions[i]['sentiment']['confidence']:.3f}, Emotion={predictions[i]['emotion']['confidence']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'sentiment_accuracy': sentiment_accuracy,\n",
    "        'sentiment_f1_macro': sentiment_f1_macro,\n",
    "        'emotion_accuracy': emotion_accuracy,\n",
    "        'emotion_f1_macro': emotion_f1_macro,\n",
    "        'predictions': predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30df5480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Running final evaluation...\n",
      "🚀 Final Evaluation with Fixed Predictor\n",
      "==================================================\n",
      "📥 Loading model with fixed predictor...\n",
      "Model loaded from multitask_model_optimized/final_model\n",
      "✅ Fixed Model loaded successfully!\n",
      "Device: cuda\n",
      "Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "📥 Loading annotated data...\n",
      "📊 Data loaded: 95 samples\n",
      "\n",
      "🔮 Running inference...\n",
      "\n",
      "📊 Calculating metrics...\n",
      "\n",
      "🎯 FINAL EVALUATION RESULTS\n",
      "==================================================\n",
      "📊 SENTIMENT CLASSIFICATION:\n",
      "   Accuracy:    0.5895 (58.95%)\n",
      "   Macro F1:    0.3723\n",
      "\n",
      "😊 EMOTION CLASSIFICATION:\n",
      "   Accuracy:    0.1474 (14.74%)\n",
      "   Macro F1:    0.1260\n",
      "\n",
      "🏆 COMBINED PERFORMANCE:\n",
      "   Average Accuracy: 0.3684\n",
      "   Average Macro F1: 0.2492\n",
      "\n",
      "📝 EXAMPLE PREDICTIONS:\n",
      "\n",
      "Sample 1:\n",
      "  Text: ya screw username really looking forward note 7 iphone 6plus issue screen userna...\n",
      "  True:  Sentiment=Negative, Emotion=Sadness\n",
      "  Pred:  Sentiment=Negative ✅, Emotion=Surprise ❌\n",
      "  Conf:  Sentiment=0.896, Emotion=0.356\n",
      "\n",
      "Sample 2:\n",
      "  Text: username samsung galaxy note7 still more user than lg v20 htc bolt oneplus 3t...\n",
      "  True:  Sentiment=Neutral, Emotion=Surprise\n",
      "  Pred:  Sentiment=Positive ❌, Emotion=Anger ❌\n",
      "  Conf:  Sentiment=0.971, Emotion=0.598\n",
      "\n",
      "Sample 3:\n",
      "  Text: traded note 7 s7 edge really hope samsung username good deal note 8 u username f...\n",
      "  True:  Sentiment=Negative, Emotion=Sadness\n",
      "  Pred:  Sentiment=Negative ✅, Emotion=Surprise ❌\n",
      "  Conf:  Sentiment=0.791, Emotion=0.556\n",
      "\n",
      "Sample 4:\n",
      "  Text: reading username report battery design failed glad didnt hang safety reason but ...\n",
      "  True:  Sentiment=Positive, Emotion=Sadness\n",
      "  Pred:  Sentiment=Negative ❌, Emotion=Surprise ❌\n",
      "  Conf:  Sentiment=0.999, Emotion=0.394\n",
      "\n",
      "Sample 5:\n",
      "  Text: maybe phone unique username feature explode username some user cry...\n",
      "  True:  Sentiment=Negative, Emotion=Anger\n",
      "  Pred:  Sentiment=Negative ✅, Emotion=Sadness ❌\n",
      "  Conf:  Sentiment=0.953, Emotion=0.238\n"
     ]
    }
   ],
   "source": [
    "# Run the final evaluation\n",
    "print(\"🔧 Running final evaluation...\")\n",
    "results = evaluate_model_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9d3b8",
   "metadata": {},
   "source": [
    "## BERTweet Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31fc6b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed MultiTaskEvaluator defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Fixed MultiTaskEvaluator for BERTweet\n",
    "class FixedMultiTaskEvaluator:\n",
    "    \"\"\"\n",
    "    Fixed evaluation class that handles different max_length values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: MultiTaskTransformer,\n",
    "        tokenizer,\n",
    "        sentiment_encoder: LabelEncoder,\n",
    "        emotion_encoder: LabelEncoder,\n",
    "        device: torch.device,\n",
    "        max_length: int = 128  # Make it configurable\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentiment_encoder = sentiment_encoder\n",
    "        self.emotion_encoder = emotion_encoder\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def evaluate_dataset(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        batch_size: int = 32\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Evaluate model on a dataset with fixed max_length\n",
    "        \"\"\"\n",
    "        dataset = MultiTaskDataset(\n",
    "            texts=texts,\n",
    "            sentiment_labels=sentiment_labels,\n",
    "            emotion_labels=emotion_labels,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,  # Use the same max_length as training\n",
    "            sentiment_label_encoder=self.sentiment_encoder,\n",
    "            emotion_label_encoder=self.emotion_encoder\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_true_labels = []\n",
    "        emotion_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Only pass the required inputs to avoid token_type_ids issue\n",
    "                model_inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(self.device),\n",
    "                    'attention_mask': batch['attention_mask'].to(self.device)\n",
    "                }\n",
    "                \n",
    "                outputs = self.model(**model_inputs)\n",
    "                \n",
    "                # Get predictions\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                # Store results\n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_true_labels.extend(batch['sentiment_labels'].numpy())\n",
    "                emotion_true_labels.extend(batch['emotion_labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = self._calculate_metrics(\n",
    "            sentiment_predictions=sentiment_predictions,\n",
    "            emotion_predictions=emotion_predictions,\n",
    "            sentiment_true_labels=sentiment_true_labels,\n",
    "            emotion_true_labels=emotion_true_labels\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_metrics(\n",
    "        self,\n",
    "        sentiment_predictions: List[int],\n",
    "        emotion_predictions: List[int],\n",
    "        sentiment_true_labels: List[int],\n",
    "        emotion_true_labels: List[int]\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"Calculate simplified metrics: only accuracy and macro F1\"\"\"\n",
    "        \n",
    "        # Sentiment metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_true_labels, sentiment_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_true_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        # Emotion metrics\n",
    "        emotion_accuracy = accuracy_score(emotion_true_labels, emotion_predictions)\n",
    "        emotion_f1_macro = f1_score(emotion_true_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment': {\n",
    "                'accuracy': sentiment_accuracy,\n",
    "                'f1_macro': sentiment_f1_macro,\n",
    "                'predictions': sentiment_predictions,\n",
    "                'true_labels': sentiment_true_labels\n",
    "            },\n",
    "            'emotion': {\n",
    "                'accuracy': emotion_accuracy,\n",
    "                'f1_macro': emotion_f1_macro,\n",
    "                'predictions': emotion_predictions,\n",
    "                'true_labels': emotion_true_labels\n",
    "            },\n",
    "            'combined': {\n",
    "                'average_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "                'average_f1': (sentiment_f1_macro + emotion_f1_macro) / 2\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"✅ Fixed MultiTaskEvaluator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0001a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed ultra-lightweight training function ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Fixed BERTweet Training Function\n",
    "def run_ultra_lightweight_training_fixed(\n",
    "    reddit_data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    model_name: str = \"vinai/bertweet-base\",\n",
    "    output_dir: str = \"./bertweet_model_ultra_light\",\n",
    "    max_external_samples: int = 500\n",
    "):\n",
    "    \"\"\"\n",
    "    Fixed ultra-lightweight training for BERTweet\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Fixed Ultra-Lightweight Training\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Aggressive memory cleanup first\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n1️⃣ Loading minimal external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare very small external data\n",
    "    external_data_splits, sentiment_encoder, emotion_encoder = prepare_external_data_for_multitask(\n",
    "        sentiment_data, emotion_data, max_samples=max_external_samples\n",
    "    )\n",
    "    \n",
    "    # Load Reddit data\n",
    "    print(\"\\n2️⃣ Loading Reddit data...\")\n",
    "    reddit_df = pd.read_csv(reddit_data_path)\n",
    "    reddit_evaluation_data = prepare_reddit_data_for_evaluation(\n",
    "        reddit_df, sentiment_encoder, emotion_encoder\n",
    "    )\n",
    "    \n",
    "    # Ultra-lightweight config with consistent max_length\n",
    "    config = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir,\n",
    "        num_epochs=2,       # Minimal epochs\n",
    "        batch_size=1,       # Smallest possible batch\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.05,  # Minimal warmup\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        alpha=0.5,\n",
    "        adaptive_alpha=False,  # Disable to save memory\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_dropout_prob=0.1,\n",
    "        classifier_dropout=0.1,\n",
    "        max_length=128,     # Keep consistent max_length\n",
    "        save_total_limit=1  # Keep only 1 checkpoint\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n3️⃣ Ultra-lightweight config:\")\n",
    "    print(f\"  Model: {model_name}\")\n",
    "    print(f\"  Batch size: {config.batch_size}\")\n",
    "    print(f\"  Max length: {config.max_length}\")\n",
    "    print(f\"  Training samples: {len(external_data_splits['train']['texts'])}\")\n",
    "    print(f\"  Epochs: {config.num_epochs}\")\n",
    "    \n",
    "    # Clear memory before model\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Initialize trainer\n",
    "    print(f\"\\n4️⃣ Initializing trainer...\")\n",
    "    trainer = MultiTaskTrainer(\n",
    "        config=config,\n",
    "        sentiment_num_classes=len(sentiment_encoder.classes_),\n",
    "        emotion_num_classes=len(emotion_encoder.classes_)\n",
    "    )\n",
    "    \n",
    "    # Setup with gradient checkpointing\n",
    "    print(f\"\\n5️⃣ Setting up with memory optimizations...\")\n",
    "    trainer.setup(external_data_splits, sentiment_encoder, emotion_encoder)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    if hasattr(trainer.model.shared_encoder, 'gradient_checkpointing_enable'):\n",
    "        trainer.model.shared_encoder.gradient_checkpointing_enable()\n",
    "        print(\"✅ Gradient checkpointing enabled\")\n",
    "    \n",
    "    # Train with memory monitoring\n",
    "    print(f\"\\n6️⃣ Training with memory monitoring...\")\n",
    "    try:\n",
    "        history = trainer.train()\n",
    "        print(\"✅ Training completed!\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"❌ Still out of memory: {e}\")\n",
    "            print(\"💡 Try restarting kernel and using even smaller batch_size=1\")\n",
    "            return None, None\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Clear memory before evaluation\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    # Evaluate with FIXED evaluator\n",
    "    print(f\"\\n7️⃣ Evaluating with fixed evaluator...\")\n",
    "    evaluator = FixedMultiTaskEvaluator(  # Use the fixed evaluator\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.tokenizer,\n",
    "        sentiment_encoder=sentiment_encoder,\n",
    "        emotion_encoder=emotion_encoder,\n",
    "        device=device,\n",
    "        max_length=config.max_length  # Use the same max_length as training\n",
    "    )\n",
    "    \n",
    "    reddit_results = evaluator.evaluate_dataset(\n",
    "        texts=reddit_evaluation_data['texts'],\n",
    "        sentiment_labels=reddit_evaluation_data['sentiment_labels'],\n",
    "        emotion_labels=reddit_evaluation_data['emotion_labels'],\n",
    "        batch_size=1  # Ultra small batch for evaluation\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    save_model_and_encoders(\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.tokenizer,\n",
    "        sentiment_encoder=sentiment_encoder,\n",
    "        emotion_encoder=emotion_encoder,\n",
    "        output_dir=os.path.join(output_dir, 'final_model')\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n📈 Results:\")\n",
    "    print(f\"Sentiment Accuracy: {reddit_results['sentiment']['accuracy']:.4f}\")\n",
    "    print(f\"Emotion Accuracy: {reddit_results['emotion']['accuracy']:.4f}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    aggressive_memory_cleanup()\n",
    "    \n",
    "    return trainer.model, reddit_results\n",
    "\n",
    "print(\"✅ Fixed ultra-lightweight training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aafbe2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Restarting BERTweet Training with Fixed Function\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 2.16 GB\n",
      "  Cached: 2.92 GB\n",
      "🔥 Step 1: Fixed Ultra-Lightweight BERTweet Training\n",
      "--------------------------------------------------\n",
      "🚀 Starting Fixed Ultra-Lightweight Training\n",
      "==================================================\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 2.16 GB\n",
      "  Cached: 2.35 GB\n",
      "\n",
      "1️⃣ Loading minimal external datasets...\n",
      "Loading external datasets for training...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 val\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 val\n",
      "🔄 Preparing external datasets for multitask training...\n",
      "✅ External data prepared:\n",
      "  Train samples: 400\n",
      "  Validation samples: 100\n",
      "  Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "  Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "📈 Training set class distribution:\n",
      "  Sentiment 'Negative': 192 samples\n",
      "  Sentiment 'Neutral': 24 samples\n",
      "  Sentiment 'Positive': 184 samples\n",
      "  Emotion 'Anger': 129 samples\n",
      "  Emotion 'Fear': 69 samples\n",
      "  Emotion 'Joy': 50 samples\n",
      "  Emotion 'No Emotion': 58 samples\n",
      "  Emotion 'Sadness': 71 samples\n",
      "  Emotion 'Surprise': 23 samples\n",
      "\n",
      "2️⃣ Loading Reddit data...\n",
      "🔄 Preparing Reddit data for evaluation...\n",
      "✅ Reddit evaluation data prepared: 95 samples\n",
      "\n",
      "3️⃣ Ultra-lightweight config:\n",
      "  Model: vinai/bertweet-base\n",
      "  Batch size: 1\n",
      "  Max length: 128\n",
      "  Training samples: 400\n",
      "  Epochs: 2\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 2.16 GB\n",
      "  Cached: 2.35 GB\n",
      "\n",
      "4️⃣ Initializing trainer...\n",
      "\n",
      "5️⃣ Setting up with memory optimizations...\n",
      "✅ Setup complete!\n",
      "  Model: vinai/bertweet-base\n",
      "  Training samples: 400\n",
      "  Validation samples: 100\n",
      "  Training steps per epoch: 400\n",
      "  Total training steps: 800\n",
      "✅ Gradient checkpointing enabled\n",
      "\n",
      "6️⃣ Training with memory monitoring...\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.7064 | LR: 1.00e-05 | Alpha: 0.500\n",
      "  Batch 40/400 | Loss: 1.7924 | LR: 2.00e-05 | Alpha: 0.500\n",
      "  Batch 60/400 | Loss: 1.7242 | LR: 1.95e-05 | Alpha: 0.500\n",
      "  Batch 80/400 | Loss: 1.6665 | LR: 1.89e-05 | Alpha: 0.500\n",
      "  Batch 100/400 | Loss: 1.6861 | LR: 1.84e-05 | Alpha: 0.500\n",
      "  Batch 120/400 | Loss: 1.7279 | LR: 1.79e-05 | Alpha: 0.500\n",
      "  Batch 140/400 | Loss: 1.7625 | LR: 1.74e-05 | Alpha: 0.500\n",
      "  Batch 160/400 | Loss: 1.7494 | LR: 1.68e-05 | Alpha: 0.500\n",
      "  Batch 180/400 | Loss: 1.7886 | LR: 1.63e-05 | Alpha: 0.500\n",
      "  Batch 200/400 | Loss: 1.8043 | LR: 1.58e-05 | Alpha: 0.500\n",
      "  Batch 220/400 | Loss: 1.8014 | LR: 1.53e-05 | Alpha: 0.500\n",
      "  Batch 240/400 | Loss: 1.7839 | LR: 1.47e-05 | Alpha: 0.500\n",
      "  Batch 260/400 | Loss: 1.7785 | LR: 1.42e-05 | Alpha: 0.500\n",
      "  Batch 280/400 | Loss: 1.7786 | LR: 1.37e-05 | Alpha: 0.500\n",
      "  Batch 300/400 | Loss: 1.7678 | LR: 1.32e-05 | Alpha: 0.500\n",
      "  Batch 320/400 | Loss: 1.7743 | LR: 1.26e-05 | Alpha: 0.500\n",
      "  Batch 340/400 | Loss: 1.7650 | LR: 1.21e-05 | Alpha: 0.500\n",
      "  Batch 360/400 | Loss: 1.7584 | LR: 1.16e-05 | Alpha: 0.500\n",
      "  Batch 380/400 | Loss: 1.7636 | LR: 1.11e-05 | Alpha: 0.500\n",
      "  Batch 400/400 | Loss: 1.7642 | LR: 1.05e-05 | Alpha: 0.500\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.7642\n",
      "  Val Loss: 1.4264\n",
      "  Sentiment Accuracy: 0.4900\n",
      "  Emotion Accuracy: 0.2600\n",
      "  Combined Score: 0.3750\n",
      "  Alpha: 0.500\n",
      "  Learning Rate: 1.05e-05\n",
      "Model saved to ./bertweet_model_ultra_light\\checkpoint-epoch-1\n",
      "Model saved to ./bertweet_model_ultra_light\\best_model\n",
      "\n",
      "📍 Epoch 2/2\n",
      "--------------------------------------------------\n",
      "  Batch 20/400 | Loss: 1.5974 | LR: 1.00e-05 | Alpha: 0.500\n",
      "  Batch 40/400 | Loss: 1.5951 | LR: 9.47e-06 | Alpha: 0.500\n",
      "  Batch 60/400 | Loss: 1.6034 | LR: 8.95e-06 | Alpha: 0.500\n",
      "  Batch 80/400 | Loss: 1.6183 | LR: 8.42e-06 | Alpha: 0.500\n",
      "  Batch 100/400 | Loss: 1.6092 | LR: 7.89e-06 | Alpha: 0.500\n",
      "  Batch 120/400 | Loss: 1.5959 | LR: 7.37e-06 | Alpha: 0.500\n",
      "  Batch 140/400 | Loss: 1.6047 | LR: 6.84e-06 | Alpha: 0.500\n",
      "  Batch 160/400 | Loss: 1.6253 | LR: 6.32e-06 | Alpha: 0.500\n",
      "  Batch 180/400 | Loss: 1.6317 | LR: 5.79e-06 | Alpha: 0.500\n",
      "  Batch 200/400 | Loss: 1.6144 | LR: 5.26e-06 | Alpha: 0.500\n",
      "  Batch 220/400 | Loss: 1.6038 | LR: 4.74e-06 | Alpha: 0.500\n",
      "  Batch 240/400 | Loss: 1.5964 | LR: 4.21e-06 | Alpha: 0.500\n",
      "  Batch 260/400 | Loss: 1.5946 | LR: 3.68e-06 | Alpha: 0.500\n",
      "  Batch 280/400 | Loss: 1.5961 | LR: 3.16e-06 | Alpha: 0.500\n",
      "  Batch 300/400 | Loss: 1.6011 | LR: 2.63e-06 | Alpha: 0.500\n",
      "  Batch 320/400 | Loss: 1.5922 | LR: 2.11e-06 | Alpha: 0.500\n",
      "  Batch 340/400 | Loss: 1.5894 | LR: 1.58e-06 | Alpha: 0.500\n",
      "  Batch 360/400 | Loss: 1.5967 | LR: 1.05e-06 | Alpha: 0.500\n",
      "  Batch 380/400 | Loss: 1.5906 | LR: 5.26e-07 | Alpha: 0.500\n",
      "  Batch 400/400 | Loss: 1.5904 | LR: 0.00e+00 | Alpha: 0.500\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.5904\n",
      "  Val Loss: 1.4898\n",
      "  Sentiment Accuracy: 0.4500\n",
      "  Emotion Accuracy: 0.1000\n",
      "  Combined Score: 0.2750\n",
      "  Alpha: 0.500\n",
      "  Learning Rate: 0.00e+00\n",
      "Model saved to ./bertweet_model_ultra_light\\checkpoint-epoch-2\n",
      "\n",
      "🎉 Training completed!\n",
      "Best combined score: 0.3750\n",
      "✅ Training completed!\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 4.31 GB\n",
      "  Cached: 4.63 GB\n",
      "\n",
      "7️⃣ Evaluating with fixed evaluator...\n",
      "Model saved to ./bertweet_model_ultra_light\\final_model\n",
      "✅ Model and encoders saved to: ./bertweet_model_ultra_light\\final_model\n",
      "\n",
      "📈 Results:\n",
      "Sentiment Accuracy: 0.2947\n",
      "Emotion Accuracy: 0.0316\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 4.31 GB\n",
      "  Cached: 4.63 GB\n",
      "\n",
      "🎉 BERTweet Initial Training Successful!\n",
      "📊 Initial Results:\n",
      "   Sentiment Accuracy: 0.2947\n",
      "   Emotion Accuracy: 0.0316\n",
      "   Combined Score: 0.1632\n"
     ]
    }
   ],
   "source": [
    "# Cell: Restart BERTweet Training with Fixed Function\n",
    "print(\"🚀 Restarting BERTweet Training with Fixed Function\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear memory first\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Run fixed ultra-lightweight training with BERTweet\n",
    "print(\"🔥 Step 1: Fixed Ultra-Lightweight BERTweet Training\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "bertweet_model, bertweet_results = run_ultra_lightweight_training_fixed(\n",
    "    reddit_data_path=\"annotated_reddit_posts.csv\",\n",
    "    model_name=\"vinai/bertweet-base\",  # Using BERTweet instead of DeBERTa\n",
    "    output_dir=\"./bertweet_model_ultra_light\",\n",
    "    max_external_samples=500  # Small training set for initial training\n",
    ")\n",
    "\n",
    "if bertweet_model is not None:\n",
    "    print(\"\\n🎉 BERTweet Initial Training Successful!\")\n",
    "    print(f\"📊 Initial Results:\")\n",
    "    print(f\"   Sentiment Accuracy: {bertweet_results['sentiment']['accuracy']:.4f}\")\n",
    "    print(f\"   Emotion Accuracy: {bertweet_results['emotion']['accuracy']:.4f}\")\n",
    "    print(f\"   Combined Score: {(bertweet_results['sentiment']['accuracy'] + bertweet_results['emotion']['accuracy'])/2:.4f}\")\n",
    "else:\n",
    "    print(\"\\n❌ Initial training failed!\")\n",
    "    print(\"💡 Troubleshooting options:\")\n",
    "    print(\"1. Restart kernel completely\")\n",
    "    print(\"2. Use CPU training: device = torch.device('cpu')\")\n",
    "    print(\"3. Reduce batch size further\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e686e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Import Required Libraries for BERTweet Hyperparameter Tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66636ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet hyperparameter tuning class defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Fixed Hyperparameter Tuning for BERTweet (With Imports)\n",
    "class BERTweetHyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning for BERTweet using the same Bayesian optimization method as DeBERTa\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        reddit_data_path: str,\n",
    "        n_trials: int = 15,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        max_external_samples: int = 2000\n",
    "    ):\n",
    "        self.reddit_data_path = reddit_data_path\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        self.max_external_samples = max_external_samples\n",
    "        \n",
    "        # Load external datasets for training (same as DeBERTa)\n",
    "        print(\"🔄 Loading external datasets for BERTweet hyperparameter tuning...\")\n",
    "        sentiment_data, emotion_data = load_external_datasets()\n",
    "        \n",
    "        # Prepare external data splits\n",
    "        self.external_data_splits, self.sentiment_encoder, self.emotion_encoder = prepare_external_data_for_multitask(\n",
    "            sentiment_data, emotion_data, max_samples=max_external_samples\n",
    "        )\n",
    "        \n",
    "        # Load Reddit data for evaluation\n",
    "        reddit_df = pd.read_csv(reddit_data_path)\n",
    "        self.reddit_evaluation_data = prepare_reddit_data_for_evaluation(\n",
    "            reddit_df, self.sentiment_encoder, self.emotion_encoder\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ BERTweet Hyperparameter tuner initialized\")\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Training data: {len(self.external_data_splits['train']['texts'])} external samples\")\n",
    "        print(f\"Evaluation data: {len(self.reddit_evaluation_data['texts'])} Reddit samples\")\n",
    "        print(f\"Trials: {n_trials}\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function for BERTweet - same as DeBERTa\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters - same ranges as DeBERTa\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [2, 4, 8])\n",
    "        alpha = trial.suggest_float('alpha', 0.3, 0.7)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.05, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.4)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.001, 0.1)\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.05, 0.2)\n",
    "        num_epochs = trial.suggest_int('num_epochs', 2, 5)\n",
    "        max_length = trial.suggest_categorical('max_length', [128, 256])\n",
    "        \n",
    "        # Create configuration\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,  # BERTweet model\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            alpha=alpha,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            adaptive_alpha=False,\n",
    "            output_dir=f\"./temp_bertweet_trial_{trial.number}\",\n",
    "            save_strategy=\"no\",\n",
    "            save_total_limit=1\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory before trial\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = MultiTaskTrainer(\n",
    "                config=config,\n",
    "                sentiment_num_classes=len(self.sentiment_encoder.classes_),\n",
    "                emotion_num_classes=len(self.emotion_encoder.classes_)\n",
    "            )\n",
    "            \n",
    "            # Setup with external data\n",
    "            trainer.setup(self.external_data_splits, self.sentiment_encoder, self.emotion_encoder)\n",
    "            \n",
    "            # Train model on external data\n",
    "            history = trainer.train()\n",
    "            \n",
    "            # Clear memory before evaluation\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Evaluate on Reddit data using FIXED evaluator\n",
    "            evaluator = FixedMultiTaskEvaluator(\n",
    "                model=trainer.model,\n",
    "                tokenizer=trainer.tokenizer,\n",
    "                sentiment_encoder=self.sentiment_encoder,\n",
    "                emotion_encoder=self.emotion_encoder,\n",
    "                device=device,\n",
    "                max_length=config.max_length  # Use same max_length as training\n",
    "            )\n",
    "            \n",
    "            reddit_results = evaluator.evaluate_dataset(\n",
    "                texts=self.reddit_evaluation_data['texts'],\n",
    "                sentiment_labels=self.reddit_evaluation_data['sentiment_labels'],\n",
    "                emotion_labels=self.reddit_evaluation_data['emotion_labels'],\n",
    "                batch_size=2  # Small batch for evaluation\n",
    "            )\n",
    "            \n",
    "            # Combined score based on Reddit evaluation\n",
    "            combined_score = (\n",
    "                reddit_results['sentiment']['accuracy'] + \n",
    "                reddit_results['emotion']['accuracy']\n",
    "            ) / 2\n",
    "            \n",
    "            print(f\"Trial {trial.number}: Combined Score = {combined_score:.4f} \"\n",
    "                  f\"(Sentiment: {reddit_results['sentiment']['accuracy']:.4f}, \"\n",
    "                  f\"Emotion: {reddit_results['emotion']['accuracy']:.4f})\")\n",
    "            \n",
    "            # Clean up\n",
    "            del trainer, evaluator\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            return combined_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            aggressive_memory_cleanup()\n",
    "            return 0.0\n",
    "    \n",
    "    def tune(self) -> optuna.Study:\n",
    "        \"\"\"Run hyperparameter optimization - same method as DeBERTa\"\"\"\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
    "        )\n",
    "        \n",
    "        print(f\"🔍 Starting BERTweet hyperparameter optimization...\")\n",
    "        print(f\"This will run {self.n_trials} trials - each training and evaluating a model\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n🏆 BERTweet Optimization completed!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best combined score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "print(\"✅ BERTweet hyperparameter tuning class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb42ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Step 2: BERTweet Hyperparameter Optimization (Same Method as DeBERTa)\n",
      "--------------------------------------------------\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 0.02 GB\n",
      "  Cached: 2.35 GB\n",
      "🔍 Starting BERTweet hyperparameter optimization...\n",
      "🔄 Loading external datasets for BERTweet hyperparameter tuning...\n",
      "Loading external datasets for training...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 val\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 val\n",
      "🔄 Preparing external datasets for multitask training...\n",
      "✅ External data prepared:\n",
      "  Train samples: 1600\n",
      "  Validation samples: 400\n",
      "  Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "  Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "📈 Training set class distribution:\n",
      "  Sentiment 'Negative': 721 samples\n",
      "  Sentiment 'Neutral': 101 samples\n",
      "  Sentiment 'Positive': 778 samples\n",
      "  Emotion 'Anger': 522 samples\n",
      "  Emotion 'Fear': 267 samples\n",
      "  Emotion 'Joy': 186 samples\n",
      "  Emotion 'No Emotion': 231 samples\n",
      "  Emotion 'Sadness': 281 samples\n",
      "  Emotion 'Surprise': 113 samples\n",
      "🔄 Preparing Reddit data for evaluation...\n",
      "✅ Reddit evaluation data prepared: 95 samples\n",
      "✅ BERTweet Hyperparameter tuner initialized\n",
      "Model: vinai/bertweet-base\n",
      "Training data: 1600 external samples\n",
      "Evaluation data: 95 Reddit samples\n",
      "Trials: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:28:39,047] A new study created in memory with name: no-name-55e08ef5-017c-4c93-8c3f-fc8a65e3be63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting BERTweet hyperparameter optimization...\n",
      "This will run 15 trials - each training and evaluating a model\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 0.02 GB\n",
      "  Cached: 0.04 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:28:42,250] Trial 0 finished with value: 0.0 and parameters: {'learning_rate': 2.368863950364079e-05, 'batch_size': 2, 'alpha': 0.36240745617697456, 'hidden_dropout_prob': 0.08899863008405066, 'classifier_dropout': 0.11742508365045984, 'weight_decay': 0.08675143843171859, 'warmup_ratio': 0.14016725176148134, 'num_epochs': 4, 'max_length': 256}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete!\n",
      "  Model: vinai/bertweet-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 3200\n",
      "🚀 Starting training for 4 epochs...\n",
      "\n",
      "📍 Epoch 1/4\n",
      "--------------------------------------------------\n",
      "Trial 0 failed: The expanded size of the tensor (256) must match the existing size (130) at non-singleton dimension 1.  Target sizes: [2, 256].  Tensor sizes: [1, 130]\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 0.56 GB\n",
      "  Cached: 0.60 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 0.02 GB\n",
      "  Cached: 0.04 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 11:28:45,078] Trial 1 finished with value: 0.0 and parameters: {'learning_rate': 6.798962421591133e-05, 'batch_size': 2, 'alpha': 0.4216968971838151, 'hidden_dropout_prob': 0.18118910790805948, 'classifier_dropout': 0.22958350559263474, 'weight_decay': 0.029831684879606152, 'warmup_ratio': 0.14177793420835694, 'num_epochs': 2, 'max_length': 256}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete!\n",
      "  Model: vinai/bertweet-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 1600\n",
      "🚀 Starting training for 2 epochs...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "--------------------------------------------------\n",
      "Trial 1 failed: The expanded size of the tensor (256) must match the existing size (130) at non-singleton dimension 1.  Target sizes: [2, 256].  Tensor sizes: [1, 130]\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 0.56 GB\n",
      "  Cached: 0.60 GB\n",
      "🧹 Memory cleaned!\n",
      "  Allocated: 0.02 GB\n",
      "  Cached: 0.04 GB\n",
      "✅ Setup complete!\n",
      "  Model: vinai/bertweet-base\n",
      "  Training samples: 1600\n",
      "  Validation samples: 400\n",
      "  Training steps per epoch: 800\n",
      "  Total training steps: 4000\n",
      "🚀 Starting training for 5 epochs...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 2.2177 | LR: 2.39e-06 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 2.1162 | LR: 4.78e-06 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 2.0796 | LR: 7.18e-06 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 2.0345 | LR: 9.57e-06 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 2.0481 | LR: 1.20e-05 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 1.9716 | LR: 1.44e-05 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 1.9689 | LR: 1.67e-05 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 1.9286 | LR: 1.91e-05 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 1.9249 | LR: 2.15e-05 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 1.9170 | LR: 2.39e-05 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 1.8974 | LR: 2.63e-05 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 1.9200 | LR: 2.86e-05 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 1.9354 | LR: 2.84e-05 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 1.9434 | LR: 2.83e-05 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 1.9351 | LR: 2.81e-05 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 1.9203 | LR: 2.80e-05 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 1.9358 | LR: 2.78e-05 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 1.9365 | LR: 2.77e-05 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 1.9295 | LR: 2.75e-05 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 1.9295 | LR: 2.74e-05 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 1.9219 | LR: 2.72e-05 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 1.9177 | LR: 2.71e-05 | Alpha: 0.537\n",
      "  Batch 460/800 | Loss: 1.9045 | LR: 2.69e-05 | Alpha: 0.537\n",
      "  Batch 480/800 | Loss: 1.8978 | LR: 2.67e-05 | Alpha: 0.537\n",
      "  Batch 500/800 | Loss: 1.8928 | LR: 2.66e-05 | Alpha: 0.537\n",
      "  Batch 520/800 | Loss: 1.8848 | LR: 2.64e-05 | Alpha: 0.537\n",
      "  Batch 540/800 | Loss: 1.8923 | LR: 2.63e-05 | Alpha: 0.537\n",
      "  Batch 560/800 | Loss: 1.8907 | LR: 2.61e-05 | Alpha: 0.537\n",
      "  Batch 580/800 | Loss: 1.8861 | LR: 2.60e-05 | Alpha: 0.537\n",
      "  Batch 600/800 | Loss: 1.8894 | LR: 2.58e-05 | Alpha: 0.537\n",
      "  Batch 620/800 | Loss: 1.8841 | LR: 2.57e-05 | Alpha: 0.537\n",
      "  Batch 640/800 | Loss: 1.8869 | LR: 2.55e-05 | Alpha: 0.537\n",
      "  Batch 660/800 | Loss: 1.8826 | LR: 2.54e-05 | Alpha: 0.537\n",
      "  Batch 680/800 | Loss: 1.8863 | LR: 2.52e-05 | Alpha: 0.537\n",
      "  Batch 700/800 | Loss: 1.8854 | LR: 2.51e-05 | Alpha: 0.537\n",
      "  Batch 720/800 | Loss: 1.8802 | LR: 2.49e-05 | Alpha: 0.537\n",
      "  Batch 740/800 | Loss: 1.8744 | LR: 2.48e-05 | Alpha: 0.537\n",
      "  Batch 760/800 | Loss: 1.8724 | LR: 2.46e-05 | Alpha: 0.537\n",
      "  Batch 780/800 | Loss: 1.8711 | LR: 2.45e-05 | Alpha: 0.537\n",
      "  Batch 800/800 | Loss: 1.8684 | LR: 2.43e-05 | Alpha: 0.537\n",
      "📊 Epoch 1 Results:\n",
      "  Train Loss: 1.8684\n",
      "  Val Loss: 2.3403\n",
      "  Sentiment Accuracy: 0.0575\n",
      "  Emotion Accuracy: 0.0850\n",
      "  Combined Score: 0.0713\n",
      "  Alpha: 0.537\n",
      "  Learning Rate: 2.43e-05\n",
      "\n",
      "📍 Epoch 2/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.5447 | LR: 2.42e-05 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 1.5673 | LR: 2.40e-05 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 1.7062 | LR: 2.39e-05 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 1.7506 | LR: 2.37e-05 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 1.7312 | LR: 2.36e-05 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 1.7137 | LR: 2.34e-05 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 1.7496 | LR: 2.33e-05 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 1.7322 | LR: 2.31e-05 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 1.7098 | LR: 2.29e-05 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 1.6968 | LR: 2.28e-05 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 1.6722 | LR: 2.26e-05 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 1.6852 | LR: 2.25e-05 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 1.7006 | LR: 2.23e-05 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 1.7085 | LR: 2.22e-05 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 1.6991 | LR: 2.20e-05 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 1.6927 | LR: 2.19e-05 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 1.6832 | LR: 2.17e-05 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 1.6785 | LR: 2.16e-05 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 1.6789 | LR: 2.14e-05 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 1.6744 | LR: 2.13e-05 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 1.6598 | LR: 2.11e-05 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 1.6539 | LR: 2.10e-05 | Alpha: 0.537\n",
      "  Batch 460/800 | Loss: 1.6515 | LR: 2.08e-05 | Alpha: 0.537\n",
      "  Batch 480/800 | Loss: 1.6509 | LR: 2.07e-05 | Alpha: 0.537\n",
      "  Batch 500/800 | Loss: 1.6436 | LR: 2.05e-05 | Alpha: 0.537\n",
      "  Batch 520/800 | Loss: 1.6402 | LR: 2.04e-05 | Alpha: 0.537\n",
      "  Batch 540/800 | Loss: 1.6412 | LR: 2.02e-05 | Alpha: 0.537\n",
      "  Batch 560/800 | Loss: 1.6407 | LR: 2.01e-05 | Alpha: 0.537\n",
      "  Batch 580/800 | Loss: 1.6389 | LR: 1.99e-05 | Alpha: 0.537\n",
      "  Batch 600/800 | Loss: 1.6373 | LR: 1.98e-05 | Alpha: 0.537\n",
      "  Batch 620/800 | Loss: 1.6318 | LR: 1.96e-05 | Alpha: 0.537\n",
      "  Batch 640/800 | Loss: 1.6248 | LR: 1.95e-05 | Alpha: 0.537\n",
      "  Batch 660/800 | Loss: 1.6224 | LR: 1.93e-05 | Alpha: 0.537\n",
      "  Batch 680/800 | Loss: 1.6180 | LR: 1.91e-05 | Alpha: 0.537\n",
      "  Batch 700/800 | Loss: 1.6196 | LR: 1.90e-05 | Alpha: 0.537\n",
      "  Batch 720/800 | Loss: 1.6179 | LR: 1.88e-05 | Alpha: 0.537\n",
      "  Batch 740/800 | Loss: 1.6147 | LR: 1.87e-05 | Alpha: 0.537\n",
      "  Batch 760/800 | Loss: 1.6139 | LR: 1.85e-05 | Alpha: 0.537\n",
      "  Batch 780/800 | Loss: 1.6123 | LR: 1.84e-05 | Alpha: 0.537\n",
      "  Batch 800/800 | Loss: 1.6097 | LR: 1.82e-05 | Alpha: 0.537\n",
      "📊 Epoch 2 Results:\n",
      "  Train Loss: 1.6097\n",
      "  Val Loss: 1.7211\n",
      "  Sentiment Accuracy: 0.0575\n",
      "  Emotion Accuracy: 0.1525\n",
      "  Combined Score: 0.1050\n",
      "  Alpha: 0.537\n",
      "  Learning Rate: 1.82e-05\n",
      "\n",
      "📍 Epoch 3/5\n",
      "--------------------------------------------------\n",
      "  Batch 20/800 | Loss: 1.5040 | LR: 1.81e-05 | Alpha: 0.537\n",
      "  Batch 40/800 | Loss: 1.4201 | LR: 1.79e-05 | Alpha: 0.537\n",
      "  Batch 60/800 | Loss: 1.4595 | LR: 1.78e-05 | Alpha: 0.537\n",
      "  Batch 80/800 | Loss: 1.4385 | LR: 1.76e-05 | Alpha: 0.537\n",
      "  Batch 100/800 | Loss: 1.4193 | LR: 1.75e-05 | Alpha: 0.537\n",
      "  Batch 120/800 | Loss: 1.4749 | LR: 1.73e-05 | Alpha: 0.537\n",
      "  Batch 140/800 | Loss: 1.4792 | LR: 1.72e-05 | Alpha: 0.537\n",
      "  Batch 160/800 | Loss: 1.4863 | LR: 1.70e-05 | Alpha: 0.537\n",
      "  Batch 180/800 | Loss: 1.4821 | LR: 1.69e-05 | Alpha: 0.537\n",
      "  Batch 200/800 | Loss: 1.4903 | LR: 1.67e-05 | Alpha: 0.537\n",
      "  Batch 220/800 | Loss: 1.4895 | LR: 1.66e-05 | Alpha: 0.537\n",
      "  Batch 240/800 | Loss: 1.4838 | LR: 1.64e-05 | Alpha: 0.537\n",
      "  Batch 260/800 | Loss: 1.4946 | LR: 1.63e-05 | Alpha: 0.537\n",
      "  Batch 280/800 | Loss: 1.4846 | LR: 1.61e-05 | Alpha: 0.537\n",
      "  Batch 300/800 | Loss: 1.4900 | LR: 1.60e-05 | Alpha: 0.537\n",
      "  Batch 320/800 | Loss: 1.4942 | LR: 1.58e-05 | Alpha: 0.537\n",
      "  Batch 340/800 | Loss: 1.4920 | LR: 1.57e-05 | Alpha: 0.537\n",
      "  Batch 360/800 | Loss: 1.4923 | LR: 1.55e-05 | Alpha: 0.537\n",
      "  Batch 380/800 | Loss: 1.4956 | LR: 1.54e-05 | Alpha: 0.537\n",
      "  Batch 400/800 | Loss: 1.4962 | LR: 1.52e-05 | Alpha: 0.537\n",
      "  Batch 420/800 | Loss: 1.5006 | LR: 1.50e-05 | Alpha: 0.537\n",
      "  Batch 440/800 | Loss: 1.4972 | LR: 1.49e-05 | Alpha: 0.537\n"
     ]
    }
   ],
   "source": [
    "# Cell: Run BERTweet Hyperparameter Tuning\n",
    "print(\"\\n🔍 Step 2: BERTweet Hyperparameter Optimization (Same Method as DeBERTa)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Clear memory first\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Run BERTweet hyperparameter tuning using the same Bayesian optimization method\n",
    "print(\"🔍 Starting BERTweet hyperparameter optimization...\")\n",
    "\n",
    "try:\n",
    "    # Initialize BERTweet tuner\n",
    "    bertweet_tuner = BERTweetHyperparameterTuner(\n",
    "        reddit_data_path=\"annotated_reddit_posts.csv\",\n",
    "        n_trials=15,  # Same number of trials as DeBERTa\n",
    "        model_name=\"vinai/bertweet-base\",\n",
    "        max_external_samples=2000\n",
    "    )\n",
    "    \n",
    "    # Run the same Bayesian optimization process\n",
    "    bertweet_study = bertweet_tuner.tune()\n",
    "    \n",
    "    print(\"\\n✅ BERTweet hyperparameter optimization completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ BERTweet hyperparameter optimization failed: {e}\")\n",
    "    print(\"💡 This might be due to memory constraints or other issues\")\n",
    "    bertweet_study = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad1b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
