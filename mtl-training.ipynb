{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ed9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multitask Learning Framework for Sentiment and Emotion Classification\n",
    "    \n",
    "    Features:\n",
    "    - Shared transformer encoder (RoBERTa, BERTweet, DeBERTa)\n",
    "    - Task-specific attention heads\n",
    "    - Parallel classification heads\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"roberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        freeze_encoder: bool = False\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load configuration and adjust dropout\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared transformer encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Freeze encoder if specified\n",
    "        if freeze_encoder:\n",
    "            for param in self.shared_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for shared + task-specific\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for shared + task-specific\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        task: Optional[str] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs [batch_size, seq_len]\n",
    "            attention_mask: Attention mask [batch_size, seq_len]\n",
    "            task: Optional task specification (\"sentiment\", \"emotion\", or None for both)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing logits for requested tasks\n",
    "        \"\"\"\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get sequence output [batch_size, seq_len, hidden_size]\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention to capture common linguistic features\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        \n",
    "        # Pool shared features (use [CLS] token or mean pooling)\n",
    "        shared_pooled = shared_attended[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        if task is None or task == \"sentiment\":\n",
    "            # Task-specific attention for sentiment\n",
    "            sentiment_attended, sentiment_weights = self.sentiment_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "            sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "            \n",
    "            # Pool sentiment features\n",
    "            sentiment_pooled = sentiment_attended[:, 0, :]  # [CLS] token\n",
    "            \n",
    "            # Combine shared and task-specific features\n",
    "            sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "            \n",
    "            # Sentiment classification\n",
    "            sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "            outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "            outputs[\"sentiment_attention_weights\"] = sentiment_weights\n",
    "        \n",
    "        # Emotion branch\n",
    "        if task is None or task == \"emotion\":\n",
    "            # Task-specific attention for emotion\n",
    "            emotion_attended, emotion_weights = self.emotion_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "            emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "            \n",
    "            # Pool emotion features\n",
    "            emotion_pooled = emotion_attended[:, 0, :]  # [CLS] token\n",
    "            \n",
    "            # Combine shared and task-specific features\n",
    "            emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "            \n",
    "            # Emotion classification\n",
    "            emotion_logits = self.emotion_classifier(emotion_features)\n",
    "            outputs[\"emotion_logits\"] = emotion_logits\n",
    "            outputs[\"emotion_attention_weights\"] = emotion_weights\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Model configuration options\n",
    "MODEL_CONFIGS = {\n",
    "    \"roberta-base\": {\n",
    "        \"name\": \"roberta-base\",\n",
    "        \"description\": \"Standard RoBERTa base model\"\n",
    "    },\n",
    "    \"bertweet\": {\n",
    "        \"name\": \"vinai/bertweet-base\",\n",
    "        \"description\": \"BERTweet optimized for social media text\"\n",
    "    },\n",
    "    \"deberta\": {\n",
    "        \"name\": \"microsoft/deberta-base\",\n",
    "        \"description\": \"DeBERTa with enhanced attention mechanism\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Multitask model architecture defined!\")\n",
    "print(\"Available models:\", list(MODEL_CONFIGS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c35e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset Class for Multitask Learning\n",
    "class MultiTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for multitask learning with sentiment and emotion labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        sentiment_label_encoder=None,\n",
    "        emotion_label_encoder=None\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sentiment_label_encoder = sentiment_label_encoder\n",
    "        self.emotion_label_encoder = emotion_label_encoder\n",
    "        \n",
    "        # Validate data\n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have the same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "def create_stratified_sampler(sentiment_labels: List[int], emotion_labels: List[int]) -> WeightedRandomSampler:\n",
    "    \"\"\"\n",
    "    Create a weighted random sampler for stratified sampling\n",
    "    considering both sentiment and emotion class distributions\n",
    "    \"\"\"\n",
    "    # Combine labels to create compound classes for stratification\n",
    "    compound_labels = [f\"{s}_{e}\" for s, e in zip(sentiment_labels, emotion_labels)]\n",
    "    \n",
    "    # Calculate class weights\n",
    "    unique_labels = list(set(compound_labels))\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels,\n",
    "        y=compound_labels\n",
    "    )\n",
    "    \n",
    "    # Create weight dictionary\n",
    "    weight_dict = dict(zip(unique_labels, class_weights))\n",
    "    \n",
    "    # Assign weights to each sample\n",
    "    sample_weights = [weight_dict[label] for label in compound_labels]\n",
    "    \n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "def prepare_multitask_data(\n",
    "    df: pd.DataFrame,\n",
    "    sentiment_column: str = 'sentiment',\n",
    "    emotion_column: str = 'emotion',\n",
    "    text_column: str = 'text_content',\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[Dict, LabelEncoder, LabelEncoder]:\n",
    "    \"\"\"\n",
    "    Prepare data for multitask learning\n",
    "    \n",
    "    Returns:\n",
    "        data_splits: Dictionary containing train/val splits\n",
    "        sentiment_encoder: Fitted sentiment label encoder\n",
    "        emotion_encoder: Fitted emotion label encoder\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    texts = df[text_column].tolist()\n",
    "    sentiment_labels_text = df[sentiment_column].tolist()\n",
    "    emotion_labels_text = df[emotion_column].tolist()\n",
    "    \n",
    "    # Encode labels\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    sentiment_labels = sentiment_encoder.fit_transform(sentiment_labels_text)\n",
    "    emotion_labels = emotion_encoder.fit_transform(emotion_labels_text)\n",
    "    \n",
    "    # Create stratification labels (compound of both tasks)\n",
    "    stratify_labels = [f\"{s}_{e}\" for s, e in zip(sentiment_labels, emotion_labels)]\n",
    "    \n",
    "    # Split data\n",
    "    (train_texts, val_texts, \n",
    "     train_sentiment, val_sentiment,\n",
    "     train_emotion, val_emotion) = train_test_split(\n",
    "        texts, sentiment_labels, emotion_labels,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_labels\n",
    "    )\n",
    "    \n",
    "    data_splits = {\n",
    "        'train': {\n",
    "            'texts': train_texts,\n",
    "            'sentiment_labels': train_sentiment.tolist(),\n",
    "            'emotion_labels': train_emotion.tolist()\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': val_texts,\n",
    "            'sentiment_labels': val_sentiment.tolist(),\n",
    "            'emotion_labels': val_emotion.tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Data prepared:\")\n",
    "    print(f\"  Train samples: {len(train_texts)}\")\n",
    "    print(f\"  Validation samples: {len(val_texts)}\")\n",
    "    print(f\"  Sentiment classes: {len(sentiment_encoder.classes_)} {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"  Emotion classes: {len(emotion_encoder.classes_)} {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return data_splits, sentiment_encoder, emotion_encoder\n",
    "\n",
    "print(\"âœ… Dataset preparation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18169d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Loss Function with Weighting\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted loss function for multitask learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.5,\n",
    "        sentiment_class_weights: Optional[torch.Tensor] = None,\n",
    "        emotion_class_weights: Optional[torch.Tensor] = None,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight parameter between sentiment and emotion loss (0.3-0.7)\n",
    "            sentiment_class_weights: Class weights for sentiment imbalance\n",
    "            emotion_class_weights: Class weights for emotion imbalance\n",
    "        \"\"\"\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.device = device or torch.device('cpu')\n",
    "        \n",
    "        # Initialize loss functions with class weights\n",
    "        self.sentiment_loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=sentiment_class_weights.to(self.device) if sentiment_class_weights is not None else None\n",
    "        )\n",
    "        self.emotion_loss_fn = nn.CrossEntropyLoss(\n",
    "            weight=emotion_class_weights.to(self.device) if emotion_class_weights is not None else None\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        sentiment_logits: torch.Tensor,\n",
    "        emotion_logits: torch.Tensor,\n",
    "        sentiment_labels: torch.Tensor,\n",
    "        emotion_labels: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Calculate weighted multitask loss\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing individual and combined losses\n",
    "        \"\"\"\n",
    "        # Calculate individual losses\n",
    "        sentiment_loss = self.sentiment_loss_fn(sentiment_logits, sentiment_labels)\n",
    "        emotion_loss = self.emotion_loss_fn(emotion_logits, emotion_labels)\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = self.alpha * sentiment_loss + (1 - self.alpha) * emotion_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'sentiment_loss': sentiment_loss,\n",
    "            'emotion_loss': emotion_loss,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "    \n",
    "    def update_alpha(self, new_alpha: float):\n",
    "        \"\"\"Update alpha parameter during training\"\"\"\n",
    "        self.alpha = max(0.3, min(0.7, new_alpha))  # Constrain to [0.3, 0.7]\n",
    "\n",
    "def compute_class_weights_from_labels(labels: List[int], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Compute class weights for imbalanced datasets\"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=unique_labels,\n",
    "        y=labels\n",
    "    )\n",
    "    return torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "class AdaptiveAlphaScheduler:\n",
    "    \"\"\"\n",
    "    Adaptive alpha scheduler that adjusts the loss weighting based on task performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_alpha: float = 0.5, adaptation_rate: float = 0.1):\n",
    "        self.alpha = initial_alpha\n",
    "        self.adaptation_rate = adaptation_rate\n",
    "        self.sentiment_history = []\n",
    "        self.emotion_history = []\n",
    "    \n",
    "    def step(self, sentiment_accuracy: float, emotion_accuracy: float) -> float:\n",
    "        \"\"\"\n",
    "        Adjust alpha based on relative task performance\n",
    "        Better performing task gets lower weight to balance learning\n",
    "        \"\"\"\n",
    "        self.sentiment_history.append(sentiment_accuracy)\n",
    "        self.emotion_history.append(emotion_accuracy)\n",
    "        \n",
    "        if len(self.sentiment_history) >= 2:\n",
    "            # Calculate performance difference\n",
    "            sentiment_trend = sentiment_accuracy - np.mean(self.sentiment_history[-3:])\n",
    "            emotion_trend = emotion_accuracy - np.mean(self.emotion_history[-3:])\n",
    "            \n",
    "            # Adjust alpha: if sentiment is improving faster, decrease its weight\n",
    "            if sentiment_trend > emotion_trend:\n",
    "                self.alpha -= self.adaptation_rate\n",
    "            elif emotion_trend > sentiment_trend:\n",
    "                self.alpha += self.adaptation_rate\n",
    "            \n",
    "            # Constrain alpha to [0.3, 0.7]\n",
    "            self.alpha = max(0.3, min(0.7, self.alpha))\n",
    "        \n",
    "        return self.alpha\n",
    "\n",
    "print(\"âœ… Loss functions and schedulers defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ccbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Utilities\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration class for training parameters\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"roberta-base\",\n",
    "        max_length: int = 512,\n",
    "        batch_size: int = 16,\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 5,\n",
    "        warmup_ratio: float = 0.1,\n",
    "        weight_decay: float = 0.01,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        alpha: float = 0.5,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        adaptive_alpha: bool = True,\n",
    "        save_strategy: str = \"epoch\",\n",
    "        evaluation_strategy: str = \"epoch\",\n",
    "        output_dir: str = \"./multitask_model\",\n",
    "        logging_steps: int = 50,\n",
    "        save_total_limit: int = 3\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.alpha = alpha\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_dropout_prob = attention_dropout_prob\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.adaptive_alpha = adaptive_alpha\n",
    "        self.save_strategy = save_strategy\n",
    "        self.evaluation_strategy = evaluation_strategy\n",
    "        self.output_dir = output_dir\n",
    "        self.logging_steps = logging_steps\n",
    "        self.save_total_limit = save_total_limit\n",
    "\n",
    "def create_optimizer_and_scheduler(\n",
    "    model: nn.Module,\n",
    "    config: TrainingConfig,\n",
    "    num_training_steps: int\n",
    ") -> Tuple[AdamW, LambdaLR]:\n",
    "    \"\"\"\n",
    "    Create optimizer and learning rate scheduler\n",
    "    \"\"\"\n",
    "    # Separate parameters for different learning rates\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() \n",
    "                      if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() \n",
    "                      if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # AdamW optimizer\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=config.learning_rate,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Linear warmup scheduler\n",
    "    num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 3, min_delta: float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        \n",
    "    def __call__(self, score: float) -> bool:\n",
    "        \"\"\"Returns True if training should be stopped\"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "class ModelCheckpointer:\n",
    "    \"\"\"Model checkpointing utility\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str, save_total_limit: int = 3):\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.saved_checkpoints = []\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        tokenizer,\n",
    "        optimizer: AdamW,\n",
    "        scheduler: LambdaLR,\n",
    "        epoch: int,\n",
    "        metrics: Dict,\n",
    "        is_best: bool = False\n",
    "    ):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-epoch-{epoch}\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model and tokenizer\n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        tokenizer.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save training state\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "        \n",
    "        # Save best model separately\n",
    "        if is_best:\n",
    "            best_dir = os.path.join(self.output_dir, 'best_model')\n",
    "            os.makedirs(best_dir, exist_ok=True)\n",
    "            model.save_pretrained(best_dir)\n",
    "            tokenizer.save_pretrained(best_dir)\n",
    "        \n",
    "        # Manage checkpoint limit\n",
    "        self.saved_checkpoints.append(checkpoint_dir)\n",
    "        if len(self.saved_checkpoints) > self.save_total_limit:\n",
    "            old_checkpoint = self.saved_checkpoints.pop(0)\n",
    "            if os.path.exists(old_checkpoint) and 'best_model' not in old_checkpoint:\n",
    "                import shutil\n",
    "                shutil.rmtree(old_checkpoint)\n",
    "\n",
    "print(\"âœ… Training utilities defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d432163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training Loop\n",
    "class MultiTaskTrainer:\n",
    "    \"\"\"\n",
    "    Main trainer class for multitask learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfig,\n",
    "        sentiment_num_classes: int,\n",
    "        emotion_num_classes: int\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize components\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = None\n",
    "        self.loss_fn = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.alpha_scheduler = None\n",
    "        self.early_stopping = None\n",
    "        self.checkpointer = None\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_loss': [],\n",
    "            'train_emotion_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_loss': [],\n",
    "            'val_emotion_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'alpha': [],\n",
    "            'learning_rate': []\n",
    "        }\n",
    "    \n",
    "    def setup(\n",
    "        self,\n",
    "        data_splits: Dict,\n",
    "        sentiment_encoder: LabelEncoder,\n",
    "        emotion_encoder: LabelEncoder\n",
    "    ):\n",
    "        \"\"\"Setup model, loss function, and training components\"\"\"\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = MultiTaskTransformer(\n",
    "            model_name=self.config.model_name,\n",
    "            sentiment_num_classes=self.sentiment_num_classes,\n",
    "            emotion_num_classes=self.emotion_num_classes,\n",
    "            hidden_dropout_prob=self.config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=self.config.attention_dropout_prob,\n",
    "            classifier_dropout=self.config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Compute class weights\n",
    "        sentiment_weights = compute_class_weights_from_labels(\n",
    "            data_splits['train']['sentiment_labels'], self.device\n",
    "        )\n",
    "        emotion_weights = compute_class_weights_from_labels(\n",
    "            data_splits['train']['emotion_labels'], self.device\n",
    "        )\n",
    "        \n",
    "        # Initialize loss function\n",
    "        self.loss_fn = MultiTaskLoss(\n",
    "            alpha=self.config.alpha,\n",
    "            sentiment_class_weights=sentiment_weights,\n",
    "            emotion_class_weights=emotion_weights,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        self.train_dataset = MultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length,\n",
    "            sentiment_label_encoder=sentiment_encoder,\n",
    "            emotion_label_encoder=emotion_encoder\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = MultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length,\n",
    "            sentiment_label_encoder=sentiment_encoder,\n",
    "            emotion_label_encoder=emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_sampler = create_stratified_sampler(\n",
    "            data_splits['train']['sentiment_labels'],\n",
    "            data_splits['train']['emotion_labels']\n",
    "        ) if len(data_splits['train']['texts']) > 50 else None\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            sampler=train_sampler,\n",
    "            shuffle=(train_sampler is None),\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer, self.scheduler = create_optimizer_and_scheduler(\n",
    "            self.model, self.config, num_training_steps\n",
    "        )\n",
    "        \n",
    "        # Initialize utilities\n",
    "        if self.config.adaptive_alpha:\n",
    "            self.alpha_scheduler = AdaptiveAlphaScheduler(\n",
    "                initial_alpha=self.config.alpha\n",
    "            )\n",
    "        \n",
    "        self.early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "        self.checkpointer = ModelCheckpointer(\n",
    "            self.config.output_dir,\n",
    "            self.config.save_total_limit\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Setup complete!\")\n",
    "        print(f\"  Model: {self.config.model_name}\")\n",
    "        print(f\"  Training samples: {len(self.train_dataset)}\")\n",
    "        print(f\"  Validation samples: {len(self.val_dataset)}\")\n",
    "        print(f\"  Training steps per epoch: {len(self.train_loader)}\")\n",
    "        print(f\"  Total training steps: {num_training_steps}\")\n",
    "    \n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_sentiment_loss = 0.0\n",
    "        total_emotion_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss_dict = self.loss_fn(\n",
    "                sentiment_logits=outputs['sentiment_logits'],\n",
    "                emotion_logits=outputs['emotion_logits'],\n",
    "                sentiment_labels=sentiment_labels,\n",
    "                emotion_labels=emotion_labels\n",
    "            )\n",
    "            \n",
    "            loss = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                self.config.max_grad_norm\n",
    "            )\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            total_loss += loss.item()\n",
    "            total_sentiment_loss += loss_dict['sentiment_loss'].item()\n",
    "            total_emotion_loss += loss_dict['emotion_loss'].item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Logging\n",
    "            if (batch_idx + 1) % self.config.logging_steps == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                current_lr = self.scheduler.get_last_lr()[0]\n",
    "                print(f\"  Batch {batch_idx + 1}/{len(self.train_loader)} | \"\n",
    "                      f\"Loss: {avg_loss:.4f} | \"\n",
    "                      f\"LR: {current_lr:.2e} | \"\n",
    "                      f\"Alpha: {self.loss_fn.alpha:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'train_loss': total_loss / num_batches,\n",
    "            'train_sentiment_loss': total_sentiment_loss / num_batches,\n",
    "            'train_emotion_loss': total_emotion_loss / num_batches\n",
    "        }\n",
    "    \n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_sentiment_loss = 0.0\n",
    "        total_emotion_loss = 0.0\n",
    "        \n",
    "        sentiment_predictions = []\n",
    "        sentiment_true_labels = []\n",
    "        emotion_predictions = []\n",
    "        emotion_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss_dict = self.loss_fn(\n",
    "                    sentiment_logits=outputs['sentiment_logits'],\n",
    "                    emotion_logits=outputs['emotion_logits'],\n",
    "                    sentiment_labels=sentiment_labels,\n",
    "                    emotion_labels=emotion_labels\n",
    "                )\n",
    "                \n",
    "                # Accumulate losses\n",
    "                total_loss += loss_dict['total_loss'].item()\n",
    "                total_sentiment_loss += loss_dict['sentiment_loss'].item()\n",
    "                total_emotion_loss += loss_dict['emotion_loss'].item()\n",
    "                \n",
    "                # Predictions\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                sentiment_true_labels.extend(sentiment_labels.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                emotion_true_labels.extend(emotion_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        num_batches = len(self.val_loader)\n",
    "        sentiment_accuracy = accuracy_score(sentiment_true_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_true_labels, emotion_predictions)\n",
    "        \n",
    "        return {\n",
    "            'val_loss': total_loss / num_batches,\n",
    "            'val_sentiment_loss': total_sentiment_loss / num_batches,\n",
    "            'val_emotion_loss': total_emotion_loss / num_batches,\n",
    "            'val_sentiment_accuracy': sentiment_accuracy,\n",
    "            'val_emotion_accuracy': emotion_accuracy,\n",
    "            'sentiment_predictions': sentiment_predictions,\n",
    "            'sentiment_true_labels': sentiment_true_labels,\n",
    "            'emotion_predictions': emotion_predictions,\n",
    "            'emotion_true_labels': emotion_true_labels\n",
    "        }\n",
    "    \n",
    "    def train(self) -> Dict[str, List]:\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"ðŸš€ Starting training for {self.config.num_epochs} epochs...\")\n",
    "        \n",
    "        best_combined_score = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\nðŸ“ Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Train for one epoch\n",
    "            train_metrics = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_metrics = self.evaluate()\n",
    "            \n",
    "            # Update alpha if adaptive\n",
    "            if self.alpha_scheduler:\n",
    "                new_alpha = self.alpha_scheduler.step(\n",
    "                    val_metrics['val_sentiment_accuracy'],\n",
    "                    val_metrics['val_emotion_accuracy']\n",
    "                )\n",
    "                self.loss_fn.update_alpha(new_alpha)\n",
    "            \n",
    "            # Calculate combined score for checkpointing\n",
    "            combined_score = (\n",
    "                val_metrics['val_sentiment_accuracy'] + \n",
    "                val_metrics['val_emotion_accuracy']\n",
    "            ) / 2\n",
    "            \n",
    "            is_best = combined_score > best_combined_score\n",
    "            if is_best:\n",
    "                best_combined_score = combined_score\n",
    "            \n",
    "            # Log metrics\n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            \n",
    "            print(f\"ðŸ“Š Epoch {epoch + 1} Results:\")\n",
    "            print(f\"  Train Loss: {train_metrics['train_loss']:.4f}\")\n",
    "            print(f\"  Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "            print(f\"  Sentiment Accuracy: {val_metrics['val_sentiment_accuracy']:.4f}\")\n",
    "            print(f\"  Emotion Accuracy: {val_metrics['val_emotion_accuracy']:.4f}\")\n",
    "            print(f\"  Combined Score: {combined_score:.4f}\")\n",
    "            print(f\"  Alpha: {self.loss_fn.alpha:.3f}\")\n",
    "            print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "            \n",
    "            # Save history\n",
    "            self.training_history['epoch'].append(epoch + 1)\n",
    "            self.training_history['train_loss'].append(train_metrics['train_loss'])\n",
    "            self.training_history['train_sentiment_loss'].append(train_metrics['train_sentiment_loss'])\n",
    "            self.training_history['train_emotion_loss'].append(train_metrics['train_emotion_loss'])\n",
    "            self.training_history['val_loss'].append(val_metrics['val_loss'])\n",
    "            self.training_history['val_sentiment_loss'].append(val_metrics['val_sentiment_loss'])\n",
    "            self.training_history['val_emotion_loss'].append(val_metrics['val_emotion_loss'])\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_metrics['val_sentiment_accuracy'])\n",
    "            self.training_history['val_emotion_accuracy'].append(val_metrics['val_emotion_accuracy'])\n",
    "            self.training_history['alpha'].append(self.loss_fn.alpha)\n",
    "            self.training_history['learning_rate'].append(current_lr)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if self.config.save_strategy == \"epoch\":\n",
    "                self.checkpointer.save_checkpoint(\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    optimizer=self.optimizer,\n",
    "                    scheduler=self.scheduler,\n",
    "                    epoch=epoch + 1,\n",
    "                    metrics=val_metrics,\n",
    "                    is_best=is_best\n",
    "                )\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(combined_score):\n",
    "                print(f\"â¹ï¸ Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Training completed!\")\n",
    "        print(f\"Best combined score: {best_combined_score:.4f}\")\n",
    "        \n",
    "        return self.training_history\n",
    "\n",
    "print(\"âœ… Training loop defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6908f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Evaluation Functions\n",
    "class MultiTaskEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multitask models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: MultiTaskTransformer,\n",
    "        tokenizer,\n",
    "        sentiment_encoder: LabelEncoder,\n",
    "        emotion_encoder: LabelEncoder,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentiment_encoder = sentiment_encoder\n",
    "        self.emotion_encoder = emotion_encoder\n",
    "        self.device = device\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def evaluate_dataset(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        batch_size: int = 32\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Evaluate model on a dataset\n",
    "        \"\"\"\n",
    "        dataset = MultiTaskDataset(\n",
    "            texts=texts,\n",
    "            sentiment_labels=sentiment_labels,\n",
    "            emotion_labels=emotion_labels,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=512,\n",
    "            sentiment_label_encoder=self.sentiment_encoder,\n",
    "            emotion_label_encoder=self.emotion_encoder\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_true_labels = []\n",
    "        emotion_true_labels = []\n",
    "        sentiment_probabilities = []\n",
    "        emotion_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Get predictions and probabilities\n",
    "                sentiment_logits = outputs['sentiment_logits']\n",
    "                emotion_logits = outputs['emotion_logits']\n",
    "                \n",
    "                sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "                emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "                emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                # Store results\n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_true_labels.extend(batch['sentiment_labels'].numpy())\n",
    "                emotion_true_labels.extend(batch['emotion_labels'].numpy())\n",
    "                sentiment_probabilities.extend(sentiment_probs.cpu().numpy())\n",
    "                emotion_probabilities.extend(emotion_probs.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = self._calculate_metrics(\n",
    "            sentiment_predictions=sentiment_predictions,\n",
    "            emotion_predictions=emotion_predictions,\n",
    "            sentiment_true_labels=sentiment_true_labels,\n",
    "            emotion_true_labels=emotion_true_labels,\n",
    "            sentiment_probabilities=sentiment_probabilities,\n",
    "            emotion_probabilities=emotion_probabilities\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_metrics(\n",
    "        self,\n",
    "        sentiment_predictions: List[int],\n",
    "        emotion_predictions: List[int],\n",
    "        sentiment_true_labels: List[int],\n",
    "        emotion_true_labels: List[int],\n",
    "        sentiment_probabilities: List[np.ndarray],\n",
    "        emotion_probabilities: List[np.ndarray]\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "        \n",
    "        # Sentiment metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_true_labels, sentiment_predictions)\n",
    "        sentiment_precision, sentiment_recall, sentiment_f1, _ = precision_recall_fscore_support(\n",
    "            sentiment_true_labels, sentiment_predictions, average='macro', zero_division=0\n",
    "        )\n",
    "        sentiment_f1_weighted = precision_recall_fscore_support(\n",
    "            sentiment_true_labels, sentiment_predictions, average='weighted', zero_division=0\n",
    "        )[2]\n",
    "        \n",
    "        # Emotion metrics\n",
    "        emotion_accuracy = accuracy_score(emotion_true_labels, emotion_predictions)\n",
    "        emotion_precision, emotion_recall, emotion_f1, _ = precision_recall_fscore_support(\n",
    "            emotion_true_labels, emotion_predictions, average='macro', zero_division=0\n",
    "        )\n",
    "        emotion_f1_weighted = precision_recall_fscore_support(\n",
    "            emotion_true_labels, emotion_predictions, average='weighted', zero_division=0\n",
    "        )[2]\n",
    "        \n",
    "        # Classification reports\n",
    "        sentiment_report = classification_report(\n",
    "            sentiment_true_labels, sentiment_predictions,\n",
    "            target_names=self.sentiment_encoder.classes_,\n",
    "            output_dict=True, zero_division=0\n",
    "        )\n",
    "        \n",
    "        emotion_report = classification_report(\n",
    "            emotion_true_labels, emotion_predictions,\n",
    "            target_names=self.emotion_encoder.classes_,\n",
    "            output_dict=True, zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Confusion matrices\n",
    "        sentiment_cm = confusion_matrix(sentiment_true_labels, sentiment_predictions)\n",
    "        emotion_cm = confusion_matrix(emotion_true_labels, emotion_predictions)\n",
    "        \n",
    "        return {\n",
    "            'sentiment': {\n",
    "                'accuracy': sentiment_accuracy,\n",
    "                'precision': sentiment_precision,\n",
    "                'recall': sentiment_recall,\n",
    "                'f1_macro': sentiment_f1,\n",
    "                'f1_weighted': sentiment_f1_weighted,\n",
    "                'classification_report': sentiment_report,\n",
    "                'confusion_matrix': sentiment_cm,\n",
    "                'predictions': sentiment_predictions,\n",
    "                'true_labels': sentiment_true_labels,\n",
    "                'probabilities': sentiment_probabilities\n",
    "            },\n",
    "            'emotion': {\n",
    "                'accuracy': emotion_accuracy,\n",
    "                'precision': emotion_precision,\n",
    "                'recall': emotion_recall,\n",
    "                'f1_macro': emotion_f1,\n",
    "                'f1_weighted': emotion_f1_weighted,\n",
    "                'classification_report': emotion_report,\n",
    "                'confusion_matrix': emotion_cm,\n",
    "                'predictions': emotion_predictions,\n",
    "                'true_labels': emotion_true_labels,\n",
    "                'probabilities': emotion_probabilities\n",
    "            },\n",
    "            'combined': {\n",
    "                'average_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "                'average_f1': (sentiment_f1 + emotion_f1) / 2\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def plot_training_history(self, history: Dict[str, List], save_path: str = None):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Loss plots\n",
    "        axes[0, 0].plot(history['epoch'], history['train_loss'], label='Train', marker='o')\n",
    "        axes[0, 0].plot(history['epoch'], history['val_loss'], label='Validation', marker='s')\n",
    "        axes[0, 0].set_title('Total Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Task-specific losses\n",
    "        axes[0, 1].plot(history['epoch'], history['train_sentiment_loss'], label='Train Sentiment', marker='o')\n",
    "        axes[0, 1].plot(history['epoch'], history['val_sentiment_loss'], label='Val Sentiment', marker='s')\n",
    "        axes[0, 1].plot(history['epoch'], history['train_emotion_loss'], label='Train Emotion', marker='^')\n",
    "        axes[0, 1].plot(history['epoch'], history['val_emotion_loss'], label='Val Emotion', marker='d')\n",
    "        axes[0, 1].set_title('Task-Specific Losses')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Accuracy plots\n",
    "        axes[0, 2].plot(history['epoch'], history['val_sentiment_accuracy'], label='Sentiment', marker='o')\n",
    "        axes[0, 2].plot(history['epoch'], history['val_emotion_accuracy'], label='Emotion', marker='s')\n",
    "        axes[0, 2].set_title('Validation Accuracy')\n",
    "        axes[0, 2].set_xlabel('Epoch')\n",
    "        axes[0, 2].set_ylabel('Accuracy')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True)\n",
    "        \n",
    "        # Alpha evolution\n",
    "        axes[1, 0].plot(history['epoch'], history['alpha'], label='Alpha', marker='o', color='red')\n",
    "        axes[1, 0].set_title('Loss Weight (Alpha)')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Alpha')\n",
    "        axes[1, 0].grid(True)\n",
    "        axes[1, 0].set_ylim([0.2, 0.8])\n",
    "        \n",
    "        # Learning rate\n",
    "        axes[1, 1].plot(history['epoch'], history['learning_rate'], label='Learning Rate', marker='o', color='green')\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].grid(True)\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        \n",
    "        # Combined metrics\n",
    "        combined_accuracy = [(s + e) / 2 for s, e in zip(history['val_sentiment_accuracy'], history['val_emotion_accuracy'])]\n",
    "        axes[1, 2].plot(history['epoch'], combined_accuracy, label='Combined Accuracy', marker='o', color='purple')\n",
    "        axes[1, 2].set_title('Combined Performance')\n",
    "        axes[1, 2].set_xlabel('Epoch')\n",
    "        axes[1, 2].set_ylabel('Average Accuracy')\n",
    "        axes[1, 2].legend()\n",
    "        axes[1, 2].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Training history plot saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self, results: Dict, save_path: str = None):\n",
    "        \"\"\"Plot confusion matrices for both tasks\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Sentiment confusion matrix\n",
    "        sns.heatmap(\n",
    "            results['sentiment']['confusion_matrix'],\n",
    "            annot=True, fmt='d',\n",
    "            xticklabels=self.sentiment_encoder.classes_,\n",
    "            yticklabels=self.sentiment_encoder.classes_,\n",
    "            ax=axes[0],\n",
    "            cmap='Blues'\n",
    "        )\n",
    "        axes[0].set_title('Sentiment Classification\\nConfusion Matrix')\n",
    "        axes[0].set_xlabel('Predicted')\n",
    "        axes[0].set_ylabel('Actual')\n",
    "        \n",
    "        # Emotion confusion matrix\n",
    "        sns.heatmap(\n",
    "            results['emotion']['confusion_matrix'],\n",
    "            annot=True, fmt='d',\n",
    "            xticklabels=self.emotion_encoder.classes_,\n",
    "            yticklabels=self.emotion_encoder.classes_,\n",
    "            ax=axes[1],\n",
    "            cmap='Oranges'\n",
    "        )\n",
    "        axes[1].set_title('Emotion Classification\\nConfusion Matrix')\n",
    "        axes[1].set_xlabel('Predicted')\n",
    "        axes[1].set_ylabel('Actual')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Confusion matrices saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"âœ… Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Inference Functions\n",
    "class MultiTaskPredictor:\n",
    "    \"\"\"\n",
    "    Inference class for multitask model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        sentiment_encoder_path: str,\n",
    "        emotion_encoder_path: str,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = MultiTaskTransformer.from_pretrained(model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load label encoders\n",
    "        import joblib\n",
    "        self.sentiment_encoder = joblib.load(sentiment_encoder_path)\n",
    "        self.emotion_encoder = joblib.load(emotion_encoder_path)\n",
    "        \n",
    "        print(f\"âœ… Model loaded successfully!\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Sentiment classes: {list(self.sentiment_encoder.classes_)}\")\n",
    "        print(f\"Emotion classes: {list(self.emotion_encoder.classes_)}\")\n",
    "    \n",
    "    def predict_single(\n",
    "        self,\n",
    "        text: str,\n",
    "        return_probabilities: bool = True,\n",
    "        return_attention: bool = False\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Predict sentiment and emotion for a single text\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get predictions\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            \n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            sentiment_pred_id = torch.argmax(sentiment_logits, dim=-1).item()\n",
    "            emotion_pred_id = torch.argmax(emotion_logits, dim=-1).item()\n",
    "            \n",
    "            # Decode predictions\n",
    "            sentiment_label = self.sentiment_encoder.inverse_transform([sentiment_pred_id])[0]\n",
    "            emotion_label = self.emotion_encoder.inverse_transform([emotion_pred_id])[0]\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'sentiment': {\n",
    "                    'label': sentiment_label,\n",
    "                    'confidence': sentiment_probs[0][sentiment_pred_id].item(),\n",
    "                    'class_id': sentiment_pred_id\n",
    "                },\n",
    "                'emotion': {\n",
    "                    'label': emotion_label,\n",
    "                    'confidence': emotion_probs[0][emotion_pred_id].item(),\n",
    "                    'class_id': emotion_pred_id\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if return_probabilities:\n",
    "                result['sentiment']['probabilities'] = {\n",
    "                    class_name: prob.item() for class_name, prob in \n",
    "                    zip(self.sentiment_encoder.classes_, sentiment_probs[0])\n",
    "                }\n",
    "                result['emotion']['probabilities'] = {\n",
    "                    class_name: prob.item() for class_name, prob in \n",
    "                    zip(self.emotion_encoder.classes_, emotion_probs[0])\n",
    "                }\n",
    "            \n",
    "            if return_attention:\n",
    "                result['sentiment']['attention_weights'] = outputs['sentiment_attention_weights']\n",
    "                result['emotion']['attention_weights'] = outputs['emotion_attention_weights']\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_batch(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        batch_size: int = 32,\n",
    "        return_probabilities: bool = False\n",
    "    ) -> List[Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        Predict sentiment and emotion for a batch of texts\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                \n",
    "                sentiment_logits = outputs['sentiment_logits']\n",
    "                emotion_logits = outputs['emotion_logits']\n",
    "                \n",
    "                sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "                emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "                emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                # Process each item in batch\n",
    "                for j in range(len(batch_texts)):\n",
    "                    sentiment_pred_id = sentiment_preds[j].item()\n",
    "                    emotion_pred_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    sentiment_label = self.sentiment_encoder.inverse_transform([sentiment_pred_id])[0]\n",
    "                    emotion_label = self.emotion_encoder.inverse_transform([emotion_pred_id])[0]\n",
    "                    \n",
    "                    result = {\n",
    "                        'text': batch_texts[j],\n",
    "                        'sentiment': {\n",
    "                            'label': sentiment_label,\n",
    "                            'confidence': sentiment_probs[j][sentiment_pred_id].item(),\n",
    "                            'class_id': sentiment_pred_id\n",
    "                        },\n",
    "                        'emotion': {\n",
    "                            'label': emotion_label,\n",
    "                            'confidence': emotion_probs[j][emotion_pred_id].item(),\n",
    "                            'class_id': emotion_pred_id\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    if return_probabilities:\n",
    "                        result['sentiment']['probabilities'] = {\n",
    "                            class_name: prob.item() for class_name, prob in \n",
    "                            zip(self.sentiment_encoder.classes_, sentiment_probs[j])\n",
    "                        }\n",
    "                        result['emotion']['probabilities'] = {\n",
    "                            class_name: prob.item() for class_name, prob in \n",
    "                            zip(self.emotion_encoder.classes_, emotion_probs[j])\n",
    "                        }\n",
    "                    \n",
    "                    results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def save_model_and_encoders(\n",
    "    model: MultiTaskTransformer,\n",
    "    tokenizer,\n",
    "    sentiment_encoder: LabelEncoder,\n",
    "    emotion_encoder: LabelEncoder,\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"Save complete model with encoders\"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save encoders\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    # Save model configuration\n",
    "    config = {\n",
    "        'sentiment_classes': list(sentiment_encoder.classes_),\n",
    "        'emotion_classes': list(emotion_encoder.classes_),\n",
    "        'sentiment_num_classes': len(sentiment_encoder.classes_),\n",
    "        'emotion_num_classes': len(emotion_encoder.classes_)\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'model_config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Model and encoders saved to: {output_dir}\")\n",
    "\n",
    "print(\"âœ… Inference functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Execution and Example Usage\n",
    "\n",
    "def run_multitask_training(\n",
    "    data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    model_name: str = \"roberta-base\",\n",
    "    output_dir: str = \"./multitask_model\",\n",
    "    config_overrides: Dict = None\n",
    ") -> Tuple[MultiTaskTransformer, Dict]:\n",
    "    \"\"\"\n",
    "    Main function to run complete multitask training pipeline\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting Multitask Learning Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"ðŸ“ Loading data...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    \n",
    "    # Prepare data\n",
    "    data_splits, sentiment_encoder, emotion_encoder = prepare_multitask_data(df)\n",
    "    \n",
    "    # Create training configuration\n",
    "    config = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir,\n",
    "        num_epochs=8,\n",
    "        batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        alpha=0.5,\n",
    "        adaptive_alpha=True,\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_dropout_prob=0.1,\n",
    "        classifier_dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Apply any configuration overrides\n",
    "    if config_overrides:\n",
    "        for key, value in config_overrides.items():\n",
    "            if hasattr(config, key):\n",
    "                setattr(config, key, value)\n",
    "                print(f\"Updated config.{key} = {value}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = MultiTaskTrainer(\n",
    "        config=config,\n",
    "        sentiment_num_classes=len(sentiment_encoder.classes_),\n",
    "        emotion_num_classes=len(emotion_encoder.classes_)\n",
    "    )\n",
    "    \n",
    "    # Setup training\n",
    "    trainer.setup(data_splits, sentiment_encoder, emotion_encoder)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nðŸ‹ï¸ Training model...\")\n",
    "    history = trainer.train()\n",
    "    \n",
    "    # Save final model\n",
    "    save_model_and_encoders(\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.tokenizer,\n",
    "        sentiment_encoder=sentiment_encoder,\n",
    "        emotion_encoder=emotion_encoder,\n",
    "        output_dir=os.path.join(output_dir, 'final_model')\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"\\nðŸ“Š Final evaluation...\")\n",
    "    evaluator = MultiTaskEvaluator(\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.tokenizer,\n",
    "        sentiment_encoder=sentiment_encoder,\n",
    "        emotion_encoder=emotion_encoder,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_results = evaluator.evaluate_dataset(\n",
    "        texts=data_splits['val']['texts'],\n",
    "        sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "        emotion_labels=data_splits['val']['emotion_labels']\n",
    "    )\n",
    "    \n",
    "    # Print results summary\n",
    "    print(f\"\\nðŸ“ˆ Final Results Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Sentiment Classification:\")\n",
    "    print(f\"  Accuracy: {val_results['sentiment']['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro): {val_results['sentiment']['f1_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted): {val_results['sentiment']['f1_weighted']:.4f}\")\n",
    "    print(f\"\\nEmotion Classification:\")\n",
    "    print(f\"  Accuracy: {val_results['emotion']['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro): {val_results['emotion']['f1_macro']:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted): {val_results['emotion']['f1_weighted']:.4f}\")\n",
    "    print(f\"\\nCombined Performance:\")\n",
    "    print(f\"  Average Accuracy: {val_results['combined']['average_accuracy']:.4f}\")\n",
    "    print(f\"  Average F1-Score: {val_results['combined']['average_f1']:.4f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(f\"\\nðŸ“Š Creating visualizations...\")\n",
    "    os.makedirs(os.path.join(output_dir, 'plots'), exist_ok=True)\n",
    "    \n",
    "    # Plot training history\n",
    "    evaluator.plot_training_history(\n",
    "        history, \n",
    "        save_path=os.path.join(output_dir, 'plots', 'training_history.png')\n",
    "    )\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    evaluator.plot_confusion_matrices(\n",
    "        val_results,\n",
    "        save_path=os.path.join(output_dir, 'plots', 'confusion_matrices.png')\n",
    "    )\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_file = os.path.join(output_dir, 'evaluation_results.json')\n",
    "    \n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    def convert_for_json(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_for_json(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_for_json(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    serializable_results = convert_for_json(val_results)\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Training completed successfully!\")\n",
    "    print(f\"ðŸ“ Output saved to: {output_dir}\")\n",
    "    print(f\"ðŸ“ Final model: {os.path.join(output_dir, 'final_model')}\")\n",
    "    \n",
    "    return trainer.model, val_results\n",
    "\n",
    "# Example usage and testing\n",
    "def test_multitask_model():\n",
    "    \"\"\"Test the multitask model with sample texts\"\"\"\n",
    "    print(\"\\nðŸ§ª Testing Multitask Model\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Sample test cases\n",
    "    test_texts = [\n",
    "        \"I absolutely love this product! It's amazing and makes me so happy! ðŸ˜\",\n",
    "        \"This is terrible... I hate it so much. It makes me really angry! ðŸ˜ \",\n",
    "        \"The service was okay, nothing special. Just neutral feelings about it.\",\n",
    "        \"I'm so excited about this! Can't wait to try it out! ðŸŽ‰\",\n",
    "        \"This is really scary and makes me worried about the future. ðŸ˜°\",\n",
    "        \"What a surprise! I never expected this to happen!\"\n",
    "    ]\n",
    "    \n",
    "    # Load trained model (update path as needed)\n",
    "    model_path = \"./multitask_model/final_model\"\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        predictor = MultiTaskPredictor(\n",
    "            model_path=model_path,\n",
    "            sentiment_encoder_path=os.path.join(model_path, 'sentiment_encoder.pkl'),\n",
    "            emotion_encoder_path=os.path.join(model_path, 'emotion_encoder.pkl'),\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\"\\nðŸ”® Predictions:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for text in test_texts:\n",
    "            result = predictor.predict_single(text, return_probabilities=True)\n",
    "            \n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Sentiment: {result['sentiment']['label']} \"\n",
    "                  f\"(confidence: {result['sentiment']['confidence']:.3f})\")\n",
    "            print(f\"Emotion: {result['emotion']['label']} \"\n",
    "                  f\"(confidence: {result['emotion']['confidence']:.3f})\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    else:\n",
    "        print(f\"âš ï¸ Model not found at {model_path}\")\n",
    "        print(\"Please run training first!\")\n",
    "\n",
    "print(\"âœ… Main execution functions defined!\")\n",
    "print(\"\\nðŸŽ¯ Ready to start multitask learning!\")\n",
    "print(\"\\nTo begin training, run:\")\n",
    "print(\"model, results = run_multitask_training()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0507cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Hyperparameter Tuning with Optuna (Optional)\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "class MultiTaskHyperparameterTuner:\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning for multitask learning using Optuna\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        n_trials: int = 20,\n",
    "        cv_folds: int = 3,\n",
    "        model_name: str = \"roberta-base\"\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.n_trials = n_trials\n",
    "        self.cv_folds = cv_folds\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Load and prepare data\n",
    "        df = pd.read_csv(data_path)\n",
    "        self.data_splits, self.sentiment_encoder, self.emotion_encoder = prepare_multitask_data(df)\n",
    "        \n",
    "        print(f\"âœ… Hyperparameter tuner initialized\")\n",
    "        print(f\"Data: {len(df)} samples\")\n",
    "        print(f\"Trials: {n_trials}\")\n",
    "        print(f\"CV Folds: {cv_folds}\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "        alpha = trial.suggest_float('alpha', 0.3, 0.7)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.05, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.5)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.01, 0.3)\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.05, 0.2)\n",
    "        num_epochs = trial.suggest_int('num_epochs', 3, 8)\n",
    "        \n",
    "        # Create configuration\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            alpha=alpha,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            adaptive_alpha=False,  # Disable for consistent comparison\n",
    "            output_dir=f\"./temp_trial_{trial.number}\",\n",
    "            save_strategy=\"no\"  # Don't save during tuning\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Initialize trainer\n",
    "            trainer = MultiTaskTrainer(\n",
    "                config=config,\n",
    "                sentiment_num_classes=len(self.sentiment_encoder.classes_),\n",
    "                emotion_num_classes=len(self.emotion_encoder.classes_)\n",
    "            )\n",
    "            \n",
    "            # Setup with reduced data for faster tuning\n",
    "            trainer.setup(self.data_splits, self.sentiment_encoder, self.emotion_encoder)\n",
    "            \n",
    "            # Train model\n",
    "            history = trainer.train()\n",
    "            \n",
    "            # Calculate final combined score\n",
    "            final_sentiment_acc = history['val_sentiment_accuracy'][-1]\n",
    "            final_emotion_acc = history['val_emotion_accuracy'][-1]\n",
    "            combined_score = (final_sentiment_acc + final_emotion_acc) / 2\n",
    "            \n",
    "            # Clean up\n",
    "            del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            return combined_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def tune(self) -> optuna.Study:\n",
    "        \"\"\"Run hyperparameter optimization\"\"\"\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ” Starting hyperparameter optimization...\")\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nðŸ† Optimization completed!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "def run_hyperparameter_tuning(\n",
    "    data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    n_trials: int = 20,\n",
    "    model_name: str = \"roberta-base\"\n",
    "):\n",
    "    \"\"\"Run hyperparameter tuning and train final model with best params\"\"\"\n",
    "    \n",
    "    # Run tuning\n",
    "    tuner = MultiTaskHyperparameterTuner(\n",
    "        data_path=data_path,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    study = tuner.tune()\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(f\"\\nðŸš€ Training final model with best hyperparameters...\")\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    model, results = run_multitask_training(\n",
    "        data_path=data_path,\n",
    "        model_name=model_name,\n",
    "        output_dir=\"./multitask_model_optimized\",\n",
    "        config_overrides=best_params\n",
    "    )\n",
    "    \n",
    "    # Save tuning results\n",
    "    import pickle\n",
    "    with open(\"./multitask_model_optimized/hyperparameter_study.pkl\", 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "    \n",
    "    return model, results, study\n",
    "\n",
    "print(\"âœ… Hyperparameter tuning functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Run Training\n",
    "# Now let's run the multitask training on your data!\n",
    "\n",
    "# Basic training with default parameters\n",
    "print(\"ðŸš€ Starting Multitask Training...\")\n",
    "model, results = run_multitask_training(\n",
    "    data_path=\"annotated_reddit_posts.csv\",\n",
    "    model_name=\"roberta-base\",  # or try \"vinai/bertweet-base\" for social media text\n",
    "    output_dir=\"./multitask_model\"\n",
    ")\n",
    "\n",
    "# Test the trained model\n",
    "test_multitask_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1985e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Optional - Run Hyperparameter Tuning\n",
    "# Uncomment to run hyperparameter optimization (takes longer)\n",
    "\n",
    "# print(\"ðŸ” Starting Hyperparameter Optimization...\")\n",
    "# optimized_model, optimized_results, study = run_hyperparameter_tuning(\n",
    "#     data_path=\"annotated_reddit_posts.csv\",\n",
    "#     n_trials=15,  # Adjust based on your computational budget\n",
    "#     model_name=\"roberta-base\"\n",
    "# )\n",
    "\n",
    "print(\"âœ… Multitask Learning Framework Complete!\")\n",
    "print(\"\"\"\n",
    "ðŸŽ¯ What you can do now:\n",
    "1. Use the trained model for inference\n",
    "2. Fine-tune on additional data  \n",
    "3. Experiment with different transformer models\n",
    "4. Adjust loss weighting (alpha parameter)\n",
    "5. Try different attention mechanisms\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
