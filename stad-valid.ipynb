{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af9d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Libraries imported and paths configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model paths\n",
    "MODEL_PATHS = {\n",
    "    'roberta_sentiment': 'roberta_sentiment_model_optimized',\n",
    "    'roberta_emotion': 'roberta_emotion_model_optimized', \n",
    "    'deberta_multitask': 'deberta_optimized',\n",
    "    'bertweet_multitask': 'bertweet_model_ultra_light'\n",
    "}\n",
    "\n",
    "DATA_PATH = 'annotated_reddit_posts.csv'\n",
    "\n",
    "print(\"Libraries imported and paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dae91f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multitask model architecture defined!\n",
      "Available models: ['bertweet', 'deberta']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# Cell 2: Multitask Model Architecture (FIXED)\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multitask Learning Framework for Sentiment and Emotion Classification\n",
    "    \n",
    "    Features:\n",
    "    - Shared transformer encoder (BERTweet, DeBERTa)\n",
    "    - Task-specific attention heads\n",
    "    - Parallel classification heads\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"microsoft/deberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        freeze_encoder: bool = False\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load configuration and adjust dropout\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared transformer encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Freeze encoder if specified\n",
    "        if freeze_encoder:\n",
    "            for param in self.shared_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for shared + task-specific\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for shared + task-specific\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        task: Optional[str] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs [batch_size, seq_len]\n",
    "            attention_mask: Attention mask [batch_size, seq_len]\n",
    "            task: Optional task specification (\"sentiment\", \"emotion\", or None for both)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing logits for requested tasks\n",
    "        \"\"\"\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get sequence output [batch_size, seq_len, hidden_size]\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention to capture common linguistic features\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        \n",
    "        # Pool shared features (use [CLS] token or mean pooling)\n",
    "        shared_pooled = shared_attended[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        if task is None or task == \"sentiment\":\n",
    "            # Task-specific attention for sentiment\n",
    "            sentiment_attended, sentiment_weights = self.sentiment_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "            sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "            \n",
    "            # Pool sentiment features\n",
    "            sentiment_pooled = sentiment_attended[:, 0, :]  # [CLS] token\n",
    "            \n",
    "            # Combine shared and task-specific features\n",
    "            sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "            \n",
    "            # Sentiment classification\n",
    "            sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "            outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "            outputs[\"sentiment_attention_weights\"] = sentiment_weights\n",
    "        \n",
    "        # Emotion branch\n",
    "        if task is None or task == \"emotion\":\n",
    "            # Task-specific attention for emotion\n",
    "            emotion_attended, emotion_weights = self.emotion_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "            emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "            \n",
    "            # Pool emotion features\n",
    "            emotion_pooled = emotion_attended[:, 0, :]  # [CLS] token\n",
    "            \n",
    "            # Combine shared and task-specific features\n",
    "            emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "            \n",
    "            # Emotion classification\n",
    "            emotion_logits = self.emotion_classifier(emotion_features)\n",
    "            outputs[\"emotion_logits\"] = emotion_logits\n",
    "            outputs[\"emotion_attention_weights\"] = emotion_weights\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    # ‚úÖ ADD THESE MISSING HUGGING FACE COMPATIBLE METHODS\n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        \"\"\"Save the model in Hugging Face compatible format\"\"\"\n",
    "        import os\n",
    "        import json\n",
    "        \n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"sentiment_num_classes\": self.sentiment_num_classes,\n",
    "            \"emotion_num_classes\": self.emotion_num_classes,\n",
    "            \"model_type\": \"MultiTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"Model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load the model in Hugging Face compatible format\"\"\"\n",
    "        import os\n",
    "        import json\n",
    "        \n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            sentiment_num_classes=config[\"sentiment_num_classes\"],\n",
    "            emotion_num_classes=config[\"emotion_num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        return model\n",
    "\n",
    "# Model configuration options\n",
    "MODEL_CONFIGS = {\n",
    "    \"bertweet\": {\n",
    "        \"name\": \"vinai/bertweet-base\",\n",
    "        \"description\": \"BERTweet optimized for social media text\"\n",
    "    },\n",
    "    \"deberta\": {\n",
    "        \"name\": \"microsoft/deberta-base\",\n",
    "        \"description\": \"DeBERTa with enhanced attention mechanism\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Multitask model architecture defined!\")\n",
    "print(\"Available models:\", list(MODEL_CONFIGS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "910a9154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictor classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define Model Loading Functions\n",
    "class RoBERTaSingleTaskPredictor:\n",
    "    \n",
    "    def __init__(self, model_path: str, task_type: str, max_length: int = 512):\n",
    "        self.device = device\n",
    "        self.task_type = task_type  # 'sentiment' or 'emotion'\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"üì• Loading RoBERTa {task_type} model from {model_path}\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load label encoder if available\n",
    "        encoder_path = os.path.join(model_path, f'{task_type}_encoder.pkl')\n",
    "        if os.path.exists(encoder_path):\n",
    "            self.label_encoder = joblib.load(encoder_path)\n",
    "        else:\n",
    "            # Create default encoder\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            if task_type == 'sentiment':\n",
    "                self.label_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "            else:  # emotion\n",
    "                self.label_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "        \n",
    "        print(f\"‚úÖ RoBERTa {task_type} model loaded successfully\")\n",
    "        print(f\"   Classes: {list(self.label_encoder.classes_)}\")\n",
    "    \n",
    "    def predict_batch(self, texts: List[str], batch_size: int = 16) -> List[Dict]:\n",
    "        \"\"\"Predict for a batch of texts\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                # Process results\n",
    "                for j in range(len(batch_texts)):\n",
    "                    pred_id = preds[j].item()\n",
    "                    confidence = probs[j][pred_id].item()\n",
    "                    \n",
    "                    # Handle out of range predictions\n",
    "                    if pred_id >= len(self.label_encoder.classes_):\n",
    "                        pred_id = 0\n",
    "                    \n",
    "                    label = self.label_encoder.classes_[pred_id]\n",
    "                    \n",
    "                    result = {\n",
    "                        'text': batch_texts[j],\n",
    "                        'predicted_label': label,\n",
    "                        'confidence': confidence,\n",
    "                        'class_id': pred_id\n",
    "                    }\n",
    "                    results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class MultiTaskPredictor:\n",
    "    \n",
    "    def __init__(self, model_path: str, model_name: str, max_length: int = 128):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"üì• Loading {model_name} multitask model from {model_path}\")\n",
    "        \n",
    "        # Find the actual model directory\n",
    "        model_dir = self._find_model_directory(model_path)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        except:\n",
    "            # Fallback to original model name\n",
    "            original_name = \"microsoft/deberta-base\" if \"deberta\" in model_name.lower() else \"vinai/bertweet-base\"\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(original_name)\n",
    "        \n",
    "        # Load multitask model\n",
    "        self.model = self._load_multitask_model(model_dir)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load label encoders\n",
    "        self.sentiment_encoder, self.emotion_encoder = self._load_encoders(model_dir)\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} multitask model loaded successfully\")\n",
    "        print(f\"   Sentiment classes: {list(self.sentiment_encoder.classes_)}\")\n",
    "        print(f\"   Emotion classes: {list(self.emotion_encoder.classes_)}\")\n",
    "    \n",
    "    def _find_model_directory(self, base_path: str) -> str:\n",
    "        \"\"\"Find the actual model directory\"\"\"\n",
    "        possible_dirs = [\n",
    "            base_path,\n",
    "            os.path.join(base_path, 'best_model'),\n",
    "            os.path.join(base_path, 'final_model'),\n",
    "            os.path.join(base_path, 'checkpoint-epoch-1'),\n",
    "            os.path.join(base_path, 'checkpoint-epoch-2')\n",
    "        ]\n",
    "        \n",
    "        for dir_path in possible_dirs:\n",
    "            if os.path.exists(dir_path) and os.path.exists(os.path.join(dir_path, 'pytorch_model.bin')):\n",
    "                return dir_path\n",
    "        \n",
    "        raise FileNotFoundError(f\"No valid model found in {base_path}\")\n",
    "    \n",
    "    def _load_multitask_model(self, model_dir: str):\n",
    "        \"\"\"Load the multitask model\"\"\"\n",
    "        try:\n",
    "            # Try loading with from_pretrained (if it was saved properly)\n",
    "            return MultiTaskTransformer.from_pretrained(model_dir)\n",
    "        except:\n",
    "            # Manual loading\n",
    "            with open(os.path.join(model_dir, 'config.json'), 'r') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            model = MultiTaskTransformer(\n",
    "                model_name=config.get(\"model_name\", \"microsoft/deberta-base\"),\n",
    "                sentiment_num_classes=config.get(\"sentiment_num_classes\", 3),\n",
    "                emotion_num_classes=config.get(\"emotion_num_classes\", 6)\n",
    "            )\n",
    "            \n",
    "            state_dict = torch.load(os.path.join(model_dir, 'pytorch_model.bin'), map_location='cpu')\n",
    "            model.load_state_dict(state_dict)\n",
    "            return model\n",
    "    \n",
    "    def _load_encoders(self, model_dir: str):\n",
    "        \"\"\"Load label encoders\"\"\"\n",
    "        sentiment_path = os.path.join(model_dir, 'sentiment_encoder.pkl')\n",
    "        emotion_path = os.path.join(model_dir, 'emotion_encoder.pkl')\n",
    "        \n",
    "        if os.path.exists(sentiment_path) and os.path.exists(emotion_path):\n",
    "            sentiment_encoder = joblib.load(sentiment_path)\n",
    "            emotion_encoder = joblib.load(emotion_path)\n",
    "        else:\n",
    "            # Default encoders\n",
    "            sentiment_encoder = LabelEncoder()\n",
    "            emotion_encoder = LabelEncoder()\n",
    "            sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "            emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "        \n",
    "        return sentiment_encoder, emotion_encoder\n",
    "    \n",
    "    def predict_batch(self, texts: List[str], batch_size: int = 8) -> List[Dict]:\n",
    "        \"\"\"Predict both sentiment and emotion for a batch of texts\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            \n",
    "            # Only pass required inputs\n",
    "            model_inputs = {\n",
    "                'input_ids': inputs['input_ids'].to(self.device),\n",
    "                'attention_mask': inputs['attention_mask'].to(self.device)\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**model_inputs)\n",
    "                \n",
    "                sentiment_logits = outputs['sentiment_logits']\n",
    "                emotion_logits = outputs['emotion_logits']\n",
    "                \n",
    "                sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "                emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "                emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "                \n",
    "                # Process results\n",
    "                for j in range(len(batch_texts)):\n",
    "                    sent_id = sentiment_preds[j].item()\n",
    "                    emot_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    # Handle out of range\n",
    "                    if sent_id >= len(self.sentiment_encoder.classes_):\n",
    "                        sent_id = 0\n",
    "                    if emot_id >= len(self.emotion_encoder.classes_):\n",
    "                        emot_id = 0\n",
    "                    \n",
    "                    result = {\n",
    "                        'text': batch_texts[j],\n",
    "                        'sentiment': {\n",
    "                            'label': self.sentiment_encoder.classes_[sent_id],\n",
    "                            'confidence': sentiment_probs[j][sent_id].item(),\n",
    "                            'class_id': sent_id\n",
    "                        },\n",
    "                        'emotion': {\n",
    "                            'label': self.emotion_encoder.classes_[emot_id],\n",
    "                            'confidence': emotion_probs[j][emot_id].item(),\n",
    "                            'class_id': emot_id\n",
    "                        }\n",
    "                    }\n",
    "                    results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"Model predictor classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20d2bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Evaluation Functions\n",
    "def calculate_metrics(true_labels: List[str], pred_labels: List[str], task_name: str) -> Dict:\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    macro_f1 = f1_score(true_labels, pred_labels, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'task': task_name,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'classification_report': classification_report(true_labels, pred_labels, zero_division=0)\n",
    "    }\n",
    "\n",
    "def evaluate_single_task_model(predictor, texts: List[str], true_labels: List[str], \n",
    "                              task_name: str, model_name: str) -> Dict:\n",
    "    print(f\"üîÆ Evaluating {model_name} for {task_name}...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = predictor.predict_batch(texts)\n",
    "    pred_labels = [pred['predicted_label'] for pred in predictions]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(true_labels, pred_labels, task_name)\n",
    "    \n",
    "    # Add model info\n",
    "    metrics['model_name'] = model_name\n",
    "    metrics['model_type'] = 'single_task'\n",
    "    metrics['predictions'] = predictions\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_multitask_model(predictor, texts: List[str], true_sentiments: List[str], \n",
    "                            true_emotions: List[str], model_name: str) -> Dict:\n",
    "    \"\"\"Evaluate a multitask model\"\"\"\n",
    "    print(f\"üîÆ Evaluating {model_name} for both tasks...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = predictor.predict_batch(texts)\n",
    "    \n",
    "    # Extract predictions\n",
    "    pred_sentiments = [pred['sentiment']['label'] for pred in predictions]\n",
    "    pred_emotions = [pred['emotion']['label'] for pred in predictions]\n",
    "    \n",
    "    # Calculate metrics for both tasks\n",
    "    sentiment_metrics = calculate_metrics(true_sentiments, pred_sentiments, 'sentiment')\n",
    "    emotion_metrics = calculate_metrics(true_emotions, pred_emotions, 'emotion')\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'model_type': 'multitask',\n",
    "        'sentiment': sentiment_metrics,\n",
    "        'emotion': emotion_metrics,\n",
    "        'combined_accuracy': (sentiment_metrics['accuracy'] + emotion_metrics['accuracy']) / 2,\n",
    "        'combined_macro_f1': (sentiment_metrics['macro_f1'] + emotion_metrics['macro_f1']) / 2,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29b5522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from annotated_reddit_posts.csv\n",
      "‚úÖ Data loaded: 95 samples\n",
      "   Sentiment classes: ['Negative', 'Neutral', 'Positive']\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Data\n",
    "def load_and_prepare_data(data_path: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    print(f\"Loading data from {data_path}\")\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    texts = df['text_content'].tolist()\n",
    "    sentiments = df['sentiment'].tolist()\n",
    "    emotions = df['emotion'].tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded: {len(texts)} samples\")\n",
    "    print(f\"   Sentiment classes: {sorted(set(sentiments))}\")\n",
    "    print(f\"   Emotion classes: {sorted(set(emotions))}\")\n",
    "    \n",
    "    return texts, sentiments, emotions\n",
    "\n",
    "# Load the data\n",
    "texts, true_sentiments, true_emotions = load_and_prepare_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb851d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading All Models\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ Loading RoBERTa Sentiment Model\n",
      "üì• Loading RoBERTa sentiment model from roberta_sentiment_model_optimized\n",
      "‚úÖ RoBERTa sentiment model loaded successfully\n",
      "   Classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "\n",
      "2Ô∏è‚É£ Loading RoBERTa Emotion Model\n",
      "üì• Loading RoBERTa emotion model from roberta_emotion_model_optimized\n",
      "‚úÖ RoBERTa emotion model loaded successfully\n",
      "   Classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "3Ô∏è‚É£ Loading DeBERTa Multitask Model\n",
      "üì• Loading DeBERTa multitask model from deberta_optimized\n",
      "Model loaded from deberta_optimized\n",
      "‚úÖ DeBERTa multitask model loaded successfully\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "4Ô∏è‚É£ Loading BERTweet Multitask Model\n",
      "üì• Loading BERTweet multitask model from bertweet_model_ultra_light\n",
      "Model loaded from bertweet_model_ultra_light\\best_model\n",
      "‚úÖ BERTweet multitask model loaded successfully\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "üìä Model Loading Summary:\n",
      "   ‚úÖ Loaded: ['roberta_sentiment', 'roberta_emotion', 'deberta_multitask', 'bertweet_multitask']\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load All Models\n",
    "def load_all_models() -> Dict:\n",
    "    \"\"\"Load all four models\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    print(\"üöÄ Loading All Models\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # RoBERTa Sentiment Model\n",
    "        print(\"\\n1Ô∏è‚É£ Loading RoBERTa Sentiment Model\")\n",
    "        models['roberta_sentiment'] = RoBERTaSingleTaskPredictor(\n",
    "            MODEL_PATHS['roberta_sentiment'], \n",
    "            'sentiment'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load RoBERTa sentiment model: {e}\")\n",
    "        models['roberta_sentiment'] = None\n",
    "    \n",
    "    try:\n",
    "        # RoBERTa Emotion Model  \n",
    "        print(\"\\n2Ô∏è‚É£ Loading RoBERTa Emotion Model\")\n",
    "        models['roberta_emotion'] = RoBERTaSingleTaskPredictor(\n",
    "            MODEL_PATHS['roberta_emotion'], \n",
    "            'emotion'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load RoBERTa emotion model: {e}\")\n",
    "        models['roberta_emotion'] = None\n",
    "    \n",
    "    try:\n",
    "        # DeBERTa Multitask Model\n",
    "        print(\"\\n3Ô∏è‚É£ Loading DeBERTa Multitask Model\")\n",
    "        models['deberta_multitask'] = MultiTaskPredictor(\n",
    "            MODEL_PATHS['deberta_multitask'], \n",
    "            'DeBERTa'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load DeBERTa multitask model: {e}\")\n",
    "        models['deberta_multitask'] = None\n",
    "    \n",
    "    try:\n",
    "        # BERTweet Multitask Model\n",
    "        print(\"\\n4Ô∏è‚É£ Loading BERTweet Multitask Model\")\n",
    "        models['bertweet_multitask'] = MultiTaskPredictor(\n",
    "            MODEL_PATHS['bertweet_multitask'], \n",
    "            'BERTweet'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load BERTweet multitask model: {e}\")\n",
    "        models['bertweet_multitask'] = None\n",
    "    \n",
    "    # Summary\n",
    "    loaded_models = [k for k, v in models.items() if v is not None]\n",
    "    failed_models = [k for k, v in models.items() if v is None]\n",
    "    \n",
    "    print(f\"\\nüìä Model Loading Summary:\")\n",
    "    print(f\"   ‚úÖ Loaded: {loaded_models}\")\n",
    "    if failed_models:\n",
    "        print(f\"   ‚ùå Failed: {failed_models}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Load all models\n",
    "models = load_all_models()                                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3d3f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Single Seed Evaluation\n",
    "def run_single_evaluation(models: Dict, texts: List[str], true_sentiments: List[str], \n",
    "                         true_emotions: List[str], seed: int = 42) -> Dict:\n",
    "    \"\"\"Run evaluation for all models with a single seed\"\"\"\n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    print(f\"\\nüéØ Running Evaluation (Seed: {seed})\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = {'seed': seed, 'models': {}}\n",
    "    \n",
    "    # Evaluate RoBERTa Sentiment\n",
    "    if models['roberta_sentiment'] is not None:\n",
    "        results['models']['roberta_sentiment'] = evaluate_single_task_model(\n",
    "            models['roberta_sentiment'], texts, true_sentiments, 'sentiment', 'RoBERTa-Sentiment'\n",
    "        )\n",
    "    \n",
    "    # Evaluate RoBERTa Emotion\n",
    "    if models['roberta_emotion'] is not None:\n",
    "        results['models']['roberta_emotion'] = evaluate_single_task_model(\n",
    "            models['roberta_emotion'], texts, true_emotions, 'emotion', 'RoBERTa-Emotion'\n",
    "        )\n",
    "    \n",
    "    # Evaluate DeBERTa Multitask\n",
    "    if models['deberta_multitask'] is not None:\n",
    "        results['models']['deberta_multitask'] = evaluate_multitask_model(\n",
    "            models['deberta_multitask'], texts, true_sentiments, true_emotions, 'DeBERTa-Multitask'\n",
    "        )\n",
    "    \n",
    "    # Evaluate BERTweet Multitask\n",
    "    if models['bertweet_multitask'] is not None:\n",
    "        results['models']['bertweet_multitask'] = evaluate_multitask_model(\n",
    "            models['bertweet_multitask'], texts, true_sentiments, true_emotions, 'BERTweet-Multitask'\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run single evaluation\n",
    "single_results = run_single_evaluation(models, texts, true_sentiments, true_emotions, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5da4cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà SINGLE SEED EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ü§ñ RoBERTa-Sentiment\n",
      "----------------------------------------\n",
      "üìä SENTIMENT CLASSIFICATION:\n",
      "   Accuracy:    0.5474 (54.74%)\n",
      "   Macro F1:    0.2358\n",
      "\n",
      "ü§ñ RoBERTa-Emotion\n",
      "----------------------------------------\n",
      "üìä EMOTION CLASSIFICATION:\n",
      "   Accuracy:    0.3579 (35.79%)\n",
      "   Macro F1:    0.1613\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Print Single Seed Results\n",
    "def print_single_results(results: Dict):\n",
    "    \"\"\"Print results for single seed evaluation\"\"\"\n",
    "    print(\"\\nüìà SINGLE SEED EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_key, model_results in results['models'].items():\n",
    "        if model_results is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nü§ñ {model_results['model_name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if model_results['model_type'] == 'single_task':\n",
    "            task = model_results['task']\n",
    "            print(f\"üìä {task.upper()} CLASSIFICATION:\")\n",
    "            print(f\"   Accuracy:    {model_results['accuracy']:.4f} ({model_results['accuracy']*100:.2f}%)\")\n",
    "            print(f\"   Macro F1:    {model_results['macro_f1']:.4f}\")\n",
    "            \n",
    "        else:  # multitask\n",
    "            print(f\"üìä SENTIMENT CLASSIFICATION:\")\n",
    "            print(f\"   Accuracy:    {model_results['sentiment']['accuracy']:.4f} ({model_results['sentiment']['accuracy']*100:.2f}%)\")\n",
    "            print(f\"   Macro F1:    {model_results['sentiment']['macro_f1']:.4f}\")\n",
    "            \n",
    "            print(f\"\\nüòä EMOTION CLASSIFICATION:\")\n",
    "            print(f\"   Accuracy:    {model_results['emotion']['accuracy']:.4f} ({model_results['emotion']['accuracy']*100:.2f}%)\")\n",
    "            print(f\"   Macro F1:    {model_results['emotion']['macro_f1']:.4f}\")\n",
    "            \n",
    "            print(f\"\\nüèÜ COMBINED PERFORMANCE:\")\n",
    "            print(f\"   Avg Accuracy: {model_results['combined_accuracy']:.4f}\")\n",
    "            print(f\"   Avg Macro F1: {model_results['combined_macro_f1']:.4f}\")\n",
    "\n",
    "# Print the results\n",
    "print_single_results(single_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08d077f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé≤ Random Seed Analysis\n",
      "==================================================\n",
      "Testing seeds: [42, 123, 456, 789, 999]\n",
      "\n",
      "üîÑ Running evaluation with seed 42...\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üîÑ Running evaluation with seed 123...\n",
      "\n",
      "üéØ Running Evaluation (Seed: 123)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üîÑ Running evaluation with seed 456...\n",
      "\n",
      "üéØ Running Evaluation (Seed: 456)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üîÑ Running evaluation with seed 789...\n",
      "\n",
      "üéØ Running Evaluation (Seed: 789)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üîÑ Running evaluation with seed 999...\n",
      "\n",
      "üéØ Running Evaluation (Seed: 999)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Analyzing Stability Across Seeds\n",
      "----------------------------------------\n",
      "\n",
      "ü§ñ DEBERTA_MULTITASK\n",
      "   üìä Sentiment - Accuracy: 0.5895 ¬± 0.0000\n",
      "                Macro F1:  0.3723 ¬± 0.0000\n",
      "   üòä Emotion   - Accuracy: 0.1579 ¬± 0.0000\n",
      "                Macro F1:  0.1352 ¬± 0.0000\n",
      "\n",
      "ü§ñ ROBERTA_SENTIMENT\n",
      "   üìä Sentiment - Accuracy: 0.5474 ¬± 0.0000\n",
      "                Macro F1:  0.2358 ¬± 0.0000\n",
      "\n",
      "ü§ñ ROBERTA_EMOTION\n",
      "   üòä Emotion   - Accuracy: 0.3579 ¬± 0.0000\n",
      "                Macro F1:  0.1613 ¬± 0.0000\n",
      "\n",
      "ü§ñ BERTWEET_MULTITASK\n",
      "   üìä Sentiment - Accuracy: 0.5474 ¬± 0.0000\n",
      "                Macro F1:  0.2358 ¬± 0.0000\n",
      "   üòä Emotion   - Accuracy: 0.1895 ¬± 0.0000\n",
      "                Macro F1:  0.0722 ¬± 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Random Seed Analysis\n",
    "def run_random_seed_analysis(models: Dict, texts: List[str], true_sentiments: List[str], \n",
    "                            true_emotions: List[str], seeds: List[int] = [42, 123, 456, 789, 999]) -> Dict:\n",
    "    \"\"\"Run evaluation across multiple random seeds\"\"\"\n",
    "    print(f\"\\nüé≤ Random Seed Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Testing seeds: {seeds}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\nüîÑ Running evaluation with seed {seed}...\")\n",
    "        seed_results = run_single_evaluation(models, texts, true_sentiments, true_emotions, seed)\n",
    "        all_results.append(seed_results)\n",
    "    \n",
    "    return analyze_seed_stability(all_results)\n",
    "\n",
    "def analyze_seed_stability(all_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Analyze stability across seeds\"\"\"\n",
    "    print(f\"\\nüìä Analyzing Stability Across Seeds\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    stability_analysis = {}\n",
    "    \n",
    "    # Get all model names\n",
    "    model_names = set()\n",
    "    for result in all_results:\n",
    "        model_names.update(result['models'].keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nü§ñ {model_name.upper()}\")\n",
    "        \n",
    "        # Collect metrics across seeds\n",
    "        sentiment_accs, sentiment_f1s = [], []\n",
    "        emotion_accs, emotion_f1s = [], []\n",
    "        \n",
    "        for result in all_results:\n",
    "            if model_name not in result['models'] or result['models'][model_name] is None:\n",
    "                continue\n",
    "                \n",
    "            model_result = result['models'][model_name]\n",
    "            \n",
    "            if model_result['model_type'] == 'single_task':\n",
    "                if model_result['task'] == 'sentiment':\n",
    "                    sentiment_accs.append(model_result['accuracy'])\n",
    "                    sentiment_f1s.append(model_result['macro_f1'])\n",
    "                else:  # emotion\n",
    "                    emotion_accs.append(model_result['accuracy'])\n",
    "                    emotion_f1s.append(model_result['macro_f1'])\n",
    "            else:  # multitask\n",
    "                sentiment_accs.append(model_result['sentiment']['accuracy'])\n",
    "                sentiment_f1s.append(model_result['sentiment']['macro_f1'])\n",
    "                emotion_accs.append(model_result['emotion']['accuracy'])\n",
    "                emotion_f1s.append(model_result['emotion']['macro_f1'])\n",
    "        \n",
    "        # Calculate statistics\n",
    "        model_stability = {}\n",
    "        \n",
    "        if sentiment_accs:\n",
    "            model_stability['sentiment'] = {\n",
    "                'accuracy_mean': np.mean(sentiment_accs),\n",
    "                'accuracy_std': np.std(sentiment_accs),\n",
    "                'f1_mean': np.mean(sentiment_f1s),\n",
    "                'f1_std': np.std(sentiment_f1s)\n",
    "            }\n",
    "            print(f\"   üìä Sentiment - Accuracy: {np.mean(sentiment_accs):.4f} ¬± {np.std(sentiment_accs):.4f}\")\n",
    "            print(f\"                Macro F1:  {np.mean(sentiment_f1s):.4f} ¬± {np.std(sentiment_f1s):.4f}\")\n",
    "        \n",
    "        if emotion_accs:\n",
    "            model_stability['emotion'] = {\n",
    "                'accuracy_mean': np.mean(emotion_accs),\n",
    "                'accuracy_std': np.std(emotion_accs),\n",
    "                'f1_mean': np.mean(emotion_f1s),\n",
    "                'f1_std': np.std(emotion_f1s)\n",
    "            }\n",
    "            print(f\"   üòä Emotion   - Accuracy: {np.mean(emotion_accs):.4f} ¬± {np.std(emotion_accs):.4f}\")\n",
    "            print(f\"                Macro F1:  {np.mean(emotion_f1s):.4f} ¬± {np.std(emotion_f1s):.4f}\")\n",
    "        \n",
    "        stability_analysis[model_name] = model_stability\n",
    "    \n",
    "    return {\n",
    "        'all_results': all_results,\n",
    "        'stability_analysis': stability_analysis,\n",
    "        'seeds_tested': [r['seed'] for r in all_results]\n",
    "    }\n",
    "\n",
    "# Run random seed analysis\n",
    "seed_analysis = run_random_seed_analysis(models, texts, true_sentiments, true_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24ddfd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ FINAL MODEL COMPARISON SUMMARY\n",
      "======================================================================\n",
      "             Model      Task Accuracy (Mean) Accuracy (Std) Macro F1 (Mean) Macro F1 (Std)\n",
      " Deberta Multitask Sentiment          0.5895         0.0000          0.3723         0.0000\n",
      " Deberta Multitask   Emotion          0.1579         0.0000          0.1352         0.0000\n",
      " Roberta Sentiment Sentiment          0.5474         0.0000          0.2358         0.0000\n",
      "   Roberta Emotion   Emotion          0.3579         0.0000          0.1613         0.0000\n",
      "Bertweet Multitask Sentiment          0.5474         0.0000          0.2358         0.0000\n",
      "Bertweet Multitask   Emotion          0.1895         0.0000          0.0722         0.0000\n",
      "\n",
      "üìù Key Insights:\n",
      "   ‚Ä¢ Evaluated 5 random seeds: [42, 123, 456, 789, 999]\n",
      "   ‚Ä¢ Lower standard deviation indicates more stable performance\n",
      "   ‚Ä¢ Multitask models provide both sentiment and emotion predictions\n",
      "   ‚Ä¢ Single-task models are specialized for one task\n",
      "\n",
      "ü•á Best Performers (by Macro F1):\n",
      "   Sentiment: Deberta Multitask (0.3723)\n",
      "   Emotion:   Roberta Emotion (0.1613)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Model Comparison and Summary\n",
    "def create_model_comparison_table(stability_analysis: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Create a comparison table of all models\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name, model_stats in stability_analysis.items():\n",
    "        if 'sentiment' in model_stats:\n",
    "            rows.append({\n",
    "                'Model': model_name.replace('_', ' ').title(),\n",
    "                'Task': 'Sentiment',\n",
    "                'Accuracy (Mean)': f\"{model_stats['sentiment']['accuracy_mean']:.4f}\",\n",
    "                'Accuracy (Std)': f\"{model_stats['sentiment']['accuracy_std']:.4f}\",\n",
    "                'Macro F1 (Mean)': f\"{model_stats['sentiment']['f1_mean']:.4f}\",\n",
    "                'Macro F1 (Std)': f\"{model_stats['sentiment']['f1_std']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        if 'emotion' in model_stats:\n",
    "            rows.append({\n",
    "                'Model': model_name.replace('_', ' ').title(),\n",
    "                'Task': 'Emotion',\n",
    "                'Accuracy (Mean)': f\"{model_stats['emotion']['accuracy_mean']:.4f}\",\n",
    "                'Accuracy (Std)': f\"{model_stats['emotion']['accuracy_std']:.4f}\",\n",
    "                'Macro F1 (Mean)': f\"{model_stats['emotion']['f1_mean']:.4f}\",\n",
    "                'Macro F1 (Std)': f\"{model_stats['emotion']['f1_std']:.4f}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def print_final_summary(seed_analysis: Dict):\n",
    "    \"\"\"Print final summary of all evaluations\"\"\"\n",
    "    print(f\"\\nüèÜ FINAL MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = create_model_comparison_table(seed_analysis['stability_analysis'])\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüìù Key Insights:\")\n",
    "    print(f\"   ‚Ä¢ Evaluated {len(seed_analysis['seeds_tested'])} random seeds: {seed_analysis['seeds_tested']}\")\n",
    "    print(f\"   ‚Ä¢ Lower standard deviation indicates more stable performance\")\n",
    "    print(f\"   ‚Ä¢ Multitask models provide both sentiment and emotion predictions\")\n",
    "    print(f\"   ‚Ä¢ Single-task models are specialized for one task\")\n",
    "    \n",
    "    # Find best performers\n",
    "    sentiment_best = None\n",
    "    emotion_best = None\n",
    "    best_sent_f1 = 0\n",
    "    best_emot_f1 = 0\n",
    "    \n",
    "    for model_name, stats in seed_analysis['stability_analysis'].items():\n",
    "        if 'sentiment' in stats and stats['sentiment']['f1_mean'] > best_sent_f1:\n",
    "            best_sent_f1 = stats['sentiment']['f1_mean']\n",
    "            sentiment_best = model_name\n",
    "        \n",
    "        if 'emotion' in stats and stats['emotion']['f1_mean'] > best_emot_f1:\n",
    "            best_emot_f1 = stats['emotion']['f1_mean']\n",
    "            emotion_best = model_name\n",
    "    \n",
    "    print(f\"\\nü•á Best Performers (by Macro F1):\")\n",
    "    if sentiment_best:\n",
    "        print(f\"   Sentiment: {sentiment_best.replace('_', ' ').title()} ({best_sent_f1:.4f})\")\n",
    "    if emotion_best:\n",
    "        print(f\"   Emotion:   {emotion_best.replace('_', ' ').title()} ({best_emot_f1:.4f})\")\n",
    "\n",
    "# Print final summary\n",
    "print_final_summary(seed_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "324ef96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Bootstrap Stability Analysis\n",
      "==================================================\n",
      "Running 10 bootstrap samples (each with 80% of data)\n",
      "\n",
      "üìä Bootstrap Sample 1/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 2/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 3/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 4/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 5/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 6/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 7/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 8/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 9/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Bootstrap Sample 10/10\n",
      "\n",
      "üéØ Running Evaluation (Seed: 42)\n",
      "==================================================\n",
      "üîÆ Evaluating RoBERTa-Sentiment for sentiment...\n",
      "üîÆ Evaluating RoBERTa-Emotion for emotion...\n",
      "üîÆ Evaluating DeBERTa-Multitask for both tasks...\n",
      "üîÆ Evaluating BERTweet-Multitask for both tasks...\n",
      "\n",
      "üìä Analyzing Bootstrap Stability\n",
      "----------------------------------------\n",
      "\n",
      "ü§ñ DEBERTA_MULTITASK\n",
      "   üìä Sentiment - Accuracy: 0.6000 ¬± 0.0295\n",
      "                Macro F1:  0.3888 ¬± 0.0379\n",
      "   üòä Emotion   - Accuracy: 0.1539 ¬± 0.0317\n",
      "                Macro F1:  0.1170 ¬± 0.0259\n",
      "\n",
      "ü§ñ ROBERTA_SENTIMENT\n",
      "   üìä Sentiment - Accuracy: 0.5382 ¬± 0.0308\n",
      "                Macro F1:  0.2331 ¬± 0.0086\n",
      "\n",
      "ü§ñ ROBERTA_EMOTION\n",
      "   üòä Emotion   - Accuracy: 0.3539 ¬± 0.0379\n",
      "                Macro F1:  0.1640 ¬± 0.0191\n",
      "\n",
      "ü§ñ BERTWEET_MULTITASK\n",
      "   üìä Sentiment - Accuracy: 0.5382 ¬± 0.0308\n",
      "                Macro F1:  0.2331 ¬± 0.0086\n",
      "   üòä Emotion   - Accuracy: 0.1776 ¬± 0.0313\n",
      "                Macro F1:  0.0624 ¬± 0.0148\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Bootstrap Sampling Stability Test (Most Practical)\n",
    "def run_bootstrap_stability_analysis(models: Dict, texts: List[str], \n",
    "                                   true_sentiments: List[str], true_emotions: List[str],\n",
    "                                   n_bootstrap: int = 10, sample_ratio: float = 0.8):\n",
    "    \"\"\"\n",
    "    Test model stability using bootstrap sampling\n",
    "    This gives you meaningful variance by testing on different data subsets\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Bootstrap Stability Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Running {n_bootstrap} bootstrap samples (each with {sample_ratio*100:.0f}% of data)\")\n",
    "    \n",
    "    bootstrap_results = []\n",
    "    data_size = len(texts)\n",
    "    sample_size = int(data_size * sample_ratio)\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        print(f\"\\nüìä Bootstrap Sample {i+1}/{n_bootstrap}\")\n",
    "        \n",
    "        # Create bootstrap sample\n",
    "        np.random.seed(i * 42)  # Different seed for each bootstrap\n",
    "        indices = np.random.choice(data_size, sample_size, replace=True)\n",
    "        \n",
    "        bootstrap_texts = [texts[idx] for idx in indices]\n",
    "        bootstrap_sentiments = [true_sentiments[idx] for idx in indices]\n",
    "        bootstrap_emotions = [true_emotions[idx] for idx in indices]\n",
    "        \n",
    "        # Evaluate models on this bootstrap sample\n",
    "        bootstrap_result = run_single_evaluation(\n",
    "            models, bootstrap_texts, bootstrap_sentiments, bootstrap_emotions, seed=42\n",
    "        )\n",
    "        bootstrap_result['bootstrap_id'] = i\n",
    "        bootstrap_result['sample_indices'] = indices.tolist()\n",
    "        bootstrap_results.append(bootstrap_result)\n",
    "    \n",
    "    return analyze_bootstrap_stability(bootstrap_results)\n",
    "\n",
    "def analyze_bootstrap_stability(bootstrap_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Analyze stability across bootstrap samples\"\"\"\n",
    "    print(f\"\\nüìä Analyzing Bootstrap Stability\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    stability_analysis = {}\n",
    "    \n",
    "    # Get all model names\n",
    "    model_names = set()\n",
    "    for result in bootstrap_results:\n",
    "        model_names.update(result['models'].keys())\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nü§ñ {model_name.upper()}\")\n",
    "        \n",
    "        # Collect metrics across bootstrap samples\n",
    "        sentiment_accs, sentiment_f1s = [], []\n",
    "        emotion_accs, emotion_f1s = [], []\n",
    "        \n",
    "        for result in bootstrap_results:\n",
    "            if model_name not in result['models'] or result['models'][model_name] is None:\n",
    "                continue\n",
    "                \n",
    "            model_result = result['models'][model_name]\n",
    "            \n",
    "            if model_result['model_type'] == 'single_task':\n",
    "                if model_result['task'] == 'sentiment':\n",
    "                    sentiment_accs.append(model_result['accuracy'])\n",
    "                    sentiment_f1s.append(model_result['macro_f1'])\n",
    "                else:  # emotion\n",
    "                    emotion_accs.append(model_result['accuracy'])\n",
    "                    emotion_f1s.append(model_result['macro_f1'])\n",
    "            else:  # multitask\n",
    "                sentiment_accs.append(model_result['sentiment']['accuracy'])\n",
    "                sentiment_f1s.append(model_result['sentiment']['macro_f1'])\n",
    "                emotion_accs.append(model_result['emotion']['accuracy'])\n",
    "                emotion_f1s.append(model_result['emotion']['macro_f1'])\n",
    "        \n",
    "        # Calculate statistics\n",
    "        model_stability = {}\n",
    "        \n",
    "        if sentiment_accs:\n",
    "            model_stability['sentiment'] = {\n",
    "                'accuracy_mean': np.mean(sentiment_accs),\n",
    "                'accuracy_std': np.std(sentiment_accs),\n",
    "                'f1_mean': np.mean(sentiment_f1s),\n",
    "                'f1_std': np.std(sentiment_f1s)\n",
    "            }\n",
    "            print(f\"   üìä Sentiment - Accuracy: {np.mean(sentiment_accs):.4f} ¬± {np.std(sentiment_accs):.4f}\")\n",
    "            print(f\"                Macro F1:  {np.mean(sentiment_f1s):.4f} ¬± {np.std(sentiment_f1s):.4f}\")\n",
    "        \n",
    "        if emotion_accs:\n",
    "            model_stability['emotion'] = {\n",
    "                'accuracy_mean': np.mean(emotion_accs),\n",
    "                'accuracy_std': np.std(emotion_accs),\n",
    "                'f1_mean': np.mean(emotion_f1s),\n",
    "                'f1_std': np.std(emotion_f1s)\n",
    "            }\n",
    "            print(f\"   üòä Emotion   - Accuracy: {np.mean(emotion_accs):.4f} ¬± {np.std(emotion_accs):.4f}\")\n",
    "            print(f\"                Macro F1:  {np.mean(emotion_f1s):.4f} ¬± {np.std(emotion_f1s):.4f}\")\n",
    "        \n",
    "        stability_analysis[model_name] = model_stability\n",
    "    \n",
    "    return {\n",
    "        'bootstrap_results': bootstrap_results,\n",
    "        'stability_analysis': stability_analysis,\n",
    "        'n_bootstrap': len(bootstrap_results)\n",
    "    }\n",
    "\n",
    "# Run bootstrap analysis (this will give you meaningful variance)\n",
    "bootstrap_analysis = run_bootstrap_stability_analysis(models, texts, true_sentiments, true_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b301f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
