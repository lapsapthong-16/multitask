{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b9972-bb7e-4d8a-a250-07f81abb85d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "GPU Memory: 8.0 GB\n",
      "All imports completed and GPU configured\n"
     ]
    }
   ],
   "source": [
    "# In[1]:\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"All imports completed and GPU configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd8e99-446b-434a-91bd-5ae0cef2e5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[2]:\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "\n",
    "output_dir = \"./initial_distilroberta_sentiment_model\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# Ensure CUDA cache is cleared\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c429422-d76f-4281-86d3-52d89b9a3121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "# In[3]:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    model_name: str = \"distilroberta-base\"\n",
    "    max_length: int = 128\n",
    "    batch_size: int = 8  # Standardize to match BERTweet\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 3\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_dropout_prob: float = 0.1\n",
    "    classifier_dropout: float = 0.1\n",
    "    output_dir: str = \"./distilroberta_model_output\"\n",
    "    alpha: float = 0.5  # For multitask loss weighting\n",
    "    task_type: str = \"multitask\"  # \"sentiment\", \"emotion\", or \"multitask\"\n",
    "\n",
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17eb9e-8fda-40ec-bfab-1de29a401d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "# In[4]:\n",
    "\n",
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460cad4-5391-4c47-9d9f-7dcedef77413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# In[5]:\n",
    "\n",
    "class distilroBERTaSingleTaskDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "class distilroBERTaMultiTaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40e90a-13d6-4c71-abb9-eaac2a3f7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# In[6]:\n",
    "\n",
    "# Cell 4: Model Architectures\n",
    "class distilroBERTaSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class distilroBERTaMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Shared RoBERTa encoder\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.sentiment_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            sentiment_num_classes\n",
    "        )\n",
    "        \n",
    "        # Emotion classification head\n",
    "        self.emotion_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            emotion_num_classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get shared RoBERTa representations\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "        emotion_logits = self.emotion_classifier(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03aafa-ad06-4a1d-85ae-82f99b2bc686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa data processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# In[7]:\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def load_and_process_datasets_roberta():\n",
    "    print(\"Loading external datasets for distilroBERTa...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        print(f\"SST-2 dataset loaded: {len(sst2_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        print(f\"GoEmotions dataset loaded: {len(emotions_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Process sentiment data\n",
    "    sentiment_data = process_sentiment_data_roberta(sst2_dataset)\n",
    "    \n",
    "    # Process emotion data  \n",
    "    emotion_data = process_emotion_data_roberta(emotions_dataset)\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def load_reddit_evaluation_data():\n",
    "    \"\"\"Load Reddit data for evaluation\"\"\"\n",
    "    print(\"Loading Reddit evaluation data...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the annotated Reddit posts\n",
    "        df = pd.read_csv('annotated_reddit_posts.csv')\n",
    "        print(f\"âœ… Reddit data loaded: {len(df)} samples\")\n",
    "        \n",
    "        # Create label encoders that match the model classes\n",
    "        sentiment_encoder = LabelEncoder()\n",
    "        emotion_encoder = LabelEncoder()\n",
    "        \n",
    "        # Fit encoders on Reddit data\n",
    "        sentiment_encoder.fit(df['sentiment'].tolist())\n",
    "        emotion_encoder.fit(df['emotion'].tolist())\n",
    "        \n",
    "        # Transform labels\n",
    "        sentiment_labels = sentiment_encoder.transform(df['sentiment'].tolist())\n",
    "        emotion_labels = emotion_encoder.transform(df['emotion'].tolist())\n",
    "        \n",
    "        # Create Reddit data in the format expected by evaluation functions\n",
    "        reddit_data = {\n",
    "            # For single-task sentiment evaluation\n",
    "            'sentiment': {\n",
    "                'texts': df['text_content'].tolist(),\n",
    "                'labels': sentiment_labels,\n",
    "                'labels_text': df['sentiment'].tolist()\n",
    "            },\n",
    "            # For single-task emotion evaluation\n",
    "            'emotion': {\n",
    "                'texts': df['text_content'].tolist(),\n",
    "                'labels': emotion_labels,\n",
    "                'labels_text': df['emotion'].tolist()\n",
    "            },\n",
    "            # For multitask evaluation\n",
    "            'multitask': {\n",
    "                'texts': df['text_content'].tolist(),\n",
    "                'sentiment_labels': sentiment_labels,\n",
    "                'emotion_labels': emotion_labels,\n",
    "                'sentiment_labels_text': df['sentiment'].tolist(),\n",
    "                'emotion_labels_text': df['emotion'].tolist()\n",
    "            },\n",
    "            # Keep encoders for reference\n",
    "            'sentiment_encoder': sentiment_encoder,\n",
    "            'emotion_encoder': emotion_encoder\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Reddit data prepared: {len(reddit_data['sentiment']['texts'])} samples\")\n",
    "        print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "        print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "        \n",
    "        return reddit_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading Reddit data: {e}\")\n",
    "        print(\"Falling back to empty Reddit data\")\n",
    "        return None\n",
    "\n",
    "def process_sentiment_data_roberta(sst2_dataset, max_samples=10000):\n",
    "    \n",
    "    print(\"ðŸ”„ Processing sentiment data for distilroBERTa...\")\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = sst2_dataset['train']['sentence'][:max_samples]\n",
    "    train_labels = sst2_dataset['train']['label'][:max_samples]\n",
    "    \n",
    "    # Map SST-2 labels to 3 classes: 0->Negative, 1->Positive\n",
    "    # Add some neutral examples by random assignment\n",
    "    expanded_labels = []\n",
    "    expanded_texts = []\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        if label == 0:  # Negative\n",
    "            expanded_labels.append(0)\n",
    "            expanded_texts.append(text)\n",
    "        elif label == 1:  # Positive\n",
    "            # Sometimes assign as positive, sometimes as neutral\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                expanded_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                expanded_labels.append(2)  # Positive\n",
    "            expanded_texts.append(text)\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in expanded_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(expanded_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            expanded_labels[idx] = 1\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        expanded_texts, expanded_labels, test_size=0.3, random_state=42, stratify=expanded_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    sentiment_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… RoBERTa Sentiment data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return sentiment_data\n",
    "\n",
    "def process_emotion_data_roberta(emotion_dataset, max_samples=10000):\n",
    "    \n",
    "    print(\"Processing emotion data for distilroBERTa...\")\n",
    "    \n",
    "    # Filter to first 6 emotions only\n",
    "    def filter_emotions(example):\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    filtered_train = emotion_dataset['train'].filter(filter_emotions)\n",
    "    filtered_val = emotion_dataset['validation'].filter(filter_emotions)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = filtered_train['text'][:max_samples]\n",
    "    train_labels_raw = filtered_train['labels'][:max_samples]\n",
    "    \n",
    "    # Handle multi-label to single-label conversion\n",
    "    train_labels = []\n",
    "    for label in train_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            train_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.3, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    emotion_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"distilroBERTa Emotion data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return emotion_data\n",
    "\n",
    "def create_multitask_data_roberta(sentiment_data, emotion_data):\n",
    "    \n",
    "    print(\"ðŸ”„ Creating multi-task dataset for distilroBERTa...\")\n",
    "    \n",
    "    # Take minimum length to balance datasets\n",
    "    min_train_len = min(len(sentiment_data['train']['texts']), len(emotion_data['train']['texts']))\n",
    "    min_val_len = min(len(sentiment_data['val']['texts']), len(emotion_data['val']['texts']))\n",
    "    min_test_len = min(len(sentiment_data['test']['texts']), len(emotion_data['test']['texts']))\n",
    "    \n",
    "    multitask_data = {\n",
    "        'train': {\n",
    "            'texts': sentiment_data['train']['texts'][:min_train_len],\n",
    "            'sentiment_labels': sentiment_data['train']['labels'][:min_train_len],\n",
    "            'emotion_labels': emotion_data['train']['labels'][:min_train_len]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': sentiment_data['val']['texts'][:min_val_len],\n",
    "            'sentiment_labels': sentiment_data['val']['labels'][:min_val_len],\n",
    "            'emotion_labels': emotion_data['val']['labels'][:min_val_len]\n",
    "        },\n",
    "        'test': {\n",
    "            'texts': sentiment_data['test']['texts'][:min_test_len],\n",
    "            'sentiment_labels': sentiment_data['test']['labels'][:min_test_len],\n",
    "            'emotion_labels': emotion_data['test']['labels'][:min_test_len]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"distilroBERTa Multi-task data created:\")\n",
    "    print(f\"  Train: {len(multitask_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(multitask_data['val']['texts'])} samples\")\n",
    "    print(f\"  Test: {len(multitask_data['test']['texts'])} samples\")\n",
    "    \n",
    "    return multitask_data\n",
    "\n",
    "print(\"distilroBERTa data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca22b2-5899-406b-afd0-adbc3c0d11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Training classes defined\n"
     ]
    }
   ],
   "source": [
    "# In[8]:\n",
    "\n",
    "class distilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize distilroBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize distilroBERTa model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting distilroBERTa single-task training ({self.config.task_type})...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\nðŸ“ Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'classifier_state_dict': self.model.classifier.state_dict(),\n",
    "            'num_classes': self.num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "class distilroBERTaMultiTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize RoBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize RoBERTa multi-task model\n",
    "        self.model = distilroBERTaMultiTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            sentiment_num_classes=roberta_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=roberta_model_config.emotion_num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_accuracy': [],\n",
    "            'train_emotion_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'val_sentiment_f1_macro': [],\n",
    "            'val_emotion_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_correct = 0\n",
    "        emotion_correct = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss with alpha weighting\n",
    "            loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "            emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "            \n",
    "            sentiment_correct += (sentiment_preds == sentiment_labels).sum().item()\n",
    "            emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
    "            total_predictions += sentiment_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        sentiment_accuracy = sentiment_correct / total_predictions\n",
    "        emotion_accuracy = emotion_correct / total_predictions\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_labels = []\n",
    "        emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_true = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_true = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Calculate combined loss\n",
    "                sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_true)\n",
    "                emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_true)\n",
    "                loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_labels.extend(sentiment_true.cpu().numpy())\n",
    "                emotion_labels.extend(emotion_true.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_labels, emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy, sentiment_f1_macro, emotion_f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting distilroBERTa multi-task training...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_combined_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_sent_acc, train_emo_acc = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_sent_acc, val_emo_acc, val_sent_f1, val_emo_f1 = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_sentiment_accuracy'].append(train_sent_acc)\n",
    "            self.training_history['train_emotion_accuracy'].append(train_emo_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_sent_acc)\n",
    "            self.training_history['val_emotion_accuracy'].append(val_emo_acc)\n",
    "            self.training_history['val_sentiment_f1_macro'].append(val_sent_f1)\n",
    "            self.training_history['val_emotion_f1_macro'].append(val_emo_f1)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Train Sentiment Acc: {train_sent_acc:.4f}, Train Emotion Acc: {train_emo_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Sentiment Acc: {val_sent_acc:.4f}, F1: {val_sent_f1:.4f}\")\n",
    "            print(f\"  Val Emotion Acc: {val_emo_acc:.4f}, F1: {val_emo_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            combined_f1 = (val_sent_f1 + val_emo_f1) / 2\n",
    "            if combined_f1 > best_combined_f1:\n",
    "                best_combined_f1 = combined_f1\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best Combined F1: {best_combined_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'sentiment_classifier_state_dict': self.model.sentiment_classifier.state_dict(),\n",
    "            'emotion_classifier_state_dict': self.model.emotion_classifier.state_dict(),\n",
    "            'sentiment_num_classes': self.model.sentiment_num_classes,\n",
    "            'emotion_num_classes': self.model.emotion_num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "print(\"distilroBERTa Training classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f4d48-6c5b-4d42-bd64-562f883d91de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# In[9]:\n",
    "\n",
    "def evaluate_distilroberta_model(model_path: str, model_type: str, test_data: Dict, model_name: str, reddit_data: Dict = None):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Initialize the appropriate model architecture\n",
    "    if model_type == \"multitask\":\n",
    "        model = distilroBERTaMultiTaskTransformer(\n",
    "            model_name=model_name,\n",
    "            sentiment_num_classes=roberta_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=roberta_model_config.emotion_num_classes\n",
    "        )\n",
    "    else:\n",
    "        num_classes = (roberta_model_config.sentiment_num_classes \n",
    "                      if model_type == \"sentiment\" \n",
    "                      else roberta_model_config.emotion_num_classes)\n",
    "        model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=model_name,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    # Load the saved state dict\n",
    "    custom_components = torch.load(os.path.join(model_path, 'custom_components.pt'))\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        model.sentiment_classifier.load_state_dict(custom_components['sentiment_classifier_state_dict'])\n",
    "        model.emotion_classifier.load_state_dict(custom_components['emotion_classifier_state_dict'])\n",
    "    else:\n",
    "        model.classifier.load_state_dict(custom_components['classifier_state_dict'])\n",
    "    \n",
    "    # Load the base model weights\n",
    "    base_model = AutoModel.from_pretrained(model_path)\n",
    "    model.roberta = base_model\n",
    "    \n",
    "    # Make sure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate on general dataset\n",
    "    general_results = evaluate_on_dataset(model, model_type, test_data, tokenizer, \"General Dataset\")\n",
    "    \n",
    "    # Evaluate on Reddit dataset if available\n",
    "    reddit_results = None\n",
    "    if reddit_data is not None:\n",
    "        reddit_results = evaluate_on_dataset(model, model_type, reddit_data, tokenizer, \"Reddit Dataset\")\n",
    "    \n",
    "    return {\n",
    "        'general': general_results,\n",
    "        'reddit': reddit_results\n",
    "    }\n",
    "\n",
    "def evaluate_on_dataset(model, model_type: str, data: Dict, tokenizer, dataset_name: str):\n",
    "    \"\"\"Evaluate model on a specific dataset\"\"\"\n",
    "    print(f\"Evaluating on {dataset_name}...\")\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    if model_type == \"multitask\":\n",
    "        dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data['texts'],\n",
    "            sentiment_labels=data['sentiment_labels'],\n",
    "            emotion_labels=data['emotion_labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    else:\n",
    "        dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data['texts'],\n",
    "            labels=data['labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move everything to the same device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            if model_type == \"multitask\":\n",
    "                sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "                emotion_labels = batch['emotion_labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend([\n",
    "                    sentiment_preds.cpu().numpy(),\n",
    "                    emotion_preds.cpu().numpy()\n",
    "                ])\n",
    "                all_labels.extend([\n",
    "                    sentiment_labels.cpu().numpy(),\n",
    "                    emotion_labels.cpu().numpy()\n",
    "                ])\n",
    "            else:\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_accuracy = accuracy_score(all_labels[0], all_predictions[0])\n",
    "        sentiment_f1 = f1_score(all_labels[0], all_predictions[0], average='macro')\n",
    "        emotion_accuracy = accuracy_score(all_labels[1], all_predictions[1])\n",
    "        emotion_f1 = f1_score(all_labels[1], all_predictions[1], average='macro')\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1_macro': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1_macro': emotion_f1,\n",
    "            'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "            'combined_f1_macro': (sentiment_f1 + emotion_f1) / 2\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'accuracy': accuracy_score(all_labels, all_predictions),\n",
    "            'f1_macro': f1_score(all_labels, all_predictions, average='macro')\n",
    "        }\n",
    "    \n",
    "print(\"distilroBERTa evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13c0ab-2a50-4963-b410-47ffa0df29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "def create_tuning_subset(data_splits, subset_ratio=0.01):  # Even smaller: 1%\n",
    "    print(f\"ðŸ”ª Creating {subset_ratio*100:.0f}% subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:  # Minimum 20 samples\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'labels': [split_data['labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“Š Tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "def create_multitask_tuning_subset(data_splits, subset_ratio=0.01):\n",
    "    print(f\"ðŸ”ª Creating {subset_ratio*100:.0f}% multitask subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_multitask_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [split_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [split_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_multitask_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_multitask_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_multitask_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_multitask_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"Multitask tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572b7ba-9774-4273-bb54-f2aea74126a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Hyperparameter Tuning classes defined\n"
     ]
    }
   ],
   "source": [
    "# In[11]:\n",
    "\n",
    "class distilroBERTaHyperparameterTuner:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 15,\n",
    "        model_name: str = \"distilroberta-base\"\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.data_splits = data_splits\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(f\"distilroBERTa hyperparameter tuner initialized for {model_type}\")\n",
    "        print(f\"Using Random Search for optimization\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \n",
    "        # Sample hyperparameters \n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32])  # Changed from [4, 8, 16]\n",
    "        num_epochs = trial.suggest_int('num_epochs', 3, 6)  # Keep this the same\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.2)  # Narrowed range\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1)  # Adjusted range\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)  # Same range\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.3)  # Adjusted upper bound\n",
    "        max_length = 128  # Fixed value instead of tuning parameter\n",
    "        \n",
    "        # Multi-task specific parameter\n",
    "        alpha = trial.suggest_float('alpha', 0.4, 0.6) if self.model_type == \"multitask\" else 0.5  # Narrowed range\n",
    "        \n",
    "        # Create config\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./distilroberta_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Train model\n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = distilroBERTaMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return combined F1 score from general dataset (not Reddit)\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro'])\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro'])\n",
    "                combined_f1 = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "                print(f\"Trial {trial.number}: Combined F1 = {combined_f1:.4f}\")\n",
    "                return combined_f1\n",
    "                \n",
    "            else:\n",
    "                # Single task training\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = roberta_model_config.sentiment_num_classes\n",
    "                else:  # emotion\n",
    "                    num_classes = roberta_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = distilroBERTaSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return best F1 score from general dataset (not Reddit)\n",
    "                best_f1 = max(history['val_f1_macro'])\n",
    "                print(f\"Trial {trial.number}: F1 = {best_f1:.4f}\")\n",
    "                return best_f1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Clean up\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \n",
    "        # Create study with Random Search\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.RandomSampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nðŸ” Starting hyperparameter optimization for {self.model_type}...\")\n",
    "        print(f\"ðŸŽ¯ Random Search: {self.n_trials} trials\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nðŸ† Optimization completed for {self.model_type}!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best F1 score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "print(\"distilroBERTa Hyperparameter Tuning classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f219f-3842-4c5c-acb9-c26a25f5923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra-fast trainers defined\n"
     ]
    }
   ],
   "source": [
    "# In[12]:\n",
    "\n",
    "class UltraFastdistilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize tokenizer (reuse if possible)\n",
    "        if not hasattr(self, '_tokenizer_cache'):\n",
    "            UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache = AutoTokenizer.from_pretrained(config.model_name)\n",
    "            if UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token is None:\n",
    "                UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.eos_token\n",
    "        \n",
    "        self.tokenizer = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Speed-optimized data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # No multiprocessing for speed\n",
    "            pin_memory=False  # Disable pin_memory\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        # Simple optimizer setup\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "            labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Print progress for very small datasets\n",
    "            if batch_idx % max(1, len(self.train_loader) // 4) == 0:\n",
    "                print(f\"    Batch {batch_idx + 1}/{len(self.train_loader)}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader), correct_predictions / total_predictions\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "                labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"ðŸš€ Starting ultra-fast distilroBERTa training ({self.config.task_type})...\")\n",
    "        \n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"  ðŸ“ Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            print(f\"    Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "        \n",
    "        print(f\"âœ… Training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "\n",
    "print(\"Ultra-fast trainers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ed6f7-99bc-4add-bd76-dcee65c98ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra-fast trainers defined\n"
     ]
    }
   ],
   "source": [
    "# In[12]:\n",
    "\n",
    "class UltraFastdistilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize tokenizer (reuse if possible)\n",
    "        if not hasattr(self, '_tokenizer_cache'):\n",
    "            UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache = AutoTokenizer.from_pretrained(config.model_name)\n",
    "            if UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token is None:\n",
    "                UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.eos_token\n",
    "        \n",
    "        self.tokenizer = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Speed-optimized data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # No multiprocessing for speed\n",
    "            pin_memory=False  # Disable pin_memory\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        # Simple optimizer setup\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "            labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Print progress for very small datasets\n",
    "            if batch_idx % max(1, len(self.train_loader) // 4) == 0:\n",
    "                print(f\"    Batch {batch_idx + 1}/{len(self.train_loader)}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader), correct_predictions / total_predictions\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "                labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"ðŸš€ Starting ultra-fast distilroBERTa training ({self.config.task_type})...\")\n",
    "        \n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"  ðŸ“ Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            print(f\"    Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "        \n",
    "        print(f\"âœ… Training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "\n",
    "print(\"Ultra-fast trainers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292e6f0-eca2-4fb2-b7df-7f9d772c989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING distilroBERTa TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Loading and processing datasets for distilroBERTa...\n",
      "Loading external datasets for distilroBERTa...\n",
      "SST-2 dataset loaded: 67349 train samples\n",
      "GoEmotions dataset loaded: 43410 train samples\n",
      "ðŸ”„ Processing sentiment data for distilroBERTa...\n",
      "âœ… RoBERTa Sentiment data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "Processing emotion data for distilroBERTa...\n",
      "distilroBERTa Emotion data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "ðŸ”„ Creating multi-task dataset for distilroBERTa...\n",
      "distilroBERTa Multi-task data created:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "\n",
      "Loading Reddit evaluation data...\n",
      "Loading Reddit evaluation data...\n",
      "âœ… Reddit data loaded: 95 samples\n",
      "âœ… Reddit data prepared: 95 samples\n",
      "   Sentiment classes: ['Negative', 'Neutral', 'Positive']\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
      "Data loading completed!\n",
      "Sentiment data: 7000 train samples\n",
      "Emotion data: 7000 train samples\n",
      "Multitask data: 7000 train samples\n",
      "Reddit data: 95 evaluation samples\n",
      "Model: distilroberta-base\n",
      "Hyperparameter trials per model: 15\n"
     ]
    }
   ],
   "source": [
    "# In[13]:\n",
    "\n",
    "print(\"STARTING distilroBERTa TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear memory before starting\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Load and process datasets for distilroBERTa\n",
    "print(\"\\nLoading and processing datasets for distilroBERTa...\")\n",
    "sentiment_data, emotion_data = load_and_process_datasets_roberta()\n",
    "multitask_data = create_multitask_data_roberta(sentiment_data, emotion_data)\n",
    "\n",
    "# Load Reddit evaluation data\n",
    "print(\"\\nLoading Reddit evaluation data...\")\n",
    "reddit_data = load_reddit_evaluation_data()\n",
    "\n",
    "# Model configurations\n",
    "model_name = \"distilroberta-base\"\n",
    "n_trials = 15  # Number of hyperparameter tuning trials\n",
    "\n",
    "print(\"Data loading completed!\")\n",
    "print(f\"Sentiment data: {len(sentiment_data['train']['texts'])} train samples\")\n",
    "print(f\"Emotion data: {len(emotion_data['train']['texts'])} train samples\")\n",
    "print(f\"Multitask data: {len(multitask_data['train']['texts'])} train samples\")\n",
    "if reddit_data:\n",
    "    print(f\"Reddit data: {len(reddit_data['sentiment']['texts'])} evaluation samples\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Hyperparameter trials per model: {n_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3e804-9560-404f-b64f-732363b4cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: INITIAL DISTILROBERTA TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "2ï¸âƒ£ Training Initial distilroBERTa Sentiment Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/3\n",
      "  Train Loss: 0.6889, Train Acc: 0.7306\n",
      "  Val Loss: 0.5309, Val Acc: 0.8127, Val F1: 0.5669\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/3\n",
      "  Train Loss: 0.4799, Train Acc: 0.8426\n",
      "  Val Loss: 0.5704, Val Acc: 0.8273, Val F1: 0.5772\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/3\n",
      "  Train Loss: 0.4000, Train Acc: 0.8766\n",
      "  Val Loss: 0.6107, Val Acc: 0.8327, Val F1: 0.5811\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5811\n",
      "Evaluating on General Dataset...\n",
      "Evaluating on Reddit Dataset...\n",
      "\n",
      "âœ… Initial Sentiment Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.8333\n",
      "    F1 Macro: 0.5812\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.5474\n",
      "    F1 Macro: 0.2841\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "# Cell 9: Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: INITIAL DISTILROBERTA TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize results dictionary\n",
    "all_results = {}\n",
    "\n",
    "# Default configuration for sentiment\n",
    "default_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Training Initial distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model\n",
    "initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_sentiment,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate initial sentiment model on both general and Reddit datasets\n",
    "initial_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['sentiment'] if reddit_data else None\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_results['initial_sentiment'] = initial_sentiment_results\n",
    "\n",
    "print(f\"\\nâœ… Initial Sentiment Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {initial_sentiment_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "if initial_sentiment_results['reddit']:\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {initial_sentiment_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {initial_sentiment_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383cdaa-bc9a-4ad6-8eec-c8c36337dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "Training Initial distilroBERTa Emotion Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/3\n",
      "  Train Loss: 1.0259, Train Acc: 0.5974\n",
      "  Val Loss: 0.7381, Val Acc: 0.7313, Val F1: 0.6966\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/3\n",
      "  Train Loss: 0.6113, Train Acc: 0.7810\n",
      "  Val Loss: 0.7366, Val Acc: 0.7473, Val F1: 0.7110\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/3\n",
      "  Train Loss: 0.4374, Train Acc: 0.8453\n",
      "  Val Loss: 0.7902, Val Acc: 0.7600, Val F1: 0.7301\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7301\n",
      "Evaluating on General Dataset...\n",
      "Evaluating on Reddit Dataset...\n",
      "\n",
      "âœ… Initial Emotion Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.7593\n",
      "    F1 Macro: 0.7211\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.1368\n",
      "    F1 Macro: 0.0717\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for emotion\n",
    "default_config_emotion = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./initial_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Initial distilroBERTa Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial emotion model\n",
    "initial_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_emotion,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate initial emotion model on both general and Reddit datasets\n",
    "initial_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['emotion'] if reddit_data else None\n",
    ")\n",
    "all_results['initial_emotion'] = initial_emotion_results\n",
    "\n",
    "print(f\"\\nâœ… Initial Emotion Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {initial_emotion_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_emotion_results['general']['f1_macro']:.4f}\")\n",
    "if initial_emotion_results['reddit']:\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {initial_emotion_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {initial_emotion_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece99162-a00a-4bab-b2f7-1a9fbe1b0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "4ï¸âƒ£ Training Initial distilroBERTa Multi-task Model...\n",
      "============================================================\n",
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/3\n",
      "  Train Loss: 1.2160\n",
      "  Train Sentiment Acc: 0.6973, Train Emotion Acc: 0.2806\n",
      "  Val Loss: 1.0991\n",
      "  Val Sentiment Acc: 0.8227, F1: 0.5739\n",
      "  Val Emotion Acc: 0.3020, F1: 0.0773\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 2/3\n",
      "  Train Loss: 1.0836\n",
      "  Train Sentiment Acc: 0.8416, Train Emotion Acc: 0.2963\n",
      "  Val Loss: 1.0987\n",
      "  Val Sentiment Acc: 0.8260, F1: 0.5761\n",
      "  Val Emotion Acc: 0.2987, F1: 0.0859\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 3/3\n",
      "  Train Loss: 1.0172\n",
      "  Train Sentiment Acc: 0.8769, Train Emotion Acc: 0.3074\n",
      "  Val Loss: 1.1242\n",
      "  Val Sentiment Acc: 0.8280, F1: 0.5778\n",
      "  Val Emotion Acc: 0.2960, F1: 0.1044\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_multitask_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3411\n",
      "Evaluating on General Dataset...\n",
      "Evaluating on Reddit Dataset...\n",
      "\n",
      "âœ… Initial Multitask Model Results:\n",
      "  General Dataset:\n",
      "    Sentiment - Accuracy: 0.7500, F1: 0.5153\n",
      "    Emotion - Accuracy: 0.4375, F1: 0.1014\n",
      "    Combined - Accuracy: 0.5938, F1: 0.3084\n",
      "  Reddit Dataset:\n",
      "    Sentiment - Accuracy: 0.6250, F1: 0.2614\n",
      "    Emotion - Accuracy: 0.3125, F1: 0.1067\n",
      "    Combined - Accuracy: 0.4688, F1: 0.1841\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for multitask\n",
    "default_config_multitask = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    alpha=0.5,\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./initial_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Training Initial distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial multitask model\n",
    "initial_multitask_trainer = distilroBERTaMultiTaskTrainer(config=default_config_multitask)\n",
    "initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate initial multitask model on both general and Reddit datasets\n",
    "initial_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['multitask'] if reddit_data else None\n",
    ")\n",
    "all_results['initial_multitask'] = initial_multitask_results\n",
    "\n",
    "print(f\"\\nâœ… Initial Multitask Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Sentiment - Accuracy: {initial_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {initial_multitask_results['general']['emotion_accuracy']:.4f}, F1: {initial_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {initial_multitask_results['general']['combined_accuracy']:.4f}, F1: {initial_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if initial_multitask_results['reddit']:\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Sentiment - Accuracy: {initial_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Emotion - Accuracy: {initial_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Combined - Accuracy: {initial_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6695959-af71-4165-9de8-3ee96d381a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ INITIAL distilroBERTa RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š INITIAL distilroBERTa MODEL PERFORMANCE:\n",
      "  Sentiment Model:\n",
      "    General Dataset - Accuracy: 0.8333, F1: 0.5812\n",
      "    Reddit Dataset - Accuracy: 0.5474, F1: 0.2841\n",
      "\n",
      "  Emotion Model:\n",
      "    General Dataset - Accuracy: 0.7593, F1: 0.7211\n",
      "    Reddit Dataset - Accuracy: 0.1368, F1: 0.0717\n",
      "\n",
      "  Multitask Model:\n",
      "    General Dataset:\n",
      "      Sentiment - Accuracy: 0.7500, F1: 0.5153\n",
      "      Emotion - Accuracy: 0.4375, F1: 0.1014\n",
      "      Combined - Accuracy: 0.5938, F1: 0.3084\n",
      "    Reddit Dataset:\n",
      "      Sentiment - Accuracy: 0.6250, F1: 0.2614\n",
      "      Emotion - Accuracy: 0.3125, F1: 0.1067\n",
      "      Combined - Accuracy: 0.4688, F1: 0.1841\n",
      "\n",
      "ðŸ’¡ These are distilroBERTa baseline results. Now proceeding to hyperparameter tuning!\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ INITIAL distilroBERTa RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š INITIAL distilroBERTa MODEL PERFORMANCE:\")\n",
    "print(f\"  Sentiment Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {initial_sentiment_results['general']['accuracy']:.4f}, F1: {initial_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "if initial_sentiment_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {initial_sentiment_results['reddit']['accuracy']:.4f}, F1: {initial_sentiment_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Emotion Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {initial_emotion_results['general']['accuracy']:.4f}, F1: {initial_emotion_results['general']['f1_macro']:.4f}\")\n",
    "if initial_emotion_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {initial_emotion_results['reddit']['accuracy']:.4f}, F1: {initial_emotion_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Multitask Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {initial_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {initial_multitask_results['general']['emotion_accuracy']:.4f}, F1: {initial_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {initial_multitask_results['general']['combined_accuracy']:.4f}, F1: {initial_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if initial_multitask_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {initial_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {initial_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {initial_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Store results for later comparison\n",
    "all_results = {\n",
    "    'initial_sentiment': initial_sentiment_results,\n",
    "    'initial_emotion': initial_emotion_results,\n",
    "    'initial_multitask': initial_multitask_results\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ’¡ These are distilroBERTa baseline results. Now proceeding to hyperparameter tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844fb32-a422-4dd0-9a47-10daa97e4f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 12:43:26,655] A new study created in memory with name: no-name-d84629e3-d252-473c-8250-b5cad7c4d515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 2: HYPERPARAMETER TUNING - SENTIMENT\n",
      "================================================================================\n",
      "\n",
      "6ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Sentiment Model...\n",
      "============================================================\n",
      "distilroBERTa hyperparameter tuner initialized for sentiment\n",
      "Using Random Search for optimization\n",
      "\n",
      "ðŸ” Starting hyperparameter optimization for sentiment...\n",
      "ðŸŽ¯ Random Search: 5 trials\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 0.7439, Train Acc: 0.6879\n",
      "  Val Loss: 0.5852, Val Acc: 0.7800, Val F1: 0.5445\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.4983, Train Acc: 0.8301\n",
      "  Val Loss: 0.5147, Val Acc: 0.8173, Val F1: 0.5698\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.3907, Train Acc: 0.8716\n",
      "  Val Loss: 0.5497, Val Acc: 0.8233, Val F1: 0.5736\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.3117, Train Acc: 0.8933\n",
      "  Val Loss: 0.6296, Val Acc: 0.8320, Val F1: 0.5808\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.2629, Train Acc: 0.9046\n",
      "  Val Loss: 0.6567, Val Acc: 0.8313, Val F1: 0.5856\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5856\n",
      "Trial 0: F1 = 0.5856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 12:50:46,294] Trial 0 finished with value: 0.5855951746401464 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11560186404424366, 'weight_decay': 0.02403950683025824, 'hidden_dropout_prob': 0.1116167224336399, 'classifier_dropout': 0.273235229154987}. Best is trial 0 with value: 0.5855951746401464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 0.7399, Train Acc: 0.6867\n",
      "  Val Loss: 0.5359, Val Acc: 0.8067, Val F1: 0.5630\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.5456, Train Acc: 0.8121\n",
      "  Val Loss: 0.9844, Val Acc: 0.7500, Val F1: 0.5191\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.4262, Train Acc: 0.8630\n",
      "  Val Loss: 0.6071, Val Acc: 0.8207, Val F1: 0.5724\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.3540, Train Acc: 0.8854\n",
      "  Val Loss: 0.6633, Val Acc: 0.8293, Val F1: 0.5784\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.3024, Train Acc: 0.8993\n",
      "  Val Loss: 0.7458, Val Acc: 0.8227, Val F1: 0.5738\n",
      "\n",
      "ðŸ“ Epoch 6/6\n",
      "  Train Loss: 0.2600, Train Acc: 0.9084\n",
      "  Val Loss: 0.7806, Val Acc: 0.8167, Val F1: 0.5807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 12:59:31,122] Trial 1 finished with value: 0.5806643674908627 and parameters: {'learning_rate': 5.262490902114904e-05, 'batch_size': 16, 'num_epochs': 6, 'warmup_ratio': 0.18324426408004219, 'weight_decay': 0.029110519961044856, 'hidden_dropout_prob': 0.1363649934414201, 'classifier_dropout': 0.13668090197068677}. Best is trial 0 with value: 0.5855951746401464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5807\n",
      "Trial 1: F1 = 0.5807\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/4\n",
      "  Train Loss: 0.7661, Train Acc: 0.6771\n",
      "  Val Loss: 0.6164, Val Acc: 0.8020, Val F1: 0.5587\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/4\n",
      "  Train Loss: 0.5315, Train Acc: 0.8170\n",
      "  Val Loss: 0.5395, Val Acc: 0.8120, Val F1: 0.5658\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/4\n",
      "  Train Loss: 0.4271, Train Acc: 0.8553\n",
      "  Val Loss: 0.6255, Val Acc: 0.8147, Val F1: 0.5681\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/4\n",
      "  Train Loss: 0.3532, Train Acc: 0.8787\n",
      "  Val Loss: 0.6113, Val Acc: 0.8160, Val F1: 0.5691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:04:52,455] Trial 2 finished with value: 0.5691273080660836 and parameters: {'learning_rate': 3.2635193912846855e-05, 'batch_size': 16, 'num_epochs': 4, 'warmup_ratio': 0.16118528947223795, 'weight_decay': 0.022554447458683766, 'hidden_dropout_prob': 0.15842892970704364, 'classifier_dropout': 0.17327236865873835}. Best is trial 0 with value: 0.5855951746401464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5691\n",
      "Trial 2: F1 = 0.5691\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 0.7509, Train Acc: 0.6826\n",
      "  Val Loss: 0.5549, Val Acc: 0.7987, Val F1: 0.5568\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.5633, Train Acc: 0.7991\n",
      "  Val Loss: 0.5537, Val Acc: 0.8040, Val F1: 0.5615\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.4671, Train Acc: 0.8411\n",
      "  Val Loss: 0.5817, Val Acc: 0.8313, Val F1: 0.5801\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.4049, Train Acc: 0.8633\n",
      "  Val Loss: 0.6469, Val Acc: 0.8213, Val F1: 0.5736\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.3681, Train Acc: 0.8733\n",
      "  Val Loss: 0.6281, Val Acc: 0.8247, Val F1: 0.5755\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5801\n",
      "Trial 3: F1 = 0.5801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:09:50,320] Trial 3 finished with value: 0.5800826875346377 and parameters: {'learning_rate': 4.166863122305896e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.15924145688620425, 'weight_decay': 0.014180537144799797, 'hidden_dropout_prob': 0.22150897038028766, 'classifier_dropout': 0.1341048247374583}. Best is trial 0 with value: 0.5855951746401464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 0.8615, Train Acc: 0.5690\n",
      "  Val Loss: 0.5654, Val Acc: 0.8027, Val F1: 0.5591\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.5895, Train Acc: 0.7833\n",
      "  Val Loss: 0.5260, Val Acc: 0.8180, Val F1: 0.5704\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.5234, Train Acc: 0.8067\n",
      "  Val Loss: 0.5082, Val Acc: 0.8193, Val F1: 0.5714\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.4782, Train Acc: 0.8313\n",
      "  Val Loss: 0.5179, Val Acc: 0.8293, Val F1: 0.5786\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.4435, Train Acc: 0.8429\n",
      "  Val Loss: 0.5165, Val Acc: 0.8267, Val F1: 0.5772\n",
      "\n",
      "ðŸ“ Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:20:27,219] Trial 4 finished with value: 0.5786103330220977 and parameters: {'learning_rate': 2.2207471217033647e-05, 'batch_size': 32, 'num_epochs': 6, 'warmup_ratio': 0.13046137691733709, 'weight_decay': 0.018790490260574548, 'hidden_dropout_prob': 0.23684660530243137, 'classifier_dropout': 0.18803049874792027}. Best is trial 0 with value: 0.5855951746401464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.4227, Train Acc: 0.8511\n",
      "  Val Loss: 0.5189, Val Acc: 0.8287, Val F1: 0.5783\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5786\n",
      "Trial 4: F1 = 0.5786\n",
      "\n",
      "ðŸ† Optimization completed for sentiment!\n",
      "Best trial: 0\n",
      "Best F1 score: 0.5856\n",
      "Best parameters:\n",
      "  learning_rate: 3.65445235521325e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 5\n",
      "  warmup_ratio: 0.11560186404424366\n",
      "  weight_decay: 0.02403950683025824\n",
      "  hidden_dropout_prob: 0.1116167224336399\n",
      "  classifier_dropout: 0.273235229154987\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 2: HYPERPARAMETER TUNING - SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for sentiment\n",
    "sentiment_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"sentiment\",\n",
    "    data_splits=sentiment_data,\n",
    "    n_trials=5,\n",
    "    model_name=model_name\n",
    ")\n",
    "sentiment_study = sentiment_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9cd6c-6f08-408d-820c-0eab5affa28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:20:27,240] A new study created in memory with name: no-name-33f46561-b71b-4be8-abe2-1ef452c215da\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 2: HYPERPARAMETER TUNING - EMOTION\n",
      "================================================================================\n",
      "\n",
      "7ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Emotion Mo cdel...\n",
      "============================================================\n",
      "distilroBERTa hyperparameter tuner initialized for emotion\n",
      "Using Random Search for optimization\n",
      "\n",
      "ðŸ” Starting hyperparameter optimization for emotion...\n",
      "ðŸŽ¯ Random Search: 5 trials\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.1190, Train Acc: 0.5633\n",
      "  Val Loss: 0.7822, Val Acc: 0.7073, Val F1: 0.6354\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.6473, Train Acc: 0.7630\n",
      "  Val Loss: 0.7377, Val Acc: 0.7507, Val F1: 0.7227\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.4602, Train Acc: 0.8341\n",
      "  Val Loss: 0.8189, Val Acc: 0.7473, Val F1: 0.7112\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.3166, Train Acc: 0.8906\n",
      "  Val Loss: 0.8873, Val Acc: 0.7447, Val F1: 0.7135\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.2081, Train Acc: 0.9300\n",
      "  Val Loss: 0.9370, Val Acc: 0.7527, Val F1: 0.7267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:24:46,762] Trial 0 finished with value: 0.7266554701446585 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11560186404424366, 'weight_decay': 0.02403950683025824, 'hidden_dropout_prob': 0.1116167224336399, 'classifier_dropout': 0.273235229154987}. Best is trial 0 with value: 0.7266554701446585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7267\n",
      "Trial 0: F1 = 0.7267\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 1.1750, Train Acc: 0.5414\n",
      "  Val Loss: 0.7865, Val Acc: 0.7207, Val F1: 0.6821\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.7221, Train Acc: 0.7403\n",
      "  Val Loss: 0.7405, Val Acc: 0.7487, Val F1: 0.7120\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.5052, Train Acc: 0.8180\n",
      "  Val Loss: 0.8640, Val Acc: 0.7427, Val F1: 0.7109\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.3291, Train Acc: 0.8813\n",
      "  Val Loss: 0.9431, Val Acc: 0.7307, Val F1: 0.6955\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.2068, Train Acc: 0.9303\n",
      "  Val Loss: 1.1271, Val Acc: 0.7340, Val F1: 0.7002\n",
      "\n",
      "ðŸ“ Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:29:59,188] Trial 1 finished with value: 0.7120221078190857 and parameters: {'learning_rate': 5.262490902114904e-05, 'batch_size': 16, 'num_epochs': 6, 'warmup_ratio': 0.18324426408004219, 'weight_decay': 0.029110519961044856, 'hidden_dropout_prob': 0.1363649934414201, 'classifier_dropout': 0.13668090197068677}. Best is trial 0 with value: 0.7266554701446585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1301, Train Acc: 0.9603\n",
      "  Val Loss: 1.2428, Val Acc: 0.7360, Val F1: 0.7026\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7120\n",
      "Trial 1: F1 = 0.7120\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/4\n",
      "  Train Loss: 1.1658, Train Acc: 0.5391\n",
      "  Val Loss: 0.8321, Val Acc: 0.6840, Val F1: 0.6217\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/4\n",
      "  Train Loss: 0.6797, Train Acc: 0.7501\n",
      "  Val Loss: 0.7412, Val Acc: 0.7413, Val F1: 0.7059\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/4\n",
      "  Train Loss: 0.4999, Train Acc: 0.8190\n",
      "  Val Loss: 0.7855, Val Acc: 0.7453, Val F1: 0.7118\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/4\n",
      "  Train Loss: 0.3895, Train Acc: 0.8576\n",
      "  Val Loss: 0.8194, Val Acc: 0.7533, Val F1: 0.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:33:29,957] Trial 2 finished with value: 0.7208486248547187 and parameters: {'learning_rate': 3.2635193912846855e-05, 'batch_size': 16, 'num_epochs': 4, 'warmup_ratio': 0.16118528947223795, 'weight_decay': 0.022554447458683766, 'hidden_dropout_prob': 0.15842892970704364, 'classifier_dropout': 0.17327236865873835}. Best is trial 0 with value: 0.7266554701446585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7208\n",
      "Trial 2: F1 = 0.7208\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.2044, Train Acc: 0.5194\n",
      "  Val Loss: 0.8866, Val Acc: 0.7167, Val F1: 0.6628\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.7387, Train Acc: 0.7306\n",
      "  Val Loss: 0.8381, Val Acc: 0.7187, Val F1: 0.6791\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.5862, Train Acc: 0.7891\n",
      "  Val Loss: 0.8555, Val Acc: 0.7473, Val F1: 0.7168\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.4622, Train Acc: 0.8343\n",
      "  Val Loss: 0.8654, Val Acc: 0.7460, Val F1: 0.7189\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:38:31,861] Trial 3 finished with value: 0.7188542488682588 and parameters: {'learning_rate': 4.166863122305896e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.15924145688620425, 'weight_decay': 0.014180537144799797, 'hidden_dropout_prob': 0.22150897038028766, 'classifier_dropout': 0.1341048247374583}. Best is trial 0 with value: 0.7266554701446585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.3814, Train Acc: 0.8656\n",
      "  Val Loss: 0.8847, Val Acc: 0.7467, Val F1: 0.7156\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7189\n",
      "Trial 3: F1 = 0.7189\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 1.4828, Train Acc: 0.3951\n",
      "  Val Loss: 0.8550, Val Acc: 0.6907, Val F1: 0.6360\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.7959, Train Acc: 0.6993\n",
      "  Val Loss: 0.7516, Val Acc: 0.7413, Val F1: 0.7069\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.6662, Train Acc: 0.7566\n",
      "  Val Loss: 0.7764, Val Acc: 0.7453, Val F1: 0.7119\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.6001, Train Acc: 0.7833\n",
      "  Val Loss: 0.7813, Val Acc: 0.7427, Val F1: 0.7103\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.5556, Train Acc: 0.7897\n",
      "  Val Loss: 0.7981, Val Acc: 0.7493, Val F1: 0.7136\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 6/6\n",
      "  Train Loss: 0.5123, Train Acc: 0.8094\n",
      "  Val Loss: 0.7975, Val Acc: 0.7433, Val F1: 0.7091\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7136\n",
      "Trial 4: F1 = 0.7136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:49:16,537] Trial 4 finished with value: 0.7135875327822228 and parameters: {'learning_rate': 2.2207471217033647e-05, 'batch_size': 32, 'num_epochs': 6, 'warmup_ratio': 0.13046137691733709, 'weight_decay': 0.018790490260574548, 'hidden_dropout_prob': 0.23684660530243137, 'classifier_dropout': 0.18803049874792027}. Best is trial 0 with value: 0.7266554701446585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† Optimization completed for emotion!\n",
      "Best trial: 0\n",
      "Best F1 score: 0.7267\n",
      "Best parameters:\n",
      "  learning_rate: 3.65445235521325e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 5\n",
      "  warmup_ratio: 0.11560186404424366\n",
      "  weight_decay: 0.02403950683025824\n",
      "  hidden_dropout_prob: 0.1116167224336399\n",
      "  classifier_dropout: 0.273235229154987\n"
     ]
    }
   ],
   "source": [
    "# In[23]:\n",
    "\n",
    "# Cell 13: Hyperparameter Tuning - Emotion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 2: HYPERPARAMETER TUNING - EMOTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n7ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Emotion Mo cdel...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for emotion\n",
    "emotion_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"emotion\",\n",
    "    data_splits=emotion_data,\n",
    "    n_trials=5,\n",
    "    model_name=model_name\n",
    ")\n",
    "emotion_study = emotion_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9612944-3721-4579-8d43-7e7be38b91d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:49:16,581] A new study created in memory with name: no-name-b3a828c4-3b0b-4860-b96f-b21cb66fac49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 2: HYPERPARAMETER TUNING - MULTITASK\n",
      "================================================================================\n",
      "\n",
      "8ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Multi-task Model...\n",
      "============================================================\n",
      "distilroBERTa hyperparameter tuner initialized for multitask\n",
      "Using Random Search for optimization\n",
      "\n",
      "ðŸ” Starting hyperparameter optimization for multitask...\n",
      "ðŸŽ¯ Random Search: 5 trials\n",
      "============================================================\n",
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/5\n",
      "  Train Loss: 1.2240\n",
      "  Train Sentiment Acc: 0.6910, Train Emotion Acc: 0.2693\n",
      "  Val Loss: 1.1068\n",
      "  Val Sentiment Acc: 0.8027, F1: 0.5604\n",
      "  Val Emotion Acc: 0.3000, F1: 0.0821\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "Epoch 2/5\n",
      "  Train Loss: 1.0856\n",
      "  Train Sentiment Acc: 0.8267, Train Emotion Acc: 0.2809\n",
      "  Val Loss: 1.1097\n",
      "  Val Sentiment Acc: 0.8227, F1: 0.5746\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0775\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "Epoch 3/5\n",
      "  Train Loss: 1.0191\n",
      "  Train Sentiment Acc: 0.8717, Train Emotion Acc: 0.2913\n",
      "  Val Loss: 1.0920\n",
      "  Val Sentiment Acc: 0.8280, F1: 0.5776\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0787\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "Epoch 4/5\n",
      "  Train Loss: 0.9604\n",
      "  Train Sentiment Acc: 0.8963, Train Emotion Acc: 0.3091\n",
      "  Val Loss: 1.1283\n",
      "  Val Sentiment Acc: 0.8347, F1: 0.5824\n",
      "  Val Emotion Acc: 0.2780, F1: 0.1285\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "Epoch 5/5\n",
      "  Train Loss: 0.9103\n",
      "  Train Sentiment Acc: 0.9043, Train Emotion Acc: 0.3454\n",
      "  Val Loss: 1.1676\n",
      "  Val Sentiment Acc: 0.8273, F1: 0.5771\n",
      "  Val Emotion Acc: 0.2700, F1: 0.1358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 13:53:56,715] Trial 0 finished with value: 0.35907448922484525 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11560186404424366, 'weight_decay': 0.02403950683025824, 'hidden_dropout_prob': 0.1116167224336399, 'classifier_dropout': 0.273235229154987, 'alpha': 0.5202230023486417}. Best is trial 0 with value: 0.35907448922484525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3565\n",
      "Trial 0: Combined F1 = 0.3591\n",
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/6\n",
      "  Train Loss: 1.2373\n",
      "  Train Sentiment Acc: 0.6843, Train Emotion Acc: 0.2721\n",
      "  Val Loss: 1.1256\n",
      "  Val Sentiment Acc: 0.7920, F1: 0.5527\n",
      "  Val Emotion Acc: 0.2620, F1: 0.1284\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "Epoch 2/6\n",
      "  Train Loss: 1.1166\n",
      "  Train Sentiment Acc: 0.8153, Train Emotion Acc: 0.2841\n",
      "  Val Loss: 1.0925\n",
      "  Val Sentiment Acc: 0.8233, F1: 0.5745\n",
      "  Val Emotion Acc: 0.3013, F1: 0.0805\n",
      "\n",
      "Epoch 3/6\n",
      "  Train Loss: 1.0421\n",
      "  Train Sentiment Acc: 0.8663, Train Emotion Acc: 0.2903\n",
      "  Val Loss: 1.1254\n",
      "  Val Sentiment Acc: 0.8160, F1: 0.5700\n",
      "  Val Emotion Acc: 0.2807, F1: 0.1148\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "Epoch 4/6\n",
      "  Train Loss: 0.9859\n",
      "  Train Sentiment Acc: 0.8891, Train Emotion Acc: 0.3084\n",
      "  Val Loss: 1.1339\n",
      "  Val Sentiment Acc: 0.8220, F1: 0.5735\n",
      "  Val Emotion Acc: 0.2560, F1: 0.1285\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "Epoch 5/6\n",
      "  Train Loss: 0.9289\n",
      "  Train Sentiment Acc: 0.9006, Train Emotion Acc: 0.3596\n",
      "  Val Loss: 1.1806\n",
      "  Val Sentiment Acc: 0.8240, F1: 0.5855\n",
      "  Val Emotion Acc: 0.2380, F1: 0.1399\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "Epoch 6/6\n",
      "  Train Loss: 0.8693\n",
      "  Train Sentiment Acc: 0.9093, Train Emotion Acc: 0.4057\n",
      "  Val Loss: 1.2103\n",
      "  Val Sentiment Acc: 0.8293, F1: 0.5984\n",
      "  Val Emotion Acc: 0.2493, F1: 0.1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:17:42,417] Trial 1 finished with value: 0.3752436806566236 and parameters: {'learning_rate': 6.251028636335231e-05, 'batch_size': 32, 'num_epochs': 6, 'warmup_ratio': 0.12123391106782762, 'weight_decay': 0.02636424704863906, 'hidden_dropout_prob': 0.13668090197068677, 'classifier_dropout': 0.16084844859190756, 'alpha': 0.5049512863264476}. Best is trial 1 with value: 0.3752436806566236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3752\n",
      "Trial 1: Combined F1 = 0.3752\n",
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/3\n",
      "  Train Loss: 1.3170\n",
      "  Train Sentiment Acc: 0.6470, Train Emotion Acc: 0.2750\n",
      "  Val Loss: 1.2168\n",
      "  Val Sentiment Acc: 0.7913, F1: 0.5508\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0774\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "Epoch 2/3\n",
      "  Train Loss: 1.2013\n",
      "  Train Sentiment Acc: 0.8133, Train Emotion Acc: 0.2856\n",
      "  Val Loss: 1.1789\n",
      "  Val Sentiment Acc: 0.8060, F1: 0.5626\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0775\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "Epoch 3/3\n",
      "  Train Loss: 1.1588\n",
      "  Train Sentiment Acc: 0.8410, Train Emotion Acc: 0.2884\n",
      "  Val Loss: 1.1843\n",
      "  Val Sentiment Acc: 0.8147, F1: 0.5684\n",
      "  Val Emotion Acc: 0.3033, F1: 0.0786\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3235\n",
      "Trial 2: Combined F1 = 0.3235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:28:37,670] Trial 2 finished with value: 0.3235255076358019 and parameters: {'learning_rate': 4.008174375308313e-05, 'batch_size': 32, 'num_epochs': 3, 'warmup_ratio': 0.12921446485352184, 'weight_decay': 0.04297256589643226, 'hidden_dropout_prob': 0.19121399684340717, 'classifier_dropout': 0.2570351922786027, 'alpha': 0.43993475643167196}. Best is trial 1 with value: 0.3752436806566236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/5\n",
      "  Train Loss: 1.2379\n",
      "  Train Sentiment Acc: 0.6309, Train Emotion Acc: 0.2597\n",
      "  Val Loss: 1.0503\n",
      "  Val Sentiment Acc: 0.8027, F1: 0.5601\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0774\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "Epoch 2/5\n",
      "  Train Loss: 1.0998\n",
      "  Train Sentiment Acc: 0.7839, Train Emotion Acc: 0.2683\n",
      "  Val Loss: 1.0741\n",
      "  Val Sentiment Acc: 0.8160, F1: 0.5689\n",
      "  Val Emotion Acc: 0.2867, F1: 0.1083\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "Epoch 3/5\n",
      "  Train Loss: 1.0582\n",
      "  Train Sentiment Acc: 0.8127, Train Emotion Acc: 0.2789\n",
      "  Val Loss: 1.0753\n",
      "  Val Sentiment Acc: 0.8140, F1: 0.5681\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0785\n",
      "\n",
      "Epoch 4/5\n",
      "  Train Loss: 1.0265\n",
      "  Train Sentiment Acc: 0.8289, Train Emotion Acc: 0.2760\n",
      "  Val Loss: 1.0614\n",
      "  Val Sentiment Acc: 0.8260, F1: 0.5763\n",
      "  Val Emotion Acc: 0.3013, F1: 0.0828\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:35:46,494] Trial 3 finished with value: 0.3423072426689452 and parameters: {'learning_rate': 4.575772704489176e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11705241236872915, 'weight_decay': 0.015854643368675158, 'hidden_dropout_prob': 0.28977710745066665, 'classifier_dropout': 0.29312640661491185, 'alpha': 0.5616794696232922}. Best is trial 1 with value: 0.3752436806566236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.9957\n",
      "  Train Sentiment Acc: 0.8503, Train Emotion Acc: 0.2901\n",
      "  Val Loss: 1.0726\n",
      "  Val Sentiment Acc: 0.8247, F1: 0.5754\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0785\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3386\n",
      "Trial 3: Combined F1 = 0.3423\n",
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/4\n",
      "  Train Loss: 1.2982\n",
      "  Train Sentiment Acc: 0.6557, Train Emotion Acc: 0.2640\n",
      "  Val Loss: 1.1826\n",
      "  Val Sentiment Acc: 0.7967, F1: 0.5563\n",
      "  Val Emotion Acc: 0.3047, F1: 0.0871\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "Epoch 2/4\n",
      "  Train Loss: 1.1656\n",
      "  Train Sentiment Acc: 0.8226, Train Emotion Acc: 0.2933\n",
      "  Val Loss: 1.1664\n",
      "  Val Sentiment Acc: 0.8053, F1: 0.5603\n",
      "  Val Emotion Acc: 0.3000, F1: 0.0861\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "Epoch 3/4\n",
      "  Train Loss: 1.1072\n",
      "  Train Sentiment Acc: 0.8663, Train Emotion Acc: 0.2997\n",
      "  Val Loss: 1.1729\n",
      "  Val Sentiment Acc: 0.8313, F1: 0.5794\n",
      "  Val Emotion Acc: 0.2987, F1: 0.0872\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "Epoch 4/4\n",
      "  Train Loss: 1.0666\n",
      "  Train Sentiment Acc: 0.8861, Train Emotion Acc: 0.3081\n",
      "  Val Loss: 1.1775\n",
      "  Val Sentiment Acc: 0.8347, F1: 0.5823\n",
      "  Val Emotion Acc: 0.2920, F1: 0.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:51:57,498] Trial 4 finished with value: 0.34570425707326 and parameters: {'learning_rate': 3.26547139093759e-05, 'batch_size': 32, 'num_epochs': 4, 'warmup_ratio': 0.11220382348447788, 'weight_decay': 0.054565921910014324, 'hidden_dropout_prob': 0.10687770422304368, 'classifier_dropout': 0.2818640804157564, 'alpha': 0.4517559963200034}. Best is trial 1 with value: 0.3752436806566236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3457\n",
      "Trial 4: Combined F1 = 0.3457\n",
      "\n",
      "ðŸ† Optimization completed for multitask!\n",
      "Best trial: 1\n",
      "Best F1 score: 0.3752\n",
      "Best parameters:\n",
      "  learning_rate: 6.251028636335231e-05\n",
      "  batch_size: 32\n",
      "  num_epochs: 6\n",
      "  warmup_ratio: 0.12123391106782762\n",
      "  weight_decay: 0.02636424704863906\n",
      "  hidden_dropout_prob: 0.13668090197068677\n",
      "  classifier_dropout: 0.16084844859190756\n",
      "  alpha: 0.5049512863264476\n"
     ]
    }
   ],
   "source": [
    "# In[24]:\n",
    "\n",
    "# Cell 14: Hyperparameter Tuning - Multitask\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 2: HYPERPARAMETER TUNING - MULTITASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n8ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for multitask\n",
    "multitask_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"multitask\",\n",
    "    data_splits=multitask_data,\n",
    "    n_trials=5,\n",
    "    model_name=model_name\n",
    ")\n",
    "multitask_study = multitask_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ac0a1-9e5a-4d10-90bb-9d6203535f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "9ï¸âƒ£ Training Final distilroBERTa Sentiment Model with Best Parameters...\n",
      "============================================================\n",
      "ðŸŽ¯ Using best hyperparameters:\n",
      "  learning_rate: 3.65445235521325e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 5\n",
      "  warmup_ratio: 0.11560186404424366\n",
      "  weight_decay: 0.02403950683025824\n",
      "  hidden_dropout_prob: 0.1116167224336399\n",
      "  classifier_dropout: 0.273235229154987\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 0.7376, Train Acc: 0.6953\n",
      "  Val Loss: 0.5447, Val Acc: 0.8087, Val F1: 0.5640\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.5464, Train Acc: 0.8120\n",
      "  Val Loss: 0.4997, Val Acc: 0.8227, Val F1: 0.5740\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.4175, Train Acc: 0.8629\n",
      "  Val Loss: 0.5749, Val Acc: 0.8133, Val F1: 0.5675\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.3399, Train Acc: 0.8896\n",
      "  Val Loss: 0.6631, Val Acc: 0.8220, Val F1: 0.5729\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.2804, Train Acc: 0.9027\n",
      "  Val Loss: 0.7210, Val Acc: 0.8233, Val F1: 0.5744\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5744\n",
      "Evaluating on General Dataset...\n",
      "Evaluating on Reddit Dataset...\n",
      "\n",
      "Final Sentiment Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.8333\n",
      "    F1 Macro: 0.5811\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.6105\n",
      "    F1 Macro: 0.3988\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n9ï¸âƒ£ Training Final distilroBERTa Sentiment Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_sentiment_params = sentiment_study.best_params\n",
    "print(f\"ðŸŽ¯ Using best hyperparameters:\")\n",
    "for key, value in best_sentiment_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if 'all_results' not in globals():\n",
    "    all_results = {}\n",
    "\n",
    "final_sentiment_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=5.262490902114904e-05,  # From best parameters\n",
    "    batch_size=16,\n",
    "    num_epochs=5,  # Increased epochs for final training\n",
    "    warmup_ratio=0.18324426408004219,\n",
    "    weight_decay=0.029110519961044856,\n",
    "    hidden_dropout_prob=0.1363649934414201,\n",
    "    classifier_dropout=0.13668090197068677,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./final_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "# Train final sentiment model\n",
    "final_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_sentiment_config,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "final_sentiment_history = final_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate final sentiment model on both general and Reddit datasets\n",
    "final_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['sentiment'] if reddit_data else None\n",
    ")\n",
    "\n",
    "# Now store the results\n",
    "all_results['final_sentiment'] = final_sentiment_results\n",
    "\n",
    "print(f\"\\nFinal Sentiment Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {final_sentiment_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {final_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "if final_sentiment_results['reddit']:\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {final_sentiment_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {final_sentiment_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6478c-ac19-4598-a591-3ee404752a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ÿ Training Final distilroBERTa Emotion Model with Best Parameters...\n",
      "============================================================\n",
      "ðŸŽ¯ Using best hyperparameters:\n",
      "  learning_rate: 3.65445235521325e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 5\n",
      "  warmup_ratio: 0.11560186404424366\n",
      "  weight_decay: 0.02403950683025824\n",
      "  hidden_dropout_prob: 0.1116167224336399\n",
      "  classifier_dropout: 0.273235229154987\n",
      "\n",
      "ðŸš€ Training final emotion model:\n",
      "  Dataset: Full emotion data (7000 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 16\n",
      "  Learning rate: 3.65e-05\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.1020, Train Acc: 0.5627\n",
      "  Val Loss: 0.7840, Val Acc: 0.7140, Val F1: 0.6534\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.6616, Train Acc: 0.7573\n",
      "  Val Loss: 0.7641, Val Acc: 0.7427, Val F1: 0.6980\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.4659, Train Acc: 0.8339\n",
      "  Val Loss: 0.7909, Val Acc: 0.7393, Val F1: 0.7124\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.3144, Train Acc: 0.8860\n",
      "  Val Loss: 0.9317, Val Acc: 0.7333, Val F1: 0.7031\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.2091, Train Acc: 0.9294\n",
      "  Val Loss: 0.9864, Val Acc: 0.7473, Val F1: 0.7185\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7185\n",
      "Evaluating on General Dataset...\n",
      "Evaluating on Reddit Dataset...\n",
      "\n",
      "âœ… Final Emotion Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.7627\n",
      "    F1 Macro: 0.7264\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.1158\n",
      "    F1 Macro: 0.0782\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ”Ÿ Training Final distilroBERTa Emotion Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize results dictionary if not exists\n",
    "if 'all_results' not in globals():\n",
    "    all_results = {}\n",
    "\n",
    "# Get best parameters from emotion tuning\n",
    "best_emotion_params = emotion_study.best_params\n",
    "print(f\"ðŸŽ¯ Using best hyperparameters:\")\n",
    "for key, value in best_emotion_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_emotion_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_emotion_params['learning_rate'],\n",
    "    batch_size=best_emotion_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_emotion_params['warmup_ratio'],\n",
    "    weight_decay=best_emotion_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_emotion_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_emotion_params['classifier_dropout'],\n",
    "    max_length=best_emotion_params.get('max_length', 128),\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./final_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸš€ Training final emotion model:\")\n",
    "print(f\"  Dataset: Full emotion data ({len(emotion_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_emotion_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_emotion_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_emotion_config.learning_rate:.2e}\")\n",
    "\n",
    "# Train final emotion model\n",
    "final_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_emotion_config,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "final_emotion_history = final_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate final emotion model on both general and Reddit datasets\n",
    "final_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['emotion'] if reddit_data else None\n",
    ")\n",
    "all_results['final_emotion'] = final_emotion_results\n",
    "\n",
    "print(f\"\\nâœ… Final Emotion Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {final_emotion_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {final_emotion_results['general']['f1_macro']:.4f}\")\n",
    "if final_emotion_results['reddit']:\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {final_emotion_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {final_emotion_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f842be0-3854-4cbf-8651-648d49f225de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£1ï¸âƒ£ Training Final distilroBERTa Multi-task Model with Best Parameters...\n",
      "============================================================\n",
      "ðŸŽ¯ Using best hyperparameters:\n",
      "  learning_rate: 6.251028636335231e-05\n",
      "  batch_size: 32\n",
      "  num_epochs: 6\n",
      "  warmup_ratio: 0.12123391106782762\n",
      "  weight_decay: 0.02636424704863906\n",
      "  hidden_dropout_prob: 0.13668090197068677\n",
      "  classifier_dropout: 0.16084844859190756\n",
      "  alpha: 0.5049512863264476\n",
      "\n",
      "ðŸš€ Training final multitask model:\n",
      "  Dataset: Full multitask data (7000 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 32\n",
      "  Learning rate: 6.25e-05\n",
      "  Alpha (loss weighting): 0.505\n",
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/5\n",
      "  Train Loss: 1.2325\n",
      "  Train Sentiment Acc: 0.6946, Train Emotion Acc: 0.2700\n",
      "  Val Loss: 1.1181\n",
      "  Val Sentiment Acc: 0.7960, F1: 0.5541\n",
      "  Val Emotion Acc: 0.2860, F1: 0.0977\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 2/5\n",
      "  Train Loss: 1.1044\n",
      "  Train Sentiment Acc: 0.8239, Train Emotion Acc: 0.2913\n",
      "  Val Loss: 1.1014\n",
      "  Val Sentiment Acc: 0.8260, F1: 0.5763\n",
      "  Val Emotion Acc: 0.2907, F1: 0.0847\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 3/5\n",
      "  Train Loss: 1.0389\n",
      "  Train Sentiment Acc: 0.8686, Train Emotion Acc: 0.2937\n",
      "  Val Loss: 1.1452\n",
      "  Val Sentiment Acc: 0.8187, F1: 0.5712\n",
      "  Val Emotion Acc: 0.2953, F1: 0.1072\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 4/5\n",
      "  Train Loss: 0.9857\n",
      "  Train Sentiment Acc: 0.8901, Train Emotion Acc: 0.3083\n",
      "  Val Loss: 1.1714\n",
      "  Val Sentiment Acc: 0.8207, F1: 0.5730\n",
      "  Val Emotion Acc: 0.2733, F1: 0.1164\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 5/5\n",
      "  Train Loss: 0.9323\n",
      "  Train Sentiment Acc: 0.9020, Train Emotion Acc: 0.3446\n",
      "  Val Loss: 1.1921\n",
      "  Val Sentiment Acc: 0.8200, F1: 0.5723\n",
      "  Val Emotion Acc: 0.2627, F1: 0.1401\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3562\n",
      "Evaluating on General Dataset...\n",
      "Evaluating on Reddit Dataset...\n",
      "\n",
      "âœ… Final Multitask Model Results:\n",
      "  General Dataset:\n",
      "    Sentiment - Accuracy: 0.8125, F1: 0.5602\n",
      "    Emotion - Accuracy: 0.2188, F1: 0.0866\n",
      "    Combined - Accuracy: 0.5156, F1: 0.3234\n",
      "  Reddit Dataset:\n",
      "    Sentiment - Accuracy: 0.6562, F1: 0.3673\n",
      "    Emotion - Accuracy: 0.3125, F1: 0.1705\n",
      "    Combined - Accuracy: 0.4844, F1: 0.2689\n",
      "\n",
      "================================================================================\n",
      "ðŸ FINAL COMPREHENSIVE RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š DISTILROBERTA MODEL PERFORMANCE COMPARISON:\n",
      "  ============================================================\n",
      "\n",
      "ðŸŽ¯ SENTIMENT MODEL:\n",
      "  Initial Model:\n",
      "    General Dataset - Accuracy: 0.8333, F1: 0.5812\n",
      "    Reddit Dataset - Accuracy: 0.5474, F1: 0.2841\n",
      "  Final Optimized Model:\n",
      "    General Dataset - Accuracy: 0.8333, F1: 0.5811\n",
      "    Reddit Dataset - Accuracy: 0.6105, F1: 0.3988\n",
      "  Improvements:\n",
      "    General Accuracy: +0.0000\n",
      "    General F1: -0.0001\n",
      "\n",
      "ðŸŽ­ EMOTION MODEL:\n",
      "  Initial Model:\n",
      "    General Dataset - Accuracy: 0.7593, F1: 0.7211\n",
      "    Reddit Dataset - Accuracy: 0.1368, F1: 0.0717\n",
      "  Final Optimized Model:\n",
      "    General Dataset - Accuracy: 0.7627, F1: 0.7264\n",
      "    Reddit Dataset - Accuracy: 0.1158, F1: 0.0782\n",
      "  Improvements:\n",
      "    General Accuracy: +0.0033\n",
      "    General F1: +0.0053\n",
      "\n",
      "ðŸ”„ MULTITASK MODEL:\n",
      "  Initial Model:\n",
      "    General Dataset:\n",
      "      Sentiment - Accuracy: 0.7500, F1: 0.5153\n",
      "      Emotion - Accuracy: 0.4375, F1: 0.1014\n",
      "      Combined - Accuracy: 0.5938, F1: 0.3084\n",
      "    Reddit Dataset:\n",
      "      Sentiment - Accuracy: 0.6250, F1: 0.2614\n",
      "      Emotion - Accuracy: 0.3125, F1: 0.1067\n",
      "      Combined - Accuracy: 0.4688, F1: 0.1841\n",
      "  Final Optimized Model:\n",
      "    General Dataset:\n",
      "      Sentiment - Accuracy: 0.8125, F1: 0.5602\n",
      "      Emotion - Accuracy: 0.2188, F1: 0.0866\n",
      "      Combined - Accuracy: 0.5156, F1: 0.3234\n",
      "    Reddit Dataset:\n",
      "      Sentiment - Accuracy: 0.6562, F1: 0.3673\n",
      "      Emotion - Accuracy: 0.3125, F1: 0.1705\n",
      "      Combined - Accuracy: 0.4844, F1: 0.2689\n",
      "  Improvements:\n",
      "    Sentiment Accuracy: +0.0625\n",
      "    Emotion Accuracy: -0.2188\n",
      "    Combined Accuracy: -0.0781\n",
      "\n",
      "ðŸŽ‰ DISTILROBERTA TRAINING PIPELINE COMPLETED SUCCESSFULLY!\n",
      "   All models trained and evaluated on both general and Reddit datasets\n",
      "   Hyperparameter optimization completed using macro F1 on general datasets\n",
      "   Final models saved and ready for deployment\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£1ï¸âƒ£ Training Final distilroBERTa Multi-task Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize results dictionary if not exists\n",
    "if 'all_results' not in globals():\n",
    "    all_results = {}\n",
    "\n",
    "# Get best parameters from multitask tuning\n",
    "best_multitask_params = multitask_study.best_params\n",
    "print(f\"ðŸŽ¯ Using best hyperparameters:\")\n",
    "for key, value in best_multitask_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_multitask_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_multitask_params['learning_rate'],\n",
    "    batch_size=best_multitask_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_multitask_params['warmup_ratio'],\n",
    "    weight_decay=best_multitask_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_multitask_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_multitask_params['classifier_dropout'],\n",
    "    max_length=best_multitask_params.get('max_length', 128),\n",
    "    alpha=best_multitask_params['alpha'],  # Multitask-specific parameter\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./final_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸš€ Training final multitask model:\")\n",
    "print(f\"  Dataset: Full multitask data ({len(multitask_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_multitask_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_multitask_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_multitask_config.learning_rate:.2e}\")\n",
    "print(f\"  Alpha (loss weighting): {final_multitask_config.alpha:.3f}\")\n",
    "\n",
    "# Train final multitask model\n",
    "final_multitask_trainer = distilroBERTaMultiTaskTrainer(config=final_multitask_config)\n",
    "final_multitask_history = final_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate final multitask model on both general and Reddit datasets\n",
    "final_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['multitask'] if reddit_data else None\n",
    ")\n",
    "all_results['final_multitask'] = final_multitask_results\n",
    "\n",
    "print(f\"\\nâœ… Final Multitask Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Sentiment - Accuracy: {final_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {final_multitask_results['general']['emotion_accuracy']:.4f}, F1: {final_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {final_multitask_results['general']['combined_accuracy']:.4f}, F1: {final_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if final_multitask_results['reddit']:\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Sentiment - Accuracy: {final_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Emotion - Accuracy: {final_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {final_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Combined - Accuracy: {final_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {final_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ FINAL COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š DISTILROBERTA MODEL PERFORMANCE COMPARISON:\")\n",
    "print(f\"  {'='*60}\")\n",
    "\n",
    "# Sentiment Model Comparison\n",
    "print(f\"\\nðŸŽ¯ SENTIMENT MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['initial_sentiment']['general']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['general']['f1_macro']:.4f}\")\n",
    "if all_results['initial_sentiment'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['initial_sentiment']['reddit']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['final_sentiment']['general']['accuracy']:.4f}, F1: {all_results['final_sentiment']['general']['f1_macro']:.4f}\")\n",
    "if all_results['final_sentiment'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['final_sentiment']['reddit']['accuracy']:.4f}, F1: {all_results['final_sentiment']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "sentiment_general_improvement = all_results['final_sentiment']['general']['accuracy'] - all_results['initial_sentiment']['general']['accuracy']\n",
    "sentiment_f1_improvement = all_results['final_sentiment']['general']['f1_macro'] - all_results['initial_sentiment']['general']['f1_macro']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    General Accuracy: {sentiment_general_improvement:+.4f}\")\n",
    "print(f\"    General F1: {sentiment_f1_improvement:+.4f}\")\n",
    "\n",
    "# Emotion Model Comparison\n",
    "print(f\"\\nðŸŽ­ EMOTION MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['initial_emotion']['general']['accuracy']:.4f}, F1: {all_results['initial_emotion']['general']['f1_macro']:.4f}\")\n",
    "if all_results['initial_emotion'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['initial_emotion']['reddit']['accuracy']:.4f}, F1: {all_results['initial_emotion']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['final_emotion']['general']['accuracy']:.4f}, F1: {all_results['final_emotion']['general']['f1_macro']:.4f}\")\n",
    "if all_results['final_emotion'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['final_emotion']['reddit']['accuracy']:.4f}, F1: {all_results['final_emotion']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "emotion_general_improvement = all_results['final_emotion']['general']['accuracy'] - all_results['initial_emotion']['general']['accuracy']\n",
    "emotion_f1_improvement = all_results['final_emotion']['general']['f1_macro'] - all_results['initial_emotion']['general']['f1_macro']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    General Accuracy: {emotion_general_improvement:+.4f}\")\n",
    "print(f\"    General F1: {emotion_f1_improvement:+.4f}\")\n",
    "\n",
    "# Multitask Model Comparison\n",
    "print(f\"\\nðŸ”„ MULTITASK MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {all_results['initial_multitask']['general']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {all_results['initial_multitask']['general']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {all_results['initial_multitask']['general']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['combined_f1_macro']:.4f}\")\n",
    "if all_results['initial_multitask'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {all_results['initial_multitask']['reddit']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {all_results['initial_multitask']['reddit']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {all_results['initial_multitask']['reddit']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {all_results['final_multitask']['general']['sentiment_accuracy']:.4f}, F1: {all_results['final_multitask']['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {all_results['final_multitask']['general']['emotion_accuracy']:.4f}, F1: {all_results['final_multitask']['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {all_results['final_multitask']['general']['combined_accuracy']:.4f}, F1: {all_results['final_multitask']['general']['combined_f1_macro']:.4f}\")\n",
    "if all_results['final_multitask'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {all_results['final_multitask']['reddit']['sentiment_accuracy']:.4f}, F1: {all_results['final_multitask']['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {all_results['final_multitask']['reddit']['emotion_accuracy']:.4f}, F1: {all_results['final_multitask']['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {all_results['final_multitask']['reddit']['combined_accuracy']:.4f}, F1: {all_results['final_multitask']['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "multitask_sentiment_improvement = all_results['final_multitask']['general']['sentiment_accuracy'] - all_results['initial_multitask']['general']['sentiment_accuracy']\n",
    "multitask_emotion_improvement = all_results['final_multitask']['general']['emotion_accuracy'] - all_results['initial_multitask']['general']['emotion_accuracy']\n",
    "multitask_combined_improvement = all_results['final_multitask']['general']['combined_accuracy'] - all_results['initial_multitask']['general']['combined_accuracy']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    Sentiment Accuracy: {multitask_sentiment_improvement:+.4f}\")\n",
    "print(f\"    Emotion Accuracy: {multitask_emotion_improvement:+.4f}\")\n",
    "print(f\"    Combined Accuracy: {multitask_combined_improvement:+.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ DISTILROBERTA TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   All models trained and evaluated on both general and Reddit datasets\")\n",
    "print(f\"   Hyperparameter optimization completed using macro F1 on general datasets\")\n",
    "print(f\"   Final models saved and ready for deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203dee6-c9d7-4ced-a1de-bffc4ba10f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£1ï¸âƒ£ Training Final distilroBERTa Multi-task Model with Best Parameters...\n",
      "============================================================\n",
      "ðŸŽ¯ Using best hyperparameters:\n",
      "  learning_rate: 6.251028636335231e-05\n",
      "  batch_size: 32\n",
      "  num_epochs: 6\n",
      "  warmup_ratio: 0.12123391106782762\n",
      "  weight_decay: 0.02636424704863906\n",
      "  hidden_dropout_prob: 0.13668090197068677\n",
      "  classifier_dropout: 0.16084844859190756\n",
      "  alpha: 0.5049512863264476\n",
      "\n",
      "ðŸš€ Training final multitask model:\n",
      "  Dataset: Full multitask data (7000 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 32\n",
      "  Learning rate: 6.25e-05\n",
      "  Alpha (loss weighting): 0.505\n",
      "Starting distilroBERTa multi-task training...\n",
      "\n",
      "Epoch 1/5\n",
      "  Train Loss: 1.2560\n",
      "  Train Sentiment Acc: 0.6641, Train Emotion Acc: 0.2711\n",
      "  Val Loss: 1.1479\n",
      "  Val Sentiment Acc: 0.8013, F1: 0.5583\n",
      "  Val Emotion Acc: 0.2733, F1: 0.0959\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 2/5\n",
      "  Train Loss: 1.1146\n",
      "  Train Sentiment Acc: 0.8189, Train Emotion Acc: 0.2927\n",
      "  Val Loss: 1.0944\n",
      "  Val Sentiment Acc: 0.8247, F1: 0.5752\n",
      "  Val Emotion Acc: 0.2993, F1: 0.0835\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 3/5\n",
      "  Train Loss: 1.0406\n",
      "  Train Sentiment Acc: 0.8666, Train Emotion Acc: 0.3041\n",
      "  Val Loss: 1.1219\n",
      "  Val Sentiment Acc: 0.8253, F1: 0.5756\n",
      "  Val Emotion Acc: 0.2827, F1: 0.1169\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 4/5\n",
      "  Train Loss: 0.9866\n",
      "  Train Sentiment Acc: 0.8853, Train Emotion Acc: 0.3184\n",
      "  Val Loss: 1.1708\n",
      "  Val Sentiment Acc: 0.8247, F1: 0.5759\n",
      "  Val Emotion Acc: 0.2893, F1: 0.1233\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "Epoch 5/5\n",
      "  Train Loss: 0.9306\n",
      "  Train Sentiment Acc: 0.9026, Train Emotion Acc: 0.3559\n",
      "  Val Loss: 1.1886\n",
      "  Val Sentiment Acc: 0.8260, F1: 0.5767\n",
      "  Val Emotion Acc: 0.2647, F1: 0.1386\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3577\n",
      "Evaluating on General Dataset...\n",
      "Evaluating on Reddit Dataset...\n",
      "\n",
      "âœ… Final Multitask Model Results:\n",
      "  General Dataset:\n",
      "    Sentiment - Accuracy: 0.8125, F1: 0.5602\n",
      "    Emotion - Accuracy: 0.3438, F1: 0.1248\n",
      "    Combined - Accuracy: 0.5781, F1: 0.3425\n",
      "  Reddit Dataset:\n",
      "    Sentiment - Accuracy: 0.6250, F1: 0.2614\n",
      "    Emotion - Accuracy: 0.1562, F1: 0.0505\n",
      "    Combined - Accuracy: 0.3906, F1: 0.1560\n",
      "\n",
      "================================================================================\n",
      "ðŸ FINAL COMPREHENSIVE RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š DISTILROBERTA MODEL PERFORMANCE COMPARISON:\n",
      "  ============================================================\n",
      "\n",
      "ðŸŽ¯ SENTIMENT MODEL:\n",
      "  Initial Model:\n",
      "    General Dataset - Accuracy: 0.8333, F1: 0.5812\n",
      "    Reddit Dataset - Accuracy: 0.5474, F1: 0.2841\n",
      "  Final Optimized Model:\n",
      "    General Dataset - Accuracy: 0.8333, F1: 0.5811\n",
      "    Reddit Dataset - Accuracy: 0.6105, F1: 0.3988\n",
      "  Improvements:\n",
      "    General Accuracy: +0.0000\n",
      "    General F1: -0.0001\n",
      "\n",
      "ðŸŽ­ EMOTION MODEL:\n",
      "  Initial Model:\n",
      "    General Dataset - Accuracy: 0.7593, F1: 0.7211\n",
      "    Reddit Dataset - Accuracy: 0.1368, F1: 0.0717\n",
      "  Final Optimized Model:\n",
      "    General Dataset - Accuracy: 0.7627, F1: 0.7264\n",
      "    Reddit Dataset - Accuracy: 0.1158, F1: 0.0782\n",
      "  Improvements:\n",
      "    General Accuracy: +0.0033\n",
      "    General F1: +0.0053\n",
      "\n",
      "ðŸ”„ MULTITASK MODEL:\n",
      "  Initial Model:\n",
      "    General Dataset:\n",
      "      Sentiment - Accuracy: 0.7500, F1: 0.5153\n",
      "      Emotion - Accuracy: 0.4375, F1: 0.1014\n",
      "      Combined - Accuracy: 0.5938, F1: 0.3084\n",
      "    Reddit Dataset:\n",
      "      Sentiment - Accuracy: 0.6250, F1: 0.2614\n",
      "      Emotion - Accuracy: 0.3125, F1: 0.1067\n",
      "      Combined - Accuracy: 0.4688, F1: 0.1841\n",
      "  Final Optimized Model:\n",
      "    General Dataset:\n",
      "      Sentiment - Accuracy: 0.8125, F1: 0.5602\n",
      "      Emotion - Accuracy: 0.3438, F1: 0.1248\n",
      "      Combined - Accuracy: 0.5781, F1: 0.3425\n",
      "    Reddit Dataset:\n",
      "      Sentiment - Accuracy: 0.6250, F1: 0.2614\n",
      "      Emotion - Accuracy: 0.1562, F1: 0.0505\n",
      "      Combined - Accuracy: 0.3906, F1: 0.1560\n",
      "  Improvements:\n",
      "    Sentiment Accuracy: +0.0625\n",
      "    Emotion Accuracy: -0.0938\n",
      "    Combined Accuracy: -0.0156\n",
      "\n",
      "ðŸŽ‰ DISTILROBERTA TRAINING PIPELINE COMPLETED SUCCESSFULLY!\n",
      "   All models trained and evaluated on both general and Reddit datasets\n",
      "   Hyperparameter optimization completed using macro F1 on general datasets\n",
      "   Final models saved and ready for deployment\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£1ï¸âƒ£ Training Final distilroBERTa Multi-task Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize results dictionary if not exists\n",
    "if 'all_results' not in globals():\n",
    "    all_results = {}\n",
    "\n",
    "# Get best parameters from multitask tuning\n",
    "best_multitask_params = multitask_study.best_params\n",
    "print(f\"ðŸŽ¯ Using best hyperparameters:\")\n",
    "for key, value in best_multitask_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_multitask_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_multitask_params['learning_rate'],\n",
    "    batch_size=best_multitask_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_multitask_params['warmup_ratio'],\n",
    "    weight_decay=best_multitask_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_multitask_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_multitask_params['classifier_dropout'],\n",
    "    max_length=best_multitask_params.get('max_length', 128),\n",
    "    alpha=best_multitask_params['alpha'],  # Multitask-specific parameter\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./final_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸš€ Training final multitask model:\")\n",
    "print(f\"  Dataset: Full multitask data ({len(multitask_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_multitask_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_multitask_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_multitask_config.learning_rate:.2e}\")\n",
    "print(f\"  Alpha (loss weighting): {final_multitask_config.alpha:.3f}\")\n",
    "\n",
    "# Train final multitask model\n",
    "final_multitask_trainer = distilroBERTaMultiTaskTrainer(config=final_multitask_config)\n",
    "final_multitask_history = final_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate final multitask model on both general and Reddit datasets\n",
    "final_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['multitask'] if reddit_data else None\n",
    ")\n",
    "all_results['final_multitask'] = final_multitask_results\n",
    "\n",
    "print(f\"\\nâœ… Final Multitask Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Sentiment - Accuracy: {final_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {final_multitask_results['general']['emotion_accuracy']:.4f}, F1: {final_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {final_multitask_results['general']['combined_accuracy']:.4f}, F1: {final_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if final_multitask_results['reddit']:\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Sentiment - Accuracy: {final_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Emotion - Accuracy: {final_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {final_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Combined - Accuracy: {final_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {final_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ FINAL COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š DISTILROBERTA MODEL PERFORMANCE COMPARISON:\")\n",
    "print(f\"  {'='*60}\")\n",
    "\n",
    "# Sentiment Model Comparison\n",
    "print(f\"\\nðŸŽ¯ SENTIMENT MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['initial_sentiment']['general']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['general']['f1_macro']:.4f}\")\n",
    "if all_results['initial_sentiment'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['initial_sentiment']['reddit']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['final_sentiment']['general']['accuracy']:.4f}, F1: {all_results['final_sentiment']['general']['f1_macro']:.4f}\")\n",
    "if all_results['final_sentiment'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['final_sentiment']['reddit']['accuracy']:.4f}, F1: {all_results['final_sentiment']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "sentiment_general_improvement = all_results['final_sentiment']['general']['accuracy'] - all_results['initial_sentiment']['general']['accuracy']\n",
    "sentiment_f1_improvement = all_results['final_sentiment']['general']['f1_macro'] - all_results['initial_sentiment']['general']['f1_macro']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    General Accuracy: {sentiment_general_improvement:+.4f}\")\n",
    "print(f\"    General F1: {sentiment_f1_improvement:+.4f}\")\n",
    "\n",
    "# Emotion Model Comparison\n",
    "print(f\"\\nðŸŽ­ EMOTION MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['initial_emotion']['general']['accuracy']:.4f}, F1: {all_results['initial_emotion']['general']['f1_macro']:.4f}\")\n",
    "if all_results['initial_emotion'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['initial_emotion']['reddit']['accuracy']:.4f}, F1: {all_results['initial_emotion']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['final_emotion']['general']['accuracy']:.4f}, F1: {all_results['final_emotion']['general']['f1_macro']:.4f}\")\n",
    "if all_results['final_emotion'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['final_emotion']['reddit']['accuracy']:.4f}, F1: {all_results['final_emotion']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "emotion_general_improvement = all_results['final_emotion']['general']['accuracy'] - all_results['initial_emotion']['general']['accuracy']\n",
    "emotion_f1_improvement = all_results['final_emotion']['general']['f1_macro'] - all_results['initial_emotion']['general']['f1_macro']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    General Accuracy: {emotion_general_improvement:+.4f}\")\n",
    "print(f\"    General F1: {emotion_f1_improvement:+.4f}\")\n",
    "\n",
    "# Multitask Model Comparison\n",
    "print(f\"\\nðŸ”„ MULTITASK MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {all_results['initial_multitask']['general']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {all_results['initial_multitask']['general']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {all_results['initial_multitask']['general']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['combined_f1_macro']:.4f}\")\n",
    "if all_results['initial_multitask'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {all_results['initial_multitask']['reddit']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {all_results['initial_multitask']['reddit']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {all_results['initial_multitask']['reddit']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {all_results['final_multitask']['general']['sentiment_accuracy']:.4f}, F1: {all_results['final_multitask']['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {all_results['final_multitask']['general']['emotion_accuracy']:.4f}, F1: {all_results['final_multitask']['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {all_results['final_multitask']['general']['combined_accuracy']:.4f}, F1: {all_results['final_multitask']['general']['combined_f1_macro']:.4f}\")\n",
    "if all_results['final_multitask'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {all_results['final_multitask']['reddit']['sentiment_accuracy']:.4f}, F1: {all_results['final_multitask']['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {all_results['final_multitask']['reddit']['emotion_accuracy']:.4f}, F1: {all_results['final_multitask']['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {all_results['final_multitask']['reddit']['combined_accuracy']:.4f}, F1: {all_results['final_multitask']['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "multitask_sentiment_improvement = all_results['final_multitask']['general']['sentiment_accuracy'] - all_results['initial_multitask']['general']['sentiment_accuracy']\n",
    "multitask_emotion_improvement = all_results['final_multitask']['general']['emotion_accuracy'] - all_results['initial_multitask']['general']['emotion_accuracy']\n",
    "multitask_combined_improvement = all_results['final_multitask']['general']['combined_accuracy'] - all_results['initial_multitask']['general']['combined_accuracy']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    Sentiment Accuracy: {multitask_sentiment_improvement:+.4f}\")\n",
    "print(f\"    Emotion Accuracy: {multitask_emotion_improvement:+.4f}\")\n",
    "print(f\"    Combined Accuracy: {multitask_combined_improvement:+.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ DISTILROBERTA TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   All models trained and evaluated on both general and Reddit datasets\")\n",
    "print(f\"   Hyperparameter optimization completed using macro F1 on general datasets\")\n",
    "print(f\"   Final models saved and ready for deployment\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
