{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig, \n",
    "    RobertaTokenizer, RobertaForSequenceClassification\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import re\n",
    "\n",
    "# XAI Libraries\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from captum.attr import IntegratedGradients, TokenReferenceBase, visualization\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory for visualizations\n",
    "os.makedirs(\"xai_visualizations\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ XAI libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384686e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define MultiTaskTransformer Architecture\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task learning model for sentiment and emotion classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"microsoft/deberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        freeze_encoder: bool = False\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load configuration and adjust dropout\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared transformer encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Freeze encoder if specified\n",
    "        if freeze_encoder:\n",
    "            for param in self.shared_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        task: Optional[str] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        shared_pooled = shared_attended[:, 0, :]\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        if task is None or task == \"sentiment\":\n",
    "            sentiment_attended, sentiment_weights = self.sentiment_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "            sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "            sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "            sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "            sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "            outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "        \n",
    "        # Emotion branch\n",
    "        if task is None or task == \"emotion\":\n",
    "            emotion_attended, emotion_weights = self.emotion_attention(\n",
    "                sequence_output, sequence_output, sequence_output,\n",
    "                key_padding_mask=~attention_mask.bool()\n",
    "            )\n",
    "            emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "            emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "            emotion_pooled = emotion_attended[:, 0, :]\n",
    "            emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "            emotion_logits = self.emotion_classifier(emotion_features)\n",
    "            outputs[\"emotion_logits\"] = emotion_logits\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load the model\"\"\"\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            sentiment_num_classes=config[\"sentiment_num_classes\"],\n",
    "            emotion_num_classes=config[\"emotion_num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "print(\"‚úÖ MultiTaskTransformer architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Text Preprocessing Functions\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Replace newlines and multiple spaces\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Basic cleaning while preserving social media content\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_reddit_text(text: str) -> str:\n",
    "    \"\"\"Preprocess Reddit-specific text\"\"\"\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Handle Reddit-specific patterns\n",
    "    text = re.sub(r'username', '@user', text)  # Replace username placeholders\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove Reddit formatting\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)  # Remove parenthetical notes\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def load_and_preprocess_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess the annotated Reddit posts\"\"\"\n",
    "    print(f\"üì• Loading data from {data_path}...\")\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Preprocess text\n",
    "    df['cleaned_text'] = df['text_content'].apply(preprocess_reddit_text)\n",
    "    \n",
    "    # Create label encoders\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    sentiment_encoder.fit(df['sentiment'].tolist())\n",
    "    emotion_encoder.fit(df['emotion'].tolist())\n",
    "    \n",
    "    df['sentiment_encoded'] = sentiment_encoder.transform(df['sentiment'])\n",
    "    df['emotion_encoded'] = emotion_encoder.transform(df['emotion'])\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded: {len(df)} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return df, sentiment_encoder, emotion_encoder\n",
    "\n",
    "# Load the data\n",
    "reddit_df, data_sentiment_encoder, data_emotion_encoder = load_and_preprocess_data(\"annotated_reddit_posts.csv\")\n",
    "print(\"‚úÖ Text preprocessing functions defined and data loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Loading Functions\n",
    "class ModelLoader:\n",
    "    \"\"\"Centralized model loading for all model types\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_roberta_sentiment(model_path: str):\n",
    "        \"\"\"Load RoBERTa sentiment model\"\"\"\n",
    "        print(f\"üì• Loading RoBERTa sentiment model from {model_path}...\")\n",
    "        \n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Load label encoder\n",
    "        encoder_path = os.path.join(model_path, 'sentiment_encoder.pkl')\n",
    "        label_encoder = joblib.load(encoder_path)\n",
    "        \n",
    "        print(f\"‚úÖ RoBERTa sentiment model loaded!\")\n",
    "        print(f\"   Classes: {list(label_encoder.classes_)}\")\n",
    "        \n",
    "        return model, tokenizer, label_encoder\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_roberta_emotion(model_path: str):\n",
    "        \"\"\"Load RoBERTa emotion model\"\"\"\n",
    "        print(f\"üì• Loading RoBERTa emotion model from {model_path}...\")\n",
    "        \n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Load label encoder\n",
    "        encoder_path = os.path.join(model_path, 'emotion_encoder.pkl')\n",
    "        label_encoder = joblib.load(encoder_path)\n",
    "        \n",
    "        print(f\"‚úÖ RoBERTa emotion model loaded!\")\n",
    "        print(f\"   Classes: {list(label_encoder.classes_)}\")\n",
    "        \n",
    "        return model, tokenizer, label_encoder\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_multitask_model(model_path: str, model_type: str):\n",
    "        \"\"\"Load multitask model (DeBERTa or BERTweet)\"\"\"\n",
    "        print(f\"üì• Loading {model_type} multitask model from {model_path}...\")\n",
    "        \n",
    "        # Determine model name based on type\n",
    "        if model_type.lower() == 'deberta':\n",
    "            model_name = \"microsoft/deberta-base\"\n",
    "        elif model_type.lower() == 'bertweet':\n",
    "            model_name = \"vinai/bertweet-base\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load model\n",
    "        try:\n",
    "            # Try to load from config.json\n",
    "            model = MultiTaskTransformer.from_pretrained(model_path)\n",
    "        except:\n",
    "            # Fallback: load manually\n",
    "            sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "            emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "            \n",
    "            model = MultiTaskTransformer(\n",
    "                model_name=model_name,\n",
    "                sentiment_num_classes=len(sentiment_encoder.classes_),\n",
    "                emotion_num_classes=len(emotion_encoder.classes_)\n",
    "            )\n",
    "            \n",
    "            state_dict_path = os.path.join(model_path, 'pytorch_model.bin')\n",
    "            state_dict = torch.load(state_dict_path, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Load label encoders\n",
    "        sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "        emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "        \n",
    "        print(f\"‚úÖ {model_type} multitask model loaded!\")\n",
    "        print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "        print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "        \n",
    "        return model, tokenizer, sentiment_encoder, emotion_encoder\n",
    "\n",
    "# Load all models\n",
    "print(\"üöÄ Loading All Models\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    roberta_sentiment_model, roberta_sentiment_tokenizer, roberta_sentiment_encoder = \\\n",
    "        ModelLoader.load_roberta_sentiment(\"roberta_sentiment_model_optimized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load RoBERTa sentiment model: {e}\")\n",
    "    roberta_sentiment_model = None\n",
    "\n",
    "try:\n",
    "    roberta_emotion_model, roberta_emotion_tokenizer, roberta_emotion_encoder = \\\n",
    "        ModelLoader.load_roberta_emotion(\"roberta_emotion_model_optimized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load RoBERTa emotion model: {e}\")\n",
    "    roberta_emotion_model = None\n",
    "\n",
    "try:\n",
    "    deberta_model, deberta_tokenizer, deberta_sentiment_encoder, deberta_emotion_encoder = \\\n",
    "        ModelLoader.load_multitask_model(\"deberta_optimized\", \"deberta\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load DeBERTa model: {e}\")\n",
    "    deberta_model = None\n",
    "\n",
    "try:\n",
    "    bertweet_model, bertweet_tokenizer, bertweet_sentiment_encoder, bertweet_emotion_encoder = \\\n",
    "        ModelLoader.load_multitask_model(\"bertweet_model_ultra_light\", \"bertweet\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load BERTweet model: {e}\")\n",
    "    bertweet_model = None\n",
    "\n",
    "print(\"\\n‚úÖ Model loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Prediction Functions\n",
    "class PredictionEngine:\n",
    "    \"\"\"Generate predictions from all models\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_roberta_sentiment(text: str, model, tokenizer, label_encoder):\n",
    "        \"\"\"Get sentiment prediction from RoBERTa\"\"\"\n",
    "        if model is None:\n",
    "            return None\n",
    "            \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                          padding=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            prediction = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        return {\n",
    "            'predicted_class': label_encoder.classes_[prediction],\n",
    "            'predicted_id': prediction,\n",
    "            'probabilities': probabilities.cpu().numpy()[0],\n",
    "            'logits': logits.cpu().numpy()[0]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_roberta_emotion(text: str, model, tokenizer, label_encoder):\n",
    "        \"\"\"Get emotion prediction from RoBERTa\"\"\"\n",
    "        if model is None:\n",
    "            return None\n",
    "            \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                          padding=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            prediction = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        return {\n",
    "            'predicted_class': label_encoder.classes_[prediction],\n",
    "            'predicted_id': prediction,\n",
    "            'probabilities': probabilities.cpu().numpy()[0],\n",
    "            'logits': logits.cpu().numpy()[0]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict_multitask(text: str, model, tokenizer, sentiment_encoder, emotion_encoder, max_length=128):\n",
    "        \"\"\"Get predictions from multitask model\"\"\"\n",
    "        if model is None:\n",
    "            return None, None\n",
    "            \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                          padding=True, max_length=max_length).to(device)\n",
    "        \n",
    "        # Filter out token_type_ids if present\n",
    "        filtered_inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask']\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**filtered_inputs)\n",
    "            \n",
    "            # Sentiment\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            sentiment_pred = torch.argmax(sentiment_logits, dim=-1).item()\n",
    "            \n",
    "            # Emotion\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            emotion_pred = torch.argmax(emotion_logits, dim=-1).item()\n",
    "        \n",
    "        sentiment_result = {\n",
    "            'predicted_class': sentiment_encoder.classes_[sentiment_pred],\n",
    "            'predicted_id': sentiment_pred,\n",
    "            'probabilities': sentiment_probs.cpu().numpy()[0],\n",
    "            'logits': sentiment_logits.cpu().numpy()[0]\n",
    "        }\n",
    "        \n",
    "        emotion_result = {\n",
    "            'predicted_class': emotion_encoder.classes_[emotion_pred],\n",
    "            'predicted_id': emotion_pred,\n",
    "            'probabilities': emotion_probs.cpu().numpy()[0],\n",
    "            'logits': emotion_logits.cpu().numpy()[0]\n",
    "        }\n",
    "        \n",
    "        return sentiment_result, emotion_result\n",
    "\n",
    "def generate_all_predictions(text: str) -> Dict:\n",
    "    \"\"\"Generate predictions from all available models\"\"\"\n",
    "    print(f\"üîÆ Generating predictions for text: '{text[:100]}...'\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # RoBERTa Sentiment\n",
    "    if roberta_sentiment_model is not None:\n",
    "        results['roberta_sentiment'] = PredictionEngine.predict_roberta_sentiment(\n",
    "            text, roberta_sentiment_model, roberta_sentiment_tokenizer, roberta_sentiment_encoder\n",
    "        )\n",
    "    \n",
    "    # RoBERTa Emotion\n",
    "    if roberta_emotion_model is not None:\n",
    "        results['roberta_emotion'] = PredictionEngine.predict_roberta_emotion(\n",
    "            text, roberta_emotion_model, roberta_emotion_tokenizer, roberta_emotion_encoder\n",
    "        )\n",
    "    \n",
    "    # DeBERTa Multitask\n",
    "    if deberta_model is not None:\n",
    "        sent_pred, emot_pred = PredictionEngine.predict_multitask(\n",
    "            text, deberta_model, deberta_tokenizer, \n",
    "            deberta_sentiment_encoder, deberta_emotion_encoder\n",
    "        )\n",
    "        results['deberta_sentiment'] = sent_pred\n",
    "        results['deberta_emotion'] = emot_pred\n",
    "    \n",
    "    # BERTweet Multitask\n",
    "    if bertweet_model is not None:\n",
    "        sent_pred, emot_pred = PredictionEngine.predict_multitask(\n",
    "            text, bertweet_model, bertweet_tokenizer, \n",
    "            bertweet_sentiment_encoder, bertweet_emotion_encoder\n",
    "        )\n",
    "        results['bertweet_sentiment'] = sent_pred\n",
    "        results['bertweet_emotion'] = emot_pred\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Prediction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf61abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: SHAP Implementation\n",
    "class SHAPExplainer:\n",
    "    \"\"\"SHAP explanations for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.explainers = {}\n",
    "    \n",
    "    def create_prediction_function(self, model, tokenizer, task='sentiment', max_length=512):\n",
    "        \"\"\"Create prediction function for SHAP\"\"\"\n",
    "        \n",
    "        def predict_fn(texts):\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            \n",
    "            predictions = []\n",
    "            for text in texts:\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                                 padding=True, max_length=max_length).to(device)\n",
    "                \n",
    "                # Handle different model types\n",
    "                if hasattr(model, 'shared_encoder'):  # Multitask model\n",
    "                    filtered_inputs = {\n",
    "                        'input_ids': inputs['input_ids'],\n",
    "                        'attention_mask': inputs['attention_mask']\n",
    "                    }\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**filtered_inputs)\n",
    "                        if task == 'sentiment':\n",
    "                            logits = outputs['sentiment_logits']\n",
    "                        else:\n",
    "                            logits = outputs['emotion_logits']\n",
    "                else:  # Single task model\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                        logits = outputs.logits\n",
    "                \n",
    "                probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "                predictions.append(probs[0])\n",
    "            \n",
    "            return np.array(predictions)\n",
    "        \n",
    "        return predict_fn\n",
    "    \n",
    "    def explain_text(self, text: str, model, tokenizer, task='sentiment', model_name='model'):\n",
    "        \"\"\"Generate SHAP explanation for text\"\"\"\n",
    "        print(f\"üîç Generating SHAP explanation for {model_name} ({task})...\")\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"‚ùå {model_name} model not available\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Create prediction function\n",
    "            predict_fn = self.create_prediction_function(model, tokenizer, task)\n",
    "            \n",
    "            # Create explainer\n",
    "            explainer = shap.Explainer(predict_fn, masker=shap.maskers.Text())\n",
    "            \n",
    "            # Generate explanation\n",
    "            shap_values = explainer([text])\n",
    "            \n",
    "            return {\n",
    "                'explainer': explainer,\n",
    "                'shap_values': shap_values,\n",
    "                'text': text,\n",
    "                'model_name': model_name,\n",
    "                'task': task\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå SHAP explanation failed for {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_shap_explanation(self, shap_result, save_path=None):\n",
    "        \"\"\"Plot SHAP explanation\"\"\"\n",
    "        if shap_result is None:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            shap.plots.text(shap_result['shap_values'][0], display=False)\n",
    "            \n",
    "            if save_path:\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.title(f\"SHAP Explanation: {shap_result['model_name']} ({shap_result['task']})\")\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå SHAP plotting failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ SHAP explainer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4370595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: LIME Implementation\n",
    "class LIMEExplainer:\n",
    "    \"\"\"LIME explanations for text classification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.explainer = LimeTextExplainer(\n",
    "            class_names=['Negative', 'Neutral', 'Positive'],  # Will be updated per task\n",
    "            mode='classification'\n",
    "        )\n",
    "    \n",
    "    def create_prediction_function(self, model, tokenizer, label_encoder, task='sentiment', max_length=512):\n",
    "        \"\"\"Create prediction function for LIME\"\"\"\n",
    "        \n",
    "        def predict_fn(texts):\n",
    "            predictions = []\n",
    "            for text in texts:\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                                 padding=True, max_length=max_length).to(device)\n",
    "                \n",
    "                # Handle different model types\n",
    "                if hasattr(model, 'shared_encoder'):  # Multitask model\n",
    "                    filtered_inputs = {\n",
    "                        'input_ids': inputs['input_ids'],\n",
    "                        'attention_mask': inputs['attention_mask']\n",
    "                    }\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**filtered_inputs)\n",
    "                        if task == 'sentiment':\n",
    "                            logits = outputs['sentiment_logits']\n",
    "                        else:\n",
    "                            logits = outputs['emotion_logits']\n",
    "                else:  # Single task model\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                        logits = outputs.logits\n",
    "                \n",
    "                probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "                predictions.append(probs[0])\n",
    "            \n",
    "            return np.array(predictions)\n",
    "        \n",
    "        return predict_fn\n",
    "    \n",
    "    def explain_text(self, text: str, model, tokenizer, label_encoder, task='sentiment', model_name='model'):\n",
    "        \"\"\"Generate LIME explanation for text\"\"\"\n",
    "        print(f\"üîç Generating LIME explanation for {model_name} ({task})...\")\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"‚ùå {model_name} model not available\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Update class names\n",
    "            self.explainer.class_names = list(label_encoder.classes_)\n",
    "            \n",
    "            # Create prediction function\n",
    "            predict_fn = self.create_prediction_function(model, tokenizer, label_encoder, task)\n",
    "            \n",
    "            # Generate explanation\n",
    "            explanation = self.explainer.explain_instance(\n",
    "                text, predict_fn, num_features=20, num_samples=1000\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'explanation': explanation,\n",
    "                'text': text,\n",
    "                'model_name': model_name,\n",
    "                'task': task,\n",
    "                'class_names': list(label_encoder.classes_)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå LIME explanation failed for {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_lime_explanation(self, lime_result, save_path=None):\n",
    "        \"\"\"Plot LIME explanation\"\"\"\n",
    "        if lime_result is None:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Create figure\n",
    "            fig = lime_result['explanation'].as_pyplot_figure()\n",
    "            fig.suptitle(f\"LIME Explanation: {lime_result['model_name']} ({lime_result['task']})\")\n",
    "            \n",
    "            if save_path:\n",
    "                fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            # Also show as HTML (for notebook display)\n",
    "            print(\"HTML Visualization:\")\n",
    "            lime_result['explanation'].show_in_notebook(text=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå LIME plotting failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ LIME explainer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Integrated Gradients Implementation\n",
    "class IntegratedGradientsExplainer:\n",
    "    \"\"\"Integrated Gradients explanations using Captum\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.baseline_token_id = 0  # [PAD] token usually\n",
    "    \n",
    "    def create_forward_function(self, model, task='sentiment'):\n",
    "        \"\"\"Create forward function for Captum\"\"\"\n",
    "        \n",
    "        def forward_fn(input_ids, attention_mask):\n",
    "            # Handle different model types\n",
    "            if hasattr(model, 'shared_encoder'):  # Multitask model\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                if task == 'sentiment':\n",
    "                    return outputs['sentiment_logits']\n",
    "                else:\n",
    "                    return outputs['emotion_logits']\n",
    "            else:  # Single task model\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                return outputs.logits\n",
    "        \n",
    "        return forward_fn\n",
    "    \n",
    "    def explain_text(self, text: str, model, tokenizer, task='sentiment', model_name='model', max_length=512):\n",
    "        \"\"\"Generate Integrated Gradients explanation\"\"\"\n",
    "        print(f\"üîç Generating Integrated Gradients explanation for {model_name} ({task})...\")\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"‚ùå {model_name} model not available\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                             padding=True, max_length=max_length)\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            \n",
    "            # Create forward function\n",
    "            forward_fn = self.create_forward_function(model, task)\n",
    "            \n",
    "            # Initialize Integrated Gradients\n",
    "            ig = IntegratedGradients(forward_fn)\n",
    "            \n",
    "            # Create baseline (all PAD tokens)\n",
    "            baseline = torch.zeros_like(input_ids).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                logits = forward_fn(input_ids, attention_mask)\n",
    "                pred_class = torch.argmax(logits, dim=-1).item()\n",
    "            \n",
    "            # Calculate attributions\n",
    "            attributions = ig.attribute(\n",
    "                inputs=(input_ids, attention_mask),\n",
    "                baselines=(baseline, attention_mask),\n",
    "                target=pred_class,\n",
    "                n_steps=50\n",
    "            )\n",
    "            \n",
    "            # Get attribution scores for input_ids\n",
    "            input_attributions = attributions[0]\n",
    "            \n",
    "            # Convert to tokens\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "            \n",
    "            return {\n",
    "                'attributions': input_attributions,\n",
    "                'tokens': tokens,\n",
    "                'input_ids': input_ids,\n",
    "                'predicted_class': pred_class,\n",
    "                'text': text,\n",
    "                'model_name': model_name,\n",
    "                'task': task\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Integrated Gradients explanation failed for {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_ig_explanation(self, ig_result, save_path=None):\n",
    "        \"\"\"Plot Integrated Gradients explanation\"\"\"\n",
    "        if ig_result is None:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Get attribution scores and tokens\n",
    "            attributions = ig_result['attributions'][0].cpu().numpy()\n",
    "            tokens = ig_result['tokens']\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            \n",
    "            # Normalize attributions for color mapping\n",
    "            max_abs_attr = max(abs(attributions.min()), abs(attributions.max()))\n",
    "            normalized_attrs = attributions / max_abs_attr if max_abs_attr > 0 else attributions\n",
    "            \n",
    "            # Create color map\n",
    "            colors = plt.cm.RdYlBu_r(normalized_attrs * 0.5 + 0.5)\n",
    "            \n",
    "            # Plot tokens with colors\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            \n",
    "            y_pos = 0.5\n",
    "            x_pos = 0.1\n",
    "            x_step = 0.8 / len(tokens)\n",
    "            \n",
    "            for i, (token, attr, color) in enumerate(zip(tokens, attributions, colors)):\n",
    "                # Clean token for display\n",
    "                clean_token = token.replace('ƒ†', '').replace('‚ñÅ', '')\n",
    "                if clean_token.startswith('##'):\n",
    "                    clean_token = clean_token[2:]\n",
    "                \n",
    "                # Skip special tokens for cleaner visualization\n",
    "                if clean_token in ['<s>', '</s>', '<pad>', '[CLS]', '[SEP]']:\n",
    "                    continue\n",
    "                \n",
    "                # Plot token with background color\n",
    "                bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.7)\n",
    "                ax.text(x_pos, y_pos, clean_token, fontsize=10, ha='center', va='center',\n",
    "                       bbox=bbox_props, transform=ax.transAxes)\n",
    "                \n",
    "                x_pos += x_step\n",
    "            \n",
    "            # Add color bar\n",
    "            sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlBu_r, \n",
    "                                     norm=plt.Normalize(vmin=-max_abs_attr, vmax=max_abs_attr))\n",
    "            sm.set_array([])\n",
    "            cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', pad=0.1, shrink=0.8)\n",
    "            cbar.set_label('Attribution Score', fontsize=12)\n",
    "            \n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"Integrated Gradients: {ig_result['model_name']} ({ig_result['task']})\", \n",
    "                        fontsize=14, fontweight='bold', pad=20)\n",
    "            \n",
    "            if save_path:\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå IG plotting failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ Integrated Gradients explainer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: XAI Analysis Engine\n",
    "class XAIAnalysisEngine:\n",
    "    \"\"\"Comprehensive XAI analysis for all models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.shap_explainer = SHAPExplainer()\n",
    "        self.lime_explainer = LIMEExplainer()\n",
    "        self.ig_explainer = IntegratedGradientsExplainer()\n",
    "    \n",
    "    def analyze_single_text(self, text: str, save_visualizations=True, post_id=None):\n",
    "        \"\"\"Complete XAI analysis for a single text\"\"\"\n",
    "        print(f\"\\nüöÄ Starting XAI Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Text: '{text[:100]}...'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Generate predictions first\n",
    "        predictions = generate_all_predictions(text)\n",
    "        \n",
    "        # Display predictions\n",
    "        print(\"\\nüìä PREDICTIONS:\")\n",
    "        print(\"-\" * 30)\n",
    "        for model_task, pred in predictions.items():\n",
    "            if pred is not None:\n",
    "                print(f\"{model_task}: {pred['predicted_class']} (conf: {pred['probabilities'].max():.3f})\")\n",
    "        \n",
    "        results = {'predictions': predictions, 'explanations': {}}\n",
    "        \n",
    "        # XAI Analysis for each available model\n",
    "        model_configs = [\n",
    "            ('roberta_sentiment', roberta_sentiment_model, roberta_sentiment_tokenizer, roberta_sentiment_encoder, 'sentiment'),\n",
    "            ('roberta_emotion', roberta_emotion_model, roberta_emotion_tokenizer, roberta_emotion_encoder, 'emotion'),\n",
    "            ('deberta_sentiment', deberta_model, deberta_tokenizer, deberta_sentiment_encoder, 'sentiment'),\n",
    "            ('deberta_emotion', deberta_model, deberta_tokenizer, deberta_emotion_encoder, 'emotion'),\n",
    "            ('bertweet_sentiment', bertweet_model, bertweet_tokenizer, bertweet_sentiment_encoder, 'sentiment'),\n",
    "            ('bertweet_emotion', bertweet_model, bertweet_tokenizer, bertweet_emotion_encoder, 'emotion'),\n",
    "        ]\n",
    "        \n",
    "        for model_name, model, tokenizer, encoder, task in model_configs:\n",
    "            if model is None or encoder is None:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüîç Analyzing {model_name}...\")\n",
    "            \n",
    "            # Create save paths if needed\n",
    "            post_suffix = f\"_post_{post_id}\" if post_id else \"\"\n",
    "            \n",
    "            # SHAP Analysis\n",
    "            shap_result = self.shap_explainer.explain_text(text, model, tokenizer, task, model_name)\n",
    "            if shap_result and save_visualizations:\n",
    "                save_path = f\"xai_visualizations/shap_{model_name}{post_suffix}.png\"\n",
    "                self.shap_explainer.plot_shap_explanation(shap_result, save_path)\n",
    "            \n",
    "            # LIME Analysis\n",
    "            lime_result = self.lime_explainer.explain_text(text, model, tokenizer, encoder, task, model_name)\n",
    "            if lime_result and save_visualizations:\n",
    "                save_path = f\"xai_visualizations/lime_{model_name}{post_suffix}.png\"\n",
    "                self.lime_explainer.plot_lime_explanation(lime_result, save_path)\n",
    "            \n",
    "            # Integrated Gradients Analysis\n",
    "            max_length = 128 if 'bertweet' in model_name or 'deberta' in model_name else 512\n",
    "            ig_result = self.ig_explainer.explain_text(text, model, tokenizer, task, model_name, max_length)\n",
    "            if ig_result and save_visualizations:\n",
    "                save_path = f\"xai_visualizations/ig_{model_name}{post_suffix}.png\"\n",
    "                self.ig_explainer.plot_ig_explanation(ig_result, save_path)\n",
    "            \n",
    "            # Store results\n",
    "            results['explanations'][model_name] = {\n",
    "                'shap': shap_result,\n",
    "                'lime': lime_result,\n",
    "                'ig': ig_result\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_analyze(self, df: pd.DataFrame, num_samples=5, random_sample=True):\n",
    "        \"\"\"Analyze multiple Reddit posts\"\"\"\n",
    "        print(f\"\\nüîÑ Starting Batch XAI Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Select samples\n",
    "        if random_sample:\n",
    "            sample_df = df.sample(n=min(num_samples, len(df)), random_state=42)\n",
    "        else:\n",
    "            sample_df = df.head(num_samples)\n",
    "        \n",
    "        batch_results = {}\n",
    "        \n",
    "        for idx, row in sample_df.iterrows():\n",
    "            post_id = row['id']\n",
    "            text = row['cleaned_text']\n",
    "            \n",
    "            print(f\"\\nüìù Analyzing Post {idx + 1}/{num_samples} (ID: {post_id})\")\n",
    "            print(f\"True labels - Sentiment: {row['sentiment']}, Emotion: {row['emotion']}\")\n",
    "            \n",
    "            result = self.analyze_single_text(text, save_visualizations=True, post_id=post_id)\n",
    "            batch_results[post_id] = result\n",
    "        \n",
    "        return batch_results\n",
    "\n",
    "print(\"‚úÖ XAI Analysis Engine defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdde533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Run XAI Analysis on Sample Reddit Post\n",
    "# Select a sample Reddit post for analysis\n",
    "sample_post = reddit_df.iloc[0]  # Use first post, or change index as needed\n",
    "\n",
    "print(\"üéØ Selected Reddit Post for XAI Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ID: {sample_post['id']}\")\n",
    "print(f\"True Sentiment: {sample_post['sentiment']}\")\n",
    "print(f\"True Emotion: {sample_post['emotion']}\")\n",
    "print(f\"Text: {sample_post['cleaned_text']}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize XAI engine\n",
    "xai_engine = XAIAnalysisEngine()\n",
    "\n",
    "# Run comprehensive analysis\n",
    "sample_text = sample_post['cleaned_text']\n",
    "xai_results = xai_engine.analyze_single_text(\n",
    "    text=sample_text,\n",
    "    save_visualizations=True,\n",
    "    post_id=sample_post['id']\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ XAI Analysis Complete!\")\n",
    "print(f\"Visualizations saved to 'xai_visualizations/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ef4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Batch Processing (Optional)\n",
    "# Uncomment and run this cell to analyze multiple posts\n",
    "\n",
    "\"\"\"\n",
    "print(\"üîÑ Running Batch XAI Analysis on Multiple Posts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze 3 random posts (adjust number as needed)\n",
    "batch_results = xai_engine.batch_analyze(\n",
    "    df=reddit_df,\n",
    "    num_samples=3,\n",
    "    random_sample=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Batch Analysis Complete!\")\n",
    "print(f\"Analyzed {len(batch_results)} posts\")\n",
    "print(f\"All visualizations saved to 'xai_visualizations/' directory\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìä Batch Analysis Summary:\")\n",
    "for post_id, results in batch_results.items():\n",
    "    print(f\"\\nPost {post_id}:\")\n",
    "    for model_name, pred in results['predictions'].items():\n",
    "        if pred is not None:\n",
    "            print(f\"  {model_name}: {pred['predicted_class']}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Error Handling and Model Status Check\n",
    "def check_model_status():\n",
    "    \"\"\"Check which models are loaded and available\"\"\"\n",
    "    print(\"üîç Model Status Check\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    models_status = {\n",
    "        \"RoBERTa Sentiment\": roberta_sentiment_model is not None,\n",
    "        \"RoBERTa Emotion\": roberta_emotion_model is not None,\n",
    "        \"DeBERTa Multitask\": deberta_model is not None,\n",
    "        \"BERTweet Multitask\": bertweet_model is not None,\n",
    "    }\n",
    "    \n",
    "    for model_name, status in models_status.items():\n",
    "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"{status_icon} {model_name}: {'Available' if status else 'Not loaded'}\")\n",
    "    \n",
    "    available_count = sum(models_status.values())\n",
    "    total_count = len(models_status)\n",
    "    \n",
    "    print(f\"\\nüìä Summary: {available_count}/{total_count} models loaded successfully\")\n",
    "    \n",
    "    if available_count == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No models loaded! Please check your model paths and try reloading.\")\n",
    "    elif available_count < total_count:\n",
    "        print(\"\\n‚ö†Ô∏è  Some models failed to load. XAI analysis will only work for loaded models.\")\n",
    "    else:\n",
    "        print(\"\\nüéâ All models loaded successfully! Full XAI analysis available.\")\n",
    "    \n",
    "    return models_status\n",
    "\n",
    "# Run status check\n",
    "model_status = check_model_status()\n",
    "\n",
    "print(\"\\n‚úÖ XAI System Ready!\")\n",
    "print(\"üöÄ You can now run XAI analysis on any Reddit post text!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
