{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14cfa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# HuggingFace imports\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Captum imports for Integrated Gradients\n",
    "from captum.attr import IntegratedGradients, visualization as viz\n",
    "from captum.attr import TokenReferenceBase, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8a3d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe89f0",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03b6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \"\"\"Single-task BERTweet transformer for sentiment classification\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetSingleTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # BERTweet encoder\n",
    "        self.encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Classification head optimized for BERTweet\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load single-task BERTweet model from HuggingFace format\"\"\"\n",
    "        import torch\n",
    "        import json\n",
    "        import os\n",
    "\n",
    "        # Load saved config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Initialize model architecture\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            num_classes=config[\"num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Load state dict from HuggingFace-style checkpoint\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "\n",
    "        # Rename keys if they use HuggingFace format (e.g., \"bertweet.\")\n",
    "        renamed_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            new_k = k\n",
    "            if k.startswith(\"bertweet.\"):\n",
    "                new_k = k.replace(\"bertweet.\", \"encoder.\")\n",
    "            renamed_state_dict[new_k] = v\n",
    "\n",
    "        model.load_state_dict(renamed_state_dict, strict=False)\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3baf03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \"\"\"Multi-task BERTweet transformer for sentiment AND emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetMultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared BERTweet encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        # Emotion classification head  \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for classifier in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in classifier:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        pooled_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "        emotion_logits = self.emotion_classifier(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load multitask BERTweet model\"\"\"\n",
    "        import torch\n",
    "        import json\n",
    "        from transformers import AutoModel\n",
    "\n",
    "        # Load saved config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Initialize model with architecture info\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            sentiment_num_classes=config[\"sentiment_num_classes\"],\n",
    "            emotion_num_classes=config[\"emotion_num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Load HuggingFace-style state dict (actual weights)\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "\n",
    "        # Handle HuggingFace checkpoint key mismatch\n",
    "        renamed_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            new_k = k\n",
    "            if k.startswith(\"bertweet.\"):\n",
    "                new_k = k.replace(\"bertweet.\", \"encoder.\")\n",
    "            renamed_state_dict[new_k] = v\n",
    "\n",
    "        model.load_state_dict(renamed_state_dict, strict=False)\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e2a5d",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bertweet_models(\n",
    "    single_task_path: str = \"./bertweet_trained_models_seeds/bertweet_sentiment_seed_42\",\n",
    "    multitask_path: str = \"./bertweet_trained_models_seeds/bertweet_multitask_seed_42\"\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load both single-task and multitask BERTweet models\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (single_task_dict, multitask_dict) containing models, tokenizers, and encoders\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Loading BERTweet models...\")\n",
    "    \n",
    "    # Load single-task model\n",
    "    print(f\"üì• Loading single-task model from {single_task_path}\")\n",
    "    single_task_model = BERTweetSingleTaskTransformer.from_pretrained(single_task_path)\n",
    "    single_task_model.to(device)\n",
    "    single_task_model.eval()\n",
    "    \n",
    "    # Load tokenizer for single-task\n",
    "    single_task_tokenizer = AutoTokenizer.from_pretrained(single_task_path)\n",
    "    if single_task_tokenizer.pad_token is None:\n",
    "        single_task_tokenizer.pad_token = single_task_tokenizer.eos_token\n",
    "    \n",
    "    # Load label encoder for single-task\n",
    "    single_task_encoder = joblib.load(os.path.join(single_task_path, 'sentiment_encoder.pkl'))\n",
    "    \n",
    "    # Load multitask model\n",
    "    print(f\"üì• Loading multitask model from {multitask_path}\")\n",
    "    multitask_model = BERTweetMultiTaskTransformer.from_pretrained(multitask_path)\n",
    "    multitask_model.to(device)\n",
    "    multitask_model.eval()\n",
    "    \n",
    "    # Load tokenizer for multitask\n",
    "    multitask_tokenizer = AutoTokenizer.from_pretrained(multitask_path)\n",
    "    if multitask_tokenizer.pad_token is None:\n",
    "        multitask_tokenizer.pad_token = multitask_tokenizer.eos_token\n",
    "    \n",
    "    # Load label encoders for multitask\n",
    "    multitask_sentiment_encoder = joblib.load(os.path.join(multitask_path, 'sentiment_encoder.pkl'))\n",
    "    \n",
    "    single_task_dict = {\n",
    "        'model': single_task_model,\n",
    "        'tokenizer': single_task_tokenizer,\n",
    "        'sentiment_encoder': single_task_encoder,\n",
    "        'type': 'single_task'\n",
    "    }\n",
    "    \n",
    "    multitask_dict = {\n",
    "        'model': multitask_model,\n",
    "        'tokenizer': multitask_tokenizer,\n",
    "        'sentiment_encoder': multitask_sentiment_encoder,\n",
    "        'type': 'multitask'\n",
    "    }\n",
    "    \n",
    "    print(\"‚úÖ Models loaded successfully!\")\n",
    "    return single_task_dict, multitask_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e546023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_examples(data_path: str = \"data/cleaned_reddit_posts.csv\", num_examples: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load test examples from the available data\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the test data\n",
    "        num_examples: Number of examples to load\n",
    "        \n",
    "    Returns:\n",
    "        List of text examples for analysis\n",
    "    \"\"\"\n",
    "    print(f\"üìä Loading {num_examples} test examples from {data_path}\")\n",
    "    \n",
    "    if data_path.endswith('.csv'):\n",
    "        df = pd.read_csv(data_path)\n",
    "        texts = df['text_content'].head(num_examples).tolist()\n",
    "    else:\n",
    "        # Handle JSON files if needed\n",
    "        with open(data_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        texts = [item['text'] for item in data[:num_examples]]\n",
    "    \n",
    "    # Clean and filter texts\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str) and len(text.strip()) > 10:\n",
    "            # Basic cleaning\n",
    "            cleaned_text = text.strip()\n",
    "            if len(cleaned_text.split()) >= 5:  # At least 5 words\n",
    "                cleaned_texts.append(cleaned_text)\n",
    "        \n",
    "        if len(cleaned_texts) >= num_examples:\n",
    "            break\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(cleaned_texts)} valid test examples\")\n",
    "    return cleaned_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435ae10",
   "metadata": {},
   "source": [
    "## IG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTweetIntegratedGradients:\n",
    "    \"\"\"Integrated Gradients explainer for BERTweet models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.baseline_token_id = 0  # [PAD] token\n",
    "        \n",
    "    def _create_forward_function(self, model, task='sentiment'):\n",
    "        \"\"\"Create forward function for Captum IG\"\"\"\n",
    "        \n",
    "        def forward_fn(input_ids, attention_mask=None):\n",
    "            if attention_mask is None:\n",
    "                attention_mask = torch.ones_like(input_ids)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if hasattr(model, 'shared_encoder'):  # Multitask model\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    return outputs['sentiment_logits']\n",
    "                else:  # Single task model\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    return outputs['logits']\n",
    "        \n",
    "        return forward_fn\n",
    "    \n",
    "    def generate_attributions(\n",
    "        self, \n",
    "        text: str, \n",
    "        model_dict: Dict, \n",
    "        max_length: int = 128,\n",
    "        n_steps: int = 50\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate Integrated Gradients attributions for a text\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to explain\n",
    "            model_dict: Dictionary containing model, tokenizer, encoder\n",
    "            max_length: Maximum sequence length\n",
    "            n_steps: Number of integration steps\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with attributions and metadata\n",
    "        \"\"\"\n",
    "        model = model_dict['model']\n",
    "        tokenizer = model_dict['tokenizer']\n",
    "        \n",
    "        print(f\"üîç Generating IG attributions for {model_dict['type']} model...\")\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        # Create reference (baseline) inputs\n",
    "        ref_input_ids = torch.zeros_like(input_ids)\n",
    "        ref_attention_mask = torch.zeros_like(attention_mask)\n",
    "        \n",
    "        # Set up Integrated Gradients\n",
    "        forward_fn = self._create_forward_function(model, 'sentiment')\n",
    "        ig = IntegratedGradients(forward_fn)\n",
    "        \n",
    "        # Generate attributions\n",
    "        try:\n",
    "            attributions = ig.attribute(\n",
    "                inputs=(input_ids, attention_mask),\n",
    "                baselines=(ref_input_ids, ref_attention_mask),\n",
    "                n_steps=n_steps,\n",
    "                return_convergence_delta=True\n",
    "            )\n",
    "            \n",
    "            # Extract input_ids attributions\n",
    "            input_attributions = attributions[0][0]\n",
    "            convergence_delta = attributions[1]\n",
    "            \n",
    "            # Get tokens\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "            \n",
    "            # Calculate attribution scores per token\n",
    "            attribution_scores = input_attributions.sum(dim=-1).cpu().numpy()\n",
    "            \n",
    "            # Make prediction for predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = forward_fn(input_ids, attention_mask)\n",
    "                predictions = F.softmax(outputs, dim=-1)\n",
    "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "                confidence = predictions.max().item()\n",
    "            \n",
    "            # Clean tokens (remove special tokens for visualization)\n",
    "            clean_tokens = []\n",
    "            clean_scores = []\n",
    "            for i, (token, score) in enumerate(zip(tokens, attribution_scores)):\n",
    "                if token not in ['<s>', '</s>', '<pad>']:\n",
    "                    clean_tokens.append(token)\n",
    "                    clean_scores.append(score)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'tokens': clean_tokens,\n",
    "                'attribution_scores': clean_scores,\n",
    "                'predicted_class': predicted_class,\n",
    "                'confidence': confidence,\n",
    "                'convergence_delta': convergence_delta.item(),\n",
    "                'model_type': model_dict['type'],\n",
    "                'raw_tokens': tokens,\n",
    "                'raw_scores': attribution_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Generated attributions for {len(clean_tokens)} tokens\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating attributions: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9976ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_explanations(\n",
    "    texts: List[str], \n",
    "    single_task_dict: Dict, \n",
    "    multitask_dict: Dict\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Generate IG explanations for all texts using both models\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input texts\n",
    "        single_task_dict: Single-task model dictionary\n",
    "        multitask_dict: Multitask model dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with explanations for each model type\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Generating explanations for all texts and models...\")\n",
    "    \n",
    "    ig_explainer = BERTweetIntegratedGradients()\n",
    "    \n",
    "    results = {\n",
    "        'single_task': [],\n",
    "        'multitask': []\n",
    "    }\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"\\nüìù Processing text {i+1}/{len(texts)}\")\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        \n",
    "        # Single-task explanations\n",
    "        single_result = ig_explainer.generate_attributions(text, single_task_dict)\n",
    "        if single_result:\n",
    "            results['single_task'].append(single_result)\n",
    "        \n",
    "        # Multitask explanations\n",
    "        multi_result = ig_explainer.generate_attributions(text, multitask_dict)\n",
    "        if multi_result:\n",
    "            results['multitask'].append(multi_result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated explanations for {len(results['single_task'])} single-task and {len(results['multitask'])} multitask examples\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4c342",
   "metadata": {},
   "source": [
    "## Eval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24740ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(\n",
    "    text: str,\n",
    "    model_dict: Dict,\n",
    "    attribution_result: Dict,\n",
    "    top_k_ratios: List[float] = [0.1, 0.2, 0.3, 0.5]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate faithfulness using deletion-based approach\n",
    "    \n",
    "    Args:\n",
    "        text: Original text\n",
    "        model_dict: Model dictionary\n",
    "        attribution_result: Attribution results from IG\n",
    "        top_k_ratios: Ratios of top tokens to remove\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with faithfulness scores\n",
    "    \"\"\"\n",
    "    print(\"üìä Evaluating faithfulness...\")\n",
    "    \n",
    "    model = model_dict['model']\n",
    "    tokenizer = model_dict['tokenizer']\n",
    "    \n",
    "    # Get original prediction\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, 'shared_encoder'):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            original_probs = F.softmax(outputs['sentiment_logits'], dim=-1)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            original_probs = F.softmax(outputs['logits'], dim=-1)\n",
    "    \n",
    "    original_confidence = original_probs.max().item()\n",
    "    original_prediction = torch.argmax(original_probs, dim=-1).item()\n",
    "    \n",
    "    faithfulness_scores = {}\n",
    "    \n",
    "    # Get attribution scores and tokens\n",
    "    tokens = attribution_result['raw_tokens']\n",
    "    scores = attribution_result['raw_scores']\n",
    "    \n",
    "    for ratio in top_k_ratios:\n",
    "        try:\n",
    "            # Calculate how many tokens to remove\n",
    "            num_tokens_to_remove = max(1, int(len(tokens) * ratio))\n",
    "            \n",
    "            # Get indices of top-k most important tokens\n",
    "            top_indices = np.argsort(np.abs(scores))[-num_tokens_to_remove:]\n",
    "            \n",
    "            # Create modified input by masking top tokens\n",
    "            modified_input_ids = input_ids.clone()\n",
    "            for idx in top_indices:\n",
    "                if idx < modified_input_ids.shape[1]:\n",
    "                    modified_input_ids[0, idx] = tokenizer.pad_token_id\n",
    "            \n",
    "            # Get prediction for modified input\n",
    "            with torch.no_grad():\n",
    "                if hasattr(model, 'shared_encoder'):\n",
    "                    outputs = model(input_ids=modified_input_ids, attention_mask=attention_mask)\n",
    "                    modified_probs = F.softmax(outputs['sentiment_logits'], dim=-1)\n",
    "                else:\n",
    "                    outputs = model(input_ids=modified_input_ids, attention_mask=attention_mask)\n",
    "                    modified_probs = F.softmax(outputs['logits'], dim=-1)\n",
    "            \n",
    "            modified_confidence = modified_probs.max().item()\n",
    "            \n",
    "            # Calculate faithfulness as confidence drop\n",
    "            confidence_drop = original_confidence - modified_confidence\n",
    "            faithfulness_scores[f'faithfulness_top_{ratio}'] = confidence_drop\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error in faithfulness evaluation for ratio {ratio}: {e}\")\n",
    "            faithfulness_scores[f'faithfulness_top_{ratio}'] = 0.0\n",
    "    \n",
    "    return faithfulness_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba019c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stability(\n",
    "    text: str,\n",
    "    model_dict: Dict,\n",
    "    attribution_result: Dict,\n",
    "    num_perturbations: int = 5\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate stability using input perturbations\n",
    "    \n",
    "    Args:\n",
    "        text: Original text\n",
    "        model_dict: Model dictionary\n",
    "        attribution_result: Original attribution results\n",
    "        num_perturbations: Number of perturbations to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with stability scores\n",
    "    \"\"\"\n",
    "    print(\"üìä Evaluating stability...\")\n",
    "    \n",
    "    ig_explainer = BERTweetIntegratedGradients()\n",
    "    original_scores = np.array(attribution_result['attribution_scores'])\n",
    "    \n",
    "    perturbed_scores = []\n",
    "    \n",
    "    for i in range(num_perturbations):\n",
    "        try:\n",
    "            # Create perturbed text\n",
    "            perturbed_text = create_text_perturbation(text)\n",
    "            \n",
    "            # Generate attributions for perturbed text\n",
    "            perturbed_result = ig_explainer.generate_attributions(perturbed_text, model_dict)\n",
    "            \n",
    "            if perturbed_result and len(perturbed_result['attribution_scores']) > 0:\n",
    "                # Align scores (pad or truncate to match original length)\n",
    "                perturbed_score = np.array(perturbed_result['attribution_scores'])\n",
    "                min_len = min(len(original_scores), len(perturbed_score))\n",
    "                \n",
    "                if min_len > 0:\n",
    "                    orig_aligned = original_scores[:min_len]\n",
    "                    pert_aligned = perturbed_score[:min_len]\n",
    "                    perturbed_scores.append(pert_aligned)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error generating perturbation {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not perturbed_scores:\n",
    "        return {'stability_cosine': 0.0, 'stability_correlation': 0.0}\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    cosine_similarities = []\n",
    "    correlations = []\n",
    "    \n",
    "    for pert_scores in perturbed_scores:\n",
    "        min_len = min(len(original_scores), len(pert_scores))\n",
    "        if min_len > 1:\n",
    "            orig_aligned = original_scores[:min_len]\n",
    "            pert_aligned = pert_scores[:min_len]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            if np.linalg.norm(orig_aligned) > 0 and np.linalg.norm(pert_aligned) > 0:\n",
    "                cos_sim = cosine_similarity([orig_aligned], [pert_aligned])[0, 0]\n",
    "                cosine_similarities.append(cos_sim)\n",
    "            \n",
    "            # Correlation\n",
    "            if len(set(orig_aligned)) > 1 and len(set(pert_aligned)) > 1:\n",
    "                corr = np.corrcoef(orig_aligned, pert_aligned)[0, 1]\n",
    "                if not np.isnan(corr):\n",
    "                    correlations.append(corr)\n",
    "    \n",
    "    # Average stability scores\n",
    "    avg_cosine = np.mean(cosine_similarities) if cosine_similarities else 0.0\n",
    "    avg_correlation = np.mean(correlations) if correlations else 0.0\n",
    "    \n",
    "    return {\n",
    "        'stability_cosine': avg_cosine,\n",
    "        'stability_correlation': avg_correlation,\n",
    "        'num_valid_perturbations': len(perturbed_scores)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_perturbation(text: str) -> str:\n",
    "    \"\"\"Create a perturbed version of the text\"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) < 2:\n",
    "        return text\n",
    "    \n",
    "    perturbation_type = random.choice(['synonym', 'punctuation', 'order'])\n",
    "    \n",
    "    if perturbation_type == 'synonym':\n",
    "        # Replace one word with a synonym\n",
    "        word_idx = random.randint(0, len(words) - 1)\n",
    "        original_word = words[word_idx].lower().strip('.,!?;')\n",
    "        \n",
    "        # Try to find synonyms\n",
    "        synonyms = []\n",
    "        try:\n",
    "            for syn in wordnet.synsets(original_word):\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() != original_word and '_' not in lemma.name():\n",
    "                        synonyms.append(lemma.name())\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if synonyms:\n",
    "            words[word_idx] = random.choice(synonyms)\n",
    "    \n",
    "    elif perturbation_type == 'punctuation':\n",
    "        # Add/remove punctuation\n",
    "        if random.choice([True, False]) and not text.endswith('.'):\n",
    "            return text + '.'\n",
    "        else:\n",
    "            return text.rstrip('.,!?;')\n",
    "    \n",
    "    elif perturbation_type == 'order' and len(words) > 2:\n",
    "        # Swap two adjacent words\n",
    "        idx = random.randint(0, len(words) - 2)\n",
    "        words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f44a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fairness(attribution_result: Dict) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate fairness by analyzing attribution toward sensitive terms\n",
    "    \n",
    "    Args:\n",
    "        attribution_result: Attribution results from IG\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with fairness metrics\n",
    "    \"\"\"\n",
    "    print(\"üìä Evaluating fairness...\")\n",
    "    \n",
    "    # Define sensitive word categories\n",
    "    emotion_words = ['happy', 'sad', 'angry', 'excited', 'disappointed', 'frustrated', \n",
    "                    'joy', 'fear', 'love', 'hate', 'good', 'bad', 'terrible', 'amazing']\n",
    "    \n",
    "    bias_words = ['he', 'she', 'his', 'her', 'him', 'man', 'woman', 'male', 'female',\n",
    "                 'guy', 'girl', 'boy', 'dude', 'lady']\n",
    "    \n",
    "    tokens = [token.lower().strip('‚ñÅ') for token in attribution_result['tokens']]\n",
    "    scores = attribution_result['attribution_scores']\n",
    "    \n",
    "    # Calculate attribution statistics for different word types\n",
    "    emotion_attributions = []\n",
    "    bias_attributions = []\n",
    "    other_attributions = []\n",
    "    \n",
    "    for token, score in zip(tokens, scores):\n",
    "        clean_token = token.lower().strip('.,!?;')\n",
    "        \n",
    "        if clean_token in emotion_words:\n",
    "            emotion_attributions.append(abs(score))\n",
    "        elif clean_token in bias_words:\n",
    "            bias_attributions.append(abs(score))\n",
    "        else:\n",
    "            other_attributions.append(abs(score))\n",
    "    \n",
    "    # Calculate fairness metrics\n",
    "    fairness_metrics = {}\n",
    "    \n",
    "    # Emotion word focus\n",
    "    if emotion_attributions and other_attributions:\n",
    "        emotion_mean = np.mean(emotion_attributions)\n",
    "        other_mean = np.mean(other_attributions)\n",
    "        fairness_metrics['emotion_bias_ratio'] = emotion_mean / (other_mean + 1e-8)\n",
    "    else:\n",
    "        fairness_metrics['emotion_bias_ratio'] = 0.0\n",
    "    \n",
    "    # Gender/demographic bias\n",
    "    if bias_attributions and other_attributions:\n",
    "        bias_mean = np.mean(bias_attributions)\n",
    "        other_mean = np.mean(other_attributions)\n",
    "        fairness_metrics['demographic_bias_ratio'] = bias_mean / (other_mean + 1e-8)\n",
    "    else:\n",
    "        fairness_metrics['demographic_bias_ratio'] = 0.0\n",
    "    \n",
    "    # Attribution concentration (Gini coefficient approximation)\n",
    "    if len(scores) > 1:\n",
    "        abs_scores = np.abs(scores)\n",
    "        sorted_scores = np.sort(abs_scores)\n",
    "        n = len(sorted_scores)\n",
    "        cumsum = np.cumsum(sorted_scores)\n",
    "        gini = (2 * np.sum((np.arange(1, n + 1) * sorted_scores))) / (n * np.sum(sorted_scores)) - (n + 1) / n\n",
    "        fairness_metrics['attribution_concentration'] = gini\n",
    "    else:\n",
    "        fairness_metrics['attribution_concentration'] = 0.0\n",
    "    \n",
    "    return fairness_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9442158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_explanations(explanations: Dict[str, List[Dict]]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate all explanations using faithfulness, stability, and fairness metrics\n",
    "    \n",
    "    Args:\n",
    "        explanations: Dictionary with explanations for each model type\n",
    "        \n",
    "    Returns:\n",
    "        Comprehensive evaluation results\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Evaluating all explanations...\")\n",
    "    \n",
    "    # We'll need to reload models for faithfulness evaluation\n",
    "    single_task_dict, multitask_dict = load_bertweet_models()\n",
    "    \n",
    "    evaluation_results = {\n",
    "        'single_task': [],\n",
    "        'multitask': []\n",
    "    }\n",
    "    \n",
    "    # Evaluate single-task explanations\n",
    "    print(\"\\nüìä Evaluating single-task explanations...\")\n",
    "    for i, result in enumerate(explanations['single_task']):\n",
    "        print(f\"Evaluating single-task example {i+1}\")\n",
    "        \n",
    "        # Faithfulness\n",
    "        faithfulness = evaluate_faithfulness(result['text'], single_task_dict, result)\n",
    "        \n",
    "        # Stability  \n",
    "        stability = evaluate_stability(result['text'], single_task_dict, result)\n",
    "        \n",
    "        # Fairness\n",
    "        fairness = evaluate_fairness(result)\n",
    "        \n",
    "        # Combine all metrics\n",
    "        eval_result = {\n",
    "            'text': result['text'],\n",
    "            'predicted_class': result['predicted_class'],\n",
    "            'confidence': result['confidence'],\n",
    "            'convergence_delta': result['convergence_delta'],\n",
    "            **faithfulness,\n",
    "            **stability,\n",
    "            **fairness\n",
    "        }\n",
    "        \n",
    "        evaluation_results['single_task'].append(eval_result)\n",
    "    \n",
    "    # Evaluate multitask explanations\n",
    "    print(\"\\nüìä Evaluating multitask explanations...\")\n",
    "    for i, result in enumerate(explanations['multitask']):\n",
    "        print(f\"Evaluating multitask example {i+1}\")\n",
    "        \n",
    "        # Faithfulness\n",
    "        faithfulness = evaluate_faithfulness(result['text'], multitask_dict, result)\n",
    "        \n",
    "        # Stability\n",
    "        stability = evaluate_stability(result['text'], multitask_dict, result)\n",
    "        \n",
    "        # Fairness\n",
    "        fairness = evaluate_fairness(result)\n",
    "        \n",
    "        # Combine all metrics\n",
    "        eval_result = {\n",
    "            'text': result['text'],\n",
    "            'predicted_class': result['predicted_class'],\n",
    "            'confidence': result['confidence'],\n",
    "            'convergence_delta': result['convergence_delta'],\n",
    "            **faithfulness,\n",
    "            **stability,\n",
    "            **fairness\n",
    "        }\n",
    "        \n",
    "        evaluation_results['multitask'].append(eval_result)\n",
    "    \n",
    "    print(\"‚úÖ Evaluation complete!\")\n",
    "    return evaluation_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1074a60",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attributions(attribution_result: Dict, save_path: Optional[str] = None):\n",
    "    \"\"\"Visualize token attributions as a heatmap\"\"\"\n",
    "    \n",
    "    tokens = attribution_result['tokens']\n",
    "    scores = attribution_result['attribution_scores']\n",
    "    model_type = attribution_result['model_type']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 4))\n",
    "    \n",
    "    # Normalize scores for color mapping\n",
    "    abs_max = max(abs(min(scores)), abs(max(scores)))\n",
    "    normalized_scores = [score / abs_max for score in scores] if abs_max > 0 else scores\n",
    "    \n",
    "    # Create color map\n",
    "    colors = plt.cm.RdYlBu_r([(score + 1) / 2 for score in normalized_scores])\n",
    "    \n",
    "    # Plot bars\n",
    "    bars = ax.bar(range(len(tokens)), [abs(score) for score in normalized_scores], \n",
    "                  color=colors, alpha=0.8)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Attribution Score (Absolute)')\n",
    "    ax.set_title(f'Token Attributions - {model_type.replace(\"_\", \" \").title()} Model\\n'\n",
    "                f'Predicted Class: {attribution_result[\"predicted_class\"]} '\n",
    "                f'(Confidence: {attribution_result[\"confidence\"]:.3f})')\n",
    "    \n",
    "    # Add color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlBu_r, \n",
    "                              norm=plt.Normalize(vmin=-abs_max, vmax=abs_max))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Attribution Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved visualization to {save_path}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_comparison(evaluation_results: Dict[str, Dict], save_path: Optional[str] = None):\n",
    "    \"\"\"Plot comparison of evaluation metrics between models\"\"\"\n",
    "    \n",
    "    # Extract metrics for comparison\n",
    "    single_task_results = evaluation_results['single_task']\n",
    "    multitask_results = evaluation_results['multitask']\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    def calc_avg_metrics(results):\n",
    "        metrics = {}\n",
    "        for key in ['faithfulness_top_0.1', 'faithfulness_top_0.2', 'faithfulness_top_0.3',\n",
    "                   'stability_cosine', 'stability_correlation', 'emotion_bias_ratio',\n",
    "                   'demographic_bias_ratio', 'attribution_concentration']:\n",
    "            values = [r.get(key, 0) for r in results if key in r]\n",
    "            metrics[key] = np.mean(values) if values else 0\n",
    "        return metrics\n",
    "    \n",
    "    single_avg = calc_avg_metrics(single_task_results)\n",
    "    multi_avg = calc_avg_metrics(multitask_results)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Explainability Metrics Comparison: Single-Task vs Multitask BERTweet', fontsize=16)\n",
    "    \n",
    "    # 1. Faithfulness comparison\n",
    "    faithfulness_metrics = ['faithfulness_top_0.1', 'faithfulness_top_0.2', 'faithfulness_top_0.3']\n",
    "    single_faith = [single_avg[m] for m in faithfulness_metrics]\n",
    "    multi_faith = [multi_avg[m] for m in faithfulness_metrics]\n",
    "    \n",
    "    x_pos = np.arange(len(faithfulness_metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x_pos - width/2, single_faith, width, label='Single-Task', alpha=0.8)\n",
    "    axes[0, 0].bar(x_pos + width/2, multi_faith, width, label='Multitask', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Deletion Ratio')\n",
    "    axes[0, 0].set_ylabel('Confidence Drop')\n",
    "    axes[0, 0].set_title('Faithfulness (Higher = Better)')\n",
    "    axes[0, 0].set_xticks(x_pos)\n",
    "    axes[0, 0].set_xticklabels(['10%', '20%', '30%'])\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Stability comparison\n",
    "    stability_metrics = ['stability_cosine', 'stability_correlation']\n",
    "    single_stab = [single_avg[m] for m in stability_metrics]\n",
    "    multi_stab = [multi_avg[m] for m in stability_metrics]\n",
    "    \n",
    "    x_pos = np.arange(len(stability_metrics))\n",
    "    axes[0, 1].bar(x_pos - width/2, single_stab, width, label='Single-Task', alpha=0.8)\n",
    "    axes[0, 1].bar(x_pos + width/2, multi_stab, width, label='Multitask', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Stability Metric')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_title('Stability (Higher = Better)')\n",
    "    axes[0, 1].set_xticks(x_pos)\n",
    "    axes[0, 1].set_xticklabels(['Cosine Sim.', 'Correlation'])\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fairness comparison\n",
    "    fairness_metrics = ['emotion_bias_ratio', 'demographic_bias_ratio']\n",
    "    single_fair = [single_avg[m] for m in fairness_metrics]\n",
    "    multi_fair = [multi_avg[m] for m in fairness_metrics]\n",
    "    \n",
    "    x_pos = np.arange(len(fairness_metrics))\n",
    "    axes[1, 0].bar(x_pos - width/2, single_fair, width, label='Single-Task', alpha=0.8)\n",
    "    axes[1, 0].bar(x_pos + width/2, multi_fair, width, label='Multitask', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Bias Type')\n",
    "    axes[1, 0].set_ylabel('Bias Ratio')\n",
    "    axes[1, 0].set_title('Fairness - Bias Ratios (Lower = Better)')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(['Emotion Bias', 'Demographic Bias'])\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Attribution concentration\n",
    "    concentration_data = [single_avg['attribution_concentration'], multi_avg['attribution_concentration']]\n",
    "    axes[1, 1].bar(['Single-Task', 'Multitask'], concentration_data, alpha=0.8, \n",
    "                   color=['skyblue', 'lightcoral'])\n",
    "    axes[1, 1].set_ylabel('Gini Coefficient')\n",
    "    axes[1, 1].set_title('Attribution Concentration (Lower = More Distributed)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved comparison plot to {save_path}\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e84200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(evaluation_results: Dict[str, Dict]) -> str:\n",
    "    \"\"\"Generate a comprehensive summary report\"\"\"\n",
    "    \n",
    "    single_task_results = evaluation_results['single_task']\n",
    "    multitask_results = evaluation_results['multitask']\n",
    "    \n",
    "    def calc_summary_stats(results, model_name):\n",
    "        stats = {}\n",
    "        metrics = ['faithfulness_top_0.2', 'stability_cosine', 'emotion_bias_ratio', 'attribution_concentration']\n",
    "        \n",
    "        for metric in metrics:\n",
    "            values = [r.get(metric, 0) for r in results if metric in r]\n",
    "            if values:\n",
    "                stats[f'{metric}_mean'] = np.mean(values)\n",
    "                stats[f'{metric}_std'] = np.std(values)\n",
    "            else:\n",
    "                stats[f'{metric}_mean'] = 0\n",
    "                stats[f'{metric}_std'] = 0\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    single_stats = calc_summary_stats(single_task_results, 'Single-Task')\n",
    "    multi_stats = calc_summary_stats(multitask_results, 'Multitask')\n",
    "    \n",
    "    report = f\"\"\"\n",
    "    ================================================================================\n",
    "    üß† BERTWEET EXPLAINABILITY ANALYSIS REPORT\n",
    "    ================================================================================\n",
    "    \n",
    "    üìä ANALYSIS OVERVIEW:\n",
    "    - Models Compared: Single-Task BERTweet vs Multitask BERTweet (Sentiment Head)\n",
    "    - Test Examples: {len(single_task_results)} examples\n",
    "    - Explanation Method: Integrated Gradients\n",
    "    - Evaluation Metrics: Faithfulness, Stability, Fairness\n",
    "    \n",
    "    ================================================================================\n",
    "    üìà FAITHFULNESS RESULTS (Confidence drop when removing top 20% tokens):\n",
    "    \n",
    "    Single-Task Model:\n",
    "    - Mean: {single_stats['faithfulness_top_0.2_mean']:.4f} ¬± {single_stats['faithfulness_top_0.2_std']:.4f}\n",
    "    \n",
    "    Multitask Model:\n",
    "    - Mean: {multi_stats['faithfulness_top_0.2_mean']:.4f} ¬± {multi_stats['faithfulness_top_0.2_std']:.4f}\n",
    "    \n",
    "    Interpretation: Higher values indicate more faithful explanations.\n",
    "    {'‚úÖ Multitask model shows higher faithfulness' if multi_stats['faithfulness_top_0.2_mean'] > single_stats['faithfulness_top_0.2_mean'] else '‚úÖ Single-task model shows higher faithfulness'}\n",
    "    \n",
    "    ================================================================================\n",
    "    üîÑ STABILITY RESULTS (Cosine similarity across perturbations):\n",
    "    \n",
    "    Single-Task Model:\n",
    "    - Mean: {single_stats['stability_cosine_mean']:.4f} ¬± {single_stats['stability_cosine_std']:.4f}\n",
    "    \n",
    "    Multitask Model:\n",
    "    - Mean: {multi_stats['stability_cosine_mean']:.4f} ¬± {multi_stats['stability_cosine_std']:.4f}\n",
    "    \n",
    "    Interpretation: Higher values indicate more stable explanations.\n",
    "    {'‚úÖ Multitask model shows higher stability' if multi_stats['stability_cosine_mean'] > single_stats['stability_cosine_mean'] else '‚úÖ Single-task model shows higher stability'}\n",
    "    \n",
    "    ================================================================================\n",
    "    ‚öñÔ∏è FAIRNESS RESULTS (Emotion bias ratio):\n",
    "    \n",
    "    Single-Task Model:\n",
    "    - Mean: {single_stats['emotion_bias_ratio_mean']:.4f} ¬± {single_stats['emotion_bias_ratio_std']:.4f}\n",
    "    \n",
    "    Multitask Model:\n",
    "    - Mean: {multi_stats['emotion_bias_ratio_mean']:.4f} ¬± {multi_stats['emotion_bias_ratio_std']:.4f}\n",
    "    \n",
    "    Interpretation: Lower values indicate less biased explanations.\n",
    "    {'‚úÖ Multitask model shows lower bias' if multi_stats['emotion_bias_ratio_mean'] < single_stats['emotion_bias_ratio_mean'] else '‚úÖ Single-task model shows lower bias'}\n",
    "    \n",
    "    ================================================================================\n",
    "    üìä ATTRIBUTION CONCENTRATION (Gini coefficient):\n",
    "    \n",
    "    Single-Task Model:\n",
    "    - Mean: {single_stats['attribution_concentration_mean']:.4f} ¬± {single_stats['attribution_concentration_std']:.4f}\n",
    "    \n",
    "    Multitask Model:\n",
    "    - Mean: {multi_stats['attribution_concentration_mean']:.4f} ¬± {multi_stats['attribution_concentration_std']:.4f}\n",
    "    \n",
    "    Interpretation: Lower values indicate more distributed attributions.\n",
    "    {'‚úÖ Multitask model shows more distributed attributions' if multi_stats['attribution_concentration_mean'] < single_stats['attribution_concentration_mean'] else '‚úÖ Single-task model shows more distributed attributions'}\n",
    "    \n",
    "    ================================================================================\n",
    "    üîç KEY FINDINGS:\n",
    "    \n",
    "    1. FAITHFULNESS: The {'multitask' if multi_stats['faithfulness_top_0.2_mean'] > single_stats['faithfulness_top_0.2_mean'] else 'single-task'} model produces more faithful explanations,\n",
    "       meaning its important tokens have greater impact on predictions.\n",
    "    \n",
    "    2. STABILITY: The {'multitask' if multi_stats['stability_cosine_mean'] > single_stats['stability_cosine_mean'] else 'single-task'} model generates more stable explanations\n",
    "       that remain consistent across input perturbations.\n",
    "    \n",
    "    3. FAIRNESS: The {'multitask' if multi_stats['emotion_bias_ratio_mean'] < single_stats['emotion_bias_ratio_mean'] else 'single-task'} model shows less bias toward emotion words,\n",
    "       suggesting more balanced attention to different types of features.\n",
    "    \n",
    "    4. CONCENTRATION: The {'multitask' if multi_stats['attribution_concentration_mean'] < single_stats['attribution_concentration_mean'] else 'single-task'} model distributes attributions more evenly\n",
    "       across tokens rather than concentrating on few words.\n",
    "    \n",
    "    ================================================================================\n",
    "    üí° IMPLICATIONS:\n",
    "    \n",
    "    Multitask training appears to {'improve' if (multi_stats['faithfulness_top_0.2_mean'] > single_stats['faithfulness_top_0.2_mean'] and multi_stats['stability_cosine_mean'] > single_stats['stability_cosine_mean']) else 'have mixed effects on'} explanation quality for sentiment analysis.\n",
    "    The shared representation learning may lead to {'more robust and interpretable' if multi_stats['stability_cosine_mean'] > single_stats['stability_cosine_mean'] else 'different'} feature attribution patterns.\n",
    "    \n",
    "    This suggests that multitask learning {'enhances' if multi_stats['faithfulness_top_0.2_mean'] > single_stats['faithfulness_top_0.2_mean'] else 'affects'} the model's ability to identify\n",
    "    relevant sentiment indicators in a {'more explainable' if multi_stats['stability_cosine_mean'] > single_stats['stability_cosine_mean'] else 'different'} manner.\n",
    "    \n",
    "    ================================================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247d948",
   "metadata": {},
   "source": [
    "## Main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
