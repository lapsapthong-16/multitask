{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf2db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "GPU Memory: 8.0 GB\n",
      "All imports completed and GPU configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"All imports completed and GPU configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06713064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "\n",
    "output_dir = \"./initial_distilroberta_sentiment_model\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# Ensure CUDA cache is cleared\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb27104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    model_name: str = \"distilroberta-base\"\n",
    "    max_length: int = 128\n",
    "    batch_size: int = 8  # Standardize to match BERTweet\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 3\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_dropout_prob: float = 0.1\n",
    "    classifier_dropout: float = 0.1\n",
    "    output_dir: str = \"./distilroberta_model_output\"\n",
    "    alpha: float = 0.5  # For multitask loss weighting\n",
    "    task_type: str = \"multitask\"  # \"sentiment\", \"emotion\", or \"multitask\"\n",
    "\n",
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20511cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fbea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "class distilroBERTaSingleTaskDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "class distilroBERTaMultiTaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ca8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Architectures\n",
    "class distilroBERTaSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class distilroBERTaMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Shared RoBERTa encoder\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.sentiment_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            sentiment_num_classes\n",
    "        )\n",
    "        \n",
    "        # Emotion classification head\n",
    "        self.emotion_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            emotion_num_classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get shared RoBERTa representations\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "        emotion_logits = self.emotion_classifier(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8f19424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa data processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def aggressive_memory_cleanup():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def load_and_process_datasets_roberta():\n",
    "    print(\"Loading external datasets for distilroBERTa...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        print(f\"SST-2 dataset loaded: {len(sst2_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        print(f\"GoEmotions dataset loaded: {len(emotions_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Process sentiment data\n",
    "    sentiment_data = process_sentiment_data_roberta(sst2_dataset)\n",
    "    \n",
    "    # Process emotion data  \n",
    "    emotion_data = process_emotion_data_roberta(emotions_dataset)\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def process_sentiment_data_roberta(sst2_dataset, max_samples=10000):\n",
    "    \n",
    "    print(\"ðŸ”„ Processing sentiment data for distilroBERTa...\")\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = sst2_dataset['train']['sentence'][:max_samples]\n",
    "    train_labels = sst2_dataset['train']['label'][:max_samples]\n",
    "    \n",
    "    # Map SST-2 labels to 3 classes: 0->Negative, 1->Positive\n",
    "    # Add some neutral examples by random assignment\n",
    "    expanded_labels = []\n",
    "    expanded_texts = []\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        if label == 0:  # Negative\n",
    "            expanded_labels.append(0)\n",
    "            expanded_texts.append(text)\n",
    "        elif label == 1:  # Positive\n",
    "            # Sometimes assign as positive, sometimes as neutral\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                expanded_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                expanded_labels.append(2)  # Positive\n",
    "            expanded_texts.append(text)\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in expanded_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(expanded_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            expanded_labels[idx] = 1\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        expanded_texts, expanded_labels, test_size=0.3, random_state=42, stratify=expanded_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    sentiment_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… RoBERTa Sentiment data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return sentiment_data\n",
    "\n",
    "def process_emotion_data_roberta(emotion_dataset, max_samples=10000):\n",
    "    \n",
    "    print(\"Processing emotion data for distilroBERTa...\")\n",
    "    \n",
    "    # Filter to first 6 emotions only\n",
    "    def filter_emotions(example):\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    filtered_train = emotion_dataset['train'].filter(filter_emotions)\n",
    "    filtered_val = emotion_dataset['validation'].filter(filter_emotions)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = filtered_train['text'][:max_samples]\n",
    "    train_labels_raw = filtered_train['labels'][:max_samples]\n",
    "    \n",
    "    # Handle multi-label to single-label conversion\n",
    "    train_labels = []\n",
    "    for label in train_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            train_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.3, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    emotion_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"distilroBERTa Emotion data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return emotion_data\n",
    "\n",
    "def create_multitask_data_roberta(sentiment_data, emotion_data):\n",
    "    \n",
    "    print(\"ðŸ”„ Creating multi-task dataset for distilroBERTa...\")\n",
    "    \n",
    "    # Take minimum length to balance datasets\n",
    "    min_train_len = min(len(sentiment_data['train']['texts']), len(emotion_data['train']['texts']))\n",
    "    min_val_len = min(len(sentiment_data['val']['texts']), len(emotion_data['val']['texts']))\n",
    "    min_test_len = min(len(sentiment_data['test']['texts']), len(emotion_data['test']['texts']))\n",
    "    \n",
    "    multitask_data = {\n",
    "        'train': {\n",
    "            'texts': sentiment_data['train']['texts'][:min_train_len],\n",
    "            'sentiment_labels': sentiment_data['train']['labels'][:min_train_len],\n",
    "            'emotion_labels': emotion_data['train']['labels'][:min_train_len]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': sentiment_data['val']['texts'][:min_val_len],\n",
    "            'sentiment_labels': sentiment_data['val']['labels'][:min_val_len],\n",
    "            'emotion_labels': emotion_data['val']['labels'][:min_val_len]\n",
    "        },\n",
    "        'test': {\n",
    "            'texts': sentiment_data['test']['texts'][:min_test_len],\n",
    "            'sentiment_labels': sentiment_data['test']['labels'][:min_test_len],\n",
    "            'emotion_labels': emotion_data['test']['labels'][:min_test_len]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"distilroBERTa Multi-task data created:\")\n",
    "    print(f\"  Train: {len(multitask_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(multitask_data['val']['texts'])} samples\")\n",
    "    print(f\"  Test: {len(multitask_data['test']['texts'])} samples\")\n",
    "    \n",
    "    return multitask_data\n",
    "\n",
    "print(\"distilroBERTa data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d1443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Training classes defined!\n"
     ]
    }
   ],
   "source": [
    "class distilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize distilroBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize distilroBERTa model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting distilroBERTa single-task training ({self.config.task_type})...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\nðŸ“ Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'classifier_state_dict': self.model.classifier.state_dict(),\n",
    "            'num_classes': self.num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "class distilroBERTaMultiTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize RoBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize RoBERTa multi-task model\n",
    "        self.model = distilroBERTaMultiTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            sentiment_num_classes=roberta_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=roberta_model_config.emotion_num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_accuracy': [],\n",
    "            'train_emotion_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'val_sentiment_f1_macro': [],\n",
    "            'val_emotion_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_correct = 0\n",
    "        emotion_correct = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss with alpha weighting\n",
    "            loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "            emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "            \n",
    "            sentiment_correct += (sentiment_preds == sentiment_labels).sum().item()\n",
    "            emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
    "            total_predictions += sentiment_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        sentiment_accuracy = sentiment_correct / total_predictions\n",
    "        emotion_accuracy = emotion_correct / total_predictions\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_labels = []\n",
    "        emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_true = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_true = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Calculate combined loss\n",
    "                sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_true)\n",
    "                emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_true)\n",
    "                loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_labels.extend(sentiment_true.cpu().numpy())\n",
    "                emotion_labels.extend(emotion_true.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_labels, emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy, sentiment_f1_macro, emotion_f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting distilroBERTa multi-task training...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_combined_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_sent_acc, train_emo_acc = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_sent_acc, val_emo_acc, val_sent_f1, val_emo_f1 = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_sentiment_accuracy'].append(train_sent_acc)\n",
    "            self.training_history['train_emotion_accuracy'].append(train_emo_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_sent_acc)\n",
    "            self.training_history['val_emotion_accuracy'].append(val_emo_acc)\n",
    "            self.training_history['val_sentiment_f1_macro'].append(val_sent_f1)\n",
    "            self.training_history['val_emotion_f1_macro'].append(val_emo_f1)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Train Sentiment Acc: {train_sent_acc:.4f}, Train Emotion Acc: {train_emo_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Sentiment Acc: {val_sent_acc:.4f}, F1: {val_sent_f1:.4f}\")\n",
    "            print(f\"  Val Emotion Acc: {val_emo_acc:.4f}, F1: {val_emo_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            combined_f1 = (val_sent_f1 + val_emo_f1) / 2\n",
    "            if combined_f1 > best_combined_f1:\n",
    "                best_combined_f1 = combined_f1\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best Combined F1: {best_combined_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'sentiment_classifier_state_dict': self.model.sentiment_classifier.state_dict(),\n",
    "            'emotion_classifier_state_dict': self.model.emotion_classifier.state_dict(),\n",
    "            'sentiment_num_classes': self.model.sentiment_num_classes,\n",
    "            'emotion_num_classes': self.model.emotion_num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "print(\"distilroBERTa Training classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8a89274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_distilroberta_model(model_path: str, model_type: str, test_data: Dict, model_name: str):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Initialize the appropriate model architecture\n",
    "    if model_type == \"multitask\":\n",
    "        model = distilroBERTaMultiTaskTransformer(\n",
    "            model_name=model_name,\n",
    "            sentiment_num_classes=roberta_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=roberta_model_config.emotion_num_classes\n",
    "        )\n",
    "    else:\n",
    "        num_classes = (roberta_model_config.sentiment_num_classes \n",
    "                      if model_type == \"sentiment\" \n",
    "                      else roberta_model_config.emotion_num_classes)\n",
    "        model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=model_name,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    # Load the saved state dict\n",
    "    custom_components = torch.load(os.path.join(model_path, 'custom_components.pt'))\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        model.sentiment_classifier.load_state_dict(custom_components['sentiment_classifier_state_dict'])\n",
    "        model.emotion_classifier.load_state_dict(custom_components['emotion_classifier_state_dict'])\n",
    "    else:\n",
    "        model.classifier.load_state_dict(custom_components['classifier_state_dict'])\n",
    "    \n",
    "    # Load the base model weights\n",
    "    base_model = AutoModel.from_pretrained(model_path)\n",
    "    model.roberta = base_model\n",
    "    \n",
    "    # Make sure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Rest of the evaluation code remains the same...\n",
    "    # Create dataset and dataloader\n",
    "    if model_type == \"multitask\":\n",
    "        dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            sentiment_labels=test_data['sentiment_labels'],\n",
    "            emotion_labels=test_data['emotion_labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    else:\n",
    "        dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            labels=test_data['labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move everything to the same device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            if model_type == \"multitask\":\n",
    "                sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "                emotion_labels = batch['emotion_labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend([\n",
    "                    sentiment_preds.cpu().numpy(),\n",
    "                    emotion_preds.cpu().numpy()\n",
    "                ])\n",
    "                all_labels.extend([\n",
    "                    sentiment_labels.cpu().numpy(),\n",
    "                    emotion_labels.cpu().numpy()\n",
    "                ])\n",
    "            else:\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_accuracy = accuracy_score(all_labels[0], all_predictions[0])\n",
    "        sentiment_f1 = f1_score(all_labels[0], all_predictions[0], average='macro')\n",
    "        emotion_accuracy = accuracy_score(all_labels[1], all_predictions[1])\n",
    "        emotion_f1 = f1_score(all_labels[1], all_predictions[1], average='macro')\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1_macro': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1_macro': emotion_f1,\n",
    "            'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "            'combined_f1_macro': (sentiment_f1 + emotion_f1) / 2\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'accuracy': accuracy_score(all_labels, all_predictions),\n",
    "            'f1_macro': f1_score(all_labels, all_predictions, average='macro')\n",
    "        }\n",
    "    \n",
    "print(\"distilroBERTa evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb93e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuning_subset(data_splits, subset_ratio=0.01):  # Even smaller: 1%\n",
    "    print(f\"ðŸ”ª Creating {subset_ratio*100:.0f}% subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:  # Minimum 20 samples\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'labels': [split_data['labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“Š Tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "def create_multitask_tuning_subset(data_splits, subset_ratio=0.01):\n",
    "    print(f\"ðŸ”ª Creating {subset_ratio*100:.0f}% multitask subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_multitask_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [split_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [split_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_multitask_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_multitask_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_multitask_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_multitask_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"Multitask tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "087a4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… distilroBERTa Hyperparameter Tuning classes defined!\n"
     ]
    }
   ],
   "source": [
    "class distilroBERTaHyperparameterTuner:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 15,\n",
    "        model_name: str = \"distilroberta-base\"\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.data_splits = data_splits\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(f\"distilroBERTa hyperparameter tuner initialized for {model_type}\")\n",
    "        print(f\"Using Random Search for optimization\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \n",
    "        # Sample hyperparameters \n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32])  # Changed from [4, 8, 16]\n",
    "        num_epochs = trial.suggest_int('num_epochs', 3, 6)  # Keep this the same\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.2)  # Narrowed range\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1)  # Adjusted range\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)  # Same range\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.3)  # Adjusted upper bound\n",
    "        max_length = 128  # Fixed value instead of tuning parameter\n",
    "        \n",
    "        # Multi-task specific parameter\n",
    "        alpha = trial.suggest_float('alpha', 0.4, 0.6) if self.model_type == \"multitask\" else 0.5  # Narrowed range\n",
    "        \n",
    "        # Create config\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./distilroberta_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Train model\n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = distilroBERTaMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return combined F1 score\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro'])\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro'])\n",
    "                combined_f1 = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "                print(f\"Trial {trial.number}: Combined F1 = {combined_f1:.4f}\")\n",
    "                return combined_f1\n",
    "                \n",
    "            else:\n",
    "                # Single task training\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = roberta_model_config.sentiment_num_classes\n",
    "                else:  # emotion\n",
    "                    num_classes = roberta_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = distilroBERTaSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return best F1 score\n",
    "                best_f1 = max(history['val_f1_macro'])\n",
    "                print(f\"Trial {trial.number}: F1 = {best_f1:.4f}\")\n",
    "                return best_f1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Clean up\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \n",
    "        # Create study with Random Search\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.RandomSampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nðŸ” Starting hyperparameter optimization for {self.model_type}...\")\n",
    "        print(f\"ðŸŽ¯ Random Search: {self.n_trials} trials\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nðŸ† Optimization completed for {self.model_type}!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best F1 score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "print(\"distilroBERTa Hyperparameter Tuning classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0e31cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra-fast trainers defined\n"
     ]
    }
   ],
   "source": [
    "class UltraFastdistilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize tokenizer (reuse if possible)\n",
    "        if not hasattr(self, '_tokenizer_cache'):\n",
    "            UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache = AutoTokenizer.from_pretrained(config.model_name)\n",
    "            if UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token is None:\n",
    "                UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.eos_token\n",
    "        \n",
    "        self.tokenizer = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Speed-optimized data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # No multiprocessing for speed\n",
    "            pin_memory=False  # Disable pin_memory\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        # Simple optimizer setup\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "            labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Print progress for very small datasets\n",
    "            if batch_idx % max(1, len(self.train_loader) // 4) == 0:\n",
    "                print(f\"    Batch {batch_idx + 1}/{len(self.train_loader)}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader), correct_predictions / total_predictions\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "                labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"ðŸš€ Starting ultra-fast distilroBERTa training ({self.config.task_type})...\")\n",
    "        \n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"  ðŸ“ Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            print(f\"    Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "        \n",
    "        print(f\"âœ… Training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "\n",
    "print(\"Ultra-fast trainers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "187a96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING distilroBERTa TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ Loading and processing datasets for distilroBERTa...\n",
      "ðŸ“¥ Loading external datasets for RoBERTa...\n",
      "SST-2 dataset loaded: 67349 train samples\n",
      "GoEmotions dataset loaded: 43410 train samples\n",
      "ðŸ”„ Processing sentiment data for RoBERTa...\n",
      "âœ… RoBERTa Sentiment data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "ðŸ”„ Processing emotion data for RoBERTa...\n",
      "âœ… RoBERTa Emotion data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "ðŸ”„ Creating multi-task dataset for RoBERTa...\n",
      "âœ… RoBERTa Multi-task data created:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "Data loading completed!\n",
      "Sentiment data: 7000 train samples\n",
      "Emotion data: 7000 train samples\n",
      "Multitask data: 7000 train samples\n",
      "Model: distilroberta-base\n",
      "Hyperparameter trials per model: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"STARTING distilroBERTa TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear memory before starting\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Load and process datasets for distilroBERTa\n",
    "print(\"\\nLoading and processing datasets for distilroBERTa...\")\n",
    "sentiment_data, emotion_data = load_and_process_datasets_roberta()\n",
    "multitask_data = create_multitask_data_roberta(sentiment_data, emotion_data)\n",
    "\n",
    "# Model configurations\n",
    "model_name = \"distilroberta-base\"\n",
    "n_trials = 15  # Number of hyperparameter tuning trials\n",
    "\n",
    "print(\"Data loading completed!\")\n",
    "print(f\"Sentiment data: {len(sentiment_data['train']['texts'])} train samples\")\n",
    "print(f\"Emotion data: {len(emotion_data['train']['texts'])} train samples\")\n",
    "print(f\"Multitask data: {len(multitask_data['train']['texts'])} train samples\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Hyperparameter trials per model: {n_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "2ï¸âƒ£ Training Initial distilroBERTa Sentiment Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/3\n",
      "  Train Loss: 0.6889, Train Acc: 0.7306\n",
      "  Val Loss: 0.5309, Val Acc: 0.8127, Val F1: 0.5669\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/3\n",
      "  Train Loss: 0.4799, Train Acc: 0.8426\n",
      "  Val Loss: 0.5704, Val Acc: 0.8273, Val F1: 0.5772\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/3\n",
      "  Train Loss: 0.4000, Train Acc: 0.8766\n",
      "  Val Loss: 0.6107, Val Acc: 0.8327, Val F1: 0.5811\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5811\n",
      "\n",
      "âœ… Initial Sentiment Model Results:\n",
      "  Accuracy: 0.8333\n",
      "  F1 Macro: 0.5812\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: INITIAL DISTILROBERTA TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize results dictionary\n",
    "all_results = {}\n",
    "\n",
    "# Default configuration for sentiment\n",
    "default_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Training Initial distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model\n",
    "initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_sentiment,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate initial sentiment model\n",
    "initial_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_results['initial_sentiment'] = initial_sentiment_results\n",
    "\n",
    "print(f\"\\nâœ… Initial Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1e20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "3ï¸âƒ£ Training Initial distilroBERTa Emotion Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/3\n",
      "  Train Loss: 1.0058, Train Acc: 0.6143\n",
      "  Val Loss: 0.7677, Val Acc: 0.7180, Val F1: 0.6652\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/3\n",
      "  Train Loss: 0.6032, Train Acc: 0.7853\n",
      "  Val Loss: 0.7826, Val Acc: 0.7413, Val F1: 0.7043\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/3\n",
      "  Train Loss: 0.4370, Train Acc: 0.8466\n",
      "  Val Loss: 0.8387, Val Acc: 0.7493, Val F1: 0.7207\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7207\n",
      "\n",
      "âœ… Initial Emotion Model Results:\n",
      "  Accuracy: 0.7667\n",
      "  F1 Macro: 0.7329\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for emotion\n",
    "default_config_emotion = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./initial_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Initial distilroBERTa Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial emotion model\n",
    "initial_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_emotion,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate initial emotion model\n",
    "initial_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_emotion'] = initial_emotion_results\n",
    "\n",
    "print(f\"\\nâœ… Initial Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {initial_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "4ï¸âƒ£ Training Initial distilroBERTa Multi-task Model...\n",
      "============================================================\n",
      "ðŸš€ Starting distilroBERTa multi-task training...\n",
      "\n",
      "ðŸ“ Epoch 1/3\n",
      "  Train Loss: 1.1999\n",
      "  Train Sentiment Acc: 0.7303, Train Emotion Acc: 0.2776\n",
      "  Val Loss: 1.1355\n",
      "  Val Sentiment Acc: 0.7907, F1: 0.5520\n",
      "  Val Emotion Acc: 0.2807, F1: 0.1070\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_multitask_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/3\n",
      "  Train Loss: 1.0823\n",
      "  Train Sentiment Acc: 0.8460, Train Emotion Acc: 0.2900\n",
      "  Val Loss: 1.1108\n",
      "  Val Sentiment Acc: 0.8227, F1: 0.5735\n",
      "  Val Emotion Acc: 0.2993, F1: 0.0812\n",
      "\n",
      "ðŸ“ Epoch 3/3\n",
      "  Train Loss: 1.0223\n",
      "  Train Sentiment Acc: 0.8791, Train Emotion Acc: 0.3040\n",
      "  Val Loss: 1.1299\n",
      "  Val Sentiment Acc: 0.8260, F1: 0.5762\n",
      "  Val Emotion Acc: 0.3007, F1: 0.1022\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_multitask_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3392\n",
      "\n",
      "âœ… Initial Multitask Model Results:\n",
      "  Sentiment - Accuracy: 0.7812, F1: 0.5376\n",
      "  Emotion - Accuracy: 0.3750, F1: 0.0930\n",
      "  Combined - Accuracy: 0.5781, F1: 0.3153\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 1: INITIAL DISTILROBERTA TRAINING - MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for multitask\n",
    "default_config_multitask = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    alpha=0.5,\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./initial_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Training Initial distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial multitask model\n",
    "initial_multitask_trainer = distilroBERTaMultiTaskTrainer(config=default_config_multitask)\n",
    "initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate initial multitask model\n",
    "initial_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_multitask'] = initial_multitask_results\n",
    "\n",
    "print(f\"\\nâœ… Initial Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {initial_multitask_results['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {initial_multitask_results['emotion_accuracy']:.4f}, F1: {initial_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {initial_multitask_results['combined_accuracy']:.4f}, F1: {initial_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ INITIAL ROBERTA RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š INITIAL ROBERTA MODEL PERFORMANCE:\n",
      "  Sentiment Model:\n",
      "    Accuracy: 0.8333\n",
      "    F1 Macro: 0.5812\n",
      "\n",
      "  Emotion Model:\n",
      "    Accuracy: 0.7667\n",
      "    F1 Macro: 0.7329\n",
      "\n",
      "  Multitask Model:\n",
      "    Sentiment - Accuracy: 0.7812, F1: 0.5376\n",
      "    Emotion - Accuracy: 0.3750, F1: 0.0930\n",
      "    Combined - Accuracy: 0.5781, F1: 0.3153\n",
      "\n",
      "ðŸ’¡ These are RoBERTa baseline results. Now proceeding to hyperparameter tuning!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ INITIAL distilroBERTa RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š INITIAL distilroBERTa MODEL PERFORMANCE:\")\n",
    "print(f\"  Sentiment Model:\")\n",
    "print(f\"    Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Emotion Model:\")\n",
    "print(f\"    Accuracy: {initial_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Multitask Model:\")\n",
    "print(f\"    Sentiment - Accuracy: {initial_multitask_results['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {initial_multitask_results['emotion_accuracy']:.4f}, F1: {initial_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {initial_multitask_results['combined_accuracy']:.4f}, F1: {initial_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Store results for later comparison\n",
    "all_results = {\n",
    "    'initial_sentiment': initial_sentiment_results,\n",
    "    'initial_emotion': initial_emotion_results,\n",
    "    'initial_multitask': initial_multitask_results\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ’¡ These are distilroBERTa baseline results. Now proceeding to hyperparameter tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9aa44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 09:35:35,616] A new study created in memory with name: no-name-18a782dd-100a-4545-9f83-84aec435448a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 2: HYPERPARAMETER TUNING - SENTIMENT\n",
      "================================================================================\n",
      "\n",
      "6ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Sentiment Model...\n",
      "============================================================\n",
      "ðŸ” distilroBERTa hyperparameter tuner initialized for sentiment\n",
      "ðŸš€ Using Random Search for optimization\n",
      "\n",
      "ðŸ” Starting hyperparameter optimization for sentiment...\n",
      "ðŸŽ¯ Random Search: 5 trials\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 0.7169, Train Acc: 0.7014\n",
      "  Val Loss: 0.6337, Val Acc: 0.7847, Val F1: 0.5478\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.4947, Train Acc: 0.8310\n",
      "  Val Loss: 0.5853, Val Acc: 0.8167, Val F1: 0.5697\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.3867, Train Acc: 0.8710\n",
      "  Val Loss: 0.5884, Val Acc: 0.8227, Val F1: 0.5743\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.3105, Train Acc: 0.8954\n",
      "  Val Loss: 0.6357, Val Acc: 0.8273, Val F1: 0.5771\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.2577, Train Acc: 0.9077\n",
      "  Val Loss: 0.6929, Val Acc: 0.8253, Val F1: 0.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 09:40:04,095] Trial 0 finished with value: 0.5866923838417387 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11560186404424366, 'weight_decay': 0.02403950683025824, 'hidden_dropout_prob': 0.1116167224336399, 'classifier_dropout': 0.273235229154987}. Best is trial 0 with value: 0.5866923838417387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5867\n",
      "Trial 0: F1 = 0.5867\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 0.7140, Train Acc: 0.7076\n",
      "  Val Loss: 0.6483, Val Acc: 0.7907, Val F1: 0.5519\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.5493, Train Acc: 0.8090\n",
      "  Val Loss: 0.5175, Val Acc: 0.8147, Val F1: 0.5685\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.4350, Train Acc: 0.8561\n",
      "  Val Loss: 0.5633, Val Acc: 0.8067, Val F1: 0.5612\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.3414, Train Acc: 0.8877\n",
      "  Val Loss: 0.6135, Val Acc: 0.8153, Val F1: 0.5742\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.2821, Train Acc: 0.9019\n",
      "  Val Loss: 0.7625, Val Acc: 0.8153, Val F1: 0.5740\n",
      "\n",
      "ðŸ“ Epoch 6/6\n",
      "  Train Loss: 0.2371, Train Acc: 0.9130\n",
      "  Val Loss: 0.8092, Val Acc: 0.8113, Val F1: 0.6055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 09:45:08,074] Trial 1 finished with value: 0.6054800973145226 and parameters: {'learning_rate': 5.262490902114904e-05, 'batch_size': 16, 'num_epochs': 6, 'warmup_ratio': 0.18324426408004219, 'weight_decay': 0.029110519961044856, 'hidden_dropout_prob': 0.1363649934414201, 'classifier_dropout': 0.13668090197068677}. Best is trial 1 with value: 0.6054800973145226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.6055\n",
      "Trial 1: F1 = 0.6055\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/4\n",
      "  Train Loss: 0.7170, Train Acc: 0.7076\n",
      "  Val Loss: 0.5188, Val Acc: 0.8113, Val F1: 0.5663\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/4\n",
      "  Train Loss: 0.5157, Train Acc: 0.8161\n",
      "  Val Loss: 0.5283, Val Acc: 0.8160, Val F1: 0.5698\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/4\n",
      "  Train Loss: 0.4105, Train Acc: 0.8609\n",
      "  Val Loss: 0.5875, Val Acc: 0.8200, Val F1: 0.5716\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/4\n",
      "  Train Loss: 0.3422, Train Acc: 0.8826\n",
      "  Val Loss: 0.5949, Val Acc: 0.8287, Val F1: 0.5781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 09:48:34,112] Trial 2 finished with value: 0.5780906596524404 and parameters: {'learning_rate': 3.2635193912846855e-05, 'batch_size': 16, 'num_epochs': 4, 'warmup_ratio': 0.16118528947223795, 'weight_decay': 0.022554447458683766, 'hidden_dropout_prob': 0.15842892970704364, 'classifier_dropout': 0.17327236865873835}. Best is trial 1 with value: 0.6054800973145226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5781\n",
      "Trial 2: F1 = 0.5781\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 0.7634, Train Acc: 0.6574\n",
      "  Val Loss: 0.5673, Val Acc: 0.8027, Val F1: 0.5597\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.5698, Train Acc: 0.7937\n",
      "  Val Loss: 0.5921, Val Acc: 0.8033, Val F1: 0.5608\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.4675, Train Acc: 0.8383\n",
      "  Val Loss: 0.5779, Val Acc: 0.8233, Val F1: 0.5743\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.4059, Train Acc: 0.8649\n",
      "  Val Loss: 0.5833, Val Acc: 0.8273, Val F1: 0.5773\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.3554, Train Acc: 0.8796\n",
      "  Val Loss: 0.6216, Val Acc: 0.8280, Val F1: 0.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 09:52:52,037] Trial 3 finished with value: 0.5779689171046943 and parameters: {'learning_rate': 4.166863122305896e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.15924145688620425, 'weight_decay': 0.014180537144799797, 'hidden_dropout_prob': 0.22150897038028766, 'classifier_dropout': 0.1341048247374583}. Best is trial 1 with value: 0.6054800973145226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5780\n",
      "Trial 3: F1 = 0.5780\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 0.8752, Train Acc: 0.6129\n",
      "  Val Loss: 0.5733, Val Acc: 0.7987, Val F1: 0.5569\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.6045, Train Acc: 0.7754\n",
      "  Val Loss: 0.5241, Val Acc: 0.8160, Val F1: 0.5683\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.5295, Train Acc: 0.8067\n",
      "  Val Loss: 0.5118, Val Acc: 0.8200, Val F1: 0.5719\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.4611, Train Acc: 0.8384\n",
      "  Val Loss: 0.5493, Val Acc: 0.8207, Val F1: 0.5729\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.4373, Train Acc: 0.8423\n",
      "  Val Loss: 0.5351, Val Acc: 0.8220, Val F1: 0.5736\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 6/6\n",
      "  Train Loss: 0.4185, Train Acc: 0.8509\n",
      "  Val Loss: 0.5449, Val Acc: 0.8260, Val F1: 0.5764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 09:57:23,355] Trial 4 finished with value: 0.576372549591996 and parameters: {'learning_rate': 2.2207471217033647e-05, 'batch_size': 32, 'num_epochs': 6, 'warmup_ratio': 0.13046137691733709, 'weight_decay': 0.018790490260574548, 'hidden_dropout_prob': 0.23684660530243137, 'classifier_dropout': 0.18803049874792027}. Best is trial 1 with value: 0.6054800973145226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5764\n",
      "Trial 4: F1 = 0.5764\n",
      "\n",
      "ðŸ† Optimization completed for sentiment!\n",
      "Best trial: 1\n",
      "Best F1 score: 0.6055\n",
      "Best parameters:\n",
      "  learning_rate: 5.262490902114904e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 6\n",
      "  warmup_ratio: 0.18324426408004219\n",
      "  weight_decay: 0.029110519961044856\n",
      "  hidden_dropout_prob: 0.1363649934414201\n",
      "  classifier_dropout: 0.13668090197068677\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 2: HYPERPARAMETER TUNING - SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for sentiment\n",
    "sentiment_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"sentiment\",\n",
    "    data_splits=sentiment_data,\n",
    "    n_trials=5,\n",
    "    model_name=model_name\n",
    ")\n",
    "sentiment_study = sentiment_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a0c260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 09:57:23,364] A new study created in memory with name: no-name-173fc796-9ea2-40ad-b0a2-25078336018f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 2: HYPERPARAMETER TUNING - EMOTION\n",
      "================================================================================\n",
      "\n",
      "7ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Emotion Mo cdel...\n",
      "============================================================\n",
      "ðŸ” distilroBERTa hyperparameter tuner initialized for emotion\n",
      "ðŸš€ Using Random Search for optimization\n",
      "\n",
      "ðŸ” Starting hyperparameter optimization for emotion...\n",
      "ðŸŽ¯ Random Search: 5 trials\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.1060, Train Acc: 0.5779\n",
      "  Val Loss: 0.7892, Val Acc: 0.7153, Val F1: 0.6683\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.6432, Train Acc: 0.7629\n",
      "  Val Loss: 0.7422, Val Acc: 0.7460, Val F1: 0.7108\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.4301, Train Acc: 0.8489\n",
      "  Val Loss: 0.8253, Val Acc: 0.7367, Val F1: 0.7013\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.2803, Train Acc: 0.8999\n",
      "  Val Loss: 0.9334, Val Acc: 0.7347, Val F1: 0.7051\n",
      "\n",
      "ðŸ“ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:01:49,767] Trial 0 finished with value: 0.710796557453008 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11560186404424366, 'weight_decay': 0.02403950683025824, 'hidden_dropout_prob': 0.1116167224336399, 'classifier_dropout': 0.273235229154987}. Best is trial 0 with value: 0.710796557453008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1904, Train Acc: 0.9360\n",
      "  Val Loss: 1.0122, Val Acc: 0.7387, Val F1: 0.7067\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7108\n",
      "Trial 0: F1 = 0.7108\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 1.1516, Train Acc: 0.5406\n",
      "  Val Loss: 0.7928, Val Acc: 0.7027, Val F1: 0.6666\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.7364, Train Acc: 0.7343\n",
      "  Val Loss: 0.7401, Val Acc: 0.7360, Val F1: 0.7053\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.5075, Train Acc: 0.8204\n",
      "  Val Loss: 0.8164, Val Acc: 0.7413, Val F1: 0.7068\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.3430, Train Acc: 0.8811\n",
      "  Val Loss: 0.9156, Val Acc: 0.7447, Val F1: 0.7159\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.2168, Train Acc: 0.9276\n",
      "  Val Loss: 1.1275, Val Acc: 0.7460, Val F1: 0.7124\n",
      "\n",
      "ðŸ“ Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:07:11,102] Trial 1 finished with value: 0.7159383959312363 and parameters: {'learning_rate': 5.262490902114904e-05, 'batch_size': 16, 'num_epochs': 6, 'warmup_ratio': 0.18324426408004219, 'weight_decay': 0.029110519961044856, 'hidden_dropout_prob': 0.1363649934414201, 'classifier_dropout': 0.13668090197068677}. Best is trial 1 with value: 0.7159383959312363.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1323, Train Acc: 0.9559\n",
      "  Val Loss: 1.2203, Val Acc: 0.7427, Val F1: 0.7127\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7159\n",
      "Trial 1: F1 = 0.7159\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/4\n",
      "  Train Loss: 1.1763, Train Acc: 0.5270\n",
      "  Val Loss: 0.8126, Val Acc: 0.7153, Val F1: 0.6747\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/4\n",
      "  Train Loss: 0.6898, Train Acc: 0.7477\n",
      "  Val Loss: 0.7550, Val Acc: 0.7473, Val F1: 0.7166\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/4\n",
      "  Train Loss: 0.5152, Train Acc: 0.8090\n",
      "  Val Loss: 0.7718, Val Acc: 0.7500, Val F1: 0.7155\n",
      "\n",
      "ðŸ“ Epoch 4/4\n",
      "  Train Loss: 0.4070, Train Acc: 0.8520\n",
      "  Val Loss: 0.8190, Val Acc: 0.7560, Val F1: 0.7207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:10:50,374] Trial 2 finished with value: 0.720734484569559 and parameters: {'learning_rate': 3.2635193912846855e-05, 'batch_size': 16, 'num_epochs': 4, 'warmup_ratio': 0.16118528947223795, 'weight_decay': 0.022554447458683766, 'hidden_dropout_prob': 0.15842892970704364, 'classifier_dropout': 0.17327236865873835}. Best is trial 2 with value: 0.720734484569559.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7207\n",
      "Trial 2: F1 = 0.7207\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.2143, Train Acc: 0.5237\n",
      "  Val Loss: 0.8327, Val Acc: 0.7233, Val F1: 0.6880\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.7420, Train Acc: 0.7290\n",
      "  Val Loss: 0.7669, Val Acc: 0.7407, Val F1: 0.7077\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.5893, Train Acc: 0.7853\n",
      "  Val Loss: 0.7824, Val Acc: 0.7627, Val F1: 0.7348\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.4779, Train Acc: 0.8277\n",
      "  Val Loss: 0.8038, Val Acc: 0.7447, Val F1: 0.7129\n",
      "\n",
      "ðŸ“ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:15:19,145] Trial 3 finished with value: 0.7347866390343331 and parameters: {'learning_rate': 4.166863122305896e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.15924145688620425, 'weight_decay': 0.014180537144799797, 'hidden_dropout_prob': 0.22150897038028766, 'classifier_dropout': 0.1341048247374583}. Best is trial 3 with value: 0.7347866390343331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.3840, Train Acc: 0.8654\n",
      "  Val Loss: 0.8454, Val Acc: 0.7447, Val F1: 0.7137\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7348\n",
      "Trial 3: F1 = 0.7348\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 1.4824, Train Acc: 0.3996\n",
      "  Val Loss: 0.8392, Val Acc: 0.6960, Val F1: 0.6524\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 0.8162, Train Acc: 0.6971\n",
      "  Val Loss: 0.7875, Val Acc: 0.7020, Val F1: 0.6542\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 0.6902, Train Acc: 0.7453\n",
      "  Val Loss: 0.7810, Val Acc: 0.7380, Val F1: 0.7048\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.6104, Train Acc: 0.7739\n",
      "  Val Loss: 0.7669, Val Acc: 0.7480, Val F1: 0.7149\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.5590, Train Acc: 0.7964\n",
      "  Val Loss: 0.7884, Val Acc: 0.7560, Val F1: 0.7232\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:20:01,829] Trial 4 finished with value: 0.7231811022638341 and parameters: {'learning_rate': 2.2207471217033647e-05, 'batch_size': 32, 'num_epochs': 6, 'warmup_ratio': 0.13046137691733709, 'weight_decay': 0.018790490260574548, 'hidden_dropout_prob': 0.23684660530243137, 'classifier_dropout': 0.18803049874792027}. Best is trial 3 with value: 0.7347866390343331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5152, Train Acc: 0.8110\n",
      "  Val Loss: 0.7743, Val Acc: 0.7480, Val F1: 0.7172\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7232\n",
      "Trial 4: F1 = 0.7232\n",
      "\n",
      "ðŸ† Optimization completed for emotion!\n",
      "Best trial: 3\n",
      "Best F1 score: 0.7348\n",
      "Best parameters:\n",
      "  learning_rate: 4.166863122305896e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 5\n",
      "  warmup_ratio: 0.15924145688620425\n",
      "  weight_decay: 0.014180537144799797\n",
      "  hidden_dropout_prob: 0.22150897038028766\n",
      "  classifier_dropout: 0.1341048247374583\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Hyperparameter Tuning - Emotion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 2: HYPERPARAMETER TUNING - EMOTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n7ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Emotion Mo cdel...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for emotion\n",
    "emotion_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"emotion\",\n",
    "    data_splits=emotion_data,\n",
    "    n_trials=5,\n",
    "    model_name=model_name\n",
    ")\n",
    "emotion_study = emotion_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d421b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:20:01,865] A new study created in memory with name: no-name-bffc9898-9520-421b-82b0-a1a3228d0c28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 2: HYPERPARAMETER TUNING - MULTITASK\n",
      "================================================================================\n",
      "\n",
      "8ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Multi-task Model...\n",
      "============================================================\n",
      "ðŸ” distilroBERTa hyperparameter tuner initialized for multitask\n",
      "ðŸš€ Using Random Search for optimization\n",
      "\n",
      "ðŸ” Starting hyperparameter optimization for multitask...\n",
      "ðŸŽ¯ Random Search: 5 trials\n",
      "============================================================\n",
      "ðŸš€ Starting distilroBERTa multi-task training...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.2505\n",
      "  Train Sentiment Acc: 0.6633, Train Emotion Acc: 0.2529\n",
      "  Val Loss: 1.1267\n",
      "  Val Sentiment Acc: 0.7960, F1: 0.5541\n",
      "  Val Emotion Acc: 0.2887, F1: 0.0933\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 1.0868\n",
      "  Train Sentiment Acc: 0.8289, Train Emotion Acc: 0.2856\n",
      "  Val Loss: 1.0892\n",
      "  Val Sentiment Acc: 0.8187, F1: 0.5710\n",
      "  Val Emotion Acc: 0.2953, F1: 0.0999\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 1.0131\n",
      "  Train Sentiment Acc: 0.8757, Train Emotion Acc: 0.2841\n",
      "  Val Loss: 1.1339\n",
      "  Val Sentiment Acc: 0.8200, F1: 0.5712\n",
      "  Val Emotion Acc: 0.2860, F1: 0.1043\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.9648\n",
      "  Train Sentiment Acc: 0.8920, Train Emotion Acc: 0.3093\n",
      "  Val Loss: 1.1525\n",
      "  Val Sentiment Acc: 0.8207, F1: 0.5729\n",
      "  Val Emotion Acc: 0.2720, F1: 0.1452\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:24:33,215] Trial 0 finished with value: 0.36145588727569616 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11560186404424366, 'weight_decay': 0.02403950683025824, 'hidden_dropout_prob': 0.1116167224336399, 'classifier_dropout': 0.273235229154987, 'alpha': 0.5202230023486417}. Best is trial 0 with value: 0.36145588727569616.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.9193\n",
      "  Train Sentiment Acc: 0.9036, Train Emotion Acc: 0.3253\n",
      "  Val Loss: 1.1776\n",
      "  Val Sentiment Acc: 0.8200, F1: 0.5777\n",
      "  Val Emotion Acc: 0.2587, F1: 0.1301\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3590\n",
      "Trial 0: Combined F1 = 0.3615\n",
      "ðŸš€ Starting distilroBERTa multi-task training...\n",
      "\n",
      "ðŸ“ Epoch 1/6\n",
      "  Train Loss: 1.2498\n",
      "  Train Sentiment Acc: 0.6607, Train Emotion Acc: 0.2787\n",
      "  Val Loss: 1.1211\n",
      "  Val Sentiment Acc: 0.8113, F1: 0.5660\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0785\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/6\n",
      "  Train Loss: 1.1146\n",
      "  Train Sentiment Acc: 0.8139, Train Emotion Acc: 0.2881\n",
      "  Val Loss: 1.0961\n",
      "  Val Sentiment Acc: 0.8173, F1: 0.5703\n",
      "  Val Emotion Acc: 0.2813, F1: 0.1039\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/6\n",
      "  Train Loss: 1.0413\n",
      "  Train Sentiment Acc: 0.8641, Train Emotion Acc: 0.2939\n",
      "  Val Loss: 1.1399\n",
      "  Val Sentiment Acc: 0.8187, F1: 0.5708\n",
      "  Val Emotion Acc: 0.2860, F1: 0.1180\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/6\n",
      "  Train Loss: 0.9847\n",
      "  Train Sentiment Acc: 0.8906, Train Emotion Acc: 0.3086\n",
      "  Val Loss: 1.1616\n",
      "  Val Sentiment Acc: 0.8287, F1: 0.5783\n",
      "  Val Emotion Acc: 0.2673, F1: 0.1218\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/6\n",
      "  Train Loss: 0.9236\n",
      "  Train Sentiment Acc: 0.9050, Train Emotion Acc: 0.3477\n",
      "  Val Loss: 1.2099\n",
      "  Val Sentiment Acc: 0.8273, F1: 0.5922\n",
      "  Val Emotion Acc: 0.2553, F1: 0.1574\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "ðŸ“ Epoch 6/6\n",
      "  Train Loss: 0.8682\n",
      "  Train Sentiment Acc: 0.9113, Train Emotion Acc: 0.3930\n",
      "  Val Loss: 1.2352\n",
      "  Val Sentiment Acc: 0.8153, F1: 0.5851\n",
      "  Val Emotion Acc: 0.2480, F1: 0.1660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:29:16,213] Trial 1 finished with value: 0.379130068556821 and parameters: {'learning_rate': 6.251028636335231e-05, 'batch_size': 32, 'num_epochs': 6, 'warmup_ratio': 0.12123391106782762, 'weight_decay': 0.02636424704863906, 'hidden_dropout_prob': 0.13668090197068677, 'classifier_dropout': 0.16084844859190756, 'alpha': 0.5049512863264476}. Best is trial 1 with value: 0.379130068556821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3756\n",
      "Trial 1: Combined F1 = 0.3791\n",
      "ðŸš€ Starting distilroBERTa multi-task training...\n",
      "\n",
      "ðŸ“ Epoch 1/3\n",
      "  Train Loss: 1.3192\n",
      "  Train Sentiment Acc: 0.6476, Train Emotion Acc: 0.2769\n",
      "  Val Loss: 1.2303\n",
      "  Val Sentiment Acc: 0.7833, F1: 0.5468\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0774\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/3\n",
      "  Train Loss: 1.2068\n",
      "  Train Sentiment Acc: 0.8036, Train Emotion Acc: 0.2836\n",
      "  Val Loss: 1.1878\n",
      "  Val Sentiment Acc: 0.8193, F1: 0.5720\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0774\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/3\n",
      "  Train Loss: 1.1598\n",
      "  Train Sentiment Acc: 0.8403, Train Emotion Acc: 0.2966\n",
      "  Val Loss: 1.1824\n",
      "  Val Sentiment Acc: 0.8213, F1: 0.5729\n",
      "  Val Emotion Acc: 0.3020, F1: 0.0773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:31:39,821] Trial 2 finished with value: 0.32516057859994246 and parameters: {'learning_rate': 4.008174375308313e-05, 'batch_size': 32, 'num_epochs': 3, 'warmup_ratio': 0.12921446485352184, 'weight_decay': 0.04297256589643226, 'hidden_dropout_prob': 0.19121399684340717, 'classifier_dropout': 0.2570351922786027, 'alpha': 0.43993475643167196}. Best is trial 1 with value: 0.379130068556821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3251\n",
      "Trial 2: Combined F1 = 0.3252\n",
      "ðŸš€ Starting distilroBERTa multi-task training...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.2273\n",
      "  Train Sentiment Acc: 0.6410, Train Emotion Acc: 0.2623\n",
      "  Val Loss: 1.0858\n",
      "  Val Sentiment Acc: 0.8080, F1: 0.5629\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0774\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 1.1097\n",
      "  Train Sentiment Acc: 0.7771, Train Emotion Acc: 0.2714\n",
      "  Val Loss: 1.0570\n",
      "  Val Sentiment Acc: 0.8113, F1: 0.5662\n",
      "  Val Emotion Acc: 0.3040, F1: 0.0830\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 1.0535\n",
      "  Train Sentiment Acc: 0.8160, Train Emotion Acc: 0.2757\n",
      "  Val Loss: 1.0903\n",
      "  Val Sentiment Acc: 0.8147, F1: 0.5682\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0820\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 1.0223\n",
      "  Train Sentiment Acc: 0.8357, Train Emotion Acc: 0.2801\n",
      "  Val Loss: 1.0863\n",
      "  Val Sentiment Acc: 0.8113, F1: 0.5657\n",
      "  Val Emotion Acc: 0.2940, F1: 0.0915\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:36:02,538] Trial 3 finished with value: 0.33206021202671887 and parameters: {'learning_rate': 4.575772704489176e-05, 'batch_size': 16, 'num_epochs': 5, 'warmup_ratio': 0.11705241236872915, 'weight_decay': 0.015854643368675158, 'hidden_dropout_prob': 0.28977710745066665, 'classifier_dropout': 0.29312640661491185, 'alpha': 0.5616794696232922}. Best is trial 1 with value: 0.379130068556821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.9945\n",
      "  Train Sentiment Acc: 0.8526, Train Emotion Acc: 0.2844\n",
      "  Val Loss: 1.0886\n",
      "  Val Sentiment Acc: 0.8207, F1: 0.5726\n",
      "  Val Emotion Acc: 0.3020, F1: 0.0796\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3286\n",
      "Trial 3: Combined F1 = 0.3321\n",
      "ðŸš€ Starting distilroBERTa multi-task training...\n",
      "\n",
      "ðŸ“ Epoch 1/4\n",
      "  Train Loss: 1.3077\n",
      "  Train Sentiment Acc: 0.6654, Train Emotion Acc: 0.2711\n",
      "  Val Loss: 1.2197\n",
      "  Val Sentiment Acc: 0.7700, F1: 0.5371\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0774\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/4\n",
      "  Train Loss: 1.1711\n",
      "  Train Sentiment Acc: 0.8226, Train Emotion Acc: 0.2879\n",
      "  Val Loss: 1.1712\n",
      "  Val Sentiment Acc: 0.8120, F1: 0.5671\n",
      "  Val Emotion Acc: 0.2993, F1: 0.0833\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/4\n",
      "  Train Loss: 1.1120\n",
      "  Train Sentiment Acc: 0.8619, Train Emotion Acc: 0.3003\n",
      "  Val Loss: 1.1715\n",
      "  Val Sentiment Acc: 0.8253, F1: 0.5762\n",
      "  Val Emotion Acc: 0.2947, F1: 0.0906\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/4\n",
      "  Train Loss: 1.0749\n",
      "  Train Sentiment Acc: 0.8837, Train Emotion Acc: 0.3033\n",
      "  Val Loss: 1.1848\n",
      "  Val Sentiment Acc: 0.8267, F1: 0.5766\n",
      "  Val Emotion Acc: 0.2960, F1: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-23 10:39:00,740] Trial 4 finished with value: 0.33972565090043116 and parameters: {'learning_rate': 3.26547139093759e-05, 'batch_size': 32, 'num_epochs': 4, 'warmup_ratio': 0.11220382348447788, 'weight_decay': 0.054565921910014324, 'hidden_dropout_prob': 0.10687770422304368, 'classifier_dropout': 0.2818640804157564, 'alpha': 0.4517559963200034}. Best is trial 1 with value: 0.379130068556821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3397\n",
      "Trial 4: Combined F1 = 0.3397\n",
      "\n",
      "ðŸ† Optimization completed for multitask!\n",
      "Best trial: 1\n",
      "Best F1 score: 0.3791\n",
      "Best parameters:\n",
      "  learning_rate: 6.251028636335231e-05\n",
      "  batch_size: 32\n",
      "  num_epochs: 6\n",
      "  warmup_ratio: 0.12123391106782762\n",
      "  weight_decay: 0.02636424704863906\n",
      "  hidden_dropout_prob: 0.13668090197068677\n",
      "  classifier_dropout: 0.16084844859190756\n",
      "  alpha: 0.5049512863264476\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Hyperparameter Tuning - Multitask\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 2: HYPERPARAMETER TUNING - MULTITASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n8ï¸âƒ£ Hyperparameter Tuning for distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for multitask\n",
    "multitask_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"multitask\",\n",
    "    data_splits=multitask_data,\n",
    "    n_trials=5,\n",
    "    model_name=model_name\n",
    ")\n",
    "multitask_study = multitask_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4cf786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "9ï¸âƒ£ Training Final distilroBERTa Sentiment Model with Best Parameters...\n",
      "============================================================\n",
      "ðŸŽ¯ Using best hyperparameters:\n",
      "  learning_rate: 5.262490902114904e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 6\n",
      "  warmup_ratio: 0.18324426408004219\n",
      "  weight_decay: 0.029110519961044856\n",
      "  hidden_dropout_prob: 0.1363649934414201\n",
      "  classifier_dropout: 0.13668090197068677\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 0.7564, Train Acc: 0.6686\n",
      "  Val Loss: 0.6248, Val Acc: 0.7813, Val F1: 0.5453\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.5317, Train Acc: 0.8123\n",
      "  Val Loss: 0.5571, Val Acc: 0.7960, Val F1: 0.5561\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.4156, Train Acc: 0.8629\n",
      "  Val Loss: 0.5778, Val Acc: 0.8160, Val F1: 0.5690\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.3188, Train Acc: 0.8916\n",
      "  Val Loss: 0.7513, Val Acc: 0.8253, Val F1: 0.5750\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.2722, Train Acc: 0.9029\n",
      "  Val Loss: 0.7216, Val Acc: 0.8167, Val F1: 0.5762\n",
      "Best distilroBERTa model saved to ./final_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5762\n",
      "\n",
      "âœ… Final Sentiment Model Results:\n",
      "  Accuracy: 0.8260\n",
      "  F1 Macro: 0.5820\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n9ï¸âƒ£ Training Final distilroBERTa Sentiment Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_sentiment_params = sentiment_study.best_params\n",
    "print(f\"ðŸŽ¯ Using best hyperparameters:\")\n",
    "for key, value in best_sentiment_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if 'all_results' not in globals():\n",
    "    all_results = {}\n",
    "\n",
    "final_sentiment_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=5.262490902114904e-05,  # From best parameters\n",
    "    batch_size=16,\n",
    "    num_epochs=5,  # Increased epochs for final training\n",
    "    warmup_ratio=0.18324426408004219,\n",
    "    weight_decay=0.029110519961044856,\n",
    "    hidden_dropout_prob=0.1363649934414201,\n",
    "    classifier_dropout=0.13668090197068677,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./final_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "# Train final sentiment model\n",
    "final_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_sentiment_config,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "final_sentiment_history = final_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate final sentiment model\n",
    "final_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# Now store the results\n",
    "all_results['final_sentiment'] = final_sentiment_results\n",
    "\n",
    "print(f\"\\nFinal Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {final_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d94748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Ÿ Training Final distilroBERTa Emotion Model with Best Parameters...\n",
      "============================================================\n",
      "ðŸŽ¯ Using best hyperparameters:\n",
      "  learning_rate: 4.166863122305896e-05\n",
      "  batch_size: 16\n",
      "  num_epochs: 5\n",
      "  warmup_ratio: 0.15924145688620425\n",
      "  weight_decay: 0.014180537144799797\n",
      "  hidden_dropout_prob: 0.22150897038028766\n",
      "  classifier_dropout: 0.1341048247374583\n",
      "\n",
      "ðŸš€ Training final emotion model:\n",
      "  Dataset: Full emotion data (7000 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 16\n",
      "  Learning rate: 4.17e-05\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.2255, Train Acc: 0.5260\n",
      "  Val Loss: 0.7879, Val Acc: 0.7360, Val F1: 0.6990\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 0.7421, Train Acc: 0.7280\n",
      "  Val Loss: 0.7650, Val Acc: 0.7307, Val F1: 0.6983\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 0.5792, Train Acc: 0.7916\n",
      "  Val Loss: 0.8002, Val Acc: 0.7453, Val F1: 0.7112\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.4637, Train Acc: 0.8294\n",
      "  Val Loss: 0.8414, Val Acc: 0.7493, Val F1: 0.7181\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.3799, Train Acc: 0.8631\n",
      "  Val Loss: 0.8736, Val Acc: 0.7507, Val F1: 0.7203\n",
      "Best distilroBERTa model saved to ./final_distilroberta_emotion_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7203\n",
      "\n",
      "âœ… Final Emotion Model Results:\n",
      "  Accuracy: 0.7667\n",
      "  F1 Macro: 0.7312\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ”Ÿ Training Final distilroBERTa Emotion Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize results dictionary if not exists\n",
    "if 'all_results' not in globals():\n",
    "    all_results = {}\n",
    "\n",
    "# Get best parameters from emotion tuning\n",
    "best_emotion_params = emotion_study.best_params\n",
    "print(f\"ðŸŽ¯ Using best hyperparameters:\")\n",
    "for key, value in best_emotion_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_emotion_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_emotion_params['learning_rate'],\n",
    "    batch_size=best_emotion_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_emotion_params['warmup_ratio'],\n",
    "    weight_decay=best_emotion_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_emotion_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_emotion_params['classifier_dropout'],\n",
    "    max_length=best_emotion_params.get('max_length', 128),\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./final_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸš€ Training final emotion model:\")\n",
    "print(f\"  Dataset: Full emotion data ({len(emotion_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_emotion_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_emotion_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_emotion_config.learning_rate:.2e}\")\n",
    "\n",
    "# Train final emotion model\n",
    "final_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_emotion_config,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "final_emotion_history = final_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate final emotion model\n",
    "final_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_emotion'] = final_emotion_results\n",
    "\n",
    "print(f\"\\nâœ… Final Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {final_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0add541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£1ï¸âƒ£ Training Final distilroBERTa Multi-task Model with Best Parameters...\n",
      "============================================================\n",
      "ðŸŽ¯ Using best hyperparameters:\n",
      "  learning_rate: 6.251028636335231e-05\n",
      "  batch_size: 32\n",
      "  num_epochs: 6\n",
      "  warmup_ratio: 0.12123391106782762\n",
      "  weight_decay: 0.02636424704863906\n",
      "  hidden_dropout_prob: 0.13668090197068677\n",
      "  classifier_dropout: 0.16084844859190756\n",
      "  alpha: 0.5049512863264476\n",
      "\n",
      "ðŸš€ Training final multitask model:\n",
      "  Dataset: Full multitask data (7000 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 32\n",
      "  Learning rate: 6.25e-05\n",
      "  Alpha (loss weighting): 0.505\n",
      "ðŸš€ Starting distilroBERTa multi-task training...\n",
      "\n",
      "ðŸ“ Epoch 1/5\n",
      "  Train Loss: 1.2498\n",
      "  Train Sentiment Acc: 0.6709, Train Emotion Acc: 0.2651\n",
      "  Val Loss: 1.1177\n",
      "  Val Sentiment Acc: 0.8113, F1: 0.5657\n",
      "  Val Emotion Acc: 0.3027, F1: 0.0774\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 2/5\n",
      "  Train Loss: 1.1076\n",
      "  Train Sentiment Acc: 0.8221, Train Emotion Acc: 0.2896\n",
      "  Val Loss: 1.1047\n",
      "  Val Sentiment Acc: 0.8207, F1: 0.5729\n",
      "  Val Emotion Acc: 0.2973, F1: 0.0798\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 3/5\n",
      "  Train Loss: 1.0364\n",
      "  Train Sentiment Acc: 0.8696, Train Emotion Acc: 0.2956\n",
      "  Val Loss: 1.1295\n",
      "  Val Sentiment Acc: 0.8247, F1: 0.5755\n",
      "  Val Emotion Acc: 0.2873, F1: 0.1024\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 4/5\n",
      "  Train Loss: 0.9793\n",
      "  Train Sentiment Acc: 0.8923, Train Emotion Acc: 0.3150\n",
      "  Val Loss: 1.1746\n",
      "  Val Sentiment Acc: 0.8193, F1: 0.5717\n",
      "  Val Emotion Acc: 0.2860, F1: 0.1108\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "ðŸ“ Epoch 5/5\n",
      "  Train Loss: 0.9319\n",
      "  Train Sentiment Acc: 0.9036, Train Emotion Acc: 0.3466\n",
      "  Val Loss: 1.2057\n",
      "  Val Sentiment Acc: 0.8147, F1: 0.5708\n",
      "  Val Emotion Acc: 0.2633, F1: 0.1547\n",
      "Best distilroBERTa model saved to ./final_distilroberta_multitask_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3627\n",
      "\n",
      "âœ… Final Multitask Model Results:\n",
      "  Sentiment - Accuracy: 0.8125, F1: 0.5602\n",
      "  Emotion - Accuracy: 0.3125, F1: 0.7312\n",
      "  Combined - Accuracy: 0.5625, F1: 0.3519\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£1ï¸âƒ£ Training Final distilroBERTa Multi-task Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize results dictionary if not exists\n",
    "if 'all_results' not in globals():\n",
    "    all_results = {}\n",
    "\n",
    "# Get best parameters from multitask tuning\n",
    "best_multitask_params = multitask_study.best_params\n",
    "print(f\"ðŸŽ¯ Using best hyperparameters:\")\n",
    "for key, value in best_multitask_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_multitask_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_multitask_params['learning_rate'],\n",
    "    batch_size=best_multitask_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_multitask_params['warmup_ratio'],\n",
    "    weight_decay=best_multitask_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_multitask_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_multitask_params['classifier_dropout'],\n",
    "    max_length=best_multitask_params.get('max_length', 128),\n",
    "    alpha=best_multitask_params['alpha'],  # Multitask-specific parameter\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./final_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸš€ Training final multitask model:\")\n",
    "print(f\"  Dataset: Full multitask data ({len(multitask_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_multitask_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_multitask_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_multitask_config.learning_rate:.2e}\")\n",
    "print(f\"  Alpha (loss weighting): {final_multitask_config.alpha:.3f}\")\n",
    "\n",
    "# Train final multitask model\n",
    "final_multitask_trainer = distilroBERTaMultiTaskTrainer(config=final_multitask_config)\n",
    "final_multitask_history = final_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate final multitask model\n",
    "final_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_multitask'] = final_multitask_results\n",
    "\n",
    "print(f\"\\nâœ… Final Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {final_multitask_results['sentiment_accuracy']:.4f}, F1: {final_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {final_multitask_results['emotion_accuracy']:.4f}, F1: {final_emotion_results['f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {final_multitask_results['combined_accuracy']:.4f}, F1: {final_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
