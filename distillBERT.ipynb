{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bf2db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "GPU Memory: 8.0 GB\n",
      "All imports completed and GPU configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports for RoBERTa Pipeline\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"All imports completed and GPU configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bb27104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Classes for distilroBERTa\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    model_name: str = \"distilroberta-base\"\n",
    "    max_length: int = 128\n",
    "    batch_size: int = 8  # Standardize to match BERTweet\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 3\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_dropout_prob: float = 0.1\n",
    "    classifier_dropout: float = 0.1\n",
    "    output_dir: str = \"./distilroberta_model_output\"\n",
    "    alpha: float = 0.5  # For multitask loss weighting\n",
    "    task_type: str = \"multitask\"  # \"sentiment\", \"emotion\", or \"multitask\"\n",
    "\n",
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c20511cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "441fbea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Classes for distilroBERTa\n",
    "class distilroBERTaSingleTaskDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "class distilroBERTaMultiTaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2ca8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Architectures\n",
    "class distilroBERTaSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class distilroBERTaMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Shared RoBERTa encoder\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.sentiment_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            sentiment_num_classes\n",
    "        )\n",
    "        \n",
    "        # Emotion classification head\n",
    "        self.emotion_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            emotion_num_classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get shared RoBERTa representations\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "        emotion_logits = self.emotion_classifier(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8f19424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RoBERTa data processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data Loading and Processing Functions for RoBERTa\n",
    "def aggressive_memory_cleanup():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def load_and_process_datasets_roberta():\n",
    "    \"\"\"Load and process datasets for RoBERTa training\"\"\"\n",
    "    print(\"📥 Loading external datasets for RoBERTa...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sst2_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotions_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Process sentiment data\n",
    "    sentiment_data = process_sentiment_data_roberta(sst2_dataset)\n",
    "    \n",
    "    # Process emotion data  \n",
    "    emotion_data = process_emotion_data_roberta(emotions_dataset)\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def process_sentiment_data_roberta(sst2_dataset, max_samples=10000):\n",
    "    \"\"\"Process SST-2 dataset for RoBERTa sentiment classification\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing sentiment data for RoBERTa...\")\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = sst2_dataset['train']['sentence'][:max_samples]\n",
    "    train_labels = sst2_dataset['train']['label'][:max_samples]\n",
    "    \n",
    "    # Map SST-2 labels to 3 classes: 0->Negative, 1->Positive\n",
    "    # Add some neutral examples by random assignment\n",
    "    expanded_labels = []\n",
    "    expanded_texts = []\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        if label == 0:  # Negative\n",
    "            expanded_labels.append(0)\n",
    "            expanded_texts.append(text)\n",
    "        elif label == 1:  # Positive\n",
    "            # Sometimes assign as positive, sometimes as neutral\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                expanded_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                expanded_labels.append(2)  # Positive\n",
    "            expanded_texts.append(text)\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in expanded_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(expanded_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            expanded_labels[idx] = 1\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        expanded_texts, expanded_labels, test_size=0.3, random_state=42, stratify=expanded_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    sentiment_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ RoBERTa Sentiment data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return sentiment_data\n",
    "\n",
    "def process_emotion_data_roberta(emotion_dataset, max_samples=10000):\n",
    "    \"\"\"Process GoEmotion dataset for RoBERTa emotion classification\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing emotion data for RoBERTa...\")\n",
    "    \n",
    "    # Filter to first 6 emotions only\n",
    "    def filter_emotions(example):\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    filtered_train = emotion_dataset['train'].filter(filter_emotions)\n",
    "    filtered_val = emotion_dataset['validation'].filter(filter_emotions)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = filtered_train['text'][:max_samples]\n",
    "    train_labels_raw = filtered_train['labels'][:max_samples]\n",
    "    \n",
    "    # Handle multi-label to single-label conversion\n",
    "    train_labels = []\n",
    "    for label in train_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            train_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.3, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    emotion_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ RoBERTa Emotion data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return emotion_data\n",
    "\n",
    "def create_multitask_data_roberta(sentiment_data, emotion_data):\n",
    "    \"\"\"Create combined dataset for multi-task learning with RoBERTa\"\"\"\n",
    "    \n",
    "    print(\"🔄 Creating multi-task dataset for RoBERTa...\")\n",
    "    \n",
    "    # Take minimum length to balance datasets\n",
    "    min_train_len = min(len(sentiment_data['train']['texts']), len(emotion_data['train']['texts']))\n",
    "    min_val_len = min(len(sentiment_data['val']['texts']), len(emotion_data['val']['texts']))\n",
    "    min_test_len = min(len(sentiment_data['test']['texts']), len(emotion_data['test']['texts']))\n",
    "    \n",
    "    multitask_data = {\n",
    "        'train': {\n",
    "            'texts': sentiment_data['train']['texts'][:min_train_len],\n",
    "            'sentiment_labels': sentiment_data['train']['labels'][:min_train_len],\n",
    "            'emotion_labels': emotion_data['train']['labels'][:min_train_len]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': sentiment_data['val']['texts'][:min_val_len],\n",
    "            'sentiment_labels': sentiment_data['val']['labels'][:min_val_len],\n",
    "            'emotion_labels': emotion_data['val']['labels'][:min_val_len]\n",
    "        },\n",
    "        'test': {\n",
    "            'texts': sentiment_data['test']['texts'][:min_test_len],\n",
    "            'sentiment_labels': sentiment_data['test']['labels'][:min_test_len],\n",
    "            'emotion_labels': emotion_data['test']['labels'][:min_test_len]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ RoBERTa Multi-task data created:\")\n",
    "    print(f\"  Train: {len(multitask_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(multitask_data['val']['texts'])} samples\")\n",
    "    print(f\"  Test: {len(multitask_data['test']['texts'])} samples\")\n",
    "    \n",
    "    return multitask_data\n",
    "\n",
    "print(\"✅ RoBERTa data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5d1443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Training classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: distilroBERTa Training Classes\n",
    "class distilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize distilroBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize distilroBERTa model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting distilroBERTa single-task training ({self.config.task_type})...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'classifier_state_dict': self.model.classifier.state_dict(),\n",
    "            'num_classes': self.num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "class distilroBERTaMultiTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize RoBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize RoBERTa multi-task model\n",
    "        self.model = distilroBERTaMultiTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            sentiment_num_classes=roberta_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=roberta_model_config.emotion_num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_accuracy': [],\n",
    "            'train_emotion_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'val_sentiment_f1_macro': [],\n",
    "            'val_emotion_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \"\"\"Create data loaders for RoBERTa multi-task training\"\"\"\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_correct = 0\n",
    "        emotion_correct = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss with alpha weighting\n",
    "            loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "            emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "            \n",
    "            sentiment_correct += (sentiment_preds == sentiment_labels).sum().item()\n",
    "            emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
    "            total_predictions += sentiment_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        sentiment_accuracy = sentiment_correct / total_predictions\n",
    "        emotion_accuracy = emotion_correct / total_predictions\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_labels = []\n",
    "        emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_true = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_true = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Calculate combined loss\n",
    "                sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_true)\n",
    "                emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_true)\n",
    "                loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_labels.extend(sentiment_true.cpu().numpy())\n",
    "                emotion_labels.extend(emotion_true.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_labels, emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy, sentiment_f1_macro, emotion_f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"🚀 Starting distilroBERTa multi-task training...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_combined_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_sent_acc, train_emo_acc = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_sent_acc, val_emo_acc, val_sent_f1, val_emo_f1 = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_sentiment_accuracy'].append(train_sent_acc)\n",
    "            self.training_history['train_emotion_accuracy'].append(train_emo_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_sent_acc)\n",
    "            self.training_history['val_emotion_accuracy'].append(val_emo_acc)\n",
    "            self.training_history['val_sentiment_f1_macro'].append(val_sent_f1)\n",
    "            self.training_history['val_emotion_f1_macro'].append(val_emo_f1)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Train Sentiment Acc: {train_sent_acc:.4f}, Train Emotion Acc: {train_emo_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Sentiment Acc: {val_sent_acc:.4f}, F1: {val_sent_f1:.4f}\")\n",
    "            print(f\"  Val Emotion Acc: {val_emo_acc:.4f}, F1: {val_emo_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            combined_f1 = (val_sent_f1 + val_emo_f1) / 2\n",
    "            if combined_f1 > best_combined_f1:\n",
    "                best_combined_f1 = combined_f1\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best Combined F1: {best_combined_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'sentiment_classifier_state_dict': self.model.sentiment_classifier.state_dict(),\n",
    "            'emotion_classifier_state_dict': self.model.emotion_classifier.state_dict(),\n",
    "            'sentiment_num_classes': self.model.sentiment_num_classes,\n",
    "            'emotion_num_classes': self.model.emotion_num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "print(\"distilroBERTa Training classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8a89274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions for distilroBERTa\n",
    "def evaluate_distilroberta_model(model_path: str, model_type: str, test_data: Dict, model_name: str):\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if model_type == \"multitask\":\n",
    "        model = distilroBERTaMultiTaskTransformer.from_pretrained(model_path)\n",
    "    else:\n",
    "        num_classes = (roberta_model_config.sentiment_num_classes \n",
    "                      if model_type == \"sentiment\" \n",
    "                      else roberta_model_config.emotion_num_classes)\n",
    "        model = distilroBERTaSingleTaskTransformer.from_pretrained(model_path)\n",
    "    \n",
    "    # Make sure model is on the correct device\n",
    "    model = model.to(device)  # Add this line\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    if model_type == \"multitask\":\n",
    "        dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            sentiment_labels=test_data['sentiment_labels'],\n",
    "            emotion_labels=test_data['emotion_labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    else:\n",
    "        dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            labels=test_data['labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move everything to the same device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            if model_type == \"multitask\":\n",
    "                sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "                emotion_labels = batch['emotion_labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend([\n",
    "                    sentiment_preds.cpu().numpy(),\n",
    "                    emotion_preds.cpu().numpy()\n",
    "                ])\n",
    "                all_labels.extend([\n",
    "                    sentiment_labels.cpu().numpy(),\n",
    "                    emotion_labels.cpu().numpy()\n",
    "                ])\n",
    "            else:\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_accuracy = accuracy_score(all_labels[0], all_predictions[0])\n",
    "        sentiment_f1 = f1_score(all_labels[0], all_predictions[0], average='macro')\n",
    "        emotion_accuracy = accuracy_score(all_labels[1], all_predictions[1])\n",
    "        emotion_f1 = f1_score(all_labels[1], all_predictions[1], average='macro')\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1_macro': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1_macro': emotion_f1,\n",
    "            'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "            'combined_f1_macro': (sentiment_f1 + emotion_f1) / 2\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'accuracy': accuracy_score(all_labels, all_predictions),\n",
    "            'f1_macro': f1_score(all_labels, all_predictions, average='macro')\n",
    "        }\n",
    "    \n",
    "print(\"distilroBERTa evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eb93e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Ultra-Fast Hyperparameter Tuning Classes for distilroBERTa \n",
    "def create_tuning_subset(data_splits, subset_ratio=0.01):  # Even smaller: 1%\n",
    "    print(f\"🔪 Creating {subset_ratio*100:.0f}% subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:  # Minimum 20 samples\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'labels': [split_data['labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "def create_multitask_tuning_subset(data_splits, subset_ratio=0.01):\n",
    "    print(f\"🔪 Creating {subset_ratio*100:.0f}% multitask subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_multitask_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [split_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [split_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_multitask_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_multitask_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_multitask_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_multitask_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"Multitask tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "087a4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ distilroBERTa Hyperparameter Tuning classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Hyperparameter Tuning Classes for distilroBERTa\n",
    "class distilroBERTaHyperparameterTuner:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 15,\n",
    "        model_name: str = \"distilroberta-base\"\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.data_splits = data_splits\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(f\"🔍 distilroBERTa hyperparameter tuner initialized for {model_type}\")\n",
    "        print(f\"🚀 Using Random Search for optimization\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function for distilroBERTa\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters\n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 3, 6)\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.05, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.001, 0.1)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.4)\n",
    "        max_length = trial.suggest_categorical('max_length', [128, 256])\n",
    "        \n",
    "        # Multi-task specific parameter\n",
    "        alpha = trial.suggest_float('alpha', 0.3, 0.7) if self.model_type == \"multitask\" else 0.5\n",
    "        \n",
    "        # Create config\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./distilroberta_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Train model\n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = distilroBERTaMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return combined F1 score\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro'])\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro'])\n",
    "                combined_f1 = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "                print(f\"Trial {trial.number}: Combined F1 = {combined_f1:.4f}\")\n",
    "                return combined_f1\n",
    "                \n",
    "            else:\n",
    "                # Single task training\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = roberta_model_config.sentiment_num_classes\n",
    "                else:  # emotion\n",
    "                    num_classes = roberta_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = distilroBERTaSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return best F1 score\n",
    "                best_f1 = max(history['val_f1_macro'])\n",
    "                print(f\"Trial {trial.number}: F1 = {best_f1:.4f}\")\n",
    "                return best_f1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Clean up\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \"\"\"Run hyperparameter optimization\"\"\"\n",
    "        \n",
    "        # Create study with Random Search\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.RandomSampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🔍 Starting hyperparameter optimization for {self.model_type}...\")\n",
    "        print(f\"🎯 Random Search: {self.n_trials} trials\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n🏆 Optimization completed for {self.model_type}!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best F1 score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "print(\"✅ distilroBERTa Hyperparameter Tuning classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0e31cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra-fast trainers defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 8B: Ultra-Fast Trainers for Speed\n",
    "class UltraFastdistilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize tokenizer (reuse if possible)\n",
    "        if not hasattr(self, '_tokenizer_cache'):\n",
    "            UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache = AutoTokenizer.from_pretrained(config.model_name)\n",
    "            if UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token is None:\n",
    "                UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.eos_token\n",
    "        \n",
    "        self.tokenizer = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Speed-optimized data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # No multiprocessing for speed\n",
    "            pin_memory=False  # Disable pin_memory\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        # Simple optimizer setup\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "            labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Print progress for very small datasets\n",
    "            if batch_idx % max(1, len(self.train_loader) // 4) == 0:\n",
    "                print(f\"    Batch {batch_idx + 1}/{len(self.train_loader)}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader), correct_predictions / total_predictions\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "                labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"🚀 Starting ultra-fast distilroBERTa training ({self.config.task_type})...\")\n",
    "        \n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"  📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            print(f\"    Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "        \n",
    "        print(f\"✅ Training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "\n",
    "class UltraFastRoBERTaMultiTaskTrainer:\n",
    "    # Similar structure but for multitask...\n",
    "    pass  # Implement if needed\n",
    "\n",
    "print(\"Ultra-fast trainers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "187a96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING ROBERTA TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "1️⃣ Loading and processing datasets for distilroBERTa...\n",
      "📥 Loading external datasets for RoBERTa...\n",
      "✅ SST-2 dataset loaded: 67349 train samples\n",
      "✅ GoEmotions dataset loaded: 43410 train samples\n",
      "🔄 Processing sentiment data for RoBERTa...\n",
      "✅ RoBERTa Sentiment data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "🔄 Processing emotion data for RoBERTa...\n",
      "✅ RoBERTa Emotion data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "🔄 Creating multi-task dataset for RoBERTa...\n",
      "✅ RoBERTa Multi-task data created:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "Data loading completed!\n",
      "Sentiment data: 7000 train samples\n",
      "Emotion data: 7000 train samples\n",
      "Multitask data: 7000 train samples\n",
      "Model: distilroberta-base\n",
      "Hyperparameter trials per model: 15\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Data Loading and Initial Setup for distilroBERTa\n",
    "print(\"🚀 STARTING ROBERTA TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear memory before starting\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Load and process datasets for distilroBERTa\n",
    "print(\"\\n1️⃣ Loading and processing datasets for distilroBERTa...\")\n",
    "sentiment_data, emotion_data = load_and_process_datasets_roberta()\n",
    "multitask_data = create_multitask_data_roberta(sentiment_data, emotion_data)\n",
    "\n",
    "# Model configurations\n",
    "model_name = \"distilroberta-base\"\n",
    "n_trials = 15  # Number of hyperparameter tuning trials\n",
    "\n",
    "print(\"Data loading completed!\")\n",
    "print(f\"Sentiment data: {len(sentiment_data['train']['texts'])} train samples\")\n",
    "print(f\"Emotion data: {len(emotion_data['train']['texts'])} train samples\")\n",
    "print(f\"Multitask data: {len(multitask_data['train']['texts'])} train samples\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Hyperparameter trials per model: {n_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "069be7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: FAST INITIAL distilroberta TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "🔪 Creating 10% subset for fast initial training...\n",
      "🔪 Creating 10% subset for hyperparameter tuning...\n",
      "📊 Tuning subset created:\n",
      "  Train: 700 samples\n",
      "  Val: 150 samples\n",
      "\n",
      "2️⃣ Training Fast Initial DistilRoBERTa Sentiment Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9216, Train Acc: 0.5143\n",
      "  Val Loss: 0.9240, Val Acc: 0.3800, Val F1: 0.2024\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train initial sentiment model on subset\u001b[39;00m\n\u001b[32m     25\u001b[39m initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n\u001b[32m     26\u001b[39m     config=fast_initial_config_sentiment,\n\u001b[32m     27\u001b[39m     num_classes=roberta_model_config.sentiment_num_classes\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m initial_sentiment_history = \u001b[43minitial_sentiment_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_sentiment_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Evaluate on full test set\u001b[39;00m\n\u001b[32m     32\u001b[39m initial_sentiment_results = evaluate_distilroBERTa_model(\n\u001b[32m     33\u001b[39m     model_path=\u001b[33m\"\u001b[39m\u001b[33m./initial_distilroberta_sentiment_model/model_best\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     model_type=\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m     test_data=sentiment_data[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     36\u001b[39m     model_name=model_name\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mdistilroBERTaSingleTaskTrainer.train\u001b[39m\u001b[34m(self, data_splits)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val_f1_macro > best_f1:\n\u001b[32m    177\u001b[39m         best_f1 = val_f1_macro\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_best\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mdistilroBERTa training completed! Best F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training_history\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 190\u001b[39m, in \u001b[36mdistilroBERTaSingleTaskTrainer.save_model\u001b[39m\u001b[34m(self, is_best)\u001b[39m\n\u001b[32m    187\u001b[39m os.makedirs(model_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer.save_pretrained(model_dir)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Save custom components\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\transformers\\modeling_utils.py:2830\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   2825\u001b[39m     gc.collect()\n\u001b[32m   2827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   2828\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   2829\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2830\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2832\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "# Cell 10: Fast Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: FAST INITIAL distilroberta TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create smaller subset for initial training too\n",
    "print(\"🔪 Creating 10% subset for fast initial training...\")\n",
    "initial_sentiment_data = create_tuning_subset(sentiment_data, subset_ratio=0.1)\n",
    "\n",
    "# Faster configuration for initial training\n",
    "fast_initial_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=16,  # Larger batch size\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=2,  # Fewer epochs\n",
    "    max_length=64,  # Shorter sequences\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Training Fast Initial DistilRoBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model on subset\n",
    "initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=fast_initial_config_sentiment,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(initial_sentiment_data)\n",
    "\n",
    "# Evaluate on full test set\n",
    "initial_sentiment_results = evaluate_distilroBERTa_model(\n",
    "    model_path=\"./initial_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Initial Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "print(f\"  (Note: Trained on 10% subset for speed)\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b345dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: FAST INITIAL distilroberta TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "🔪 Creating 10% subset for fast initial training...\n",
      "🔪 Creating 10% subset for hyperparameter tuning...\n",
      "📊 Tuning subset created:\n",
      "  Train: 700 samples\n",
      "  Val: 150 samples\n",
      "\n",
      "2️⃣ Training Fast Initial DistilRoBERTa Sentiment Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9216, Train Acc: 0.5143\n",
      "  Val Loss: 0.9240, Val Acc: 0.3800, Val F1: 0.2024\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.7870, Train Acc: 0.7000\n",
      "  Val Loss: 0.7660, Val Acc: 0.7533, Val F1: 0.5197\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5197\n",
      "🔍 Evaluating distilroBERTa sentiment model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 32\u001b[39m\n",
      "\u001b[32m     29\u001b[39m initial_sentiment_history = initial_sentiment_trainer.train(initial_sentiment_data)\n",
      "\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Evaluate on full test set\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m initial_sentiment_results = \u001b[43mevaluate_distilroBERTa_model\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./initial_distilroberta_sentiment_model/model_best\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentiment_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\n",
      "\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Initial Sentiment Model Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_sentiment_results[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mevaluate_distilroBERTa_model\u001b[39m\u001b[34m(model_path, model_type, test_data, model_name)\u001b[39m\n",
      "\u001b[32m     65\u001b[39m input_ids = batch[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(device)\n",
      "\u001b[32m     66\u001b[39m attention_mask = batch[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type == \u001b[33m\"\u001b[39m\u001b[33mmultitask\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Multi-task predictions\u001b[39;00m\n",
      "\u001b[32m     72\u001b[39m     sentiment_preds = torch.argmax(outputs[\u001b[33m'\u001b[39m\u001b[33msentiment_logits\u001b[39m\u001b[33m'\u001b[39m], dim=-\u001b[32m1\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mdistilroBERTaSingleTaskTransformer.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n",
      "\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n",
      "\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Get RoBERTa outputs\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Use [CLS] token representation\u001b[39;00m\n",
      "\u001b[32m     31\u001b[39m     pooled_output = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# [CLS] token\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:912\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    909\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    910\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    921\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:122\u001b[39m, in \u001b[36mRobertaEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n",
      "\u001b[32m    119\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=\u001b[38;5;28mself\u001b[39m.position_ids.device)\n",
      "\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    123\u001b[39m token_type_embeddings = \u001b[38;5;28mself\u001b[39m.token_type_embeddings(token_type_ids)\n",
      "\u001b[32m    125\u001b[39m embeddings = inputs_embeds + token_type_embeddings\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n",
      "\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n",
      "\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n",
      "\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n",
      "\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n",
      "\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n",
      "\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n",
      "\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# Cell 10: Fast Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: FAST INITIAL distilroberta TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create smaller subset for initial training too\n",
    "print(\"🔪 Creating 10% subset for fast initial training...\")\n",
    "initial_sentiment_data = create_tuning_subset(sentiment_data, subset_ratio=0.1)\n",
    "\n",
    "# Faster configuration for initial training\n",
    "fast_initial_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=16,  # Larger batch size\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=2,  # Fewer epochs\n",
    "    max_length=64,  # Shorter sequences\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Training Fast Initial DistilRoBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model on subset\n",
    "initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=fast_initial_config_sentiment,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(initial_sentiment_data)\n",
    "\n",
    "# Evaluate on full test set\n",
    "initial_sentiment_results = evaluate_distilroBERTa_model(\n",
    "    model_path=\"./initial_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Initial Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "print(f\"  (Note: Trained on 10% subset for speed)\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: FAST INITIAL distilroberta TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "🔪 Creating 10% subset for fast initial training...\n",
      "🔪 Creating 10% subset for hyperparameter tuning...\n",
      "📊 Tuning subset created:\n",
      "  Train: 700 samples\n",
      "  Val: 150 samples\n",
      "\n",
      "2️⃣ Training Fast Initial DistilRoBERTa Sentiment Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9216, Train Acc: 0.5143\n",
      "  Val Loss: 0.9240, Val Acc: 0.3800, Val F1: 0.2024\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.7870, Train Acc: 0.7000\n",
      "  Val Loss: 0.7660, Val Acc: 0.7533, Val F1: 0.5197\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5197\n",
      "🔍 Evaluating distilroBERTa sentiment model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 32\u001b[39m\n",
      "\u001b[32m     29\u001b[39m initial_sentiment_history = initial_sentiment_trainer.train(initial_sentiment_data)\n",
      "\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Evaluate on full test set\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m initial_sentiment_results = \u001b[43mevaluate_distilroBERTa_model\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./initial_distilroberta_sentiment_model/model_best\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43msentiment_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\n",
      "\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Initial Sentiment Model Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_sentiment_results[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mevaluate_distilroBERTa_model\u001b[39m\u001b[34m(model_path, model_type, test_data, model_name)\u001b[39m\n",
      "\u001b[32m     65\u001b[39m input_ids = batch[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(device)\n",
      "\u001b[32m     66\u001b[39m attention_mask = batch[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type == \u001b[33m\"\u001b[39m\u001b[33mmultitask\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Multi-task predictions\u001b[39;00m\n",
      "\u001b[32m     72\u001b[39m     sentiment_preds = torch.argmax(outputs[\u001b[33m'\u001b[39m\u001b[33msentiment_logits\u001b[39m\u001b[33m'\u001b[39m], dim=-\u001b[32m1\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mdistilroBERTaSingleTaskTransformer.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n",
      "\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n",
      "\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Get RoBERTa outputs\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Use [CLS] token representation\u001b[39;00m\n",
      "\u001b[32m     31\u001b[39m     pooled_output = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# [CLS] token\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:912\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n",
      "\u001b[32m    909\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    910\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    913\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    921\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:122\u001b[39m, in \u001b[36mRobertaEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n",
      "\u001b[32m    119\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=\u001b[38;5;28mself\u001b[39m.position_ids.device)\n",
      "\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    123\u001b[39m token_type_embeddings = \u001b[38;5;28mself\u001b[39m.token_type_embeddings(token_type_ids)\n",
      "\u001b[32m    125\u001b[39m embeddings = inputs_embeds + token_type_embeddings\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n",
      "\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n",
      "\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n",
      "\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n",
      "\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n",
      "\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n",
      "\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n",
      "\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# Cell 10: Fast Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: FAST INITIAL distilroberta TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create smaller subset for initial training too\n",
    "print(\"🔪 Creating 10% subset for fast initial training...\")\n",
    "initial_sentiment_data = create_tuning_subset(sentiment_data, subset_ratio=0.1)\n",
    "\n",
    "# Faster configuration for initial training\n",
    "fast_initial_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=16,  # Larger batch size\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=2,  # Fewer epochs\n",
    "    max_length=64,  # Shorter sequences\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Training Fast Initial DistilRoBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model on subset\n",
    "initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=fast_initial_config_sentiment,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(initial_sentiment_data)\n",
    "\n",
    "# Evaluate on full test set\n",
    "initial_sentiment_results = evaluate_distilroBERTa_model(\n",
    "    model_path=\"./initial_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Initial Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "print(f\"  (Note: Trained on 10% subset for speed)\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug Cell: Check what model is actually being used\n",
    "print(\"🔍 DEBUGGING MODEL LOADING...\")\n",
    "\n",
    "# Check the current model_name variable\n",
    "print(f\"Current model_name variable: {model_name}\")\n",
    "\n",
    "# Test loading the model directly\n",
    "import time\n",
    "print(f\"\\n⏱️ Testing direct model loading...\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "load_time_tokenizer = time.time() - start_time\n",
    "print(f\"Tokenizer loaded in: {load_time_tokenizer:.1f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_model = AutoModel.from_pretrained(model_name)\n",
    "load_time_model = time.time() - start_time\n",
    "print(f\"Model loaded in: {load_time_model:.1f} seconds\")\n",
    "\n",
    "# Check model size\n",
    "total_params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"Model parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
    "\n",
    "# Check model type\n",
    "print(f\"Model type: {type(test_model)}\")\n",
    "print(f\"Model config: {test_model.config.model_type}\")\n",
    "\n",
    "# Expected sizes:\n",
    "# distilroberta-base: ~82M parameters  \n",
    "# bert-tiny: ~4M parameters\n",
    "\n",
    "if total_params > 100e6:\n",
    "    print(\"⚠️ WARNING: This is still a large model (>100M params)\")\n",
    "elif total_params > 50e6:\n",
    "    print(\"✅ Medium-sized model (50-100M params)\")\n",
    "else:\n",
    "    print(\"✅ Small model (<50M params)\")\n",
    "\n",
    "# Clean up\n",
    "del test_tokenizer, test_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for sentiment\n",
    "default_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Training Initial distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model\n",
    "initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_sentiment,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate initial sentiment model\n",
    "initial_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_sentiment'] = initial_sentiment_results\n",
    "\n",
    "print(f\"\\n✅ Initial Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Initial Emotion Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for emotion\n",
    "default_config_emotion = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./initial_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n3️⃣ Training Initial distilroBERTa Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial emotion model\n",
    "initial_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_emotion,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate initial emotion model\n",
    "initial_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_emotion'] = initial_emotion_results\n",
    "\n",
    "print(f\"\\n✅ Initial Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {initial_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Initial Multitask Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for multitask\n",
    "default_config_multitask = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    alpha=0.5,\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./initial_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n4️⃣ Training Initial distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial multitask model\n",
    "initial_multitask_trainer = distilroBERTaMultiTaskTrainer(config=default_config_multitask)\n",
    "initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate initial multitask model\n",
    "initial_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_multitask'] = initial_multitask_results\n",
    "\n",
    "print(f\"\\n✅ Initial Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {initial_multitask_results['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {initial_multitask_results['emotion_accuracy']:.4f}, F1: {initial_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {initial_multitask_results['combined_accuracy']:.4f}, F1: {initial_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Initial Results Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 INITIAL ROBERTA RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 INITIAL ROBERTA MODEL PERFORMANCE:\")\n",
    "print(f\"  Sentiment Model:\")\n",
    "print(f\"    Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Emotion Model:\")\n",
    "print(f\"    Accuracy: {initial_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Multitask Model:\")\n",
    "print(f\"    Sentiment - Accuracy: {initial_multitask_results['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {initial_multitask_results['emotion_accuracy']:.4f}, F1: {initial_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {initial_multitask_results['combined_accuracy']:.4f}, F1: {initial_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Store results for later comparison\n",
    "all_results = {\n",
    "    'initial_sentiment': initial_sentiment_results,\n",
    "    'initial_emotion': initial_emotion_results,\n",
    "    'initial_multitask': initial_multitask_results\n",
    "}\n",
    "\n",
    "print(f\"\\n💡 These are RoBERTa baseline results. Now proceeding to hyperparameter tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9aa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Hyperparameter Tuning - Sentiment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n6️⃣ Hyperparameter Tuning for distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for sentiment\n",
    "sentiment_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"sentiment\",\n",
    "    data_splits=sentiment_data,\n",
    "    n_trials=15,\n",
    "    model_name=model_name\n",
    ")\n",
    "sentiment_study = sentiment_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e334a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Hyperparameter Tuning - Emotion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - EMOTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n7️⃣ Hyperparameter Tuning for distilroBERTa Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for emotion\n",
    "emotion_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"emotion\",\n",
    "    data_splits=emotion_data,\n",
    "    n_trials=15,\n",
    "    model_name=model_name\n",
    ")\n",
    "emotion_study = emotion_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c743ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Hyperparameter Tuning - Multitask\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - MULTITASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n8️⃣ Hyperparameter Tuning for distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for multitask\n",
    "multitask_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"multitask\",\n",
    "    data_splits=multitask_data,\n",
    "    n_trials=15,\n",
    "    model_name=model_name\n",
    ")\n",
    "multitask_study = multitask_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0516f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Final Training - Sentiment with Best Parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n9️⃣ Training Final distilroBERTa Sentiment Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from sentiment tuning\n",
    "best_sentiment_params = sentiment_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_sentiment_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_sentiment_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_sentiment_params['learning_rate'],\n",
    "    batch_size=best_sentiment_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_sentiment_params['warmup_ratio'],\n",
    "    weight_decay=best_sentiment_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_sentiment_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_sentiment_params['classifier_dropout'],\n",
    "    max_length=best_sentiment_params.get('max_length', 128),\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./final_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "# Train final sentiment model\n",
    "final_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_sentiment_config,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "final_sentiment_history = final_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate final sentiment model\n",
    "final_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_sentiment'] = final_sentiment_results\n",
    "\n",
    "print(f\"\\n✅ Final Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {final_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Final Emotion Model Training with Best Parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🔟 Training Final distilroBERTa Emotion Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from emotion tuning\n",
    "best_emotion_params = emotion_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_emotion_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_emotion_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_emotion_params['learning_rate'],\n",
    "    batch_size=best_emotion_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_emotion_params['warmup_ratio'],\n",
    "    weight_decay=best_emotion_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_emotion_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_emotion_params['classifier_dropout'],\n",
    "    max_length=best_emotion_params.get('max_length', 128),\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./final_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final emotion model:\")\n",
    "print(f\"  Dataset: Full emotion data ({len(emotion_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_emotion_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_emotion_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_emotion_config.learning_rate:.2e}\")\n",
    "\n",
    "# Train final emotion model\n",
    "final_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_emotion_config,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "final_emotion_history = final_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate final emotion model\n",
    "final_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_emotion'] = final_emotion_results\n",
    "\n",
    "print(f\"\\n✅ Final Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {final_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Final Multitask Model Training with Best Parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1️⃣1️⃣ Training Final distilroBERTa Multi-task Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from multitask tuning\n",
    "best_multitask_params = multitask_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_multitask_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_multitask_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_multitask_params['learning_rate'],\n",
    "    batch_size=best_multitask_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_multitask_params['warmup_ratio'],\n",
    "    weight_decay=best_multitask_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_multitask_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_multitask_params['classifier_dropout'],\n",
    "    max_length=best_multitask_params.get('max_length', 128),\n",
    "    alpha=best_multitask_params['alpha'],  # Multitask-specific parameter\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./final_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final multitask model:\")\n",
    "print(f\"  Dataset: Full multitask data ({len(multitask_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_multitask_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_multitask_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_multitask_config.learning_rate:.2e}\")\n",
    "print(f\"  Alpha (loss weighting): {final_multitask_config.alpha:.3f}\")\n",
    "\n",
    "# Train final multitask model\n",
    "final_multitask_trainer = distilroBERTaMultiTaskTrainer(config=final_multitask_config)\n",
    "final_multitask_history = final_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate final multitask model\n",
    "final_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_multitask'] = final_multitask_results\n",
    "\n",
    "print(f\"\\n✅ Final Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {final_multitask_results['sentiment_accuracy']:.4f}, F1: {final_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {final_multitask_results['emotion_accuracy']:.4f}, F1: {final_emotion_results['f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {final_multitask_results['combined_accuracy']:.4f}, F1: {final_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a250d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Final Results Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMODEL PERFORMANCE COMPARISON:\")\n",
    "\n",
    "print(f\"\\n1️⃣ Sentiment Task:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    Accuracy: {all_results['initial_sentiment']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['initial_sentiment']['f1_macro']:.4f}\")\n",
    "print(f\"  Optimized Model:\")\n",
    "print(f\"    Accuracy: {all_results['final_sentiment']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['final_sentiment']['f1_macro']:.4f}\")\n",
    "print(f\"  Improvement:\")\n",
    "print(f\"    Accuracy: {(all_results['final_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy'])*100:.2f}%\")\n",
    "print(f\"    F1 Macro: {(all_results['final_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n2️⃣ Emotion Task:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    Accuracy: {all_results['initial_emotion']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['initial_emotion']['f1_macro']:.4f}\")\n",
    "print(f\"  Optimized Model:\")\n",
    "print(f\"    Accuracy: {all_results['final_emotion']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['final_emotion']['f1_macro']:.4f}\")\n",
    "print(f\"  Improvement:\")\n",
    "print(f\"    Accuracy: {(all_results['final_emotion']['accuracy'] - all_results['initial_emotion']['accuracy'])*100:.2f}%\")\n",
    "print(f\"    F1 Macro: {(all_results['final_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n3️⃣ Multi-task Model:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    Sentiment - Accuracy: {all_results['initial_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {all_results['initial_multitask']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Optimized Model:\")\n",
    "print(f\"    Sentiment - Accuracy: {all_results['final_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['final_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {all_results['final_multitask']['emotion_accuracy']:.4f}, F1: {all_results['final_multitask']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Improvement:\")\n",
    "print(f\"    Sentiment - Accuracy: {(all_results['final_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy'])*100:.2f}%\")\n",
    "print(f\"    Emotion - Accuracy: {(all_results['final_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy'])*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining pipeline completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
