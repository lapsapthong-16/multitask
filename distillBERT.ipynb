{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7bf2db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "GPU Memory: 8.0 GB\n",
      "All imports completed and GPU configured\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports for RoBERTa Pipeline\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"All imports completed and GPU configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06713064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2972"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before training, add cleanup code\n",
    "import shutil\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Clean up output directory if it exists\n",
    "output_dir = \"./initial_distilroberta_sentiment_model\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# Ensure CUDA cache is cleared\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0bb27104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Classes for distilroBERTa\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    model_name: str = \"distilroberta-base\"\n",
    "    max_length: int = 128\n",
    "    batch_size: int = 8  # Standardize to match BERTweet\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 3\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_dropout_prob: float = 0.1\n",
    "    classifier_dropout: float = 0.1\n",
    "    output_dir: str = \"./distilroberta_model_output\"\n",
    "    alpha: float = 0.5  # For multitask loss weighting\n",
    "    task_type: str = \"multitask\"  # \"sentiment\", \"emotion\", or \"multitask\"\n",
    "\n",
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c20511cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration classes defined\n"
     ]
    }
   ],
   "source": [
    "class distilroBERTaModelConfig:\n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "roberta_model_config = distilroBERTaModelConfig()\n",
    "print(\"Configuration classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "441fbea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Classes for distilroBERTa\n",
    "class distilroBERTaSingleTaskDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "class distilroBERTaMultiTaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # RoBERTa tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2ca8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Architectures\n",
    "class distilroBERTaSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load RoBERTa model\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class distilroBERTaMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Shared RoBERTa encoder\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.sentiment_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            sentiment_num_classes\n",
    "        )\n",
    "        \n",
    "        # Emotion classification head\n",
    "        self.emotion_classifier = nn.Linear(\n",
    "            self.roberta.config.hidden_size, \n",
    "            emotion_num_classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get shared RoBERTa representations\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "        emotion_logits = self.emotion_classifier(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "\n",
    "print(\"distilroBERTa Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8f19424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RoBERTa data processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data Loading and Processing Functions for RoBERTa\n",
    "def aggressive_memory_cleanup():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def load_and_process_datasets_roberta():\n",
    "    \"\"\"Load and process datasets for RoBERTa training\"\"\"\n",
    "    print(\"📥 Loading external datasets for RoBERTa...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sst2_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotions_dataset['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Process sentiment data\n",
    "    sentiment_data = process_sentiment_data_roberta(sst2_dataset)\n",
    "    \n",
    "    # Process emotion data  \n",
    "    emotion_data = process_emotion_data_roberta(emotions_dataset)\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def process_sentiment_data_roberta(sst2_dataset, max_samples=10000):\n",
    "    \"\"\"Process SST-2 dataset for RoBERTa sentiment classification\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing sentiment data for RoBERTa...\")\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = sst2_dataset['train']['sentence'][:max_samples]\n",
    "    train_labels = sst2_dataset['train']['label'][:max_samples]\n",
    "    \n",
    "    # Map SST-2 labels to 3 classes: 0->Negative, 1->Positive\n",
    "    # Add some neutral examples by random assignment\n",
    "    expanded_labels = []\n",
    "    expanded_texts = []\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        if label == 0:  # Negative\n",
    "            expanded_labels.append(0)\n",
    "            expanded_texts.append(text)\n",
    "        elif label == 1:  # Positive\n",
    "            # Sometimes assign as positive, sometimes as neutral\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                expanded_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                expanded_labels.append(2)  # Positive\n",
    "            expanded_texts.append(text)\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in expanded_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(expanded_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            expanded_labels[idx] = 1\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        expanded_texts, expanded_labels, test_size=0.3, random_state=42, stratify=expanded_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    sentiment_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ RoBERTa Sentiment data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return sentiment_data\n",
    "\n",
    "def process_emotion_data_roberta(emotion_dataset, max_samples=10000):\n",
    "    \"\"\"Process GoEmotion dataset for RoBERTa emotion classification\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing emotion data for RoBERTa...\")\n",
    "    \n",
    "    # Filter to first 6 emotions only\n",
    "    def filter_emotions(example):\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    filtered_train = emotion_dataset['train'].filter(filter_emotions)\n",
    "    filtered_val = emotion_dataset['validation'].filter(filter_emotions)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = filtered_train['text'][:max_samples]\n",
    "    train_labels_raw = filtered_train['labels'][:max_samples]\n",
    "    \n",
    "    # Handle multi-label to single-label conversion\n",
    "    train_labels = []\n",
    "    for label in train_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            train_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.3, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    emotion_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels}\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ RoBERTa Emotion data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return emotion_data\n",
    "\n",
    "def create_multitask_data_roberta(sentiment_data, emotion_data):\n",
    "    \"\"\"Create combined dataset for multi-task learning with RoBERTa\"\"\"\n",
    "    \n",
    "    print(\"🔄 Creating multi-task dataset for RoBERTa...\")\n",
    "    \n",
    "    # Take minimum length to balance datasets\n",
    "    min_train_len = min(len(sentiment_data['train']['texts']), len(emotion_data['train']['texts']))\n",
    "    min_val_len = min(len(sentiment_data['val']['texts']), len(emotion_data['val']['texts']))\n",
    "    min_test_len = min(len(sentiment_data['test']['texts']), len(emotion_data['test']['texts']))\n",
    "    \n",
    "    multitask_data = {\n",
    "        'train': {\n",
    "            'texts': sentiment_data['train']['texts'][:min_train_len],\n",
    "            'sentiment_labels': sentiment_data['train']['labels'][:min_train_len],\n",
    "            'emotion_labels': emotion_data['train']['labels'][:min_train_len]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': sentiment_data['val']['texts'][:min_val_len],\n",
    "            'sentiment_labels': sentiment_data['val']['labels'][:min_val_len],\n",
    "            'emotion_labels': emotion_data['val']['labels'][:min_val_len]\n",
    "        },\n",
    "        'test': {\n",
    "            'texts': sentiment_data['test']['texts'][:min_test_len],\n",
    "            'sentiment_labels': sentiment_data['test']['labels'][:min_test_len],\n",
    "            'emotion_labels': emotion_data['test']['labels'][:min_test_len]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ RoBERTa Multi-task data created:\")\n",
    "    print(f\"  Train: {len(multitask_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(multitask_data['val']['texts'])} samples\")\n",
    "    print(f\"  Test: {len(multitask_data['test']['texts'])} samples\")\n",
    "    \n",
    "    return multitask_data\n",
    "\n",
    "print(\"✅ RoBERTa data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5d1443e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa Training classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: distilroBERTa Training Classes\n",
    "class distilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize distilroBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize distilroBERTa model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting distilroBERTa single-task training ({self.config.task_type})...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'classifier_state_dict': self.model.classifier.state_dict(),\n",
    "            'num_classes': self.num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "class distilroBERTaMultiTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize RoBERTa tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize RoBERTa multi-task model\n",
    "        self.model = distilroBERTaMultiTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            sentiment_num_classes=roberta_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=roberta_model_config.emotion_num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_accuracy': [],\n",
    "            'train_emotion_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'val_sentiment_f1_macro': [],\n",
    "            'val_emotion_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \"\"\"Create data loaders for RoBERTa multi-task training\"\"\"\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        \n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.config.warmup_ratio),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_correct = 0\n",
    "        emotion_correct = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss with alpha weighting\n",
    "            loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "            emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "            \n",
    "            sentiment_correct += (sentiment_preds == sentiment_labels).sum().item()\n",
    "            emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
    "            total_predictions += sentiment_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        sentiment_accuracy = sentiment_correct / total_predictions\n",
    "        emotion_accuracy = emotion_correct / total_predictions\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_labels = []\n",
    "        emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_true = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_true = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                # Calculate combined loss\n",
    "                sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_true)\n",
    "                emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_true)\n",
    "                loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_labels.extend(sentiment_true.cpu().numpy())\n",
    "                emotion_labels.extend(emotion_true.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_labels, emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy, sentiment_f1_macro, emotion_f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"🚀 Starting distilroBERTa multi-task training...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_combined_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_sent_acc, train_emo_acc = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_sent_acc, val_emo_acc, val_sent_f1, val_emo_f1 = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_sentiment_accuracy'].append(train_sent_acc)\n",
    "            self.training_history['train_emotion_accuracy'].append(train_emo_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_sent_acc)\n",
    "            self.training_history['val_emotion_accuracy'].append(val_emo_acc)\n",
    "            self.training_history['val_sentiment_f1_macro'].append(val_sent_f1)\n",
    "            self.training_history['val_emotion_f1_macro'].append(val_emo_f1)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Train Sentiment Acc: {train_sent_acc:.4f}, Train Emotion Acc: {train_emo_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Sentiment Acc: {val_sent_acc:.4f}, F1: {val_sent_f1:.4f}\")\n",
    "            print(f\"  Val Emotion Acc: {val_emo_acc:.4f}, F1: {val_emo_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            combined_f1 = (val_sent_f1 + val_emo_f1) / 2\n",
    "            if combined_f1 > best_combined_f1:\n",
    "                best_combined_f1 = combined_f1\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\ndistilroBERTa training completed! Best Combined F1: {best_combined_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.roberta.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        # Save custom components\n",
    "        torch.save({\n",
    "            'sentiment_classifier_state_dict': self.model.sentiment_classifier.state_dict(),\n",
    "            'emotion_classifier_state_dict': self.model.emotion_classifier.state_dict(),\n",
    "            'sentiment_num_classes': self.model.sentiment_num_classes,\n",
    "            'emotion_num_classes': self.model.emotion_num_classes,\n",
    "            'config': self.config\n",
    "        }, os.path.join(model_dir, 'custom_components.pt'))\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best distilroBERTa model saved to {model_dir}\")\n",
    "\n",
    "print(\"distilroBERTa Training classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8a89274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroBERTa evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions for distilroBERTa\n",
    "def evaluate_distilroberta_model(model_path: str, model_type: str, test_data: Dict, model_name: str):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Initialize the appropriate model architecture\n",
    "    if model_type == \"multitask\":\n",
    "        model = distilroBERTaMultiTaskTransformer(\n",
    "            model_name=model_name,\n",
    "            sentiment_num_classes=roberta_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=roberta_model_config.emotion_num_classes\n",
    "        )\n",
    "    else:\n",
    "        num_classes = (roberta_model_config.sentiment_num_classes \n",
    "                      if model_type == \"sentiment\" \n",
    "                      else roberta_model_config.emotion_num_classes)\n",
    "        model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=model_name,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    # Load the saved state dict\n",
    "    custom_components = torch.load(os.path.join(model_path, 'custom_components.pt'))\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        model.sentiment_classifier.load_state_dict(custom_components['sentiment_classifier_state_dict'])\n",
    "        model.emotion_classifier.load_state_dict(custom_components['emotion_classifier_state_dict'])\n",
    "    else:\n",
    "        model.classifier.load_state_dict(custom_components['classifier_state_dict'])\n",
    "    \n",
    "    # Load the base model weights\n",
    "    base_model = AutoModel.from_pretrained(model_path)\n",
    "    model.roberta = base_model\n",
    "    \n",
    "    # Make sure model is on the correct device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Rest of the evaluation code remains the same...\n",
    "    # Create dataset and dataloader\n",
    "    if model_type == \"multitask\":\n",
    "        dataset = distilroBERTaMultiTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            sentiment_labels=test_data['sentiment_labels'],\n",
    "            emotion_labels=test_data['emotion_labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    else:\n",
    "        dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            labels=test_data['labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move everything to the same device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            if model_type == \"multitask\":\n",
    "                sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "                emotion_labels = batch['emotion_labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend([\n",
    "                    sentiment_preds.cpu().numpy(),\n",
    "                    emotion_preds.cpu().numpy()\n",
    "                ])\n",
    "                all_labels.extend([\n",
    "                    sentiment_labels.cpu().numpy(),\n",
    "                    emotion_labels.cpu().numpy()\n",
    "                ])\n",
    "            else:\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                # Move predictions back to CPU for sklearn metrics\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_accuracy = accuracy_score(all_labels[0], all_predictions[0])\n",
    "        sentiment_f1 = f1_score(all_labels[0], all_predictions[0], average='macro')\n",
    "        emotion_accuracy = accuracy_score(all_labels[1], all_predictions[1])\n",
    "        emotion_f1 = f1_score(all_labels[1], all_predictions[1], average='macro')\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1_macro': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1_macro': emotion_f1,\n",
    "            'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "            'combined_f1_macro': (sentiment_f1 + emotion_f1) / 2\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'accuracy': accuracy_score(all_labels, all_predictions),\n",
    "            'f1_macro': f1_score(all_labels, all_predictions, average='macro')\n",
    "        }\n",
    "    \n",
    "print(\"distilroBERTa evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1eb93e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Ultra-Fast Hyperparameter Tuning Classes for distilroBERTa \n",
    "def create_tuning_subset(data_splits, subset_ratio=0.01):  # Even smaller: 1%\n",
    "    print(f\"🔪 Creating {subset_ratio*100:.0f}% subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:  # Minimum 20 samples\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'labels': [split_data['labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "def create_multitask_tuning_subset(data_splits, subset_ratio=0.01):\n",
    "    print(f\"🔪 Creating {subset_ratio*100:.0f}% multitask subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_multitask_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 20:\n",
    "            n_samples = min(20, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [split_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [split_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_multitask_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_multitask_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_multitask_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_multitask_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"Multitask tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "087a4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ distilroBERTa Hyperparameter Tuning classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Hyperparameter Tuning Classes for distilroBERTa\n",
    "class distilroBERTaHyperparameterTuner:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 15,\n",
    "        model_name: str = \"distilroberta-base\"\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.data_splits = data_splits\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(f\"🔍 distilroBERTa hyperparameter tuner initialized for {model_type}\")\n",
    "        print(f\"🚀 Using Random Search for optimization\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function for distilroBERTa\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters\n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 3, 6)\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.05, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.001, 0.1)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.4)\n",
    "        max_length = trial.suggest_categorical('max_length', [128, 256])\n",
    "        \n",
    "        # Multi-task specific parameter\n",
    "        alpha = trial.suggest_float('alpha', 0.3, 0.7) if self.model_type == \"multitask\" else 0.5\n",
    "        \n",
    "        # Create config\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./distilroberta_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Train model\n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = distilroBERTaMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return combined F1 score\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro'])\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro'])\n",
    "                combined_f1 = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "                print(f\"Trial {trial.number}: Combined F1 = {combined_f1:.4f}\")\n",
    "                return combined_f1\n",
    "                \n",
    "            else:\n",
    "                # Single task training\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = roberta_model_config.sentiment_num_classes\n",
    "                else:  # emotion\n",
    "                    num_classes = roberta_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = distilroBERTaSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return best F1 score\n",
    "                best_f1 = max(history['val_f1_macro'])\n",
    "                print(f\"Trial {trial.number}: F1 = {best_f1:.4f}\")\n",
    "                return best_f1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Clean up\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \"\"\"Run hyperparameter optimization\"\"\"\n",
    "        \n",
    "        # Create study with Random Search\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.RandomSampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🔍 Starting hyperparameter optimization for {self.model_type}...\")\n",
    "        print(f\"🎯 Random Search: {self.n_trials} trials\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n🏆 Optimization completed for {self.model_type}!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best F1 score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "print(\"✅ distilroBERTa Hyperparameter Tuning classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0e31cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra-fast trainers defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 8B: Ultra-Fast Trainers for Speed\n",
    "class UltraFastdistilroBERTaSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize tokenizer (reuse if possible)\n",
    "        if not hasattr(self, '_tokenizer_cache'):\n",
    "            UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache = AutoTokenizer.from_pretrained(config.model_name)\n",
    "            if UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token is None:\n",
    "                UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.pad_token = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache.eos_token\n",
    "        \n",
    "        self.tokenizer = UltraFastdistilroBERTaSingleTaskTrainer._tokenizer_cache\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = distilroBERTaSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.training_history = {\n",
    "            'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        train_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = distilroBERTaSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Speed-optimized data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # No multiprocessing for speed\n",
    "            pin_memory=False  # Disable pin_memory\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        # Simple optimizer setup\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(self.train_loader):\n",
    "            input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "            labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Print progress for very small datasets\n",
    "            if batch_idx % max(1, len(self.train_loader) // 4) == 0:\n",
    "                print(f\"    Batch {batch_idx + 1}/{len(self.train_loader)}\")\n",
    "        \n",
    "        return total_loss / len(self.train_loader), correct_predictions / total_predictions\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)\n",
    "                labels = batch['labels'].to(self.device, non_blocking=True)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"🚀 Starting ultra-fast distilroBERTa training ({self.config.task_type})...\")\n",
    "        \n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"  📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            print(f\"    Loss: {train_loss:.4f}, Acc: {train_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "        \n",
    "        print(f\"✅ Training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "\n",
    "class UltraFastRoBERTaMultiTaskTrainer:\n",
    "    # Similar structure but for multitask...\n",
    "    pass  # Implement if needed\n",
    "\n",
    "print(\"Ultra-fast trainers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "187a96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING distilroBERTa TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "1️⃣ Loading and processing datasets for distilroBERTa...\n",
      "📥 Loading external datasets for RoBERTa...\n",
      "✅ SST-2 dataset loaded: 67349 train samples\n",
      "✅ GoEmotions dataset loaded: 43410 train samples\n",
      "🔄 Processing sentiment data for RoBERTa...\n",
      "✅ RoBERTa Sentiment data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "🔄 Processing emotion data for RoBERTa...\n",
      "✅ RoBERTa Emotion data processed:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "🔄 Creating multi-task dataset for RoBERTa...\n",
      "✅ RoBERTa Multi-task data created:\n",
      "  Train: 7000 samples\n",
      "  Val: 1500 samples\n",
      "  Test: 1500 samples\n",
      "Data loading completed!\n",
      "Sentiment data: 7000 train samples\n",
      "Emotion data: 7000 train samples\n",
      "Multitask data: 7000 train samples\n",
      "Model: distilroberta-base\n",
      "Hyperparameter trials per model: 15\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Data Loading and Initial Setup for distilroBERTa\n",
    "print(\"🚀 STARTING distilroBERTa TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear memory before starting\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Load and process datasets for distilroBERTa\n",
    "print(\"\\n1️⃣ Loading and processing datasets for distilroBERTa...\")\n",
    "sentiment_data, emotion_data = load_and_process_datasets_roberta()\n",
    "multitask_data = create_multitask_data_roberta(sentiment_data, emotion_data)\n",
    "\n",
    "# Model configurations\n",
    "model_name = \"distilroberta-base\"\n",
    "n_trials = 15  # Number of hyperparameter tuning trials\n",
    "\n",
    "print(\"Data loading completed!\")\n",
    "print(f\"Sentiment data: {len(sentiment_data['train']['texts'])} train samples\")\n",
    "print(f\"Emotion data: {len(emotion_data['train']['texts'])} train samples\")\n",
    "print(f\"Multitask data: {len(multitask_data['train']['texts'])} train samples\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Hyperparameter trials per model: {n_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7122193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "2️⃣ Training Initial distilroBERTa Sentiment Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 0.6889, Train Acc: 0.7306\n",
      "  Val Loss: 0.5309, Val Acc: 0.8127, Val F1: 0.5669\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.4799, Train Acc: 0.8426\n",
      "  Val Loss: 0.5704, Val Acc: 0.8273, Val F1: 0.5772\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.4000, Train Acc: 0.8766\n",
      "  Val Loss: 0.6107, Val Acc: 0.8327, Val F1: 0.5811\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_sentiment_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5811\n",
      "\n",
      "✅ Initial Sentiment Model Results:\n",
      "  Accuracy: 0.8333\n",
      "  F1 Macro: 0.5812\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize results dictionary\n",
    "all_results = {}\n",
    "\n",
    "# Default configuration for sentiment\n",
    "default_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Training Initial distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model\n",
    "initial_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_sentiment,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate initial sentiment model\n",
    "initial_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_results['initial_sentiment'] = initial_sentiment_results\n",
    "\n",
    "print(f\"\\n✅ Initial Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cb1e20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "3️⃣ Training Initial distilroBERTa Emotion Model...\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 1.0058, Train Acc: 0.6143\n",
      "  Val Loss: 0.7677, Val Acc: 0.7180, Val F1: 0.6652\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.6032, Train Acc: 0.7853\n",
      "  Val Loss: 0.7826, Val Acc: 0.7413, Val F1: 0.7043\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.4370, Train Acc: 0.8466\n",
      "  Val Loss: 0.8387, Val Acc: 0.7493, Val F1: 0.7207\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_emotion_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.7207\n",
      "\n",
      "✅ Initial Emotion Model Results:\n",
      "  Accuracy: 0.7667\n",
      "  F1 Macro: 0.7329\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Initial Emotion Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for emotion\n",
    "default_config_emotion = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./initial_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n3️⃣ Training Initial distilroBERTa Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial emotion model\n",
    "initial_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=default_config_emotion,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate initial emotion model\n",
    "initial_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_emotion'] = initial_emotion_results\n",
    "\n",
    "print(f\"\\n✅ Initial Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {initial_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5466718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "4️⃣ Training Initial distilroBERTa Multi-task Model...\n",
      "============================================================\n",
      "🚀 Starting distilroBERTa multi-task training...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 1.1999\n",
      "  Train Sentiment Acc: 0.7303, Train Emotion Acc: 0.2776\n",
      "  Val Loss: 1.1355\n",
      "  Val Sentiment Acc: 0.7907, F1: 0.5520\n",
      "  Val Emotion Acc: 0.2807, F1: 0.1070\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_multitask_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 1.0823\n",
      "  Train Sentiment Acc: 0.8460, Train Emotion Acc: 0.2900\n",
      "  Val Loss: 1.1108\n",
      "  Val Sentiment Acc: 0.8227, F1: 0.5735\n",
      "  Val Emotion Acc: 0.2993, F1: 0.0812\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 1.0223\n",
      "  Train Sentiment Acc: 0.8791, Train Emotion Acc: 0.3040\n",
      "  Val Loss: 1.1299\n",
      "  Val Sentiment Acc: 0.8260, F1: 0.5762\n",
      "  Val Emotion Acc: 0.3007, F1: 0.1022\n",
      "Best distilroBERTa model saved to ./initial_distilroberta_multitask_model\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best Combined F1: 0.3392\n",
      "\n",
      "✅ Initial Multitask Model Results:\n",
      "  Sentiment - Accuracy: 0.7812, F1: 0.5376\n",
      "  Emotion - Accuracy: 0.3750, F1: 0.0930\n",
      "  Combined - Accuracy: 0.5781, F1: 0.3153\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Initial Multitask Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL DISTILROBERTA TRAINING - MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for multitask\n",
    "default_config_multitask = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    alpha=0.5,\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./initial_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n4️⃣ Training Initial distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial multitask model\n",
    "initial_multitask_trainer = distilroBERTaMultiTaskTrainer(config=default_config_multitask)\n",
    "initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate initial multitask model\n",
    "initial_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./initial_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_multitask'] = initial_multitask_results\n",
    "\n",
    "print(f\"\\n✅ Initial Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {initial_multitask_results['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {initial_multitask_results['emotion_accuracy']:.4f}, F1: {initial_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {initial_multitask_results['combined_accuracy']:.4f}, F1: {initial_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2704229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 INITIAL ROBERTA RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 INITIAL ROBERTA MODEL PERFORMANCE:\n",
      "  Sentiment Model:\n",
      "    Accuracy: 0.8333\n",
      "    F1 Macro: 0.5812\n",
      "\n",
      "  Emotion Model:\n",
      "    Accuracy: 0.7667\n",
      "    F1 Macro: 0.7329\n",
      "\n",
      "  Multitask Model:\n",
      "    Sentiment - Accuracy: 0.7812, F1: 0.5376\n",
      "    Emotion - Accuracy: 0.3750, F1: 0.0930\n",
      "    Combined - Accuracy: 0.5781, F1: 0.3153\n",
      "\n",
      "💡 These are RoBERTa baseline results. Now proceeding to hyperparameter tuning!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Initial Results Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 INITIAL ROBERTA RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 INITIAL ROBERTA MODEL PERFORMANCE:\")\n",
    "print(f\"  Sentiment Model:\")\n",
    "print(f\"    Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Emotion Model:\")\n",
    "print(f\"    Accuracy: {initial_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Multitask Model:\")\n",
    "print(f\"    Sentiment - Accuracy: {initial_multitask_results['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {initial_multitask_results['emotion_accuracy']:.4f}, F1: {initial_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {initial_multitask_results['combined_accuracy']:.4f}, F1: {initial_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Store results for later comparison\n",
    "all_results = {\n",
    "    'initial_sentiment': initial_sentiment_results,\n",
    "    'initial_emotion': initial_emotion_results,\n",
    "    'initial_multitask': initial_multitask_results\n",
    "}\n",
    "\n",
    "print(f\"\\n💡 These are RoBERTa baseline results. Now proceeding to hyperparameter tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9aa44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 20:38:22,272] A new study created in memory with name: no-name-e818d2b4-0b02-4659-a212-615b6ac1d60a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 2: HYPERPARAMETER TUNING - SENTIMENT\n",
      "================================================================================\n",
      "\n",
      "6️⃣ Hyperparameter Tuning for distilroBERTa Sentiment Model...\n",
      "============================================================\n",
      "🔍 distilroBERTa hyperparameter tuner initialized for sentiment\n",
      "🚀 Using Random Search for optimization\n",
      "\n",
      "🔍 Starting hyperparameter optimization for sentiment...\n",
      "🎯 Random Search: 15 trials\n",
      "============================================================\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 0.8141, Train Acc: 0.6893\n",
      "  Val Loss: 0.7534, Val Acc: 0.7900, Val F1: 0.5503\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.7254, Train Acc: 0.7973\n",
      "  Val Loss: 0.8328, Val Acc: 0.8060, Val F1: 0.5622\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.6750, Train Acc: 0.8306\n",
      "  Val Loss: 0.9426, Val Acc: 0.8107, Val F1: 0.5654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 20:46:21,158] Trial 0 finished with value: 0.5654105603692791 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 4, 'num_epochs': 3, 'warmup_ratio': 0.0733991780504304, 'weight_decay': 0.006750277604651747, 'hidden_dropout_prob': 0.273235229154987, 'classifier_dropout': 0.2803345035229627, 'max_length': 128}. Best is trial 0 with value: 0.5654105603692791.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_0\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5654\n",
      "Trial 0: F1 = 0.5654\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 0.8653, Train Acc: 0.6643\n",
      "  Val Loss: 0.6716, Val Acc: 0.7633, Val F1: 0.5324\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.7422, Train Acc: 0.7796\n",
      "  Val Loss: 0.8249, Val Acc: 0.7767, Val F1: 0.5422\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.5975, Train Acc: 0.8407\n",
      "  Val Loss: 0.8167, Val Acc: 0.8073, Val F1: 0.5631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 21:02:50,917] Trial 1 finished with value: 0.5631400335536304 and parameters: {'learning_rate': 9.527257190058904e-05, 'batch_size': 4, 'num_epochs': 3, 'warmup_ratio': 0.09563633644393066, 'weight_decay': 0.05295088673159155, 'hidden_dropout_prob': 0.18638900372842315, 'classifier_dropout': 0.1873687420594126, 'max_length': 128}. Best is trial 0 with value: 0.5654105603692791.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_1\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5631\n",
      "Trial 1: F1 = 0.5631\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 0.7057, Train Acc: 0.7103\n",
      "  Val Loss: 0.5383, Val Acc: 0.7987, Val F1: 0.5578\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.4646, Train Acc: 0.8406\n",
      "  Val Loss: 0.4998, Val Acc: 0.8187, Val F1: 0.5709\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.3676, Train Acc: 0.8771\n",
      "  Val Loss: 0.5827, Val Acc: 0.8200, Val F1: 0.5721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 21:07:00,292] Trial 2 finished with value: 0.5721370158820985 and parameters: {'learning_rate': 3.2005921956585e-05, 'batch_size': 16, 'num_epochs': 3, 'warmup_ratio': 0.12713516576204176, 'weight_decay': 0.05964904231734221, 'hidden_dropout_prob': 0.10929008254399955, 'classifier_dropout': 0.28226345557043153, 'max_length': 128}. Best is trial 2 with value: 0.5721370158820985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_2\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5721\n",
      "Trial 2: F1 = 0.5721\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 0.8474, Train Acc: 0.6983\n",
      "  Val Loss: 0.8214, Val Acc: 0.7627, Val F1: 0.5315\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.7227, Train Acc: 0.7986\n",
      "  Val Loss: 0.7043, Val Acc: 0.7940, Val F1: 0.5537\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.5448, Train Acc: 0.8627\n",
      "  Val Loss: 0.8870, Val Acc: 0.8027, Val F1: 0.5597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 21:13:37,185] Trial 3 finished with value: 0.5596709257543712 and parameters: {'learning_rate': 9.210273435223547e-05, 'batch_size': 4, 'num_epochs': 3, 'warmup_ratio': 0.15263495397682356, 'weight_decay': 0.04457509688022053, 'hidden_dropout_prob': 0.12440764696895577, 'classifier_dropout': 0.2485530730333811, 'max_length': 256}. Best is trial 2 with value: 0.5721370158820985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_3\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5597\n",
      "Trial 3: F1 = 0.5597\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "  Train Loss: 0.8570, Train Acc: 0.6784\n",
      "  Val Loss: 0.8430, Val Acc: 0.7673, Val F1: 0.5335\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 2/5\n",
      "  Train Loss: 0.7765, Train Acc: 0.7929\n",
      "  Val Loss: 0.8037, Val Acc: 0.8160, Val F1: 0.5686\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 3/5\n",
      "  Train Loss: 0.6889, Train Acc: 0.8307\n",
      "  Val Loss: 0.9276, Val Acc: 0.8140, Val F1: 0.5682\n",
      "\n",
      "📍 Epoch 4/5\n",
      "  Train Loss: 0.6222, Train Acc: 0.8570\n",
      "  Val Loss: 1.0354, Val Acc: 0.8167, Val F1: 0.5698\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 5/5\n",
      "  Train Loss: 0.5697, Train Acc: 0.8673\n",
      "  Val Loss: 0.9957, Val Acc: 0.8193, Val F1: 0.5714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 21:23:37,294] Trial 4 finished with value: 0.5714219534007071 and parameters: {'learning_rate': 3.0332586204364574e-05, 'batch_size': 4, 'num_epochs': 5, 'warmup_ratio': 0.07772816832882906, 'weight_decay': 0.0969888781486913, 'hidden_dropout_prob': 0.2550265646722229, 'classifier_dropout': 0.38184968246925677, 'max_length': 128}. Best is trial 2 with value: 0.5721370158820985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best distilroBERTa model saved to ./distilroberta_trial_4\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5714\n",
      "Trial 4: F1 = 0.5714\n",
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/4\n",
      "  Train Loss: 0.8249, Train Acc: 0.6574\n",
      "  Val Loss: 0.6533, Val Acc: 0.7580, Val F1: 0.5289\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_5\\model_best\n",
      "\n",
      "📍 Epoch 2/4\n",
      "  Train Loss: 0.6238, Train Acc: 0.7794\n",
      "  Val Loss: 0.6078, Val Acc: 0.7887, Val F1: 0.5490\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_5\\model_best\n",
      "\n",
      "📍 Epoch 3/4\n",
      "  Train Loss: 0.5112, Train Acc: 0.8299\n",
      "  Val Loss: 0.6730, Val Acc: 0.7987, Val F1: 0.5573\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_5\\model_best\n",
      "\n",
      "📍 Epoch 4/4\n",
      "  Train Loss: 0.4555, Train Acc: 0.8589\n",
      "  Val Loss: 0.7548, Val Acc: 0.8067, Val F1: 0.5627\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_5\\model_best\n",
      "\n",
      "distilroBERTa training completed! Best F1: 0.5627\n",
      "Trial 5: F1 = 0.5627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 22:06:07,586] Trial 5 finished with value: 0.5626823551920409 and parameters: {'learning_rate': 8.818453591655191e-05, 'batch_size': 8, 'num_epochs': 4, 'warmup_ratio': 0.10830159345342232, 'weight_decay': 0.0278635541456157, 'hidden_dropout_prob': 0.26574750183038587, 'classifier_dropout': 0.2070259980080768, 'max_length': 256}. Best is trial 2 with value: 0.5721370158820985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting distilroBERTa single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/6\n",
      "  Train Loss: 0.8409, Train Acc: 0.6341\n",
      "  Val Loss: 0.5422, Val Acc: 0.8093, Val F1: 0.5643\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_6\\model_best\n",
      "\n",
      "📍 Epoch 2/6\n",
      "  Train Loss: 0.6012, Train Acc: 0.7794\n",
      "  Val Loss: 0.5183, Val Acc: 0.8100, Val F1: 0.5650\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_6\\model_best\n",
      "\n",
      "📍 Epoch 3/6\n",
      "  Train Loss: 0.5223, Train Acc: 0.8140\n",
      "  Val Loss: 0.5563, Val Acc: 0.8027, Val F1: 0.5606\n",
      "\n",
      "📍 Epoch 4/6\n",
      "  Train Loss: 0.4759, Train Acc: 0.8349\n",
      "  Val Loss: 0.5554, Val Acc: 0.8213, Val F1: 0.5729\n",
      "Best distilroBERTa model saved to ./distilroberta_trial_6\\model_best\n",
      "\n",
      "📍 Epoch 5/6\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Hyperparameter Tuning - Sentiment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n6️⃣ Hyperparameter Tuning for distilroBERTa Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for sentiment\n",
    "sentiment_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"sentiment\",\n",
    "    data_splits=sentiment_data,\n",
    "    n_trials=15,\n",
    "    model_name=model_name\n",
    ")\n",
    "sentiment_study = sentiment_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e334a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Hyperparameter Tuning - Emotion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - EMOTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n7️⃣ Hyperparameter Tuning for distilroBERTa Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for emotion\n",
    "emotion_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"emotion\",\n",
    "    data_splits=emotion_data,\n",
    "    n_trials=15,\n",
    "    model_name=model_name\n",
    ")\n",
    "emotion_study = emotion_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c743ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Hyperparameter Tuning - Multitask\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - MULTITASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n8️⃣ Hyperparameter Tuning for distilroBERTa Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for multitask\n",
    "multitask_tuner = distilroBERTaHyperparameterTuner(\n",
    "    model_type=\"multitask\",\n",
    "    data_splits=multitask_data,\n",
    "    n_trials=15,\n",
    "    model_name=model_name\n",
    ")\n",
    "multitask_study = multitask_tuner.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0516f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Final Training - Sentiment with Best Parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n9️⃣ Training Final distilroBERTa Sentiment Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from sentiment tuning\n",
    "best_sentiment_params = sentiment_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_sentiment_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_sentiment_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_sentiment_params['learning_rate'],\n",
    "    batch_size=best_sentiment_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_sentiment_params['warmup_ratio'],\n",
    "    weight_decay=best_sentiment_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_sentiment_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_sentiment_params['classifier_dropout'],\n",
    "    max_length=best_sentiment_params.get('max_length', 128),\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./final_distilroberta_sentiment_model\"\n",
    ")\n",
    "\n",
    "# Train final sentiment model\n",
    "final_sentiment_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_sentiment_config,\n",
    "    num_classes=roberta_model_config.sentiment_num_classes\n",
    ")\n",
    "final_sentiment_history = final_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate final sentiment model\n",
    "final_sentiment_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_sentiment'] = final_sentiment_results\n",
    "\n",
    "print(f\"\\n✅ Final Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {final_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Final Emotion Model Training with Best Parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🔟 Training Final distilroBERTa Emotion Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from emotion tuning\n",
    "best_emotion_params = emotion_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_emotion_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_emotion_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_emotion_params['learning_rate'],\n",
    "    batch_size=best_emotion_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_emotion_params['warmup_ratio'],\n",
    "    weight_decay=best_emotion_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_emotion_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_emotion_params['classifier_dropout'],\n",
    "    max_length=best_emotion_params.get('max_length', 128),\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./final_distilroberta_emotion_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final emotion model:\")\n",
    "print(f\"  Dataset: Full emotion data ({len(emotion_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_emotion_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_emotion_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_emotion_config.learning_rate:.2e}\")\n",
    "\n",
    "# Train final emotion model\n",
    "final_emotion_trainer = distilroBERTaSingleTaskTrainer(\n",
    "    config=final_emotion_config,\n",
    "    num_classes=roberta_model_config.emotion_num_classes\n",
    ")\n",
    "final_emotion_history = final_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate final emotion model\n",
    "final_emotion_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_emotion'] = final_emotion_results\n",
    "\n",
    "print(f\"\\n✅ Final Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {final_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328fd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Final Multitask Model Training with Best Parameters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1️⃣1️⃣ Training Final distilroBERTa Multi-task Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from multitask tuning\n",
    "best_multitask_params = multitask_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_multitask_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training\n",
    "final_multitask_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_multitask_params['learning_rate'],\n",
    "    batch_size=best_multitask_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_multitask_params['warmup_ratio'],\n",
    "    weight_decay=best_multitask_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_multitask_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_multitask_params['classifier_dropout'],\n",
    "    max_length=best_multitask_params.get('max_length', 128),\n",
    "    alpha=best_multitask_params['alpha'],  # Multitask-specific parameter\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./final_distilroberta_multitask_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final multitask model:\")\n",
    "print(f\"  Dataset: Full multitask data ({len(multitask_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_multitask_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_multitask_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_multitask_config.learning_rate:.2e}\")\n",
    "print(f\"  Alpha (loss weighting): {final_multitask_config.alpha:.3f}\")\n",
    "\n",
    "# Train final multitask model\n",
    "final_multitask_trainer = distilroBERTaMultiTaskTrainer(config=final_multitask_config)\n",
    "final_multitask_history = final_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate final multitask model\n",
    "final_multitask_results = evaluate_distilroberta_model(\n",
    "    model_path=\"./final_distilroberta_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['final_multitask'] = final_multitask_results\n",
    "\n",
    "print(f\"\\n✅ Final Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {final_multitask_results['sentiment_accuracy']:.4f}, F1: {final_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {final_multitask_results['emotion_accuracy']:.4f}, F1: {final_emotion_results['f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {final_multitask_results['combined_accuracy']:.4f}, F1: {final_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a250d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Final Results Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nMODEL PERFORMANCE COMPARISON:\")\n",
    "\n",
    "print(f\"\\n1️⃣ Sentiment Task:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    Accuracy: {all_results['initial_sentiment']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['initial_sentiment']['f1_macro']:.4f}\")\n",
    "print(f\"  Optimized Model:\")\n",
    "print(f\"    Accuracy: {all_results['final_sentiment']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['final_sentiment']['f1_macro']:.4f}\")\n",
    "print(f\"  Improvement:\")\n",
    "print(f\"    Accuracy: {(all_results['final_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy'])*100:.2f}%\")\n",
    "print(f\"    F1 Macro: {(all_results['final_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n2️⃣ Emotion Task:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    Accuracy: {all_results['initial_emotion']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['initial_emotion']['f1_macro']:.4f}\")\n",
    "print(f\"  Optimized Model:\")\n",
    "print(f\"    Accuracy: {all_results['final_emotion']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {all_results['final_emotion']['f1_macro']:.4f}\")\n",
    "print(f\"  Improvement:\")\n",
    "print(f\"    Accuracy: {(all_results['final_emotion']['accuracy'] - all_results['initial_emotion']['accuracy'])*100:.2f}%\")\n",
    "print(f\"    F1 Macro: {(all_results['final_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n3️⃣ Multi-task Model:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    Sentiment - Accuracy: {all_results['initial_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {all_results['initial_multitask']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Optimized Model:\")\n",
    "print(f\"    Sentiment - Accuracy: {all_results['final_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['final_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {all_results['final_multitask']['emotion_accuracy']:.4f}, F1: {all_results['final_multitask']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Improvement:\")\n",
    "print(f\"    Sentiment - Accuracy: {(all_results['final_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy'])*100:.2f}%\")\n",
    "print(f\"    Emotion - Accuracy: {(all_results['final_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy'])*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining pipeline completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
