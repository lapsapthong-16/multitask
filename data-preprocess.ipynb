{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab5aa14",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "# import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55971a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RedditTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Remove some emotion-related words from stopwords to preserve sentiment\n",
    "        emotion_words = {\n",
    "            'not', 'no', 'nor', 'but', 'however', 'although', 'though',\n",
    "            'very', 'really', 'quite', 'too', 'so', 'more', 'most',\n",
    "            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "            'some', 'such', 'only', 'own', 'same', 'than', 'too', 'very'\n",
    "        }\n",
    "        self.stop_words = self.stop_words - emotion_words\n",
    "        \n",
    "        print(f\"Initialized preprocessor with {len(self.stop_words)} stopwords\")\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        \"\"\"Remove URLs from text\"\"\"\n",
    "        # Remove http/https URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove www URLs\n",
    "        text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove reddit links like /r/subreddit\n",
    "        text = re.sub(r'/r/[A-Za-z0-9_]+', '', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_mentions_hashtags(self, text):\n",
    "        \"\"\"Remove @mentions and #hashtags\"\"\"\n",
    "        # Remove @mentions\n",
    "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "        # Remove #hashtags but preserve the word (e.g., #BombsAway -> BombsAway)\n",
    "        text = re.sub(r'#([A-Za-z0-9_]+)', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags\"\"\"\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        return text\n",
    "    \n",
    "    def handle_reddit_formatting(self, text):\n",
    "        \"\"\"Handle Reddit-specific formatting\"\"\"\n",
    "        # Remove markdown links [text](url)\n",
    "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "        # Remove reddit user references like u/username\n",
    "        text = re.sub(r'u/[A-Za-z0-9_]+', '', text)\n",
    "        # Remove markdown formatting **bold** and *italic*\n",
    "        text = re.sub(r'\\*\\*([^\\*]+)\\*\\*', r'\\1', text)\n",
    "        text = re.sub(r'\\*([^\\*]+)\\*', r'\\1', text)\n",
    "        # Remove quote markers\n",
    "        text = re.sub(r'^>', '', text, flags=re.MULTILINE)\n",
    "        return text\n",
    "    \n",
    "    def convert_emojis(self, text):\n",
    "        \"\"\"Convert emojis to text descriptions\"\"\"\n",
    "        try:\n",
    "            import emoji\n",
    "            # Convert emojis to text\n",
    "            text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "            # Clean up the emoji text formatting\n",
    "            text = re.sub(r':[a-zA-Z_]+:', lambda m: m.group().replace('_', ' ').replace(':', ''), text)\n",
    "        except ImportError:\n",
    "            # If emoji package is not available, just return the text as is\n",
    "            pass\n",
    "        return text\n",
    "    \n",
    "    def clean_special_characters(self, text):\n",
    "        \"\"\"Remove special characters but preserve emotionally relevant punctuation\"\"\"\n",
    "        # Preserve ! and ? as they convey emotion\n",
    "        # First, protect exclamation and question marks\n",
    "        text = re.sub(r'!+', ' EXCLAMATION ', text)\n",
    "        text = re.sub(r'\\?+', ' QUESTION ', text)\n",
    "        \n",
    "        # Remove other punctuation except apostrophes (for contractions)\n",
    "        text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "        \n",
    "        # Restore exclamation and question marks\n",
    "        text = text.replace(' EXCLAMATION ', ' ! ')\n",
    "        text = text.replace(' QUESTION ', ' ? ')\n",
    "        \n",
    "        # Handle contractions by removing apostrophes after processing\n",
    "        text = re.sub(r\"'\", '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize_whitespace(self, text):\n",
    "        \"\"\"Remove redundant whitespace\"\"\"\n",
    "        # Replace multiple whitespace with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove stopwords while preserving sentence structure\"\"\"\n",
    "        try:\n",
    "            words = word_tokenize(text.lower())\n",
    "            filtered_words = [word for word in words if word not in self.stop_words]\n",
    "            return ' '.join(filtered_words)\n",
    "        except:\n",
    "            # Fallback: simple split if tokenization fails\n",
    "            words = text.lower().split()\n",
    "            filtered_words = [word for word in words if word not in self.stop_words]\n",
    "            return ' '.join(filtered_words)\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        \"\"\"Lemmatize words to their base form\"\"\"\n",
    "        try:\n",
    "            words = word_tokenize(text)\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            return ' '.join(lemmatized_words)\n",
    "        except:\n",
    "            # Fallback: simple split if tokenization fails\n",
    "            words = text.split()\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            return ' '.join(lemmatized_words)\n",
    "    \n",
    "    def anonymize_identifiers(self, text):\n",
    "        \"\"\"Remove or mask identifiable information\"\"\"\n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "        # Remove potential usernames (sequences of letters/numbers/underscores)\n",
    "        text = re.sub(r'\\b[A-Za-z0-9_]{8,}\\b', '[USERNAME]', text)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Apply all preprocessing steps\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        original_text = text\n",
    "        \n",
    "        # Step 1: Handle Reddit-specific formatting\n",
    "        text = self.handle_reddit_formatting(text)\n",
    "        \n",
    "        # Step 2: Remove URLs\n",
    "        text = self.remove_urls(text)\n",
    "        \n",
    "        # Step 3: Remove mentions and hashtags\n",
    "        text = self.remove_mentions_hashtags(text)\n",
    "        \n",
    "        # Step 4: Remove HTML tags\n",
    "        text = self.remove_html_tags(text)\n",
    "        \n",
    "        # Step 5: Convert emojis\n",
    "        text = self.convert_emojis(text)\n",
    "        \n",
    "        # Step 6: Anonymize identifiers\n",
    "        text = self.anonymize_identifiers(text)\n",
    "        \n",
    "        # Step 7: Clean special characters (preserve ! and ?)\n",
    "        text = self.clean_special_characters(text)\n",
    "        \n",
    "        # Step 8: Normalize whitespace\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        # Step 9: Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Step 10: Remove stopwords\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Step 11: Lemmatize\n",
    "        text = self.lemmatize_text(text)\n",
    "        \n",
    "        # Final cleanup\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e858782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load the Reddit data and apply preprocessing\"\"\"\n",
    "    \n",
    "    print(\"Loading Reddit posts data...\")\n",
    "    \n",
    "    # Load the JSON data\n",
    "    try:\n",
    "        with open('combined_y_labeled_data.json', 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Loaded {len(data)} Reddit posts/comments\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: combined_y_labeled_data.json not found!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = RedditTextPreprocessor()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"Starting text preprocessing...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed_texts = []\n",
    "    for i, text in enumerate(df['text_content']):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i+1}/{len(df)} texts...\")\n",
    "        \n",
    "        processed_text = preprocessor.preprocess_text(text)\n",
    "        processed_texts.append(processed_text)\n",
    "    \n",
    "    # Create final dataset\n",
    "    final_df = pd.DataFrame({\n",
    "        'id': df['id'],\n",
    "        'text_content': processed_texts,\n",
    "        'original_text': df['text_content'],\n",
    "        'type': df['type'],\n",
    "        'score': df['score'],\n",
    "        'subjectivity': df['subjectivity']\n",
    "    })\n",
    "    \n",
    "    # Remove entries where cleaned text is empty or too short\n",
    "    final_df = final_df[final_df['text_content'].str.len() >= 3]\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Original dataset: {len(df)} entries\")\n",
    "    print(f\"Final dataset: {len(final_df)} entries\")\n",
    "    print(f\"Removed {len(df) - len(final_df)} entries with insufficient content\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv('cleaned_reddit_posts.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nCleaned dataset saved to: cleaned_reddit_posts.csv\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE PREPROCESSING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(3, len(final_df))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Original: {final_df.iloc[i]['original_text'][:150]}...\")\n",
    "        print(f\"Cleaned:  {final_df.iloc[i]['text_content'][:150]}...\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total entries: {len(final_df)}\")\n",
    "    print(f\"Posts: {len(final_df[final_df['type'] == 'post'])}\")\n",
    "    print(f\"Comments: {len(final_df[final_df['type'] == 'comment'])}\")\n",
    "    \n",
    "    # Text length statistics\n",
    "    text_lengths = final_df['text_content'].str.len()\n",
    "    print(f\"\\nText length statistics (after cleaning):\")\n",
    "    print(f\"  Average: {text_lengths.mean():.1f} characters\")\n",
    "    print(f\"  Median: {text_lengths.median():.1f} characters\")\n",
    "    print(f\"  Min: {text_lengths.min()} characters\")\n",
    "    print(f\"  Max: {text_lengths.max()} characters\")\n",
    "    \n",
    "    # Word count statistics\n",
    "    word_counts = final_df['text_content'].str.split().str.len()\n",
    "    print(f\"\\nWord count statistics (after cleaning):\")\n",
    "    print(f\"  Average: {word_counts.mean():.1f} words\")\n",
    "    print(f\"  Median: {word_counts.median():.1f} words\")\n",
    "    print(f\"  Min: {word_counts.min()} words\")\n",
    "    print(f\"  Max: {word_counts.max()} words\")\n",
    "    \n",
    "    # Score and subjectivity statistics\n",
    "    print(f\"\\nScore statistics:\")\n",
    "    print(f\"  Average: {final_df['score'].mean():.2f}\")\n",
    "    print(f\"  Range: {final_df['score'].min()} to {final_df['score'].max()}\")\n",
    "    \n",
    "    print(f\"\\nSubjectivity statistics:\")\n",
    "    print(f\"  Average: {final_df['subjectivity'].mean():.3f}\")\n",
    "    print(f\"  Range: {final_df['subjectivity'].min():.3f} to {final_df['subjectivity'].max():.3f}\")\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reddit Posts Text Preprocessing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Install required packages if not available\n",
    "try:\n",
    "    import emoji\n",
    "except ImportError:\n",
    "    print(\"Installing emoji package...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'emoji'])\n",
    "    import emoji\n",
    "\n",
    "# Run preprocessing\n",
    "cleaned_data = load_and_preprocess_data()\n",
    "\n",
    "if cleaned_data is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PREPROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Your dataset is now ready for sentiment and emotion analysis.\")\n",
    "    print(\"Output file: cleaned_reddit_posts.csv\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Review the cleaned data\")\n",
    "    print(\"2. Apply sentiment analysis models\")\n",
    "    print(\"3. Apply emotion classification models\")\n",
    "    print(\"4. Analyze results for PR crisis insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea143913",
   "metadata": {},
   "source": [
    "## Data Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4966ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_models():\n",
    "    # VADER analyzer\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # BERTweet models\n",
    "    print(\"Loading BERTweet models...\")\n",
    "    bertweet_sentiment_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    bertweet_emotion_model = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "    \n",
    "    # RoBERTa models  \n",
    "    print(\"Loading RoBERTa models...\")\n",
    "    roberta_sentiment_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    roberta_emotion_model = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "    \n",
    "    # Create pipelines\n",
    "    bertweet_sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                                      model=bertweet_sentiment_model,\n",
    "                                      tokenizer=bertweet_sentiment_model,\n",
    "                                      max_length=512, \n",
    "                                      truncation=True)\n",
    "    \n",
    "    bertweet_emotion_pipe = pipeline(\"text-classification\", \n",
    "                                   model=bertweet_emotion_model,\n",
    "                                   tokenizer=bertweet_emotion_model,\n",
    "                                   max_length=512,\n",
    "                                   truncation=True)\n",
    "    \n",
    "    roberta_sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                                    model=roberta_sentiment_model,\n",
    "                                    tokenizer=roberta_sentiment_model,\n",
    "                                    max_length=512,\n",
    "                                    truncation=True)\n",
    "    \n",
    "    roberta_emotion_pipe = pipeline(\"text-classification\", \n",
    "                                  model=roberta_emotion_model,\n",
    "                                  tokenizer=roberta_emotion_model,\n",
    "                                  max_length=512,\n",
    "                                  truncation=True)\n",
    "    \n",
    "    print(\"All models loaded successfully!\")\n",
    "    return vader_analyzer, bertweet_sentiment_pipe, bertweet_emotion_pipe, roberta_sentiment_pipe, roberta_emotion_pipe\n",
    "\n",
    "def analyze_vader_sentiment(text, analyzer):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    \n",
    "    if compound >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def analyze_bertweet_sentiment(text, pipe):\n",
    "    try:\n",
    "        result = pipe(text)[0]\n",
    "        label = result['label']\n",
    "        # Map labels to consistent format\n",
    "        if label in ['LABEL_0', 'NEGATIVE']:\n",
    "            return \"Negative\"\n",
    "        elif label in ['LABEL_1', 'NEUTRAL']:\n",
    "            return \"Neutral\"\n",
    "        elif label in ['LABEL_2', 'POSITIVE']:\n",
    "            return \"Positive\"\n",
    "        else:\n",
    "            return label.title()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in BERTweet sentiment: {e}\")\n",
    "        return \"Neutral\"\n",
    "\n",
    "def analyze_emotion(text, pipe):\n",
    "    try:\n",
    "        result = pipe(text)[0]\n",
    "        emotion = result['label']\n",
    "        # Map to consistent emotion labels\n",
    "        emotion_mapping = {\n",
    "            'joy': 'Joy',\n",
    "            'sadness': 'Sadness', \n",
    "            'anger': 'Anger',\n",
    "            'fear': 'Fear',\n",
    "            'surprise': 'Surprise',\n",
    "            'no_emotion': 'No Emotion'\n",
    "        }\n",
    "        return emotion_mapping.get(emotion.lower(), emotion.title())\n",
    "    except Exception as e:\n",
    "        print(f\"Error in emotion analysis: {e}\")\n",
    "        return \"Neutral\"\n",
    "\n",
    "def analyze_roberta_sentiment(text, pipe):\n",
    "    try:\n",
    "        result = pipe(text)[0]\n",
    "        label = result['label']\n",
    "        # Map labels to consistent format\n",
    "        if label in ['LABEL_0', 'NEGATIVE']:\n",
    "            return \"Negative\"\n",
    "        elif label in ['LABEL_1', 'NEUTRAL']:\n",
    "            return \"Neutral\"\n",
    "        elif label in ['LABEL_2', 'POSITIVE']:\n",
    "            return \"Positive\"\n",
    "        else:\n",
    "            return label.title()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RoBERTa sentiment: {e}\")\n",
    "        return \"Neutral\"\n",
    "\n",
    "def process_dataset(input_file, output_file):\n",
    "    print(f\"Loading dataset from {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Dataset loaded: {len(df)} entries\")\n",
    "    \n",
    "    # Setup models\n",
    "    vader_analyzer, bertweet_sentiment_pipe, bertweet_emotion_pipe, roberta_sentiment_pipe, roberta_emotion_pipe = setup_models()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['sentiment_vader'] = \"\"\n",
    "    df['emotion_vader'] = \"NA\"  # VADER doesn't do emotion\n",
    "    df['sentiment_bertweet'] = \"\"\n",
    "    df['emotion_bertweet'] = \"\"\n",
    "    df['sentiment_roberta'] = \"\"\n",
    "    df['emotion_roberta'] = \"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Processing annotations...\")\n",
    "    \n",
    "    # Process each row with progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Annotating\"):\n",
    "        text = str(row['text_content'])\n",
    "        \n",
    "        # Skip if text is empty or too short\n",
    "        if len(text.strip()) < 3:\n",
    "            df.at[idx, 'sentiment_vader'] = \"Neutral\"\n",
    "            df.at[idx, 'sentiment_bertweet'] = \"Neutral\"\n",
    "            df.at[idx, 'emotion_bertweet'] = \"Neutral\"\n",
    "            df.at[idx, 'sentiment_roberta'] = \"Neutral\"\n",
    "            df.at[idx, 'emotion_roberta'] = \"Neutral\"\n",
    "            continue\n",
    "        \n",
    "        # 1. VADER Analysis\n",
    "        df.at[idx, 'sentiment_vader'] = analyze_vader_sentiment(text, vader_analyzer)\n",
    "        \n",
    "        # 2. BERTweet Analysis\n",
    "        df.at[idx, 'sentiment_bertweet'] = analyze_bertweet_sentiment(text, bertweet_sentiment_pipe)\n",
    "        df.at[idx, 'emotion_bertweet'] = analyze_emotion(text, bertweet_emotion_pipe)\n",
    "        \n",
    "        # 3. RoBERTa Analysis\n",
    "        df.at[idx, 'sentiment_roberta'] = analyze_roberta_sentiment(text, roberta_sentiment_pipe)\n",
    "        df.at[idx, 'emotion_roberta'] = analyze_emotion(text, roberta_emotion_pipe)\n",
    "    \n",
    "    # Save annotated dataset\n",
    "    print(f\"ðŸ’¾ Saving annotated dataset to {output_file}...\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… Complete! Annotated dataset saved with {len(df)} entries\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nðŸ“ˆ ANNOTATION SUMMARY:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ VADER Sentiment Distribution:\")\n",
    "    print(df['sentiment_vader'].value_counts())\n",
    "    \n",
    "    print(\"\\nðŸ¦ BERTweet Sentiment Distribution:\")\n",
    "    print(df['sentiment_bertweet'].value_counts())\n",
    "    \n",
    "    print(\"\\nðŸ¦ BERTweet Emotion Distribution:\")\n",
    "    print(df['emotion_bertweet'].value_counts())\n",
    "    \n",
    "    print(\"\\nðŸ¤– RoBERTa Sentiment Distribution:\")\n",
    "    print(df['sentiment_roberta'].value_counts())\n",
    "    \n",
    "    print(\"\\nðŸ¤– RoBERTa Emotion Distribution:\")\n",
    "    print(df['emotion_roberta'].value_counts())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Starting Sentiment & Emotion Annotation Pipeline\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "input_file = \"cleaned_reddit_posts.csv\"\n",
    "output_file = \"annotated_reddit_posts.csv\"\n",
    "\n",
    "try:\n",
    "    annotated_df = process_dataset(input_file, output_file)\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! Annotated dataset ready: {output_file}\")\n",
    "    \n",
    "    # Show sample of annotated data\n",
    "    print(\"\\nðŸ“‹ Sample of annotated data:\")\n",
    "    sample_cols = ['id', 'text_content', 'sentiment_vader', 'sentiment_bertweet', \n",
    "                    'emotion_bertweet', 'sentiment_roberta', 'emotion_roberta']\n",
    "    print(annotated_df[sample_cols].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
