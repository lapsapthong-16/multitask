{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab5aa14",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "# import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55971a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RedditTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Remove some emotion-related words from stopwords to preserve sentiment\n",
    "        emotion_words = {\n",
    "            'not', 'no', 'nor', 'but', 'however', 'although', 'though',\n",
    "            'very', 'really', 'quite', 'too', 'so', 'more', 'most',\n",
    "            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "            'some', 'such', 'only', 'own', 'same', 'than', 'too', 'very'\n",
    "        }\n",
    "        self.stop_words = self.stop_words - emotion_words\n",
    "        \n",
    "        print(f\"Initialized preprocessor with {len(self.stop_words)} stopwords\")\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        # Remove http/https URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove www URLs\n",
    "        text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove reddit links like /r/subreddit\n",
    "        text = re.sub(r'/r/[A-Za-z0-9_]+', '', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_mentions_hashtags(self, text):\n",
    "        # Remove @mentions\n",
    "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "        # Remove #hashtags but preserve the word (e.g., #BombsAway -> BombsAway)\n",
    "        text = re.sub(r'#([A-Za-z0-9_]+)', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        return text\n",
    "    \n",
    "    def handle_reddit_formatting(self, text):\n",
    "        # Remove markdown links [text](url)\n",
    "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "        # Remove reddit user references like u/username\n",
    "        text = re.sub(r'u/[A-Za-z0-9_]+', '', text)\n",
    "        # Remove markdown formatting **bold** and *italic*\n",
    "        text = re.sub(r'\\*\\*([^\\*]+)\\*\\*', r'\\1', text)\n",
    "        text = re.sub(r'\\*([^\\*]+)\\*', r'\\1', text)\n",
    "        # Remove quote markers\n",
    "        text = re.sub(r'^>', '', text, flags=re.MULTILINE)\n",
    "        return text\n",
    "    \n",
    "    def convert_emojis(self, text):\n",
    "        try:\n",
    "            import emoji\n",
    "            # Convert emojis to text\n",
    "            text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "            # Clean up the emoji text formatting\n",
    "            text = re.sub(r':[a-zA-Z_]+:', lambda m: m.group().replace('_', ' ').replace(':', ''), text)\n",
    "        except ImportError:\n",
    "            # If emoji package is not available, just return the text as is\n",
    "            pass\n",
    "        return text\n",
    "    \n",
    "    def clean_special_characters(self, text):\n",
    "        # Preserve ! and ? as they convey emotion\n",
    "        # First, protect exclamation and question marks\n",
    "        text = re.sub(r'!+', ' EXCLAMATION ', text)\n",
    "        text = re.sub(r'\\?+', ' QUESTION ', text)\n",
    "        \n",
    "        # Remove other punctuation except apostrophes (for contractions)\n",
    "        text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "        \n",
    "        # Restore exclamation and question marks\n",
    "        text = text.replace(' EXCLAMATION ', ' ! ')\n",
    "        text = text.replace(' QUESTION ', ' ? ')\n",
    "        \n",
    "        # Handle contractions by removing apostrophes after processing\n",
    "        text = re.sub(r\"'\", '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize_whitespace(self, text):\n",
    "        # Replace multiple whitespace with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        try:\n",
    "            words = word_tokenize(text.lower())\n",
    "            filtered_words = [word for word in words if word not in self.stop_words]\n",
    "            return ' '.join(filtered_words)\n",
    "        except:\n",
    "            # Fallback: simple split if tokenization fails\n",
    "            words = text.lower().split()\n",
    "            filtered_words = [word for word in words if word not in self.stop_words]\n",
    "            return ' '.join(filtered_words)\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        try:\n",
    "            words = word_tokenize(text)\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            return ' '.join(lemmatized_words)\n",
    "        except:\n",
    "            # Fallback: simple split if tokenization fails\n",
    "            words = text.split()\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            return ' '.join(lemmatized_words)\n",
    "    \n",
    "    def anonymize_identifiers(self, text):\n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "        # Remove potential usernames (sequences of letters/numbers/underscores)\n",
    "        text = re.sub(r'\\b[A-Za-z0-9_]{8,}\\b', '[USERNAME]', text)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        original_text = text\n",
    "        \n",
    "        # Step 1: Handle Reddit-specific formatting\n",
    "        text = self.handle_reddit_formatting(text)\n",
    "        \n",
    "        # Step 2: Remove URLs\n",
    "        text = self.remove_urls(text)\n",
    "        \n",
    "        # Step 3: Remove mentions and hashtags\n",
    "        text = self.remove_mentions_hashtags(text)\n",
    "        \n",
    "        # Step 4: Remove HTML tags\n",
    "        text = self.remove_html_tags(text)\n",
    "        \n",
    "        # Step 5: Convert emojis\n",
    "        text = self.convert_emojis(text)\n",
    "        \n",
    "        # Step 6: Anonymize identifiers\n",
    "        text = self.anonymize_identifiers(text)\n",
    "        \n",
    "        # Step 7: Clean special characters (preserve ! and ?)\n",
    "        text = self.clean_special_characters(text)\n",
    "        \n",
    "        # Step 8: Normalize whitespace\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        # Step 9: Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Step 10: Remove stopwords\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Step 11: Lemmatize\n",
    "        text = self.lemmatize_text(text)\n",
    "        \n",
    "        # Final cleanup\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e858782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load the JSON data\n",
    "    try:\n",
    "        with open('combined_y_labeled_data.json', 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Loaded {len(data)} Reddit posts/comments\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: combined_y_labeled_data.json not found!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = RedditTextPreprocessor()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"Starting text preprocessing...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed_texts = []\n",
    "    for i, text in enumerate(df['text_content']):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i+1}/{len(df)} texts...\")\n",
    "        \n",
    "        processed_text = preprocessor.preprocess_text(text)\n",
    "        processed_texts.append(processed_text)\n",
    "    \n",
    "    # Create final dataset\n",
    "    final_df = pd.DataFrame({\n",
    "        'id': df['id'],\n",
    "        'text_content': processed_texts,\n",
    "        'original_text': df['text_content'],\n",
    "        'type': df['type'],\n",
    "        'score': df['score'],\n",
    "        'subjectivity': df['subjectivity']\n",
    "    })\n",
    "    \n",
    "    # Remove entries where cleaned text is empty or too short\n",
    "    final_df = final_df[final_df['text_content'].str.len() >= 3]\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Original dataset: {len(df)} entries\")\n",
    "    print(f\"Final dataset: {len(final_df)} entries\")\n",
    "    print(f\"Removed {len(df) - len(final_df)} entries with insufficient content\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv('cleaned_reddit_posts.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nCleaned dataset saved to: cleaned_reddit_posts.csv\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE PREPROCESSING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(3, len(final_df))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Original: {final_df.iloc[i]['original_text'][:150]}...\")\n",
    "        print(f\"Cleaned:  {final_df.iloc[i]['text_content'][:150]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reddit Posts Text Preprocessing\")\n",
    "\n",
    "# Run preprocessing\n",
    "cleaned_data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea143913",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee83231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==1.8.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: aiohappyeyeballs==2.6.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiohttp==3.12.13 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 3)) (3.12.13)\n",
      "Requirement already satisfied: aiosignal==1.4.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: attrs==25.3.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 5)) (25.3.0)\n",
      "Requirement already satisfied: certifi==2025.6.15 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 6)) (2025.6.15)\n",
      "Requirement already satisfied: charset-normalizer==3.4.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: click==8.2.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 8)) (8.2.1)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: contourpy==1.3.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 10)) (1.3.2)\n",
      "Requirement already satisfied: cycler==0.12.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 11)) (0.12.1)\n",
      "Requirement already satisfied: datasets==3.6.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: dill==0.3.8 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 13)) (0.3.8)\n",
      "Requirement already satisfied: emoji==2.14.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 14)) (2.14.1)\n",
      "Requirement already satisfied: filelock==3.18.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 15)) (3.18.0)\n",
      "Requirement already satisfied: fonttools==4.58.5 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 16)) (4.58.5)\n",
      "Requirement already satisfied: frozenlist==1.7.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 17)) (1.7.0)\n",
      "Requirement already satisfied: fsspec==2025.5.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 18)) (2025.5.1)\n",
      "Requirement already satisfied: huggingface-hub==0.26.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 19)) (0.26.2)\n",
      "Requirement already satisfied: idna==3.10 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 20)) (3.10)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 21)) (3.1.6)\n",
      "Requirement already satisfied: joblib==1.5.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 22)) (1.5.1)\n",
      "Requirement already satisfied: kiwisolver==1.4.8 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 23)) (1.4.8)\n",
      "Requirement already satisfied: langdetect==1.0.9 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 24)) (1.0.9)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 25)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib==3.10.3 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 26)) (3.10.3)\n",
      "Requirement already satisfied: mpmath==1.3.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 27)) (1.3.0)\n",
      "Requirement already satisfied: multidict==6.6.3 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 28)) (6.6.3)\n",
      "Requirement already satisfied: multiprocess==0.70.16 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 29)) (0.70.16)\n",
      "Requirement already satisfied: networkx==3.5 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 30)) (3.5)\n",
      "Requirement already satisfied: nltk==3.9.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 31)) (3.9.1)\n",
      "Requirement already satisfied: numpy==2.3.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 32)) (2.3.0)\n",
      "Requirement already satisfied: packaging==25.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 33)) (25.0)\n",
      "Requirement already satisfied: pandas==2.3.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 34)) (2.3.0)\n",
      "Requirement already satisfied: pillow==11.3.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 35)) (11.3.0)\n",
      "Requirement already satisfied: praw==7.8.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 36)) (7.8.1)\n",
      "Requirement already satisfied: prawcore==2.4.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 37)) (2.4.0)\n",
      "Requirement already satisfied: propcache==0.3.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 38)) (0.3.2)\n",
      "Requirement already satisfied: psutil==7.0.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 39)) (7.0.0)\n",
      "Requirement already satisfied: pyarrow==20.0.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 40)) (20.0.0)\n",
      "Requirement already satisfied: pyparsing==3.2.3 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 41)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 42)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 43)) (2025.2)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 44)) (6.0.2)\n",
      "Requirement already satisfied: regex==2024.11.6 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 45)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.4 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 46)) (2.32.4)\n",
      "Requirement already satisfied: safetensors==0.5.3 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 47)) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.7.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 48)) (1.7.0)\n",
      "Requirement already satisfied: scipy==1.16.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 49)) (1.16.0)\n",
      "Requirement already satisfied: seaborn==0.13.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 50)) (0.13.2)\n",
      "Requirement already satisfied: setuptools==80.9.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 51)) (80.9.0)\n",
      "Requirement already satisfied: six==1.17.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 52)) (1.17.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 53)) (1.13.1)\n",
      "Requirement already satisfied: textblob==0.19.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 54)) (0.19.0)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 55)) (3.6.0)\n",
      "Requirement already satisfied: tokenizers==0.20.3 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 56)) (0.20.3)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 57)) (2.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 58)) (4.67.1)\n",
      "Requirement already satisfied: transformers==4.45.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 59)) (4.45.2)\n",
      "Requirement already satisfied: typing_extensions==4.14.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 60)) (4.14.0)\n",
      "Requirement already satisfied: tzdata==2025.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 61)) (2025.2)\n",
      "Requirement already satisfied: update-checker==0.18.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 62)) (0.18.0)\n",
      "Requirement already satisfied: urllib3==2.5.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 63)) (2.5.0)\n",
      "Requirement already satisfied: vaderSentiment==3.3.2 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 64)) (3.3.2)\n",
      "Requirement already satisfied: websocket-client==1.8.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 65)) (1.8.0)\n",
      "Requirement already satisfied: xxhash==3.5.0 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 66)) (3.5.0)\n",
      "Requirement already satisfied: yarl==1.20.1 in c:\\users\\hankaixin\\desktop\\multitask\\env\\lib\\site-packages (from -r requirements.txt (line 67)) (1.20.1)\n",
      "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler==0.12.1 (from -r requirements.txt (line 11))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting contourpy==1.3.2 (from -r requirements.txt (line 10))\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting colorama==0.4.6 (from -r requirements.txt (line 9))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting click==8.2.1 (from -r requirements.txt (line 8))\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting charset-normalizer==3.4.2 (from -r requirements.txt (line 7))\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting certifi==2025.6.15 (from -r requirements.txt (line 6))\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting attrs==25.3.0 (from -r requirements.txt (line 5))\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosignal==1.4.0 (from -r requirements.txt (line 4))\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting yarl==1.20.1 (from -r requirements.txt (line 67))\n",
      "  Using cached yarl-1.20.1-cp312-cp312-win_amd64.whl.metadata (76 kB)\n",
      "Collecting multidict==6.6.3 (from -r requirements.txt (line 28))\n",
      "  Using cached multidict-6.6.3-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting aiohttp==3.12.13 (from -r requirements.txt (line 3))\n",
      "  Using cached aiohttp-3.12.13-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aiohappyeyeballs==2.6.1 (from -r requirements.txt (line 2))\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting numpy==2.3.0 (from -r requirements.txt (line 32))\n",
      "  Using cached numpy-2.3.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting accelerate==1.8.1 (from -r requirements.txt (line 1))\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested fsspec==2025.5.1\n",
      "    datasets 3.6.0 depends on fsspec<=2025.3.0 and >=2023.1.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install datasets==3.6.0 and fsspec==2025.5.1 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35c03fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment counts:\n",
      "sentiment\n",
      "Negative    52\n",
      "Positive    22\n",
      "Neutral     21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Emotion counts:\n",
      "emotion\n",
      "Anger         24\n",
      "No Emotion    22\n",
      "Sadness       21\n",
      "Surprise      12\n",
      "Joy           12\n",
      "Fear           4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('annotated_reddit_posts.csv')\n",
    "\n",
    "# Count unique values in 'sentiment' column\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print('Sentiment counts:')\n",
    "print(sentiment_counts)\n",
    "print('\\n')\n",
    "\n",
    "# Count unique values in 'emotion' column\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "print('Emotion counts:')\n",
    "print(emotion_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9baacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_content</th>\n",
       "      <th>original_text</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d8kzu3m</td>\n",
       "      <td>ya screw username really looking forward note ...</td>\n",
       "      <td>Ya this screws me over completely. I was reall...</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>0.464167</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5jd0fx</td>\n",
       "      <td>username samsung galaxy note7 still more user ...</td>\n",
       "      <td>Cancelled Samsung Galaxy Note7 still has more ...</td>\n",
       "      <td>post</td>\n",
       "      <td>95</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbfyq3r</td>\n",
       "      <td>traded note 7 s7 edge really hope samsung user...</td>\n",
       "      <td>I traded my Note 7 in for an S7 Edge.  I reall...</td>\n",
       "      <td>comment</td>\n",
       "      <td>9</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbgcdgl</td>\n",
       "      <td>reading username report battery design failed ...</td>\n",
       "      <td>From reading the independent report of why its...</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbftpbx</td>\n",
       "      <td>maybe phone unique username feature explode us...</td>\n",
       "      <td>Maybe the phone's unique exploding feature (or...</td>\n",
       "      <td>comment</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       text_content  \\\n",
       "0  d8kzu3m  ya screw username really looking forward note ...   \n",
       "1   5jd0fx  username samsung galaxy note7 still more user ...   \n",
       "2  dbfyq3r  traded note 7 s7 edge really hope samsung user...   \n",
       "3  dbgcdgl  reading username report battery design failed ...   \n",
       "4  dbftpbx  maybe phone unique username feature explode us...   \n",
       "\n",
       "                                       original_text     type  score  \\\n",
       "0  Ya this screws me over completely. I was reall...  comment      2   \n",
       "1  Cancelled Samsung Galaxy Note7 still has more ...     post     95   \n",
       "2  I traded my Note 7 in for an S7 Edge.  I reall...  comment      9   \n",
       "3  From reading the independent report of why its...  comment      2   \n",
       "4  Maybe the phone's unique exploding feature (or...  comment     -4   \n",
       "\n",
       "   subjectivity sentiment   emotion  \n",
       "0      0.464167  Negative   Sadness  \n",
       "1      0.500000   Neutral  Surprise  \n",
       "2      0.414286  Negative   Sadness  \n",
       "3      0.406250  Positive   Sadness  \n",
       "4      0.800000  Negative     Anger  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee3b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment oversampled counts:\n",
      "sentiment\n",
      "Neutral     52\n",
      "Positive    52\n",
      "Negative    52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emotion oversampled counts:\n",
      "emotion\n",
      "Sadness       24\n",
      "Anger         24\n",
      "No Emotion    24\n",
      "Joy           24\n",
      "Surprise      24\n",
      "Fear          24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def oversample_minority_classes(df, label_column):\n",
    "    # Find the size of the largest class\n",
    "    max_size = df[label_column].value_counts().max()\n",
    "    # List to hold oversampled DataFrames\n",
    "    frames = []\n",
    "    # Oversample each class\n",
    "    for class_label, group in df.groupby(label_column):\n",
    "        # Sample with replacement to match max_size\n",
    "        oversampled_group = group.sample(max_size, replace=True, random_state=42)\n",
    "        frames.append(oversampled_group)\n",
    "    # Concatenate all oversampled groups\n",
    "    oversampled_df = pd.concat(frames).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return oversampled_df\n",
    "\n",
    "\n",
    "# Oversample sentiment\n",
    "oversampled_sentiment = oversample_minority_classes(df, 'sentiment')\n",
    "print(\"Sentiment oversampled counts:\")\n",
    "print(oversampled_sentiment['sentiment'].value_counts())\n",
    "\n",
    "# Oversample emotion\n",
    "oversampled_emotion = oversample_minority_classes(df, 'emotion')\n",
    "print(\"\\nEmotion oversampled counts:\")\n",
    "print(oversampled_emotion['emotion'].value_counts())\n",
    "\n",
    "# Save to new CSVs if you want\n",
    "oversampled_sentiment.to_csv('data/oversampled_sentiment.csv', index=False)\n",
    "oversampled_emotion.to_csv('data/oversampled_emotion.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
