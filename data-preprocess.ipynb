{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab5aa14",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "# import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55971a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RedditTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Remove some emotion-related words from stopwords to preserve sentiment\n",
    "        emotion_words = {\n",
    "            'not', 'no', 'nor', 'but', 'however', 'although', 'though',\n",
    "            'very', 'really', 'quite', 'too', 'so', 'more', 'most',\n",
    "            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "            'some', 'such', 'only', 'own', 'same', 'than', 'too', 'very'\n",
    "        }\n",
    "        self.stop_words = self.stop_words - emotion_words\n",
    "        \n",
    "        print(f\"Initialized preprocessor with {len(self.stop_words)} stopwords\")\n",
    "    \n",
    "    def remove_urls(self, text):\n",
    "        # Remove http/https URLs\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove www URLs\n",
    "        text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        # Remove reddit links like /r/subreddit\n",
    "        text = re.sub(r'/r/[A-Za-z0-9_]+', '', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_mentions_hashtags(self, text):\n",
    "        # Remove @mentions\n",
    "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "        # Remove #hashtags but preserve the word (e.g., #BombsAway -> BombsAway)\n",
    "        text = re.sub(r'#([A-Za-z0-9_]+)', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        return text\n",
    "    \n",
    "    def handle_reddit_formatting(self, text):\n",
    "        # Remove markdown links [text](url)\n",
    "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "        # Remove reddit user references like u/username\n",
    "        text = re.sub(r'u/[A-Za-z0-9_]+', '', text)\n",
    "        # Remove markdown formatting **bold** and *italic*\n",
    "        text = re.sub(r'\\*\\*([^\\*]+)\\*\\*', r'\\1', text)\n",
    "        text = re.sub(r'\\*([^\\*]+)\\*', r'\\1', text)\n",
    "        # Remove quote markers\n",
    "        text = re.sub(r'^>', '', text, flags=re.MULTILINE)\n",
    "        return text\n",
    "    \n",
    "    def convert_emojis(self, text):\n",
    "        try:\n",
    "            import emoji\n",
    "            # Convert emojis to text\n",
    "            text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "            # Clean up the emoji text formatting\n",
    "            text = re.sub(r':[a-zA-Z_]+:', lambda m: m.group().replace('_', ' ').replace(':', ''), text)\n",
    "        except ImportError:\n",
    "            # If emoji package is not available, just return the text as is\n",
    "            pass\n",
    "        return text\n",
    "    \n",
    "    def clean_special_characters(self, text):\n",
    "        # Preserve ! and ? as they convey emotion\n",
    "        # First, protect exclamation and question marks\n",
    "        text = re.sub(r'!+', ' EXCLAMATION ', text)\n",
    "        text = re.sub(r'\\?+', ' QUESTION ', text)\n",
    "        \n",
    "        # Remove other punctuation except apostrophes (for contractions)\n",
    "        text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "        \n",
    "        # Restore exclamation and question marks\n",
    "        text = text.replace(' EXCLAMATION ', ' ! ')\n",
    "        text = text.replace(' QUESTION ', ' ? ')\n",
    "        \n",
    "        # Handle contractions by removing apostrophes after processing\n",
    "        text = re.sub(r\"'\", '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize_whitespace(self, text):\n",
    "        # Replace multiple whitespace with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        try:\n",
    "            words = word_tokenize(text.lower())\n",
    "            filtered_words = [word for word in words if word not in self.stop_words]\n",
    "            return ' '.join(filtered_words)\n",
    "        except:\n",
    "            # simple split if tokenization fails\n",
    "            words = text.lower().split()\n",
    "            filtered_words = [word for word in words if word not in self.stop_words]\n",
    "            return ' '.join(filtered_words)\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        try:\n",
    "            words = word_tokenize(text)\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            return ' '.join(lemmatized_words)\n",
    "        except:\n",
    "            # simple split if tokenization fails\n",
    "            words = text.split()\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "            return ' '.join(lemmatized_words)\n",
    "    \n",
    "    def anonymize_identifiers(self, text):\n",
    "        # Remove phone numbers\n",
    "        text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "        # Remove potential usernames (sequences of letters/numbers/underscores)\n",
    "        text = re.sub(r'\\b[A-Za-z0-9_]{8,}\\b', '[USERNAME]', text)\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        original_text = text\n",
    "        \n",
    "        # Step 1: Handle Reddit-specific formatting\n",
    "        text = self.handle_reddit_formatting(text)\n",
    "        \n",
    "        # Step 2: Remove URLs\n",
    "        text = self.remove_urls(text)\n",
    "        \n",
    "        # Step 3: Remove mentions and hashtags\n",
    "        text = self.remove_mentions_hashtags(text)\n",
    "        \n",
    "        # Step 4: Remove HTML tags\n",
    "        text = self.remove_html_tags(text)\n",
    "        \n",
    "        # Step 5: Convert emojis\n",
    "        text = self.convert_emojis(text)\n",
    "        \n",
    "        # Step 6: Anonymize identifiers\n",
    "        text = self.anonymize_identifiers(text)\n",
    "        \n",
    "        # Step 7: Clean special characters (preserve ! and ?)\n",
    "        text = self.clean_special_characters(text)\n",
    "        \n",
    "        # Step 8: Normalize whitespace\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        # Step 9: Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Step 10: Remove stopwords\n",
    "        text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Step 11: Lemmatize\n",
    "        text = self.lemmatize_text(text)\n",
    "        \n",
    "        # Final cleanup\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e858782",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load the JSON data\n",
    "    try:\n",
    "        with open('combined_y_labeled_data.json', 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Loaded {len(data)} Reddit posts/comments\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: combined_y_labeled_data.json not found!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = RedditTextPreprocessor()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"Starting text preprocessing...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed_texts = []\n",
    "    for i, text in enumerate(df['text_content']):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing {i+1}/{len(df)} texts...\")\n",
    "        \n",
    "        processed_text = preprocessor.preprocess_text(text)\n",
    "        processed_texts.append(processed_text)\n",
    "    \n",
    "    # Create final dataset\n",
    "    final_df = pd.DataFrame({\n",
    "        'id': df['id'],\n",
    "        'text_content': processed_texts,\n",
    "        'original_text': df['text_content'],\n",
    "        'type': df['type'],\n",
    "        'score': df['score'],\n",
    "        'subjectivity': df['subjectivity']\n",
    "    })\n",
    "    \n",
    "    # Remove entries where cleaned text is empty or too short\n",
    "    final_df = final_df[final_df['text_content'].str.len() >= 3]\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"Original dataset: {len(df)} entries\")\n",
    "    print(f\"Final dataset: {len(final_df)} entries\")\n",
    "    print(f\"Removed {len(df) - len(final_df)} entries with insufficient content\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv('cleaned_reddit_posts.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nCleaned dataset saved to: cleaned_reddit_posts.csv\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE PREPROCESSING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(3, len(final_df))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Original: {final_df.iloc[i]['original_text'][:150]}...\")\n",
    "        print(f\"Cleaned:  {final_df.iloc[i]['text_content'][:150]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reddit Posts Text Preprocessing\")\n",
    "\n",
    "# Run preprocessing\n",
    "cleaned_data = load_and_preprocess_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
