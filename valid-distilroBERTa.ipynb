{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ebee31",
   "metadata": {},
   "source": [
    "# General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e5dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Libraries imported and setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports for distilroberta Seed & Bootstrap Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import random\n",
    "from collections import Counter\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./distilroberta_seed_analysis_results\", exist_ok=True)\n",
    "os.makedirs(\"./distilroberta_trained_models_seeds\", exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fc10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Utility Functions for Analysis\n",
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee15564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: distilroberta Model Architectures\n",
    "class DistilrobertaSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load distilroberta model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.distilroberta = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.distilroberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get distilroberta outputs\n",
    "        outputs = self.distilroberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class DistilrobertaMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTwe   et model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.distilroberta = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.distilroberta.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.distilroberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        shared_pooled = shared_attended[:, 0, :]\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "        outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "        \n",
    "        # Emotion branch\n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "        emotion_logits = self.emotion_classifier(emotion_features)\n",
    "        outputs[\"emotion_logits\"] = emotion_logits\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"distilroberta model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84190c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Classes for distilroberta\n",
    "class DistilrobertaDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DistilrobertaMultiTaskDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], sentiment_labels: List[int], \n",
    "                 emotion_labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"distilroberta dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3feded95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modified data loading functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Modified Data Loading Functions for General Dataset Evaluation\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def load_external_datasets() -> Tuple[Dict, Dict]:\n",
    "    print(\"Loading external datasets...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        sentiment_data = {\n",
    "            'train': sst2_dataset['train'],\n",
    "            'validation': sst2_dataset['validation']\n",
    "        }\n",
    "        print(f\"SST-2 dataset loaded: {len(sentiment_data['train'])} train, {len(sentiment_data['validation'])} validation samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        emotion_data = {\n",
    "            'train': emotions_dataset['train'],\n",
    "            'validation': emotions_dataset['validation'],\n",
    "            'test': emotions_dataset['test']  # GoEmotions has a test split\n",
    "        }\n",
    "        print(f\"GoEmotions dataset loaded: {len(emotion_data['train'])} train, {len(emotion_data['validation'])} validation, {len(emotion_data['test'])} test samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def prepare_sst2_evaluation_data(sentiment_data: Dict, max_samples: int = 1000) -> Dict:\n",
    "    print(\"Preparing SST-2 evaluation data...\")\n",
    "    \n",
    "    # Use validation split for evaluation\n",
    "    eval_texts = sentiment_data['validation']['sentence'][:max_samples]\n",
    "    eval_labels_raw = sentiment_data['validation']['label'][:max_samples]\n",
    "    \n",
    "    # Convert SST-2 binary to 3-class sentiment (same as training)\n",
    "    eval_labels = []\n",
    "    for label in eval_labels_raw:\n",
    "        if label == 0:  # Negative\n",
    "            eval_labels.append(0)\n",
    "        elif label == 1:  # Positive\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral (same as training)\n",
    "                eval_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                eval_labels.append(2)  # Positive\n",
    "    \n",
    "    # Create encoder that matches training\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    sst2_eval_data = {\n",
    "        'texts': eval_texts,\n",
    "        'sentiment_labels': eval_labels,\n",
    "        'sentiment_encoder': sentiment_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ SST-2 evaluation data prepared: {len(sst2_eval_data['texts'])} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    \n",
    "    return sst2_eval_data\n",
    "\n",
    "def prepare_goemotions_evaluation_data(emotion_data: Dict, max_samples: int = 1000) -> Dict:\n",
    "    print(\"Preparing GoEmotions evaluation data...\")\n",
    "    \n",
    "    # Use test split for evaluation (or validation if test not available)\n",
    "    eval_split = emotion_data.get('test', emotion_data.get('validation'))\n",
    "    eval_texts_all = eval_split['text']\n",
    "    eval_labels_all = eval_split['labels']\n",
    "    \n",
    "    eval_texts = []\n",
    "    eval_labels = []\n",
    "    count = 0\n",
    "    \n",
    "    for i, label in enumerate(eval_labels_all):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):\n",
    "                eval_texts.append(eval_texts_all[i])\n",
    "                eval_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                eval_texts.append(eval_texts_all[i])\n",
    "                eval_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoder that matches training\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    goemotions_eval_data = {\n",
    "        'texts': eval_texts,\n",
    "        'emotion_labels': eval_labels,\n",
    "        'emotion_encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ GoEmotions evaluation data prepared: {len(goemotions_eval_data['texts'])} samples\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return goemotions_eval_data\n",
    "\n",
    "def prepare_multitask_evaluation_data(sst2_eval_data: Dict, goemotions_eval_data: Dict) -> Dict:\n",
    "    print(\"Preparing multitask evaluation data...\")\n",
    "    \n",
    "    # Take minimum length to ensure both tasks have same number of samples\n",
    "    min_length = min(len(sst2_eval_data['texts']), len(goemotions_eval_data['texts']))\n",
    "    \n",
    "    multitask_eval_data = {\n",
    "        'texts': sst2_eval_data['texts'][:min_length],\n",
    "        'sentiment_labels': sst2_eval_data['sentiment_labels'][:min_length],\n",
    "        'emotion_labels': goemotions_eval_data['emotion_labels'][:min_length],\n",
    "        'sentiment_encoder': sst2_eval_data['sentiment_encoder'],\n",
    "        'emotion_encoder': goemotions_eval_data['emotion_encoder']\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Multitask evaluation data prepared: {len(multitask_eval_data['texts'])} samples\")\n",
    "    \n",
    "    return multitask_eval_data\n",
    "\n",
    "def prepare_distilroberta_training_data(sentiment_data: Dict, emotion_data: Dict, max_samples: int = 3000) -> Dict:\n",
    "    print(\"Preparing distilroberta training data...\")\n",
    "    \n",
    "    # Process sentiment data (SST-2)\n",
    "    sentiment_texts = sentiment_data['train']['sentence'][:max_samples]\n",
    "    sentiment_labels = sentiment_data['train']['label'][:max_samples]\n",
    "    \n",
    "    # Process emotion data (filter to first 6 classes)\n",
    "    emotion_texts = []\n",
    "    emotion_labels = []\n",
    "    count = 0\n",
    "    \n",
    "    for i, label in enumerate(emotion_data['train']['labels']):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):  # Only use first 6 emotions\n",
    "                emotion_texts.append(emotion_data['train']['text'][i])\n",
    "                emotion_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                emotion_texts.append(emotion_data['train']['text'][i])\n",
    "                emotion_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoders\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # For SST-2: 0 = Negative, 1 = Positive\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Positive'])\n",
    "    \n",
    "    # For GoEmotions: First 6 emotions\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    training_data = {\n",
    "        'sentiment_data': {\n",
    "            'texts': sentiment_texts,\n",
    "            'labels': sentiment_labels,\n",
    "            'encoder': sentiment_encoder\n",
    "        },\n",
    "        'emotion_data': {\n",
    "            'texts': emotion_texts,\n",
    "            'labels': emotion_labels,\n",
    "            'encoder': emotion_encoder\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Training data prepared:\")\n",
    "    print(f\"   Sentiment: {len(sentiment_texts)} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion: {len(emotion_texts)} samples\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "print(\"✅ Modified data loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82afd1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta training functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: distilroberta Training Functions with Best Parameters\n",
    "def train_distilroberta_single_task(\n",
    "    task_type: str,  # 'sentiment' or 'emotion'\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 5000\n",
    ") -> Tuple[any, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training distilroberta {task_type} model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Get appropriate data\n",
    "    if task_type == 'sentiment':\n",
    "        texts = training_data['sentiment_data']['texts'][:max_samples]\n",
    "        labels = training_data['sentiment_data']['labels'][:max_samples]\n",
    "        encoder = training_data['sentiment_data']['encoder']\n",
    "        num_classes = 3\n",
    "    else:  # emotion\n",
    "        texts = training_data['emotion_data']['texts'][:max_samples]\n",
    "        labels = training_data['emotion_data']['labels'][:max_samples]\n",
    "        encoder = training_data['emotion_data']['encoder']\n",
    "        num_classes = 6\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DistilrobertaSingleTaskTransformer(\n",
    "        model_name='distilroberta-base',\n",
    "        num_classes=num_classes,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = DistilrobertaDataset(texts, labels, tokenizer, max_length=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs['logits'], labels_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./distilroberta_trained_models_seeds/distilroberta_{task_type}_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"distilroberta-base\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"task_type\": task_type,\n",
    "        \"model_type\": \"DistilrobertaSingleTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoder\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(encoder, os.path.join(output_dir, f'{task_type}_encoder.pkl'))\n",
    "    \n",
    "    print(f\"✅ distilroberta {task_type} model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "def train_distilroberta_multitask(\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 2000\n",
    ") -> Tuple[any, LabelEncoder, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training distilroberta multitask model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Prepare multitask data (combine sentiment and emotion data)\n",
    "    min_length = min(len(training_data['sentiment_data']['texts']), \n",
    "                     len(training_data['emotion_data']['texts']))\n",
    "    min_length = min(min_length, max_samples)\n",
    "    \n",
    "    combined_texts = training_data['sentiment_data']['texts'][:min_length]\n",
    "    combined_sentiment_labels = training_data['sentiment_data']['labels'][:min_length]\n",
    "    combined_emotion_labels = training_data['emotion_data']['labels'][:min_length]\n",
    "    \n",
    "    sentiment_encoder = training_data['sentiment_data']['encoder']\n",
    "    emotion_encoder = training_data['emotion_data']['encoder']\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DistilrobertaMultiTaskTransformer(\n",
    "        model_name='distilroberta-base',\n",
    "        sentiment_num_classes=3,\n",
    "        emotion_num_classes=6,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = DistilrobertaMultiTaskDataset(\n",
    "        combined_texts, combined_sentiment_labels, combined_emotion_labels, \n",
    "        tokenizer, max_length=128\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    emotion_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    alpha = best_params['alpha']\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = sentiment_criterion(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = emotion_criterion(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss_batch = alpha * sentiment_loss + (1 - alpha) * emotion_loss\n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./distilroberta_trained_models_seeds/distilroberta_multitask_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"distilroberta-base\",\n",
    "        \"sentiment_num_classes\": 3,\n",
    "        \"emotion_num_classes\": 6,\n",
    "        \"model_type\": \"DistilrobertaMultiTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoders\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    print(f\"distilroberta multitask model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, sentiment_encoder, emotion_encoder\n",
    "\n",
    "print(\"distilroberta training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d9d59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions for distilroberta Models\n",
    "def evaluate_distilroberta_single_task(model, tokenizer, label_encoder, reddit_data: Dict, task_type: str) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_labels = reddit_data[f'{task_type}_labels']\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 16):  # Batch size 16\n",
    "            batch_texts = texts[i:i+16]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs['logits']\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                pred_id = preds[j].item()\n",
    "                confidence = probs[j][pred_id].item()\n",
    "                \n",
    "                # Handle out of range predictions\n",
    "                if pred_id >= len(label_encoder.classes_):\n",
    "                    pred_id = 0\n",
    "                \n",
    "                predictions.append(pred_id)\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'predictions': predictions,\n",
    "        'confidences': confidences,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "def evaluate_distilroberta_multitask(model, tokenizer, sentiment_encoder, emotion_encoder, \n",
    "                               reddit_data: Dict, max_length: int = 128) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_sentiment_labels = reddit_data['sentiment_labels']\n",
    "    true_emotion_labels = reddit_data['emotion_labels']\n",
    "    \n",
    "    sentiment_predictions = []\n",
    "    emotion_predictions = []\n",
    "    sentiment_confidences = []\n",
    "    emotion_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 8):  # Smaller batch size for multitask\n",
    "            batch_texts = texts[i:i+8]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Process sentiment\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "            \n",
    "            # Process emotion\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                # Sentiment\n",
    "                sent_id = sentiment_preds[j].item()\n",
    "                sent_conf = sentiment_probs[j][sent_id].item()\n",
    "                if sent_id >= len(sentiment_encoder.classes_):\n",
    "                    sent_id = 0\n",
    "                sentiment_predictions.append(sent_id)\n",
    "                sentiment_confidences.append(sent_conf)\n",
    "                \n",
    "                # Emotion\n",
    "                emot_id = emotion_preds[j].item()\n",
    "                emot_conf = emotion_probs[j][emot_id].item()\n",
    "                if emot_id >= len(emotion_encoder.classes_):\n",
    "                    emot_id = 0\n",
    "                emotion_predictions.append(emot_id)\n",
    "                emotion_confidences.append(emot_conf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sentiment_accuracy = accuracy_score(true_sentiment_labels, sentiment_predictions)\n",
    "    sentiment_f1 = f1_score(true_sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    emotion_accuracy = accuracy_score(true_emotion_labels, emotion_predictions)\n",
    "    emotion_f1 = f1_score(true_emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'sentiment': {\n",
    "            'accuracy': sentiment_accuracy,\n",
    "            'macro_f1': sentiment_f1,\n",
    "            'predictions': sentiment_predictions,\n",
    "            'confidences': sentiment_confidences\n",
    "        },\n",
    "        'emotion': {\n",
    "            'accuracy': emotion_accuracy,\n",
    "            'macro_f1': emotion_f1,\n",
    "            'predictions': emotion_predictions,\n",
    "            'confidences': emotion_confidences\n",
    "        },\n",
    "        'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "        'combined_f1': (sentiment_f1 + emotion_f1) / 2\n",
    "    }\n",
    "\n",
    "print(\"distilroberta evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1097d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modified distilroberta random seed analysis function defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Modified distilroberta Random Seed Analysis Function\n",
    "def run_distilroberta_seed_analysis(\n",
    "    seeds: List[int] = [42, 123, 456, 789, 999],\n",
    "    max_training_samples: int = 3000,\n",
    "    max_eval_samples: int = 1000\n",
    "):\n",
    "    \n",
    "    print(\"🎲 STARTING distilroberta RANDOM SEED ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Seeds to test: {seeds}\")\n",
    "    print(f\"Max training samples per dataset: {max_training_samples}\")\n",
    "    print(f\"Max evaluation samples per dataset: {max_eval_samples}\")\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(\"\\n🔄 Preparing distilroberta training data...\")\n",
    "    training_data = prepare_distilroberta_training_data(sentiment_data, emotion_data, max_training_samples)\n",
    "    \n",
    "    # Prepare evaluation data\n",
    "    print(\"\\n📂 Preparing evaluation datasets...\")\n",
    "    sst2_eval_data = prepare_sst2_evaluation_data(sentiment_data, max_eval_samples)\n",
    "    goemotions_eval_data = prepare_goemotions_evaluation_data(emotion_data, max_eval_samples)\n",
    "    multitask_eval_data = prepare_multitask_evaluation_data(sst2_eval_data, goemotions_eval_data)\n",
    "    \n",
    "    # Define best parameters for each distilroberta model\n",
    "    best_params = {\n",
    "        'sentiment': {\n",
    "            'learning_rate': 5.262490902114904e-05,\n",
    "            'batch_size': 16,\n",
    "            'num_epochs': 6,\n",
    "            'warmup_ratio': 0.18324426408004219,\n",
    "            'weight_decay': 0.029110519961044856,\n",
    "            'hidden_dropout_prob': 0.1363649934414201,\n",
    "            'classifier_dropout': 0.13668090197068677\n",
    "        },\n",
    "        'emotion': {\n",
    "            'learning_rate': 4.166863122305896e-05,\n",
    "            'batch_size': 16,\n",
    "            'num_epochs': 5,\n",
    "            'warmup_ratio': 0.15924145688620425,\n",
    "            'weight_decay': 0.014180537144799797,\n",
    "            'hidden_dropout_prob': 0.22150897038028766,\n",
    "            'classifier_dropout': 0.1341048247374583\n",
    "        },\n",
    "        'multitask': {\n",
    "            'learning_rate': 6.251028636335231e-05,\n",
    "            'batch_size': 32,\n",
    "            'warmup_ratio': 0.12123391106782762,\n",
    "            'weight_decay': 0.02636424704863906,\n",
    "            'hidden_dropout_prob': 0.13668090197068677,\n",
    "            'classifier_dropout': 0.16084844859190756,\n",
    "            'alpha': 0.5049512863264476\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    # Store results for each seed\n",
    "    all_results = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n🌱 TRAINING AND EVALUATING distilroberta WITH SEED {seed}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        seed_results = {}\n",
    "        \n",
    "        # 1. Train and evaluate distilroberta Sentiment on SST-2\n",
    "        print(f\"\\n1️⃣ distilroberta Sentiment on SST-2 (Seed {seed})\")\n",
    "        model, encoder = train_distilroberta_single_task(\n",
    "            'sentiment', best_params['sentiment'], seed, \n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./distilroberta_trained_models_seeds/distilroberta_sentiment_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on SST-2 validation set\n",
    "        results = evaluate_distilroberta_single_task(model, tokenizer, encoder, sst2_eval_data, 'sentiment')\n",
    "        seed_results['distilroberta_sentiment'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 2. Train and evaluate distilroberta Emotion on GoEmotions\n",
    "        print(f\"\\n2️⃣ distilroberta Emotion on GoEmotions (Seed {seed})\")\n",
    "        model, encoder = train_distilroberta_single_task(\n",
    "            'emotion', best_params['emotion'], seed,\n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./distilroberta_trained_models_seeds/distilroberta_emotion_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on GoEmotions test set\n",
    "        results = evaluate_distilroberta_single_task(model, tokenizer, encoder, goemotions_eval_data, 'emotion')\n",
    "        seed_results['distilroberta_emotion'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 3. Train and evaluate distilroberta Multitask on both datasets\n",
    "        print(f\"\\n3️⃣ distilroberta Multitask on SST-2 + GoEmotions (Seed {seed})\")\n",
    "        model, sent_enc, emot_enc = train_distilroberta_multitask(\n",
    "            best_params['multitask'], seed, training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./distilroberta_trained_models_seeds/distilroberta_multitask_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on combined test sets\n",
    "        results = evaluate_distilroberta_multitask(\n",
    "            model, tokenizer, sent_enc, emot_enc, multitask_eval_data, 128\n",
    "        )\n",
    "        seed_results['distilroberta_multitask'] = results\n",
    "        print(f\"   Sentiment - Accuracy: {results['sentiment']['accuracy']:.4f}, F1: {results['sentiment']['macro_f1']:.4f}\")\n",
    "        print(f\"   Emotion - Accuracy: {results['emotion']['accuracy']:.4f}, F1: {results['emotion']['macro_f1']:.4f}\")\n",
    "        print(f\"   Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        all_results[seed] = seed_results\n",
    "        \n",
    "        print(f\"\\n✅ Completed evaluation for seed {seed}\")\n",
    "    \n",
    "    # Analyze stability across seeds\n",
    "    print(f\"\\n📊 ANALYZING distilroberta STABILITY ACROSS SEEDS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    stability_analysis = analyze_distilroberta_seed_stability(all_results, seeds)\n",
    "    \n",
    "    # Save results\n",
    "    save_distilroberta_results(all_results, stability_analysis, seeds)\n",
    "    \n",
    "    return all_results, stability_analysis\n",
    "\n",
    "print(\"✅ Modified distilroberta random seed analysis function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c3b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_distilroberta_multitask stability analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: distilroberta Stability Analysis Functions\n",
    "def analyze_distilroberta_seed_stability(all_results: Dict, seeds: List[int]) -> Dict:\n",
    "    \n",
    "    stability_stats = {}\n",
    "    \n",
    "    # Define model-task combinations\n",
    "    evaluations = [\n",
    "        ('distilroberta_sentiment', 'sentiment'),\n",
    "        ('distilroberta_emotion', 'emotion'),\n",
    "        ('distilroberta_multitask', 'sentiment'),\n",
    "        ('distilroberta_multitask', 'emotion')\n",
    "    ]\n",
    "    \n",
    "    for model_name, task in evaluations:\n",
    "        print(f\"\\n🔍 {model_name.upper()} - {task.upper()}\")\n",
    "        \n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            if model_name in all_results[seed]:\n",
    "                result = all_results[seed][model_name]\n",
    "                \n",
    "                if model_name.endswith('_multitask'):\n",
    "                    acc = result[task]['accuracy']\n",
    "                    f1 = result[task]['macro_f1']\n",
    "                else:\n",
    "                    acc = result['accuracy']\n",
    "                    f1 = result['macro_f1']\n",
    "                \n",
    "                accuracies.append(acc)\n",
    "                f1_scores.append(f1)\n",
    "        \n",
    "        if accuracies:\n",
    "            acc_mean = np.mean(accuracies)\n",
    "            acc_std = np.std(accuracies)\n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            \n",
    "            stability_stats[f\"{model_name}_{task}\"] = {\n",
    "                'accuracy_mean': acc_mean,\n",
    "                'accuracy_std': acc_std,\n",
    "                'f1_mean': f1_mean,\n",
    "                'f1_std': f1_std,\n",
    "                'accuracy_values': accuracies,\n",
    "                'f1_values': f1_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "            print(f\"   Macro F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    return stability_stats\n",
    "\n",
    "def save_distilroberta_results(all_results: Dict, stability_analysis: Dict, seeds: List[int]):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = f\"./distilroberta_seed_analysis_results/distilroberta_raw_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        serializable_results = {}\n",
    "        for seed, seed_results in all_results.items():\n",
    "            serializable_results[str(seed)] = {}\n",
    "            for model, results in seed_results.items():\n",
    "                if isinstance(results, dict):\n",
    "                    serializable_results[str(seed)][model] = {}\n",
    "                    for key, value in results.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            serializable_results[str(seed)][model][key] = {\n",
    "                                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                                   [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in v] if isinstance(v, list) else v\n",
    "                                for k, v in value.items()\n",
    "                            }\n",
    "                        else:\n",
    "                            serializable_results[str(seed)][model][key] = float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # Save stability analysis\n",
    "    stability_file = f\"./distilroberta_seed_analysis_results/distilroberta_stability_analysis_{timestamp}.json\"\n",
    "    with open(stability_file, 'w') as f:\n",
    "        serializable_stability = {}\n",
    "        for key, stats in stability_analysis.items():\n",
    "            serializable_stability[key] = {\n",
    "                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                   [float(x) for x in v] if isinstance(v, list) else v\n",
    "                for k, v in stats.items()\n",
    "            }\n",
    "        json.dump(serializable_stability, f, indent=2)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = f\"./distilroberta_seed_analysis_results/distilroberta_summary_report_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"distilroberta RANDOM SEED ANALYSIS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Seeds tested: {seeds}\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"STABILITY ANALYSIS (Mean ± Std)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        for key, stats in stability_analysis.items():\n",
    "            model_task = key.replace('_', ' ').title()\n",
    "            f.write(f\"\\n{model_task}:\\n\")\n",
    "            f.write(f\"  Accuracy: {stats['accuracy_mean']:.4f} ± {stats['accuracy_std']:.4f}\\n\")\n",
    "            f.write(f\"  Macro F1: {stats['f1_mean']:.4f} ± {stats['f1_std']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nBest Performers (by mean F1 score):\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Find best performers\n",
    "        sentiment_best = max([k for k in stability_analysis.keys() if 'sentiment' in k], \n",
    "                           key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        emotion_best = max([k for k in stability_analysis.keys() if 'emotion' in k], \n",
    "                         key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        \n",
    "        f.write(f\"Sentiment: {sentiment_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[sentiment_best]['f1_mean']:.4f})\\n\")\n",
    "        f.write(f\"Emotion: {emotion_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[emotion_best]['f1_mean']:.4f})\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 distilroberta results saved:\")\n",
    "    print(f\"   Raw results: {results_file}\")\n",
    "    print(f\"   Stability analysis: {stability_file}\")\n",
    "    print(f\"   Summary report: {summary_file}\")\n",
    "\n",
    "print(\"train_distilroberta_multitask stability analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1784c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 STARTING distilroberta RANDOM SEED ANALYSIS\n",
      "======================================================================\n",
      "Seeds to test: [42, 123, 456, 789, 999]\n",
      "Max training samples per dataset: 3000\n",
      "Max evaluation samples per dataset: 1000\n",
      "\n",
      "📂 Loading external datasets...\n",
      "Loading external datasets...\n",
      "SST-2 dataset loaded: 67349 train, 872 validation samples\n",
      "GoEmotions dataset loaded: 43410 train, 5426 validation, 5427 test samples\n",
      "\n",
      "🔄 Preparing distilroberta training data...\n",
      "Preparing distilroberta training data...\n",
      "✅ Training data prepared:\n",
      "   Sentiment: 3000 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Positive')]\n",
      "   Emotion: 3000 samples\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "📂 Preparing evaluation datasets...\n",
      "Preparing SST-2 evaluation data...\n",
      "✅ SST-2 evaluation data prepared: 872 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "Preparing GoEmotions evaluation data...\n",
      "✅ GoEmotions evaluation data prepared: 1000 samples\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "Preparing multitask evaluation data...\n",
      "✅ Multitask evaluation data prepared: 872 samples\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 42\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment on SST-2 (Seed 42)\n",
      "🚀 Training distilroberta sentiment model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6165\n",
      "Epoch 2/3, Average Loss: 0.2808\n",
      "Epoch 3/3, Average Loss: 0.1347\n",
      "✅ distilroberta sentiment model trained and saved with seed 42\n",
      "   Accuracy: 0.5057, Macro F1: 0.3803\n",
      "\n",
      "2️⃣ distilroberta Emotion on GoEmotions (Seed 42)\n",
      "🚀 Training distilroberta emotion model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3988\n",
      "Epoch 2/3, Average Loss: 0.7659\n",
      "Epoch 3/3, Average Loss: 0.6074\n",
      "✅ distilroberta emotion model trained and saved with seed 42\n",
      "   Accuracy: 0.7370, Macro F1: 0.7038\n",
      "\n",
      "3️⃣ distilroberta Multitask on SST-2 + GoEmotions (Seed 42)\n",
      "🚀 Training distilroberta multitask model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.2885\n",
      "Epoch 2/3, Average Loss: 1.0494\n",
      "Epoch 3/3, Average Loss: 0.9663\n",
      "distilroberta multitask model trained and saved with seed 42\n",
      "   Sentiment - Accuracy: 0.5138, F1: 0.3854\n",
      "   Emotion - Accuracy: 0.2466, F1: 0.1016\n",
      "   Combined - Accuracy: 0.3802, F1: 0.2435\n",
      "\n",
      "✅ Completed evaluation for seed 42\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 123\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment on SST-2 (Seed 123)\n",
      "🚀 Training distilroberta sentiment model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6272\n",
      "Epoch 2/3, Average Loss: 0.2724\n",
      "Epoch 3/3, Average Loss: 0.1381\n",
      "✅ distilroberta sentiment model trained and saved with seed 123\n",
      "   Accuracy: 0.4931, Macro F1: 0.3767\n",
      "\n",
      "2️⃣ distilroberta Emotion on GoEmotions (Seed 123)\n",
      "🚀 Training distilroberta emotion model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.4199\n",
      "Epoch 2/3, Average Loss: 0.7823\n",
      "Epoch 3/3, Average Loss: 0.6132\n",
      "✅ distilroberta emotion model trained and saved with seed 123\n",
      "   Accuracy: 0.7280, Macro F1: 0.6921\n",
      "\n",
      "3️⃣ distilroberta Multitask on SST-2 + GoEmotions (Seed 123)\n",
      "🚀 Training distilroberta multitask model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3444\n",
      "Epoch 2/3, Average Loss: 1.0491\n",
      "Epoch 3/3, Average Loss: 0.9619\n",
      "distilroberta multitask model trained and saved with seed 123\n",
      "   Sentiment - Accuracy: 0.4989, F1: 0.3792\n",
      "   Emotion - Accuracy: 0.3119, F1: 0.0793\n",
      "   Combined - Accuracy: 0.4054, F1: 0.2292\n",
      "\n",
      "✅ Completed evaluation for seed 123\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 456\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment on SST-2 (Seed 456)\n",
      "🚀 Training distilroberta sentiment model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.5701\n",
      "Epoch 2/3, Average Loss: 0.2571\n",
      "Epoch 3/3, Average Loss: 0.1379\n",
      "✅ distilroberta sentiment model trained and saved with seed 456\n",
      "   Accuracy: 0.4989, Macro F1: 0.3781\n",
      "\n",
      "2️⃣ distilroberta Emotion on GoEmotions (Seed 456)\n",
      "🚀 Training distilroberta emotion model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3788\n",
      "Epoch 2/3, Average Loss: 0.7774\n",
      "Epoch 3/3, Average Loss: 0.6053\n",
      "✅ distilroberta emotion model trained and saved with seed 456\n",
      "   Accuracy: 0.7360, Macro F1: 0.7060\n",
      "\n",
      "3️⃣ distilroberta Multitask on SST-2 + GoEmotions (Seed 456)\n",
      "🚀 Training distilroberta multitask model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3398\n",
      "Epoch 2/3, Average Loss: 1.0783\n",
      "Epoch 3/3, Average Loss: 0.9822\n",
      "distilroberta multitask model trained and saved with seed 456\n",
      "   Sentiment - Accuracy: 0.5161, F1: 0.3882\n",
      "   Emotion - Accuracy: 0.3131, F1: 0.0795\n",
      "   Combined - Accuracy: 0.4146, F1: 0.2338\n",
      "\n",
      "✅ Completed evaluation for seed 456\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 789\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment on SST-2 (Seed 789)\n",
      "🚀 Training distilroberta sentiment model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.5848\n",
      "Epoch 2/3, Average Loss: 0.2749\n",
      "Epoch 3/3, Average Loss: 0.1374\n",
      "✅ distilroberta sentiment model trained and saved with seed 789\n",
      "   Accuracy: 0.5149, Macro F1: 0.3873\n",
      "\n",
      "2️⃣ distilroberta Emotion on GoEmotions (Seed 789)\n",
      "🚀 Training distilroberta emotion model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3661\n",
      "Epoch 2/3, Average Loss: 0.7679\n",
      "Epoch 3/3, Average Loss: 0.6051\n",
      "✅ distilroberta emotion model trained and saved with seed 789\n",
      "   Accuracy: 0.7130, Macro F1: 0.6791\n",
      "\n",
      "3️⃣ distilroberta Multitask on SST-2 + GoEmotions (Seed 789)\n",
      "🚀 Training distilroberta multitask model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3179\n",
      "Epoch 2/3, Average Loss: 1.0545\n",
      "Epoch 3/3, Average Loss: 0.9536\n",
      "distilroberta multitask model trained and saved with seed 789\n",
      "   Sentiment - Accuracy: 0.5046, F1: 0.3798\n",
      "   Emotion - Accuracy: 0.2706, F1: 0.1015\n",
      "   Combined - Accuracy: 0.3876, F1: 0.2406\n",
      "\n",
      "✅ Completed evaluation for seed 789\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 999\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment on SST-2 (Seed 999)\n",
      "🚀 Training distilroberta sentiment model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.5706\n",
      "Epoch 2/3, Average Loss: 0.2808\n",
      "Epoch 3/3, Average Loss: 0.1463\n",
      "✅ distilroberta sentiment model trained and saved with seed 999\n",
      "   Accuracy: 0.5069, Macro F1: 0.3823\n",
      "\n",
      "2️⃣ distilroberta Emotion on GoEmotions (Seed 999)\n",
      "🚀 Training distilroberta emotion model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3646\n",
      "Epoch 2/3, Average Loss: 0.7808\n",
      "Epoch 3/3, Average Loss: 0.6108\n",
      "✅ distilroberta emotion model trained and saved with seed 999\n",
      "   Accuracy: 0.7420, Macro F1: 0.7108\n",
      "\n",
      "3️⃣ distilroberta Multitask on SST-2 + GoEmotions (Seed 999)\n",
      "🚀 Training distilroberta multitask model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.2679\n",
      "Epoch 2/3, Average Loss: 1.0607\n",
      "Epoch 3/3, Average Loss: 0.9485\n",
      "distilroberta multitask model trained and saved with seed 999\n",
      "   Sentiment - Accuracy: 0.5011, F1: 0.3803\n",
      "   Emotion - Accuracy: 0.2580, F1: 0.1072\n",
      "   Combined - Accuracy: 0.3796, F1: 0.2437\n",
      "\n",
      "✅ Completed evaluation for seed 999\n",
      "\n",
      "📊 ANALYZING distilroberta STABILITY ACROSS SEEDS\n",
      "======================================================================\n",
      "\n",
      "🔍 DISTILROBERTA_SENTIMENT - SENTIMENT\n",
      "   Accuracy: 0.5039 ± 0.0074\n",
      "   Macro F1: 0.3810 ± 0.0037\n",
      "\n",
      "🔍 DISTILROBERTA_EMOTION - EMOTION\n",
      "   Accuracy: 0.7312 ± 0.0101\n",
      "   Macro F1: 0.6984 ± 0.0114\n",
      "\n",
      "🔍 DISTILROBERTA_MULTITASK - SENTIMENT\n",
      "   Accuracy: 0.5069 ± 0.0068\n",
      "   Macro F1: 0.3826 ± 0.0036\n",
      "\n",
      "🔍 DISTILROBERTA_MULTITASK - EMOTION\n",
      "   Accuracy: 0.2800 ± 0.0276\n",
      "   Macro F1: 0.0938 ± 0.0120\n",
      "\n",
      "💾 distilroberta results saved:\n",
      "   Raw results: ./distilroberta_seed_analysis_results/distilroberta_raw_results_20250725_001207.json\n",
      "   Stability analysis: ./distilroberta_seed_analysis_results/distilroberta_stability_analysis_20250725_001207.json\n",
      "   Summary report: ./distilroberta_seed_analysis_results/distilroberta_summary_report_20250725_001207.txt\n",
      "\n",
      "DISTILROBERTA RANDOM SEED ANALYSIS COMPLETED!\n",
      "============================================================\n",
      "Check the './distilroberta_seed_analysis_results/' directory for detailed results.\n",
      "\n",
      "📊 QUICK STABILITY SUMMARY:\n",
      "----------------------------------------\n",
      "\n",
      "Distilroberta Sentiment Sentiment:\n",
      "  Accuracy: 0.504 ± 0.007\n",
      "  F1 Score: 0.381 ± 0.004\n",
      "\n",
      "Distilroberta Emotion Emotion:\n",
      "  Accuracy: 0.731 ± 0.010\n",
      "  F1 Score: 0.698 ± 0.011\n",
      "\n",
      "Distilroberta Multitask Sentiment:\n",
      "  Accuracy: 0.507 ± 0.007\n",
      "  F1 Score: 0.383 ± 0.004\n",
      "\n",
      "Distilroberta Multitask Emotion:\n",
      "  Accuracy: 0.280 ± 0.028\n",
      "  F1 Score: 0.094 ± 0.012\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Run distilroberta Random Seed Analysis (Fixed)\n",
    "\n",
    "# Clear any previous results\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Run distilroberta random seed analysis with the modified function signature\n",
    "try:\n",
    "    all_results, stability_analysis = run_distilroberta_seed_analysis(\n",
    "        seeds=[42, 123, 456, 789, 999],  # 5 different seeds\n",
    "        max_training_samples=3000,  # Reduced for faster training\n",
    "        max_eval_samples=1000  # Max evaluation samples per dataset\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDISTILROBERTA RANDOM SEED ANALYSIS COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Check the './distilroberta_seed_analysis_results/' directory for detailed results.\")\n",
    "    \n",
    "    # Display quick summary\n",
    "    print(\"\\n📊 QUICK STABILITY SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Updated to match the actual structure of stability_analysis\n",
    "    for key, stats in stability_analysis.items():\n",
    "        model_task = key.replace('_', ' ').title()\n",
    "        print(f\"\\n{model_task}:\")\n",
    "        print(f\"  Accuracy: {stats['accuracy_mean']:.3f} ± {stats['accuracy_std']:.3f}\")\n",
    "        print(f\"  F1 Score: {stats['f1_mean']:.3f} ± {stats['f1_std']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during analysis: {str(e)}\")\n",
    "    print(\"🔧 Try restarting the kernel and running cells 1-9 again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b729e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta bootstrap analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: distilroberta Bootstrap Analysis Functions\n",
    "def load_distilroberta_model_for_bootstrap(model_path: str, model_type: str):\n",
    "    print(f\"📥 Loading distilroberta {model_type} model from {model_path}...\")\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_path, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        # Load multitask model\n",
    "        model = DistilrobertaMultiTaskTransformer(\n",
    "            model_name=\"distilroberta-base\",\n",
    "            sentiment_num_classes=config['sentiment_num_classes'],\n",
    "            emotion_num_classes=config['emotion_num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoders\n",
    "        sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "        emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, sentiment_encoder, emotion_encoder\n",
    "        \n",
    "    else:\n",
    "        # Load single-task model\n",
    "        model = DistilrobertaSingleTaskTransformer(\n",
    "            model_name=\"distilroberta-base\",\n",
    "            num_classes=config['num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoder\n",
    "        encoder = joblib.load(os.path.join(model_path, f'{config[\"task_type\"]}_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, encoder\n",
    "\n",
    "def evaluate_distilroberta_on_bootstrap_sample(model, tokenizer, texts, sentiment_labels, emotion_labels, \n",
    "                                        model_sentiment_encoder, model_emotion_encoder, \n",
    "                                        data_sentiment_encoder, data_emotion_encoder, \n",
    "                                        model_type=\"multitask\", max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 8):\n",
    "                batch_texts = texts[i:i+8]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': inputs['input_ids'].to(device),\n",
    "                    'attention_mask': inputs['attention_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                outputs = model(**filtered_inputs)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                for j in range(len(batch_texts)):\n",
    "                    sent_id = sentiment_preds[j].item()\n",
    "                    emot_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    if sent_id >= len(model_sentiment_encoder.classes_):\n",
    "                        sent_id = 0\n",
    "                    if emot_id >= len(model_emotion_encoder.classes_):\n",
    "                        emot_id = 0\n",
    "                    \n",
    "                    sentiment_predictions.append(sent_id)\n",
    "                    emotion_predictions.append(emot_id)\n",
    "        \n",
    "        # Map predictions to data label space\n",
    "        mapped_sentiment_preds = []\n",
    "        mapped_emotion_preds = []\n",
    "        \n",
    "        for sent_pred, emot_pred in zip(sentiment_predictions, emotion_predictions):\n",
    "            sent_class = model_sentiment_encoder.classes_[sent_pred]\n",
    "            emot_class = model_emotion_encoder.classes_[emot_pred]\n",
    "            \n",
    "            try:\n",
    "                mapped_sent = data_sentiment_encoder.transform([sent_class])[0]\n",
    "                mapped_emot = data_emotion_encoder.transform([emot_class])[0]\n",
    "            except ValueError:\n",
    "                mapped_sent = 0\n",
    "                mapped_emot = 0\n",
    "            \n",
    "            mapped_sentiment_preds.append(mapped_sent)\n",
    "            mapped_emotion_preds.append(mapped_emot)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, mapped_sentiment_preds)\n",
    "        sentiment_f1 = f1_score(sentiment_labels, mapped_sentiment_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        emotion_accuracy = accuracy_score(emotion_labels, mapped_emotion_preds)\n",
    "        emotion_f1 = f1_score(emotion_labels, mapped_emotion_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1': emotion_f1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # Single task evaluation logic here\n",
    "        pass\n",
    "\n",
    "def bootstrap_evaluation_distilroberta(model, tokenizer, data, model_sentiment_encoder, model_emotion_encoder,\n",
    "                                data_sentiment_encoder, data_emotion_encoder, \n",
    "                                n_iterations=1000, sample_size=95):\n",
    "    print(f\"🔄 Starting distilroberta bootstrap evaluation...\")\n",
    "    print(f\"   Iterations: {n_iterations}\")\n",
    "    print(f\"   Sample size: {sample_size}\")\n",
    "    \n",
    "    results = {\n",
    "        'sentiment_accuracy': [],\n",
    "        'sentiment_f1': [],\n",
    "        'emotion_accuracy': [],\n",
    "        'emotion_f1': []\n",
    "    }\n",
    "    \n",
    "    texts = data['texts']\n",
    "    sentiment_labels = data['sentiment_labels']\n",
    "    emotion_labels = data['emotion_labels']\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Bootstrap sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
    "        \n",
    "        sample_texts = [texts[idx] for idx in indices]\n",
    "        sample_sentiment_labels = [sentiment_labels[idx] for idx in indices]\n",
    "        sample_emotion_labels = [emotion_labels[idx] for idx in indices]\n",
    "        \n",
    "        # Evaluate on bootstrap sample\n",
    "        metrics = evaluate_distilroberta_on_bootstrap_sample(\n",
    "            model, tokenizer, sample_texts, sample_sentiment_labels, sample_emotion_labels,\n",
    "            model_sentiment_encoder, model_emotion_encoder,\n",
    "            data_sentiment_encoder, data_emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results['sentiment_accuracy'].append(metrics['sentiment_accuracy'])\n",
    "        results['sentiment_f1'].append(metrics['sentiment_f1'])\n",
    "        results['emotion_accuracy'].append(metrics['emotion_accuracy'])\n",
    "        results['emotion_f1'].append(metrics['emotion_f1'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"distilroberta bootstrap analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e95a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_distilroberta_bootstrap_analysis():\n",
    "    print(\"🚀 Running distilroberta Bootstrap Analysis on General Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load external datasets\n",
    "    print(\"\\n📂 Loading general evaluation datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # 2. Prepare evaluation data\n",
    "    print(\"Preparing SST-2 evaluation data...\")\n",
    "    sst2_eval_data = prepare_sst2_evaluation_data(sentiment_data)\n",
    "    print(\"Preparing GoEmotions evaluation data...\")\n",
    "    goemotions_eval_data = prepare_goemotions_evaluation_data(emotion_data)\n",
    "    print(\"Preparing multitask evaluation data...\")\n",
    "    multitask_eval_data = prepare_multitask_evaluation_data(sst2_eval_data, goemotions_eval_data)\n",
    "    \n",
    "    # 3. Load models\n",
    "    # Load single task models\n",
    "    sentiment_model_path = \"./distilroberta_trained_models_seeds/distilroberta_sentiment_seed_42\"\n",
    "    sentiment_model, sentiment_tokenizer, sentiment_encoder = load_distilroberta_model_for_bootstrap(\n",
    "        sentiment_model_path, \"sentiment\"\n",
    "    )\n",
    "    \n",
    "    emotion_model_path = \"./distilroberta_trained_models_seeds/distilroberta_emotion_seed_42\"\n",
    "    emotion_model, emotion_tokenizer, emotion_encoder = load_distilroberta_model_for_bootstrap(\n",
    "        emotion_model_path, \"emotion\"\n",
    "    )\n",
    "    \n",
    "    # Load multitask model\n",
    "    multitask_model_path = \"./distilroberta_trained_models_seeds/distilroberta_multitask_seed_42\"\n",
    "    multitask_model, multitask_tokenizer, multitask_sent_encoder, multitask_emot_encoder = load_distilroberta_model_for_bootstrap(\n",
    "        multitask_model_path, \"multitask\"\n",
    "    )\n",
    "    \n",
    "    # 4. Run bootstrap evaluation\n",
    "    print(\"\\n🔄 Starting bootstrap evaluation...\")\n",
    "    n_iterations = 1000\n",
    "    sample_size = 95\n",
    "    \n",
    "    # Initialize results dictionary for F1 scores\n",
    "    f1_results = {\n",
    "        'sentiment_single': [],\n",
    "        'emotion_single': [],\n",
    "        'multitask_sentiment': [],\n",
    "        'multitask_emotion': []\n",
    "    }\n",
    "    \n",
    "    # Run bootstrap iterations\n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Sample indices with replacement for each dataset\n",
    "        sst2_indices = np.random.choice(len(sst2_eval_data['texts']), size=sample_size, replace=True)\n",
    "        goemotions_indices = np.random.choice(len(goemotions_eval_data['texts']), size=sample_size, replace=True)\n",
    "        \n",
    "        # Prepare bootstrap samples\n",
    "        sst2_sample = {\n",
    "            'texts': [sst2_eval_data['texts'][i] for i in sst2_indices],\n",
    "            'sentiment_labels': [sst2_eval_data['sentiment_labels'][i] for i in sst2_indices]\n",
    "        }\n",
    "        \n",
    "        goemotions_sample = {\n",
    "            'texts': [goemotions_eval_data['texts'][i] for i in goemotions_indices],\n",
    "            'emotion_labels': [goemotions_eval_data['emotion_labels'][i] for i in goemotions_indices]\n",
    "        }\n",
    "        \n",
    "        multitask_sample = {\n",
    "            'texts': sst2_sample['texts'],  # Use SST-2 texts for multitask\n",
    "            'sentiment_labels': sst2_sample['sentiment_labels'],\n",
    "            'emotion_labels': [goemotions_eval_data['emotion_labels'][i] for i in sst2_indices]\n",
    "        }\n",
    "        \n",
    "        # Evaluate single task models\n",
    "        sentiment_results = evaluate_distilroberta_single_task(\n",
    "            sentiment_model, sentiment_tokenizer, sentiment_encoder, \n",
    "            sst2_sample, 'sentiment'\n",
    "        )\n",
    "        f1_results['sentiment_single'].append(sentiment_results['macro_f1'])\n",
    "        \n",
    "        emotion_results = evaluate_distilroberta_single_task(\n",
    "            emotion_model, emotion_tokenizer, emotion_encoder, \n",
    "            goemotions_sample, 'emotion'\n",
    "        )\n",
    "        f1_results['emotion_single'].append(emotion_results['macro_f1'])\n",
    "        \n",
    "        # Evaluate multitask model\n",
    "        multitask_results = evaluate_distilroberta_multitask(\n",
    "            multitask_model, multitask_tokenizer, \n",
    "            multitask_sent_encoder, multitask_emot_encoder,\n",
    "            multitask_sample\n",
    "        )\n",
    "        f1_results['multitask_sentiment'].append(multitask_results['sentiment']['macro_f1'])\n",
    "        f1_results['multitask_emotion'].append(multitask_results['emotion']['macro_f1'])\n",
    "    \n",
    "    # 5. Calculate and display statistics\n",
    "    print(\"\\ndistilroberta Bootstrap Analysis Results (General Datasets)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, f1_scores in f1_results.items():\n",
    "        values = np.array(f1_scores)\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        ci_lower = np.percentile(values, 2.5)\n",
    "        ci_upper = np.percentile(values, 97.5)\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name.replace('_', ' ').upper()} - F1\")\n",
    "        print(f\"   Mean: {mean:.4f}\")\n",
    "        print(f\"   Std:  {std:.4f}\")\n",
    "        print(f\"   95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # 6. Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"./distilroberta_seed_analysis_results/distilroberta_bootstrap_general_datasets_{timestamp}.json\"\n",
    "    \n",
    "    results_to_save = {\n",
    "        model_name: {\n",
    "            'values': [float(x) for x in values],\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)),\n",
    "            'ci_lower': float(np.percentile(values, 2.5)),\n",
    "            'ci_upper': float(np.percentile(values, 97.5))\n",
    "        }\n",
    "        for model_name, values in f1_results.items()\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Bootstrap results saved to: {results_file}\")\n",
    "    \n",
    "    return results_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa12f8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running distilroberta Bootstrap Analysis on General Datasets\n",
      "============================================================\n",
      "\n",
      "📂 Loading general evaluation datasets...\n",
      "Loading external datasets...\n",
      "SST-2 dataset loaded: 67349 train, 872 validation samples\n",
      "GoEmotions dataset loaded: 43410 train, 5426 validation, 5427 test samples\n",
      "Preparing SST-2 evaluation data...\n",
      "Preparing SST-2 evaluation data...\n",
      "✅ SST-2 evaluation data prepared: 872 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "Preparing GoEmotions evaluation data...\n",
      "Preparing GoEmotions evaluation data...\n",
      "✅ GoEmotions evaluation data prepared: 1000 samples\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "Preparing multitask evaluation data...\n",
      "Preparing multitask evaluation data...\n",
      "✅ Multitask evaluation data prepared: 872 samples\n",
      "📥 Loading distilroberta sentiment model from ./distilroberta_trained_models_seeds/distilroberta_sentiment_seed_42...\n",
      "📥 Loading distilroberta emotion model from ./distilroberta_trained_models_seeds/distilroberta_emotion_seed_42...\n",
      "📥 Loading distilroberta multitask model from ./distilroberta_trained_models_seeds/distilroberta_multitask_seed_42...\n",
      "\n",
      "🔄 Starting bootstrap evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|██████████| 1000/1000 [09:20<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distilroberta Bootstrap Analysis Results (General Datasets)\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT SINGLE - F1\n",
      "   Mean: 0.5723\n",
      "   Std:  0.0230\n",
      "   95% CI: [0.5205, 0.6121]\n",
      "\n",
      "🎯 EMOTION SINGLE - F1\n",
      "   Mean: 0.6889\n",
      "   Std:  0.0527\n",
      "   95% CI: [0.5782, 0.7901]\n",
      "\n",
      "🎯 MULTITASK SENTIMENT - F1\n",
      "   Mean: 0.5535\n",
      "   Std:  0.0250\n",
      "   95% CI: [0.5017, 0.5998]\n",
      "\n",
      "🎯 MULTITASK EMOTION - F1\n",
      "   Mean: 0.1114\n",
      "   Std:  0.0181\n",
      "   95% CI: [0.0766, 0.1471]\n",
      "\n",
      "💾 Bootstrap results saved to: ./distilroberta_seed_analysis_results/distilroberta_bootstrap_general_datasets_20250725_173938.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Run distilroberta Bootstrap Analysis\n",
    "\n",
    "# Run bootstrap analysis\n",
    "bootstrap_stats = run_distilroberta_bootstrap_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ead5e",
   "metadata": {},
   "source": [
    "# Reddit specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43b40f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Libraries imported and setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports for distilroberta Seed & Bootstrap Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import random\n",
    "from collections import Counter\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./distilroberta_seed_analysis_results\", exist_ok=True)\n",
    "os.makedirs(\"./distilroberta_trained_models_seeds\", exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31672e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Utility Functions for Analysis\n",
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e948c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: distilroberta Model Architectures\n",
    "class DistilrobertaSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load distilroberta model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.distilroberta = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.distilroberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get distilroberta outputs\n",
    "        outputs = self.distilroberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class DistilrobertaMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilroberta-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load distilroberta model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.distilroberta = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.distilroberta.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.distilroberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        shared_pooled = shared_attended[:, 0, :]\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "        outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "        \n",
    "        # Emotion branch\n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "        emotion_logits = self.emotion_classifier(emotion_features)\n",
    "        outputs[\"emotion_logits\"] = emotion_logits\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"distilroberta model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ec8a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Classes for distilroberta\n",
    "class DistilrobertaDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DistilrobertaMultiTaskDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], sentiment_labels: List[int], \n",
    "                 emotion_labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"distilroberta dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf64a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data Loading Functions for distilroberta Analysis\n",
    "def load_external_datasets() -> Tuple[Dict, Dict]:\n",
    "    print(\"Loading external datasets...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        sentiment_data = {\n",
    "            'train': sst2_dataset['train'],\n",
    "            'validation': sst2_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sentiment_data['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        emotion_data = {\n",
    "            'train': emotions_dataset['train'],\n",
    "            'validation': emotions_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotion_data['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def prepare_reddit_evaluation_data(reddit_data_path: str) -> Dict:\n",
    "    print(f\"Loading Reddit evaluation data from {reddit_data_path}...\")\n",
    "    \n",
    "    df = pd.read_csv(reddit_data_path)\n",
    "    \n",
    "    # Create label encoders that match distilroberta models\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # Fit encoders\n",
    "    sentiment_encoder.fit(df['sentiment'].tolist())\n",
    "    emotion_encoder.fit(df['emotion'].tolist())\n",
    "    \n",
    "    reddit_data = {\n",
    "        'texts': df['text_content'].tolist(),\n",
    "        'sentiment_labels_text': df['sentiment'].tolist(),\n",
    "        'emotion_labels_text': df['emotion'].tolist(),\n",
    "        'sentiment_labels': sentiment_encoder.transform(df['sentiment'].tolist()),\n",
    "        'emotion_labels': emotion_encoder.transform(df['emotion'].tolist()),\n",
    "        'sentiment_encoder': sentiment_encoder,\n",
    "        'emotion_encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Reddit data prepared: {len(reddit_data['texts'])} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return reddit_data\n",
    "\n",
    "def prepare_distilroberta_training_data(sentiment_data: Dict, emotion_data: Dict, max_samples: int = 5000):\n",
    "    \n",
    "    # Process sentiment data (SST-2 to 3 classes)\n",
    "    sentiment_texts = sentiment_data['train']['sentence'][:max_samples]\n",
    "    sentiment_labels_raw = sentiment_data['train']['label'][:max_samples]\n",
    "    \n",
    "    # Convert SST-2 binary to 3-class sentiment\n",
    "    sentiment_labels = []\n",
    "    for label in sentiment_labels_raw:\n",
    "        if label == 0:  # Negative\n",
    "            sentiment_labels.append(0)\n",
    "        elif label == 1:  # Positive\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                sentiment_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                sentiment_labels.append(2)  # Positive\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in sentiment_labels:\n",
    "        neutral_indices = np.random.choice(len(sentiment_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            sentiment_labels[idx] = 1\n",
    "    \n",
    "    # Process emotion data (filter to first 6 classes)\n",
    "    emotion_texts_all = emotion_data['train']['text']\n",
    "    emotion_labels_all = emotion_data['train']['labels']\n",
    "    \n",
    "    emotion_texts = []\n",
    "    emotion_labels = []\n",
    "    count = 0\n",
    "    for i, label in enumerate(emotion_labels_all):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):\n",
    "                emotion_texts.append(emotion_texts_all[i])\n",
    "                emotion_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                emotion_texts.append(emotion_texts_all[i])\n",
    "                emotion_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoders\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    return {\n",
    "        'sentiment_data': {\n",
    "            'texts': sentiment_texts,\n",
    "            'labels': sentiment_labels,\n",
    "            'encoder': sentiment_encoder\n",
    "        },\n",
    "        'emotion_data': {\n",
    "            'texts': emotion_texts,\n",
    "            'labels': emotion_labels,\n",
    "            'encoder': emotion_encoder\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f87b499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta training functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: distilroberta Training Functions with Best Parameters\n",
    "def train_distilroberta_single_task(\n",
    "    task_type: str,  # 'sentiment' or 'emotion'\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 5000\n",
    ") -> Tuple[any, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training distilroberta {task_type} model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Get appropriate data\n",
    "    if task_type == 'sentiment':\n",
    "        texts = training_data['sentiment_data']['texts'][:max_samples]\n",
    "        labels = training_data['sentiment_data']['labels'][:max_samples]\n",
    "        encoder = training_data['sentiment_data']['encoder']\n",
    "        num_classes = 3\n",
    "    else:  # emotion\n",
    "        texts = training_data['emotion_data']['texts'][:max_samples]\n",
    "        labels = training_data['emotion_data']['labels'][:max_samples]\n",
    "        encoder = training_data['emotion_data']['encoder']\n",
    "        num_classes = 6\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DistilrobertaSingleTaskTransformer(\n",
    "        model_name='distilroberta-base',\n",
    "        num_classes=num_classes,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = DistilrobertaDataset(texts, labels, tokenizer, max_length=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs['logits'], labels_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./distilroberta_trained_models_seeds/distilroberta_{task_type}_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"distilroberta-base\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"task_type\": task_type,\n",
    "        \"model_type\": \"DistilrobertaSingleTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoder\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(encoder, os.path.join(output_dir, f'{task_type}_encoder.pkl'))\n",
    "    \n",
    "    print(f\"✅ distilroberta {task_type} model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "def train_distilroberta_multitask(\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 2000\n",
    ") -> Tuple[any, LabelEncoder, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training distilroberta multitask model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Prepare multitask data (combine sentiment and emotion data)\n",
    "    min_length = min(len(training_data['sentiment_data']['texts']), \n",
    "                     len(training_data['emotion_data']['texts']))\n",
    "    min_length = min(min_length, max_samples)\n",
    "    \n",
    "    combined_texts = training_data['sentiment_data']['texts'][:min_length]\n",
    "    combined_sentiment_labels = training_data['sentiment_data']['labels'][:min_length]\n",
    "    combined_emotion_labels = training_data['emotion_data']['labels'][:min_length]\n",
    "    \n",
    "    sentiment_encoder = training_data['sentiment_data']['encoder']\n",
    "    emotion_encoder = training_data['emotion_data']['encoder']\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DistilrobertaMultiTaskTransformer(\n",
    "        model_name='distilroberta-base',\n",
    "        sentiment_num_classes=3,\n",
    "        emotion_num_classes=6,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = DistilrobertaMultiTaskDataset(\n",
    "        combined_texts, combined_sentiment_labels, combined_emotion_labels, \n",
    "        tokenizer, max_length=128\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    emotion_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    alpha = best_params['alpha']\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = sentiment_criterion(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = emotion_criterion(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss_batch = alpha * sentiment_loss + (1 - alpha) * emotion_loss\n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./distilroberta_trained_models_seeds/distilroberta_multitask_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"distilroberta-base\",\n",
    "        \"sentiment_num_classes\": 3,\n",
    "        \"emotion_num_classes\": 6,\n",
    "        \"model_type\": \"DistilrobertaMultiTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoders\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    print(f\"distilroberta multitask model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, sentiment_encoder, emotion_encoder\n",
    "\n",
    "print(\"distilroberta training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81d537cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions for distilroberta Models\n",
    "def evaluate_distilroberta_single_task(model, tokenizer, label_encoder, reddit_data: Dict, task_type: str) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_labels = reddit_data[f'{task_type}_labels']\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 16):  # Batch size 16\n",
    "            batch_texts = texts[i:i+16]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs['logits']\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                pred_id = preds[j].item()\n",
    "                confidence = probs[j][pred_id].item()\n",
    "                \n",
    "                # Handle out of range predictions\n",
    "                if pred_id >= len(label_encoder.classes_):\n",
    "                    pred_id = 0\n",
    "                \n",
    "                predictions.append(pred_id)\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'predictions': predictions,\n",
    "        'confidences': confidences,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "def evaluate_distilroberta_multitask(model, tokenizer, sentiment_encoder, emotion_encoder, \n",
    "                               reddit_data: Dict, max_length: int = 128) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_sentiment_labels = reddit_data['sentiment_labels']\n",
    "    true_emotion_labels = reddit_data['emotion_labels']\n",
    "    \n",
    "    sentiment_predictions = []\n",
    "    emotion_predictions = []\n",
    "    sentiment_confidences = []\n",
    "    emotion_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 8):  # Smaller batch size for multitask\n",
    "            batch_texts = texts[i:i+8]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Process sentiment\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "            \n",
    "            # Process emotion\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                # Sentiment\n",
    "                sent_id = sentiment_preds[j].item()\n",
    "                sent_conf = sentiment_probs[j][sent_id].item()\n",
    "                if sent_id >= len(sentiment_encoder.classes_):\n",
    "                    sent_id = 0\n",
    "                sentiment_predictions.append(sent_id)\n",
    "                sentiment_confidences.append(sent_conf)\n",
    "                \n",
    "                # Emotion\n",
    "                emot_id = emotion_preds[j].item()\n",
    "                emot_conf = emotion_probs[j][emot_id].item()\n",
    "                if emot_id >= len(emotion_encoder.classes_):\n",
    "                    emot_id = 0\n",
    "                emotion_predictions.append(emot_id)\n",
    "                emotion_confidences.append(emot_conf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sentiment_accuracy = accuracy_score(true_sentiment_labels, sentiment_predictions)\n",
    "    sentiment_f1 = f1_score(true_sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    emotion_accuracy = accuracy_score(true_emotion_labels, emotion_predictions)\n",
    "    emotion_f1 = f1_score(true_emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'sentiment': {\n",
    "            'accuracy': sentiment_accuracy,\n",
    "            'macro_f1': sentiment_f1,\n",
    "            'predictions': sentiment_predictions,\n",
    "            'confidences': sentiment_confidences\n",
    "        },\n",
    "        'emotion': {\n",
    "            'accuracy': emotion_accuracy,\n",
    "            'macro_f1': emotion_f1,\n",
    "            'predictions': emotion_predictions,\n",
    "            'confidences': emotion_confidences\n",
    "        },\n",
    "        'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "        'combined_f1': (sentiment_f1 + emotion_f1) / 2\n",
    "    }\n",
    "\n",
    "print(\"distilroberta evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7906c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ distilroberta random seed analysis function defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: distilroberta Random Seed Analysis Function\n",
    "def run_distilroberta_seed_analysis(\n",
    "    reddit_data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    seeds: List[int] = [42, 123, 456, 789, 999],\n",
    "    max_training_samples: int = 3000\n",
    "):\n",
    "    \n",
    "    print(\"🎲 STARTING distilroberta RANDOM SEED ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Seeds to test: {seeds}\")\n",
    "    print(f\"Max training samples per dataset: {max_training_samples}\")\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(\"\\n🔄 Preparing distilroberta training data...\")\n",
    "    training_data = prepare_distilroberta_training_data(sentiment_data, emotion_data, max_training_samples)\n",
    "    \n",
    "    # Load Reddit evaluation data\n",
    "    print(\"\\n📂 Loading Reddit evaluation data...\")\n",
    "    reddit_data = prepare_reddit_evaluation_data(reddit_data_path)\n",
    "    \n",
    "    # Define best parameters for each distilroberta model\n",
    "    best_params = {\n",
    "        'sentiment': {\n",
    "            'learning_rate': 3.65445235521325e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'emotion': {\n",
    "            'learning_rate': 3.65445235521325e-05, \n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'multitask': {\n",
    "            'learning_rate': 4.166863122305896e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15142344384136117,\n",
    "            'weight_decay': 0.06331731119758383,\n",
    "            'hidden_dropout_prob': 0.10929008254399955,\n",
    "            'classifier_dropout': 0.22150897038028766,\n",
    "            'alpha': 0.4341048247374583\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    # Store results for each seed\n",
    "    all_results = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n🌱 TRAINING AND EVALUATING distilroberta WITH SEED {seed}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        seed_results = {}\n",
    "        \n",
    "        # 1. Train and evaluate distilroberta Sentiment\n",
    "        print(f\"\\n1️⃣ distilroberta Sentiment (Seed {seed})\")\n",
    "        model, encoder = train_distilroberta_single_task(\n",
    "            'sentiment', best_params['sentiment'], seed, \n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./distilroberta_trained_models_seeds/distilroberta_sentiment_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_distilroberta_single_task(model, tokenizer, encoder, reddit_data, 'sentiment')\n",
    "        seed_results['distilroberta_sentiment'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 2. Train and evaluate distilroberta Emotion\n",
    "        print(f\"\\n2️⃣ distilroberta Emotion (Seed {seed})\")\n",
    "        model, encoder = train_distilroberta_single_task(\n",
    "            'emotion', best_params['emotion'], seed,\n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./distilroberta_trained_models_seeds/distilroberta_emotion_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_distilroberta_single_task(model, tokenizer, encoder, reddit_data, 'emotion')\n",
    "        seed_results['distilroberta_emotion'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 3. Train and evaluate distilroberta Multitask\n",
    "        print(f\"\\n3️⃣ distilroberta Multitask (Seed {seed})\")\n",
    "        model, sent_enc, emot_enc = train_distilroberta_multitask(\n",
    "            best_params['multitask'], seed, training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./distilroberta_trained_models_seeds/distilroberta_multitask_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_distilroberta_multitask(\n",
    "            model, tokenizer, sent_enc, emot_enc, reddit_data, 128\n",
    "        )\n",
    "        seed_results['distilroberta_multitask'] = results\n",
    "        print(f\"   Sentiment - Accuracy: {results['sentiment']['accuracy']:.4f}, F1: {results['sentiment']['macro_f1']:.4f}\")\n",
    "        print(f\"   Emotion - Accuracy: {results['emotion']['accuracy']:.4f}, F1: {results['emotion']['macro_f1']:.4f}\")\n",
    "        print(f\"   Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        all_results[seed] = seed_results\n",
    "        \n",
    "        print(f\"\\n✅ Completed evaluation for seed {seed}\")\n",
    "    \n",
    "    # Analyze stability across seeds\n",
    "    print(f\"\\n📊 ANALYZING distilroberta STABILITY ACROSS SEEDS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    stability_analysis = analyze_distilroberta_seed_stability(all_results, seeds)\n",
    "    \n",
    "    # Save results\n",
    "    save_distilroberta_results(all_results, stability_analysis, seeds)\n",
    "    \n",
    "    return all_results, stability_analysis\n",
    "\n",
    "print(\"✅ distilroberta random seed analysis function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc3da3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta stability analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: distilroberta Stability Analysis Functions\n",
    "def analyze_distilroberta_seed_stability(all_results: Dict, seeds: List[int]) -> Dict:\n",
    "    \n",
    "    stability_stats = {}\n",
    "    \n",
    "    # Define model-task combinations\n",
    "    evaluations = [\n",
    "        ('distilroberta_sentiment', 'sentiment'),\n",
    "        ('distilroberta_emotion', 'emotion'),\n",
    "        ('distilroberta_multitask', 'sentiment'),\n",
    "        ('distilroberta_multitask', 'emotion')\n",
    "    ]\n",
    "    \n",
    "    for model_name, task in evaluations:\n",
    "        print(f\"\\n🔍 {model_name.upper()} - {task.upper()}\")\n",
    "        \n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            if model_name in all_results[seed]:\n",
    "                result = all_results[seed][model_name]\n",
    "                \n",
    "                if model_name.endswith('_multitask'):\n",
    "                    acc = result[task]['accuracy']\n",
    "                    f1 = result[task]['macro_f1']\n",
    "                else:\n",
    "                    acc = result['accuracy']\n",
    "                    f1 = result['macro_f1']\n",
    "                \n",
    "                accuracies.append(acc)\n",
    "                f1_scores.append(f1)\n",
    "        \n",
    "        if accuracies:\n",
    "            acc_mean = np.mean(accuracies)\n",
    "            acc_std = np.std(accuracies)\n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            \n",
    "            stability_stats[f\"{model_name}_{task}\"] = {\n",
    "                'accuracy_mean': acc_mean,\n",
    "                'accuracy_std': acc_std,\n",
    "                'f1_mean': f1_mean,\n",
    "                'f1_std': f1_std,\n",
    "                'accuracy_values': accuracies,\n",
    "                'f1_values': f1_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "            print(f\"   Macro F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    return stability_stats\n",
    "\n",
    "def save_distilroberta_results(all_results: Dict, stability_analysis: Dict, seeds: List[int]):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = f\"./distilroberta_seed_analysis_results/distilroberta_raw_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        serializable_results = {}\n",
    "        for seed, seed_results in all_results.items():\n",
    "            serializable_results[str(seed)] = {}\n",
    "            for model, results in seed_results.items():\n",
    "                if isinstance(results, dict):\n",
    "                    serializable_results[str(seed)][model] = {}\n",
    "                    for key, value in results.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            serializable_results[str(seed)][model][key] = {\n",
    "                                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                                   [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in v] if isinstance(v, list) else v\n",
    "                                for k, v in value.items()\n",
    "                            }\n",
    "                        else:\n",
    "                            serializable_results[str(seed)][model][key] = float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # Save stability analysis\n",
    "    stability_file = f\"./distilroberta_seed_analysis_results/distilroberta_stability_analysis_{timestamp}.json\"\n",
    "    with open(stability_file, 'w') as f:\n",
    "        serializable_stability = {}\n",
    "        for key, stats in stability_analysis.items():\n",
    "            serializable_stability[key] = {\n",
    "                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                   [float(x) for x in v] if isinstance(v, list) else v\n",
    "                for k, v in stats.items()\n",
    "            }\n",
    "        json.dump(serializable_stability, f, indent=2)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = f\"./distilroberta_seed_analysis_results/distilroberta_summary_report_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"distilroberta RANDOM SEED ANALYSIS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Seeds tested: {seeds}\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"STABILITY ANALYSIS (Mean ± Std)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        for key, stats in stability_analysis.items():\n",
    "            model_task = key.replace('_', ' ').title()\n",
    "            f.write(f\"\\n{model_task}:\\n\")\n",
    "            f.write(f\"  Accuracy: {stats['accuracy_mean']:.4f} ± {stats['accuracy_std']:.4f}\\n\")\n",
    "            f.write(f\"  Macro F1: {stats['f1_mean']:.4f} ± {stats['f1_std']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nBest Performers (by mean F1 score):\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Find best performers\n",
    "        sentiment_best = max([k for k in stability_analysis.keys() if 'sentiment' in k], \n",
    "                           key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        emotion_best = max([k for k in stability_analysis.keys() if 'emotion' in k], \n",
    "                         key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        \n",
    "        f.write(f\"Sentiment: {sentiment_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[sentiment_best]['f1_mean']:.4f})\\n\")\n",
    "        f.write(f\"Emotion: {emotion_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[emotion_best]['f1_mean']:.4f})\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 distilroberta results saved:\")\n",
    "    print(f\"   Raw results: {results_file}\")\n",
    "    print(f\"   Stability analysis: {stability_file}\")\n",
    "    print(f\"   Summary report: {summary_file}\")\n",
    "\n",
    "print(\"distilroberta stability analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95302a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 STARTING distilroberta RANDOM SEED ANALYSIS\n",
      "======================================================================\n",
      "Seeds to test: [42, 123, 456, 789, 999]\n",
      "Max training samples per dataset: 3000\n",
      "\n",
      "📂 Loading external datasets...\n",
      "Loading external datasets...\n",
      "✅ SST-2 dataset loaded: 67349 train samples\n",
      "✅ GoEmotions dataset loaded: 43410 train samples\n",
      "\n",
      "🔄 Preparing distilroberta training data...\n",
      "\n",
      "📂 Loading Reddit evaluation data...\n",
      "Loading Reddit evaluation data from annotated_reddit_posts.csv...\n",
      "✅ Reddit data prepared: 95 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 42\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment (Seed 42)\n",
      "🚀 Training distilroberta sentiment model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.7703\n",
      "Epoch 2/3, Average Loss: 0.5131\n",
      "Epoch 3/3, Average Loss: 0.3948\n",
      "✅ distilroberta sentiment model trained and saved with seed 42\n",
      "   Accuracy: 0.6105, Macro F1: 0.4038\n",
      "\n",
      "2️⃣ distilroberta Emotion (Seed 42)\n",
      "🚀 Training distilroberta emotion model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.2839\n",
      "Epoch 2/3, Average Loss: 0.6484\n",
      "Epoch 3/3, Average Loss: 0.4398\n",
      "✅ distilroberta emotion model trained and saved with seed 42\n",
      "   Accuracy: 0.1263, Macro F1: 0.0776\n",
      "\n",
      "3️⃣ distilroberta Multitask (Seed 42)\n",
      "🚀 Training distilroberta multitask model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5711\n",
      "Epoch 2/3, Average Loss: 1.2898\n",
      "Epoch 3/3, Average Loss: 1.1880\n",
      "distilroberta multitask model trained and saved with seed 42\n",
      "   Sentiment - Accuracy: 0.5579, F1: 0.3071\n",
      "   Emotion - Accuracy: 0.2842, F1: 0.1290\n",
      "   Combined - Accuracy: 0.4211, F1: 0.2180\n",
      "\n",
      "✅ Completed evaluation for seed 42\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 123\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment (Seed 123)\n",
      "🚀 Training distilroberta sentiment model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8656\n",
      "Epoch 2/3, Average Loss: 0.5469\n",
      "Epoch 3/3, Average Loss: 0.4134\n",
      "✅ distilroberta sentiment model trained and saved with seed 123\n",
      "   Accuracy: 0.5684, Macro F1: 0.3442\n",
      "\n",
      "2️⃣ distilroberta Emotion (Seed 123)\n",
      "🚀 Training distilroberta emotion model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3028\n",
      "Epoch 2/3, Average Loss: 0.6673\n",
      "Epoch 3/3, Average Loss: 0.4629\n",
      "✅ distilroberta emotion model trained and saved with seed 123\n",
      "   Accuracy: 0.1263, Macro F1: 0.0773\n",
      "\n",
      "3️⃣ distilroberta Multitask (Seed 123)\n",
      "🚀 Training distilroberta multitask model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6029\n",
      "Epoch 2/3, Average Loss: 1.3021\n",
      "Epoch 3/3, Average Loss: 1.1895\n",
      "distilroberta multitask model trained and saved with seed 123\n",
      "   Sentiment - Accuracy: 0.5579, F1: 0.3078\n",
      "   Emotion - Accuracy: 0.2842, F1: 0.1086\n",
      "   Combined - Accuracy: 0.4211, F1: 0.2082\n",
      "\n",
      "✅ Completed evaluation for seed 123\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 456\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment (Seed 456)\n",
      "🚀 Training distilroberta sentiment model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8170\n",
      "Epoch 2/3, Average Loss: 0.5093\n",
      "Epoch 3/3, Average Loss: 0.3869\n",
      "✅ distilroberta sentiment model trained and saved with seed 456\n",
      "   Accuracy: 0.5684, Macro F1: 0.3300\n",
      "\n",
      "2️⃣ distilroberta Emotion (Seed 456)\n",
      "🚀 Training distilroberta emotion model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3090\n",
      "Epoch 2/3, Average Loss: 0.6703\n",
      "Epoch 3/3, Average Loss: 0.4576\n",
      "✅ distilroberta emotion model trained and saved with seed 456\n",
      "   Accuracy: 0.1263, Macro F1: 0.0750\n",
      "\n",
      "3️⃣ distilroberta Multitask (Seed 456)\n",
      "🚀 Training distilroberta multitask model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.4897\n",
      "Epoch 2/3, Average Loss: 1.2877\n",
      "Epoch 3/3, Average Loss: 1.1858\n",
      "distilroberta multitask model trained and saved with seed 456\n",
      "   Sentiment - Accuracy: 0.5684, F1: 0.3442\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "   Combined - Accuracy: 0.4105, F1: 0.2057\n",
      "\n",
      "✅ Completed evaluation for seed 456\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 789\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment (Seed 789)\n",
      "🚀 Training distilroberta sentiment model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8148\n",
      "Epoch 2/3, Average Loss: 0.5200\n",
      "Epoch 3/3, Average Loss: 0.4023\n",
      "✅ distilroberta sentiment model trained and saved with seed 789\n",
      "   Accuracy: 0.5474, Macro F1: 0.3314\n",
      "\n",
      "2️⃣ distilroberta Emotion (Seed 789)\n",
      "🚀 Training distilroberta emotion model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.2789\n",
      "Epoch 2/3, Average Loss: 0.6450\n",
      "Epoch 3/3, Average Loss: 0.4401\n",
      "✅ distilroberta emotion model trained and saved with seed 789\n",
      "   Accuracy: 0.1368, Macro F1: 0.0762\n",
      "\n",
      "3️⃣ distilroberta Multitask (Seed 789)\n",
      "🚀 Training distilroberta multitask model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5982\n",
      "Epoch 2/3, Average Loss: 1.2876\n",
      "Epoch 3/3, Average Loss: 1.1687\n",
      "distilroberta multitask model trained and saved with seed 789\n",
      "   Sentiment - Accuracy: 0.5684, F1: 0.3316\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "   Combined - Accuracy: 0.4105, F1: 0.1994\n",
      "\n",
      "✅ Completed evaluation for seed 789\n",
      "\n",
      "🌱 TRAINING AND EVALUATING distilroberta WITH SEED 999\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ distilroberta Sentiment (Seed 999)\n",
      "🚀 Training distilroberta sentiment model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8650\n",
      "Epoch 2/3, Average Loss: 0.5390\n",
      "Epoch 3/3, Average Loss: 0.4014\n",
      "✅ distilroberta sentiment model trained and saved with seed 999\n",
      "   Accuracy: 0.5684, Macro F1: 0.3461\n",
      "\n",
      "2️⃣ distilroberta Emotion (Seed 999)\n",
      "🚀 Training distilroberta emotion model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.2691\n",
      "Epoch 2/3, Average Loss: 0.6928\n",
      "Epoch 3/3, Average Loss: 0.4755\n",
      "✅ distilroberta emotion model trained and saved with seed 999\n",
      "   Accuracy: 0.1684, Macro F1: 0.1043\n",
      "\n",
      "3️⃣ distilroberta Multitask (Seed 999)\n",
      "🚀 Training distilroberta multitask model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5130\n",
      "Epoch 2/3, Average Loss: 1.3102\n",
      "Epoch 3/3, Average Loss: 1.1927\n",
      "distilroberta multitask model trained and saved with seed 999\n",
      "   Sentiment - Accuracy: 0.5579, F1: 0.3078\n",
      "   Emotion - Accuracy: 0.2421, F1: 0.0650\n",
      "   Combined - Accuracy: 0.4000, F1: 0.1864\n",
      "\n",
      "✅ Completed evaluation for seed 999\n",
      "\n",
      "📊 ANALYZING distilroberta STABILITY ACROSS SEEDS\n",
      "======================================================================\n",
      "\n",
      "🔍 DISTILROBERTA_SENTIMENT - SENTIMENT\n",
      "   Accuracy: 0.5726 ± 0.0206\n",
      "   Macro F1: 0.3511 ± 0.0271\n",
      "\n",
      "🔍 DISTILROBERTA_EMOTION - EMOTION\n",
      "   Accuracy: 0.1368 ± 0.0163\n",
      "   Macro F1: 0.0821 ± 0.0112\n",
      "\n",
      "🔍 DISTILROBERTA_MULTITASK - SENTIMENT\n",
      "   Accuracy: 0.5621 ± 0.0052\n",
      "   Macro F1: 0.3197 ± 0.0154\n",
      "\n",
      "🔍 DISTILROBERTA_MULTITASK - EMOTION\n",
      "   Accuracy: 0.2632 ± 0.0176\n",
      "   Macro F1: 0.0874 ± 0.0265\n",
      "❌ Error during analysis: Object of type ndarray is not JSON serializable\n",
      "🔧 Try restarting the kernel and running cells 1-9 again.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Run distilroberta Random Seed Analysis (Fixed)\n",
    "\n",
    "# Clear any previous results\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Run distilroberta random seed analysis with the fixed saving function\n",
    "try:\n",
    "    all_results, stability_analysis = run_distilroberta_seed_analysis(\n",
    "        reddit_data_path=\"annotated_reddit_posts.csv\",\n",
    "        seeds=[42, 123, 456, 789, 999],  # 5 different seeds\n",
    "        max_training_samples=3000  # Reduced for faster training\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🎉 DISTILROBERTA RANDOM SEED ANALYSIS COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Check the './distilroberta_seed_analysis_results/' directory for detailed results.\")\n",
    "    \n",
    "    # Display quick summary\n",
    "    print(\"\\n📊 QUICK STABILITY SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name in ['DISTILROBERTA_SENTIMENT', 'DISTILROBERTA_EMOTION', 'DISTILROBERTA_MULTITASK']:\n",
    "        if model_name in stability_analysis:\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            for task in ['sentiment', 'emotion']:\n",
    "                if task in stability_analysis[model_name]:\n",
    "                    metrics = stability_analysis[model_name][task]\n",
    "                    print(f\"  {task.title()}:\")\n",
    "                    print(f\"    Accuracy: {metrics.get('accuracy_mean', 0):.3f} ± {metrics.get('accuracy_std', 0):.3f}\")\n",
    "                    print(f\"    F1 Score: {metrics.get('f1_mean', 0):.3f} ± {metrics.get('f1_std', 0):.3f}\")\n",
    "                    print(f\"    Stability: {metrics.get('stability_score', 0):.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during analysis: {str(e)}\")\n",
    "    print(\"🔧 Try restarting the kernel and running cells 1-9 again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4886efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta bootstrap analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: distilroberta Bootstrap Analysis Functions\n",
    "def load_distilroberta_model_for_bootstrap(model_path: str, model_type: str):\n",
    "    print(f\"📥 Loading distilroberta {model_type} model from {model_path}...\")\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_path, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        # Load multitask model\n",
    "        model = DistilrobertaMultiTaskTransformer(\n",
    "            model_name=\"distilroberta-base\",\n",
    "            sentiment_num_classes=config['sentiment_num_classes'],\n",
    "            emotion_num_classes=config['emotion_num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoders\n",
    "        sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "        emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, sentiment_encoder, emotion_encoder\n",
    "        \n",
    "    else:\n",
    "        # Load single-task model\n",
    "        model = DistilrobertaSingleTaskTransformer(\n",
    "            model_name=\"distilroberta-base\",\n",
    "            num_classes=config['num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoder\n",
    "        encoder = joblib.load(os.path.join(model_path, f'{config[\"task_type\"]}_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, encoder\n",
    "\n",
    "def evaluate_distilroberta_on_bootstrap_sample(model, tokenizer, texts, sentiment_labels, emotion_labels, \n",
    "                                        model_sentiment_encoder, model_emotion_encoder, \n",
    "                                        data_sentiment_encoder, data_emotion_encoder, \n",
    "                                        model_type=\"multitask\", max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 8):\n",
    "                batch_texts = texts[i:i+8]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': inputs['input_ids'].to(device),\n",
    "                    'attention_mask': inputs['attention_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                outputs = model(**filtered_inputs)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                for j in range(len(batch_texts)):\n",
    "                    sent_id = sentiment_preds[j].item()\n",
    "                    emot_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    if sent_id >= len(model_sentiment_encoder.classes_):\n",
    "                        sent_id = 0\n",
    "                    if emot_id >= len(model_emotion_encoder.classes_):\n",
    "                        emot_id = 0\n",
    "                    \n",
    "                    sentiment_predictions.append(sent_id)\n",
    "                    emotion_predictions.append(emot_id)\n",
    "        \n",
    "        # Map predictions to data label space\n",
    "        mapped_sentiment_preds = []\n",
    "        mapped_emotion_preds = []\n",
    "        \n",
    "        for sent_pred, emot_pred in zip(sentiment_predictions, emotion_predictions):\n",
    "            sent_class = model_sentiment_encoder.classes_[sent_pred]\n",
    "            emot_class = model_emotion_encoder.classes_[emot_pred]\n",
    "            \n",
    "            try:\n",
    "                mapped_sent = data_sentiment_encoder.transform([sent_class])[0]\n",
    "                mapped_emot = data_emotion_encoder.transform([emot_class])[0]\n",
    "            except ValueError:\n",
    "                mapped_sent = 0\n",
    "                mapped_emot = 0\n",
    "            \n",
    "            mapped_sentiment_preds.append(mapped_sent)\n",
    "            mapped_emotion_preds.append(mapped_emot)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, mapped_sentiment_preds)\n",
    "        sentiment_f1 = f1_score(sentiment_labels, mapped_sentiment_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        emotion_accuracy = accuracy_score(emotion_labels, mapped_emotion_preds)\n",
    "        emotion_f1 = f1_score(emotion_labels, mapped_emotion_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1': emotion_f1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # Single task evaluation logic here\n",
    "        pass\n",
    "\n",
    "def bootstrap_evaluation_distilroberta(model, tokenizer, data, model_sentiment_encoder, model_emotion_encoder,\n",
    "                                data_sentiment_encoder, data_emotion_encoder, \n",
    "                                n_iterations=1000, sample_size=95):\n",
    "    print(f\"🔄 Starting distilroberta bootstrap evaluation...\")\n",
    "    print(f\"   Iterations: {n_iterations}\")\n",
    "    print(f\"   Sample size: {sample_size}\")\n",
    "    \n",
    "    results = {\n",
    "        'sentiment_accuracy': [],\n",
    "        'sentiment_f1': [],\n",
    "        'emotion_accuracy': [],\n",
    "        'emotion_f1': []\n",
    "    }\n",
    "    \n",
    "    texts = data['texts']\n",
    "    sentiment_labels = data['sentiment_labels']\n",
    "    emotion_labels = data['emotion_labels']\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Bootstrap sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
    "        \n",
    "        sample_texts = [texts[idx] for idx in indices]\n",
    "        sample_sentiment_labels = [sentiment_labels[idx] for idx in indices]\n",
    "        sample_emotion_labels = [emotion_labels[idx] for idx in indices]\n",
    "        \n",
    "        # Evaluate on bootstrap sample\n",
    "        metrics = evaluate_distilroberta_on_bootstrap_sample(\n",
    "            model, tokenizer, sample_texts, sample_sentiment_labels, sample_emotion_labels,\n",
    "            model_sentiment_encoder, model_emotion_encoder,\n",
    "            data_sentiment_encoder, data_emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results['sentiment_accuracy'].append(metrics['sentiment_accuracy'])\n",
    "        results['sentiment_f1'].append(metrics['sentiment_f1'])\n",
    "        results['emotion_accuracy'].append(metrics['emotion_accuracy'])\n",
    "        results['emotion_f1'].append(metrics['emotion_f1'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"distilroberta bootstrap analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79c6d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_distilroberta_bootstrap_analysis():\n",
    "    print(\"🚀 Running distilroberta Bootstrap Analysis on General Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load all models (single task and multitask)\n",
    "    print(\"\\n📥 Loading models...\")\n",
    "    \n",
    "    # Load single task models\n",
    "    sentiment_model_path = \"./distilroberta_trained_models_seeds/distilroberta_sentiment_seed_42\"\n",
    "    sentiment_model, sentiment_tokenizer, sentiment_encoder = load_distilroberta_model_for_bootstrap(\n",
    "        sentiment_model_path, \"sentiment\"\n",
    "    )\n",
    "    \n",
    "    emotion_model_path = \"./distilroberta_trained_models_seeds/distilroberta_emotion_seed_42\"\n",
    "    emotion_model, emotion_tokenizer, emotion_encoder = load_distilroberta_model_for_bootstrap(\n",
    "        emotion_model_path, \"emotion\"\n",
    "    )\n",
    "    \n",
    "    # Load multitask model\n",
    "    multitask_model_path = \"./distilroberta_trained_models_seeds/distilroberta_multitask_seed_42\"\n",
    "    multitask_model, multitask_tokenizer, multitask_sent_encoder, multitask_emot_encoder = load_distilroberta_model_for_bootstrap(\n",
    "        multitask_model_path, \"multitask\"\n",
    "    )\n",
    "    \n",
    "    # 2. Load evaluation data\n",
    "    print(\"\\n📂 Loading evaluation datasets...\")\n",
    "    reddit_data = prepare_reddit_evaluation_data(\"annotated_reddit_posts.csv\")\n",
    "    \n",
    "    # 3. Run bootstrap evaluation for each model\n",
    "    print(\"\\n🔄 Starting bootstrap evaluation...\")\n",
    "    n_iterations = 1000\n",
    "    sample_size = 95\n",
    "    \n",
    "    # Initialize results dictionary for F1 scores\n",
    "    f1_results = {\n",
    "        'sentiment_single': [],\n",
    "        'emotion_single': [],\n",
    "        'multitask_sentiment': [],\n",
    "        'multitask_emotion': []\n",
    "    }\n",
    "    \n",
    "    # Run bootstrap iterations\n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Sample indices with replacement\n",
    "        indices = np.random.choice(len(reddit_data['texts']), size=sample_size, replace=True)\n",
    "        \n",
    "        # Prepare bootstrap sample\n",
    "        sample_data = {\n",
    "            'texts': [reddit_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [reddit_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [reddit_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "        \n",
    "        # Evaluate single task models\n",
    "        sentiment_results = evaluate_distilroberta_single_task(\n",
    "            sentiment_model, sentiment_tokenizer, sentiment_encoder, \n",
    "            sample_data, 'sentiment'\n",
    "        )\n",
    "        f1_results['sentiment_single'].append(sentiment_results['macro_f1'])\n",
    "        \n",
    "        emotion_results = evaluate_distilroberta_single_task(\n",
    "            emotion_model, emotion_tokenizer, emotion_encoder, \n",
    "            sample_data, 'emotion'\n",
    "        )\n",
    "        f1_results['emotion_single'].append(emotion_results['macro_f1'])\n",
    "        \n",
    "        # Evaluate multitask model\n",
    "        multitask_results = evaluate_distilroberta_multitask(\n",
    "            multitask_model, multitask_tokenizer, \n",
    "            multitask_sent_encoder, multitask_emot_encoder,\n",
    "            sample_data\n",
    "        )\n",
    "        f1_results['multitask_sentiment'].append(multitask_results['sentiment']['macro_f1'])\n",
    "        f1_results['multitask_emotion'].append(multitask_results['emotion']['macro_f1'])\n",
    "    \n",
    "    # 4. Calculate and display statistics\n",
    "    print(\"\\ndistilroberta Bootstrap Analysis Results\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, f1_scores in f1_results.items():\n",
    "        values = np.array(f1_scores)\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        ci_lower = np.percentile(values, 2.5)\n",
    "        ci_upper = np.percentile(values, 97.5)\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name.replace('_', ' ').upper()} - F1\")\n",
    "        print(f\"   Mean: {mean:.4f}\")\n",
    "        print(f\"   Std:  {std:.4f}\")\n",
    "        print(f\"   95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # 5. Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"./distilroberta_seed_analysis_results/distilroberta_bootstrap_results_{timestamp}.json\"\n",
    "    \n",
    "    results_to_save = {\n",
    "        model_name: {\n",
    "            'values': [float(x) for x in values],\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)),\n",
    "            'ci_lower': float(np.percentile(values, 2.5)),\n",
    "            'ci_upper': float(np.percentile(values, 97.5))\n",
    "        }\n",
    "        for model_name, values in f1_results.items()\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Bootstrap results saved to: {results_file}\")\n",
    "    \n",
    "    return results_to_save\n",
    "\n",
    "def prepare_reddit_evaluation_data(reddit_data_path: str) -> Dict:\n",
    "    print(f\"Loading Reddit evaluation data from {reddit_data_path}...\")\n",
    "    \n",
    "    df = pd.read_csv(reddit_data_path)\n",
    "    \n",
    "    # Create label encoders that match distilroberta models\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # Fit encoders\n",
    "    sentiment_encoder.fit(df['sentiment'].tolist())\n",
    "    emotion_encoder.fit(df['emotion'].tolist())\n",
    "    \n",
    "    reddit_data = {\n",
    "        'texts': df['text_content'].tolist(),\n",
    "        'sentiment_labels_text': df['sentiment'].tolist(),\n",
    "        'emotion_labels_text': df['emotion'].tolist(),\n",
    "        'sentiment_labels': sentiment_encoder.transform(df['sentiment'].tolist()),\n",
    "        'emotion_labels': emotion_encoder.transform(df['emotion'].tolist()),\n",
    "        'sentiment_encoder': sentiment_encoder,\n",
    "        'emotion_encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Reddit data prepared: {len(reddit_data['texts'])} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return reddit_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f47ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running distilroberta Bootstrap Analysis on General Datasets\n",
      "============================================================\n",
      "\n",
      "📥 Loading models...\n",
      "📥 Loading distilroberta sentiment model from ./distilroberta_trained_models_seeds/distilroberta_sentiment_seed_42...\n",
      "📥 Loading distilroberta emotion model from ./distilroberta_trained_models_seeds/distilroberta_emotion_seed_42...\n",
      "📥 Loading distilroberta multitask model from ./distilroberta_trained_models_seeds/distilroberta_multitask_seed_42...\n",
      "\n",
      "📂 Loading evaluation datasets...\n",
      "Loading Reddit evaluation data from annotated_reddit_posts.csv...\n",
      "✅ Reddit data prepared: 95 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "🔄 Starting bootstrap evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|██████████| 1000/1000 [10:01<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "distilroberta Bootstrap Analysis Results\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT SINGLE - F1\n",
      "   Mean: 0.4016\n",
      "   Std:  0.0418\n",
      "   95% CI: [0.3165, 0.4781]\n",
      "\n",
      "🎯 EMOTION SINGLE - F1\n",
      "   Mean: 0.0763\n",
      "   Std:  0.0196\n",
      "   95% CI: [0.0398, 0.1184]\n",
      "\n",
      "🎯 MULTITASK SENTIMENT - F1\n",
      "   Mean: 0.3053\n",
      "   Std:  0.0397\n",
      "   95% CI: [0.2315, 0.3828]\n",
      "\n",
      "🎯 MULTITASK EMOTION - F1\n",
      "   Mean: 0.1272\n",
      "   Std:  0.0192\n",
      "   95% CI: [0.0897, 0.1658]\n",
      "\n",
      "💾 Bootstrap results saved to: ./distilroberta_seed_analysis_results/distilroberta_bootstrap_results_20250725_190934.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Run distilroberta Bootstrap Analysis\n",
    "\n",
    "# Run bootstrap analysis\n",
    "bootstrap_stats = run_distilroberta_bootstrap_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
