{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ebee31",
   "metadata": {},
   "source": [
    "# General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e5dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Libraries imported and setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports for BERTweet Seed & Bootstrap Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import random\n",
    "from collections import Counter\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./bertweet_seed_analysis_results\", exist_ok=True)\n",
    "os.makedirs(\"./bertweet_trained_models_seeds\", exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fc10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Utility Functions for Analysis\n",
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee15564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: BERTweet Model Architectures\n",
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.bertweet.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERTweet outputs\n",
    "        outputs = self.bertweet(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.bertweet.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.bertweet(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        shared_pooled = shared_attended[:, 0, :]\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "        outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "        \n",
    "        # Emotion branch\n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "        emotion_logits = self.emotion_classifier(emotion_features)\n",
    "        outputs[\"emotion_logits\"] = emotion_logits\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"BERTweet model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84190c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Classes for BERTweet\n",
    "class BERTweetDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BERTweetMultiTaskDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], sentiment_labels: List[int], \n",
    "                 emotion_labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"BERTweet dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3feded95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modified data loading functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Modified Data Loading Functions for General Dataset Evaluation\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def load_external_datasets() -> Tuple[Dict, Dict]:\n",
    "    print(\"Loading external datasets...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        sentiment_data = {\n",
    "            'train': sst2_dataset['train'],\n",
    "            'validation': sst2_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sentiment_data['train'])} train, {len(sentiment_data['validation'])} validation samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        emotion_data = {\n",
    "            'train': emotions_dataset['train'],\n",
    "            'validation': emotions_dataset['validation'],\n",
    "            'test': emotions_dataset['test']  # GoEmotions has a test split\n",
    "        }\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotion_data['train'])} train, {len(emotion_data['validation'])} validation, {len(emotion_data['test'])} test samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def prepare_sst2_evaluation_data(sentiment_data: Dict, max_samples: int = 1000) -> Dict:\n",
    "    \"\"\"Prepare SST-2 validation data for evaluation\"\"\"\n",
    "    print(\"Preparing SST-2 evaluation data...\")\n",
    "    \n",
    "    # Use validation split for evaluation\n",
    "    eval_texts = sentiment_data['validation']['sentence'][:max_samples]\n",
    "    eval_labels_raw = sentiment_data['validation']['label'][:max_samples]\n",
    "    \n",
    "    # Convert SST-2 binary to 3-class sentiment (same as training)\n",
    "    eval_labels = []\n",
    "    for label in eval_labels_raw:\n",
    "        if label == 0:  # Negative\n",
    "            eval_labels.append(0)\n",
    "        elif label == 1:  # Positive\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral (same as training)\n",
    "                eval_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                eval_labels.append(2)  # Positive\n",
    "    \n",
    "    # Create encoder that matches training\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    sst2_eval_data = {\n",
    "        'texts': eval_texts,\n",
    "        'sentiment_labels': eval_labels,\n",
    "        'sentiment_encoder': sentiment_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ SST-2 evaluation data prepared: {len(sst2_eval_data['texts'])} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    \n",
    "    return sst2_eval_data\n",
    "\n",
    "def prepare_goemotions_evaluation_data(emotion_data: Dict, max_samples: int = 1000) -> Dict:\n",
    "    \"\"\"Prepare GoEmotions test data for evaluation\"\"\"\n",
    "    print(\"Preparing GoEmotions evaluation data...\")\n",
    "    \n",
    "    # Use test split for evaluation (or validation if test not available)\n",
    "    eval_split = emotion_data.get('test', emotion_data.get('validation'))\n",
    "    eval_texts_all = eval_split['text']\n",
    "    eval_labels_all = eval_split['labels']\n",
    "    \n",
    "    eval_texts = []\n",
    "    eval_labels = []\n",
    "    count = 0\n",
    "    \n",
    "    for i, label in enumerate(eval_labels_all):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):\n",
    "                eval_texts.append(eval_texts_all[i])\n",
    "                eval_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                eval_texts.append(eval_texts_all[i])\n",
    "                eval_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoder that matches training\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    goemotions_eval_data = {\n",
    "        'texts': eval_texts,\n",
    "        'emotion_labels': eval_labels,\n",
    "        'emotion_encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ GoEmotions evaluation data prepared: {len(goemotions_eval_data['texts'])} samples\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return goemotions_eval_data\n",
    "\n",
    "def prepare_multitask_evaluation_data(sst2_eval_data: Dict, goemotions_eval_data: Dict) -> Dict:\n",
    "    \"\"\"Prepare combined evaluation data for multitask model\"\"\"\n",
    "    print(\"Preparing multitask evaluation data...\")\n",
    "    \n",
    "    # Take minimum length to ensure both tasks have same number of samples\n",
    "    min_length = min(len(sst2_eval_data['texts']), len(goemotions_eval_data['texts']))\n",
    "    \n",
    "    multitask_eval_data = {\n",
    "        'texts': sst2_eval_data['texts'][:min_length],\n",
    "        'sentiment_labels': sst2_eval_data['sentiment_labels'][:min_length],\n",
    "        'emotion_labels': goemotions_eval_data['emotion_labels'][:min_length],\n",
    "        'sentiment_encoder': sst2_eval_data['sentiment_encoder'],\n",
    "        'emotion_encoder': goemotions_eval_data['emotion_encoder']\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Multitask evaluation data prepared: {len(multitask_eval_data['texts'])} samples\")\n",
    "    \n",
    "    return multitask_eval_data\n",
    "\n",
    "def prepare_bertweet_training_data(sentiment_data: Dict, emotion_data: Dict, max_samples: int = 3000) -> Dict:\n",
    "    print(\"Preparing BERTweet training data...\")\n",
    "    \n",
    "    # Process sentiment data (SST-2)\n",
    "    sentiment_texts = sentiment_data['train']['sentence'][:max_samples]\n",
    "    sentiment_labels = sentiment_data['train']['label'][:max_samples]\n",
    "    \n",
    "    # Process emotion data (filter to first 6 classes)\n",
    "    emotion_texts = []\n",
    "    emotion_labels = []\n",
    "    count = 0\n",
    "    \n",
    "    for i, label in enumerate(emotion_data['train']['labels']):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):  # Only use first 6 emotions\n",
    "                emotion_texts.append(emotion_data['train']['text'][i])\n",
    "                emotion_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                emotion_texts.append(emotion_data['train']['text'][i])\n",
    "                emotion_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoders\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # For SST-2: 0 = Negative, 1 = Positive\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Positive'])\n",
    "    \n",
    "    # For GoEmotions: First 6 emotions\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    training_data = {\n",
    "        'sentiment_data': {\n",
    "            'texts': sentiment_texts,\n",
    "            'labels': sentiment_labels,\n",
    "            'encoder': sentiment_encoder\n",
    "        },\n",
    "        'emotion_data': {\n",
    "            'texts': emotion_texts,\n",
    "            'labels': emotion_labels,\n",
    "            'encoder': emotion_encoder\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Training data prepared:\")\n",
    "    print(f\"   Sentiment: {len(sentiment_texts)} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion: {len(emotion_texts)} samples\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "print(\"✅ Modified data loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82afd1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet training functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: BERTweet Training Functions with Best Parameters\n",
    "def train_bertweet_single_task(\n",
    "    task_type: str,  # 'sentiment' or 'emotion'\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 5000\n",
    ") -> Tuple[any, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet {task_type} model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Get appropriate data\n",
    "    if task_type == 'sentiment':\n",
    "        texts = training_data['sentiment_data']['texts'][:max_samples]\n",
    "        labels = training_data['sentiment_data']['labels'][:max_samples]\n",
    "        encoder = training_data['sentiment_data']['encoder']\n",
    "        num_classes = 3\n",
    "    else:  # emotion\n",
    "        texts = training_data['emotion_data']['texts'][:max_samples]\n",
    "        labels = training_data['emotion_data']['labels'][:max_samples]\n",
    "        encoder = training_data['emotion_data']['encoder']\n",
    "        num_classes = 6\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetSingleTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        num_classes=num_classes,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetDataset(texts, labels, tokenizer, max_length=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs['logits'], labels_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_{task_type}_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"task_type\": task_type,\n",
    "        \"model_type\": \"BERTweetSingleTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoder\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(encoder, os.path.join(output_dir, f'{task_type}_encoder.pkl'))\n",
    "    \n",
    "    print(f\"✅ BERTweet {task_type} model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "def train_bertweet_multitask(\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 2000\n",
    ") -> Tuple[any, LabelEncoder, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet multitask model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Prepare multitask data (combine sentiment and emotion data)\n",
    "    min_length = min(len(training_data['sentiment_data']['texts']), \n",
    "                     len(training_data['emotion_data']['texts']))\n",
    "    min_length = min(min_length, max_samples)\n",
    "    \n",
    "    combined_texts = training_data['sentiment_data']['texts'][:min_length]\n",
    "    combined_sentiment_labels = training_data['sentiment_data']['labels'][:min_length]\n",
    "    combined_emotion_labels = training_data['emotion_data']['labels'][:min_length]\n",
    "    \n",
    "    sentiment_encoder = training_data['sentiment_data']['encoder']\n",
    "    emotion_encoder = training_data['emotion_data']['encoder']\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetMultiTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        sentiment_num_classes=3,\n",
    "        emotion_num_classes=6,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetMultiTaskDataset(\n",
    "        combined_texts, combined_sentiment_labels, combined_emotion_labels, \n",
    "        tokenizer, max_length=128\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    emotion_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    alpha = best_params['alpha']\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = sentiment_criterion(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = emotion_criterion(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss_batch = alpha * sentiment_loss + (1 - alpha) * emotion_loss\n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"sentiment_num_classes\": 3,\n",
    "        \"emotion_num_classes\": 6,\n",
    "        \"model_type\": \"BERTweetMultiTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoders\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    print(f\"BERTweet multitask model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, sentiment_encoder, emotion_encoder\n",
    "\n",
    "print(\"BERTweet training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d9d59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions for BERTweet Models\n",
    "def evaluate_bertweet_single_task(model, tokenizer, label_encoder, reddit_data: Dict, task_type: str) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_labels = reddit_data[f'{task_type}_labels']\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 16):  # Batch size 16\n",
    "            batch_texts = texts[i:i+16]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs['logits']\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                pred_id = preds[j].item()\n",
    "                confidence = probs[j][pred_id].item()\n",
    "                \n",
    "                # Handle out of range predictions\n",
    "                if pred_id >= len(label_encoder.classes_):\n",
    "                    pred_id = 0\n",
    "                \n",
    "                predictions.append(pred_id)\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'predictions': predictions,\n",
    "        'confidences': confidences,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "def evaluate_bertweet_multitask(model, tokenizer, sentiment_encoder, emotion_encoder, \n",
    "                               reddit_data: Dict, max_length: int = 128) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_sentiment_labels = reddit_data['sentiment_labels']\n",
    "    true_emotion_labels = reddit_data['emotion_labels']\n",
    "    \n",
    "    sentiment_predictions = []\n",
    "    emotion_predictions = []\n",
    "    sentiment_confidences = []\n",
    "    emotion_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 8):  # Smaller batch size for multitask\n",
    "            batch_texts = texts[i:i+8]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Process sentiment\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "            \n",
    "            # Process emotion\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                # Sentiment\n",
    "                sent_id = sentiment_preds[j].item()\n",
    "                sent_conf = sentiment_probs[j][sent_id].item()\n",
    "                if sent_id >= len(sentiment_encoder.classes_):\n",
    "                    sent_id = 0\n",
    "                sentiment_predictions.append(sent_id)\n",
    "                sentiment_confidences.append(sent_conf)\n",
    "                \n",
    "                # Emotion\n",
    "                emot_id = emotion_preds[j].item()\n",
    "                emot_conf = emotion_probs[j][emot_id].item()\n",
    "                if emot_id >= len(emotion_encoder.classes_):\n",
    "                    emot_id = 0\n",
    "                emotion_predictions.append(emot_id)\n",
    "                emotion_confidences.append(emot_conf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sentiment_accuracy = accuracy_score(true_sentiment_labels, sentiment_predictions)\n",
    "    sentiment_f1 = f1_score(true_sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    emotion_accuracy = accuracy_score(true_emotion_labels, emotion_predictions)\n",
    "    emotion_f1 = f1_score(true_emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'sentiment': {\n",
    "            'accuracy': sentiment_accuracy,\n",
    "            'macro_f1': sentiment_f1,\n",
    "            'predictions': sentiment_predictions,\n",
    "            'confidences': sentiment_confidences\n",
    "        },\n",
    "        'emotion': {\n",
    "            'accuracy': emotion_accuracy,\n",
    "            'macro_f1': emotion_f1,\n",
    "            'predictions': emotion_predictions,\n",
    "            'confidences': emotion_confidences\n",
    "        },\n",
    "        'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "        'combined_f1': (sentiment_f1 + emotion_f1) / 2\n",
    "    }\n",
    "\n",
    "print(\"BERTweet evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1097d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modified BERTweet random seed analysis function defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Modified BERTweet Random Seed Analysis Function\n",
    "def run_bertweet_seed_analysis(\n",
    "    seeds: List[int] = [42, 123, 456, 789, 999],\n",
    "    max_training_samples: int = 3000,\n",
    "    max_eval_samples: int = 1000\n",
    "):\n",
    "    \n",
    "    print(\"🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Seeds to test: {seeds}\")\n",
    "    print(f\"Max training samples per dataset: {max_training_samples}\")\n",
    "    print(f\"Max evaluation samples per dataset: {max_eval_samples}\")\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(\"\\n🔄 Preparing BERTweet training data...\")\n",
    "    training_data = prepare_bertweet_training_data(sentiment_data, emotion_data, max_training_samples)\n",
    "    \n",
    "    # Prepare evaluation data\n",
    "    print(\"\\n📂 Preparing evaluation datasets...\")\n",
    "    sst2_eval_data = prepare_sst2_evaluation_data(sentiment_data, max_eval_samples)\n",
    "    goemotions_eval_data = prepare_goemotions_evaluation_data(emotion_data, max_eval_samples)\n",
    "    multitask_eval_data = prepare_multitask_evaluation_data(sst2_eval_data, goemotions_eval_data)\n",
    "    \n",
    "    # Define best parameters for each BERTweet model\n",
    "    best_params = {\n",
    "        'sentiment': {\n",
    "            'learning_rate': 3.65445235521325e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'emotion': {\n",
    "            'learning_rate': 3.65445235521325e-05, \n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'multitask': {\n",
    "            'learning_rate': 4.166863122305896e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15142344384136117,\n",
    "            'weight_decay': 0.06331731119758383,\n",
    "            'hidden_dropout_prob': 0.10929008254399955,\n",
    "            'classifier_dropout': 0.22150897038028766,\n",
    "            'alpha': 0.4341048247374583\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    # Store results for each seed\n",
    "    all_results = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n🌱 TRAINING AND EVALUATING BERTWEET WITH SEED {seed}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        seed_results = {}\n",
    "        \n",
    "        # 1. Train and evaluate BERTweet Sentiment on SST-2\n",
    "        print(f\"\\n1️⃣ BERTweet Sentiment on SST-2 (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'sentiment', best_params['sentiment'], seed, \n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_sentiment_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on SST-2 validation set\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, sst2_eval_data, 'sentiment')\n",
    "        seed_results['bertweet_sentiment'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 2. Train and evaluate BERTweet Emotion on GoEmotions\n",
    "        print(f\"\\n2️⃣ BERTweet Emotion on GoEmotions (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'emotion', best_params['emotion'], seed,\n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_emotion_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on GoEmotions test set\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, goemotions_eval_data, 'emotion')\n",
    "        seed_results['bertweet_emotion'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 3. Train and evaluate BERTweet Multitask on both datasets\n",
    "        print(f\"\\n3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed {seed})\")\n",
    "        model, sent_enc, emot_enc = train_bertweet_multitask(\n",
    "            best_params['multitask'], seed, training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on combined test sets\n",
    "        results = evaluate_bertweet_multitask(\n",
    "            model, tokenizer, sent_enc, emot_enc, multitask_eval_data, 128\n",
    "        )\n",
    "        seed_results['bertweet_multitask'] = results\n",
    "        print(f\"   Sentiment - Accuracy: {results['sentiment']['accuracy']:.4f}, F1: {results['sentiment']['macro_f1']:.4f}\")\n",
    "        print(f\"   Emotion - Accuracy: {results['emotion']['accuracy']:.4f}, F1: {results['emotion']['macro_f1']:.4f}\")\n",
    "        print(f\"   Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        all_results[seed] = seed_results\n",
    "        \n",
    "        print(f\"\\n✅ Completed evaluation for seed {seed}\")\n",
    "    \n",
    "    # Analyze stability across seeds\n",
    "    print(f\"\\n📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    stability_analysis = analyze_bertweet_seed_stability(all_results, seeds)\n",
    "    \n",
    "    # Save results\n",
    "    save_bertweet_results(all_results, stability_analysis, seeds)\n",
    "    \n",
    "    return all_results, stability_analysis\n",
    "\n",
    "print(\"✅ Modified BERTweet random seed analysis function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c3b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet stability analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: BERTweet Stability Analysis Functions\n",
    "def analyze_bertweet_seed_stability(all_results: Dict, seeds: List[int]) -> Dict:\n",
    "    \n",
    "    stability_stats = {}\n",
    "    \n",
    "    # Define model-task combinations\n",
    "    evaluations = [\n",
    "        ('bertweet_sentiment', 'sentiment'),\n",
    "        ('bertweet_emotion', 'emotion'),\n",
    "        ('bertweet_multitask', 'sentiment'),\n",
    "        ('bertweet_multitask', 'emotion')\n",
    "    ]\n",
    "    \n",
    "    for model_name, task in evaluations:\n",
    "        print(f\"\\n🔍 {model_name.upper()} - {task.upper()}\")\n",
    "        \n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            if model_name in all_results[seed]:\n",
    "                result = all_results[seed][model_name]\n",
    "                \n",
    "                if model_name.endswith('_multitask'):\n",
    "                    acc = result[task]['accuracy']\n",
    "                    f1 = result[task]['macro_f1']\n",
    "                else:\n",
    "                    acc = result['accuracy']\n",
    "                    f1 = result['macro_f1']\n",
    "                \n",
    "                accuracies.append(acc)\n",
    "                f1_scores.append(f1)\n",
    "        \n",
    "        if accuracies:\n",
    "            acc_mean = np.mean(accuracies)\n",
    "            acc_std = np.std(accuracies)\n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            \n",
    "            stability_stats[f\"{model_name}_{task}\"] = {\n",
    "                'accuracy_mean': acc_mean,\n",
    "                'accuracy_std': acc_std,\n",
    "                'f1_mean': f1_mean,\n",
    "                'f1_std': f1_std,\n",
    "                'accuracy_values': accuracies,\n",
    "                'f1_values': f1_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "            print(f\"   Macro F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    return stability_stats\n",
    "\n",
    "def save_bertweet_results(all_results: Dict, stability_analysis: Dict, seeds: List[int]):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_raw_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        serializable_results = {}\n",
    "        for seed, seed_results in all_results.items():\n",
    "            serializable_results[str(seed)] = {}\n",
    "            for model, results in seed_results.items():\n",
    "                if isinstance(results, dict):\n",
    "                    serializable_results[str(seed)][model] = {}\n",
    "                    for key, value in results.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            serializable_results[str(seed)][model][key] = {\n",
    "                                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                                   [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in v] if isinstance(v, list) else v\n",
    "                                for k, v in value.items()\n",
    "                            }\n",
    "                        else:\n",
    "                            serializable_results[str(seed)][model][key] = float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # Save stability analysis\n",
    "    stability_file = f\"./bertweet_seed_analysis_results/bertweet_stability_analysis_{timestamp}.json\"\n",
    "    with open(stability_file, 'w') as f:\n",
    "        serializable_stability = {}\n",
    "        for key, stats in stability_analysis.items():\n",
    "            serializable_stability[key] = {\n",
    "                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                   [float(x) for x in v] if isinstance(v, list) else v\n",
    "                for k, v in stats.items()\n",
    "            }\n",
    "        json.dump(serializable_stability, f, indent=2)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = f\"./bertweet_seed_analysis_results/bertweet_summary_report_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"BERTWEET RANDOM SEED ANALYSIS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Seeds tested: {seeds}\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"STABILITY ANALYSIS (Mean ± Std)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        for key, stats in stability_analysis.items():\n",
    "            model_task = key.replace('_', ' ').title()\n",
    "            f.write(f\"\\n{model_task}:\\n\")\n",
    "            f.write(f\"  Accuracy: {stats['accuracy_mean']:.4f} ± {stats['accuracy_std']:.4f}\\n\")\n",
    "            f.write(f\"  Macro F1: {stats['f1_mean']:.4f} ± {stats['f1_std']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nBest Performers (by mean F1 score):\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Find best performers\n",
    "        sentiment_best = max([k for k in stability_analysis.keys() if 'sentiment' in k], \n",
    "                           key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        emotion_best = max([k for k in stability_analysis.keys() if 'emotion' in k], \n",
    "                         key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        \n",
    "        f.write(f\"Sentiment: {sentiment_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[sentiment_best]['f1_mean']:.4f})\\n\")\n",
    "        f.write(f\"Emotion: {emotion_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[emotion_best]['f1_mean']:.4f})\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 BERTweet results saved:\")\n",
    "    print(f\"   Raw results: {results_file}\")\n",
    "    print(f\"   Stability analysis: {stability_file}\")\n",
    "    print(f\"   Summary report: {summary_file}\")\n",
    "\n",
    "print(\"BERTweet stability analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\n",
      "======================================================================\n",
      "Seeds to test: [42, 123, 456, 789, 999]\n",
      "Max training samples per dataset: 3000\n",
      "Max evaluation samples per dataset: 1000\n",
      "\n",
      "📂 Loading external datasets...\n",
      "Loading external datasets...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 validation samples\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 validation, 5427 test samples\n",
      "\n",
      "🔄 Preparing BERTweet training data...\n",
      "Preparing BERTweet training data...\n",
      "✅ Training data prepared:\n",
      "   Sentiment: 3000 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Positive')]\n",
      "   Emotion: 3000 samples\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "📂 Preparing evaluation datasets...\n",
      "Preparing SST-2 evaluation data...\n",
      "✅ SST-2 evaluation data prepared: 872 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "Preparing GoEmotions evaluation data...\n",
      "✅ GoEmotions evaluation data prepared: 1000 samples\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "Preparing multitask evaluation data...\n",
      "✅ Multitask evaluation data prepared: 872 samples\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 42\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 42)\n",
      "🚀 Training BERTweet sentiment model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.5872\n",
      "Epoch 2/3, Average Loss: 0.2380\n",
      "Epoch 3/3, Average Loss: 0.1185\n",
      "✅ BERTweet sentiment model trained and saved with seed 42\n",
      "   Accuracy: 0.5161, Macro F1: 0.3870\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 42)\n",
      "🚀 Training BERTweet emotion model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3897\n",
      "Epoch 2/3, Average Loss: 0.6943\n",
      "Epoch 3/3, Average Loss: 0.4847\n",
      "✅ BERTweet emotion model trained and saved with seed 42\n",
      "   Accuracy: 0.7630, Macro F1: 0.7356\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 42)\n",
      "🚀 Training BERTweet multitask model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.4689\n",
      "Epoch 2/3, Average Loss: 1.1706\n",
      "Epoch 3/3, Average Loss: 1.0815\n",
      "BERTweet multitask model trained and saved with seed 42\n",
      "   Sentiment - Accuracy: 0.5138, F1: 0.3880\n",
      "   Emotion - Accuracy: 0.2821, F1: 0.1081\n",
      "   Combined - Accuracy: 0.3979, F1: 0.2480\n",
      "\n",
      "✅ Completed evaluation for seed 42\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 123\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 123)\n",
      "🚀 Training BERTweet sentiment model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6605\n",
      "Epoch 2/3, Average Loss: 0.3059\n",
      "Epoch 3/3, Average Loss: 0.1587\n",
      "✅ BERTweet sentiment model trained and saved with seed 123\n",
      "   Accuracy: 0.5034, Macro F1: 0.3844\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 123)\n",
      "🚀 Training BERTweet emotion model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3615\n",
      "Epoch 2/3, Average Loss: 0.6956\n",
      "Epoch 3/3, Average Loss: 0.4921\n",
      "✅ BERTweet emotion model trained and saved with seed 123\n",
      "   Accuracy: 0.7590, Macro F1: 0.7324\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 123)\n",
      "🚀 Training BERTweet multitask model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.4639\n",
      "Epoch 2/3, Average Loss: 1.1781\n",
      "Epoch 3/3, Average Loss: 1.0722\n",
      "BERTweet multitask model trained and saved with seed 123\n",
      "   Sentiment - Accuracy: 0.5138, F1: 0.3873\n",
      "   Emotion - Accuracy: 0.2959, F1: 0.0953\n",
      "   Combined - Accuracy: 0.4048, F1: 0.2413\n",
      "\n",
      "✅ Completed evaluation for seed 123\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 456\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 456)\n",
      "🚀 Training BERTweet sentiment model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6452\n",
      "Epoch 2/3, Average Loss: 0.2622\n",
      "Epoch 3/3, Average Loss: 0.1454\n",
      "✅ BERTweet sentiment model trained and saved with seed 456\n",
      "   Accuracy: 0.5161, Macro F1: 0.3876\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 456)\n",
      "🚀 Training BERTweet emotion model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3945\n",
      "Epoch 2/3, Average Loss: 0.7019\n",
      "Epoch 3/3, Average Loss: 0.4818\n",
      "✅ BERTweet emotion model trained and saved with seed 456\n",
      "   Accuracy: 0.7520, Macro F1: 0.7237\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 456)\n",
      "🚀 Training BERTweet multitask model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.4671\n",
      "Epoch 2/3, Average Loss: 1.1752\n",
      "Epoch 3/3, Average Loss: 1.0721\n",
      "BERTweet multitask model trained and saved with seed 456\n",
      "   Sentiment - Accuracy: 0.5092, F1: 0.3884\n",
      "   Emotion - Accuracy: 0.3131, F1: 0.0796\n",
      "   Combined - Accuracy: 0.4111, F1: 0.2340\n",
      "\n",
      "✅ Completed evaluation for seed 456\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 789\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 789)\n",
      "🚀 Training BERTweet sentiment model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6038\n",
      "Epoch 2/3, Average Loss: 0.2392\n",
      "Epoch 3/3, Average Loss: 0.1423\n",
      "✅ BERTweet sentiment model trained and saved with seed 789\n",
      "   Accuracy: 0.5195, Macro F1: 0.3890\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 789)\n",
      "🚀 Training BERTweet emotion model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3752\n",
      "Epoch 2/3, Average Loss: 0.6841\n",
      "Epoch 3/3, Average Loss: 0.4946\n",
      "✅ BERTweet emotion model trained and saved with seed 789\n",
      "   Accuracy: 0.7370, Macro F1: 0.7081\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 789)\n",
      "🚀 Training BERTweet multitask model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5368\n",
      "Epoch 2/3, Average Loss: 1.3511\n",
      "Epoch 3/3, Average Loss: 1.2951\n",
      "BERTweet multitask model trained and saved with seed 789\n",
      "   Sentiment - Accuracy: 0.4541, F1: 0.3569\n",
      "   Emotion - Accuracy: 0.2833, F1: 0.0999\n",
      "   Combined - Accuracy: 0.3687, F1: 0.2284\n",
      "\n",
      "✅ Completed evaluation for seed 789\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 999\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 999)\n",
      "🚀 Training BERTweet sentiment model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.5978\n",
      "Epoch 2/3, Average Loss: 0.2331\n",
      "Epoch 3/3, Average Loss: 0.1184\n",
      "✅ BERTweet sentiment model trained and saved with seed 999\n",
      "   Accuracy: 0.5172, Macro F1: 0.3887\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 999)\n",
      "🚀 Training BERTweet emotion model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3259\n",
      "Epoch 2/3, Average Loss: 0.6866\n",
      "Epoch 3/3, Average Loss: 0.4806\n",
      "✅ BERTweet emotion model trained and saved with seed 999\n",
      "   Accuracy: 0.7500, Macro F1: 0.7276\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 999)\n",
      "🚀 Training BERTweet multitask model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5399\n",
      "Epoch 2/3, Average Loss: 1.3173\n",
      "Epoch 3/3, Average Loss: 1.1432\n",
      "BERTweet multitask model trained and saved with seed 999\n",
      "   Sentiment - Accuracy: 0.5046, F1: 0.3791\n",
      "   Emotion - Accuracy: 0.3119, F1: 0.0795\n",
      "   Combined - Accuracy: 0.4083, F1: 0.2293\n",
      "\n",
      "✅ Completed evaluation for seed 999\n",
      "\n",
      "📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\n",
      "======================================================================\n",
      "\n",
      "🔍 BERTWEET_SENTIMENT - SENTIMENT\n",
      "   Accuracy: 0.5144 ± 0.0056\n",
      "   Macro F1: 0.3874 ± 0.0016\n",
      "\n",
      "🔍 BERTWEET_EMOTION - EMOTION\n",
      "   Accuracy: 0.7522 ± 0.0089\n",
      "   Macro F1: 0.7255 ± 0.0096\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - SENTIMENT\n",
      "   Accuracy: 0.4991 ± 0.0227\n",
      "   Macro F1: 0.3799 ± 0.0120\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - EMOTION\n",
      "   Accuracy: 0.2972 ± 0.0134\n",
      "   Macro F1: 0.0925 ± 0.0113\n",
      "\n",
      "💾 BERTweet results saved:\n",
      "   Raw results: ./bertweet_seed_analysis_results/bertweet_raw_results_20250725_154625.json\n",
      "   Stability analysis: ./bertweet_seed_analysis_results/bertweet_stability_analysis_20250725_154625.json\n",
      "   Summary report: ./bertweet_seed_analysis_results/bertweet_summary_report_20250725_154625.txt\n",
      "\n",
      "🎉 BERTWEET RANDOM SEED ANALYSIS COMPLETED!\n",
      "============================================================\n",
      "Check the './bertweet_seed_analysis_results/' directory for detailed results.\n",
      "\n",
      "📊 QUICK STABILITY SUMMARY:\n",
      "----------------------------------------\n",
      "\n",
      "Bertweet Sentiment Sentiment:\n",
      "  Accuracy: 0.514 ± 0.006\n",
      "  F1 Score: 0.387 ± 0.002\n",
      "\n",
      "Bertweet Emotion Emotion:\n",
      "  Accuracy: 0.752 ± 0.009\n",
      "  F1 Score: 0.725 ± 0.010\n",
      "\n",
      "Bertweet Multitask Sentiment:\n",
      "  Accuracy: 0.499 ± 0.023\n",
      "  F1 Score: 0.380 ± 0.012\n",
      "\n",
      "Bertweet Multitask Emotion:\n",
      "  Accuracy: 0.297 ± 0.013\n",
      "  F1 Score: 0.092 ± 0.011\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Run BERTweet Random Seed Analysis (Fixed)\n",
    "\n",
    "# Clear any previous results\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Run BERTweet random seed analysis with the modified function signature\n",
    "try:\n",
    "    all_results, stability_analysis = run_bertweet_seed_analysis(\n",
    "        seeds=[42, 123, 456, 789, 999],  # 5 different seeds\n",
    "        max_training_samples=3000,  # Reduced for faster training\n",
    "        max_eval_samples=1000  # Max evaluation samples per dataset\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBERTWEET RANDOM SEED ANALYSIS COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Check the './bertweet_seed_analysis_results/' directory for detailed results.\")\n",
    "    \n",
    "    # Display quick summary\n",
    "    print(\"\\n📊 QUICK STABILITY SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Updated to match the actual structure of stability_analysis\n",
    "    for key, stats in stability_analysis.items():\n",
    "        model_task = key.replace('_', ' ').title()\n",
    "        print(f\"\\n{model_task}:\")\n",
    "        print(f\"  Accuracy: {stats['accuracy_mean']:.3f} ± {stats['accuracy_std']:.3f}\")\n",
    "        print(f\"  F1 Score: {stats['f1_mean']:.3f} ± {stats['f1_std']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during analysis: {str(e)}\")\n",
    "    print(\"🔧 Try restarting the kernel and running cells 1-9 again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b729e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet bootstrap analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: BERTweet Bootstrap Analysis Functions\n",
    "def load_bertweet_model_for_bootstrap(model_path: str, model_type: str):\n",
    "    print(f\"📥 Loading BERTweet {model_type} model from {model_path}...\")\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_path, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        # Load multitask model\n",
    "        model = BERTweetMultiTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            sentiment_num_classes=config['sentiment_num_classes'],\n",
    "            emotion_num_classes=config['emotion_num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoders\n",
    "        sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "        emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, sentiment_encoder, emotion_encoder\n",
    "        \n",
    "    else:\n",
    "        # Load single-task model\n",
    "        model = BERTweetSingleTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            num_classes=config['num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoder\n",
    "        encoder = joblib.load(os.path.join(model_path, f'{config[\"task_type\"]}_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, encoder\n",
    "\n",
    "def evaluate_bertweet_on_bootstrap_sample(model, tokenizer, texts, sentiment_labels, emotion_labels, \n",
    "                                        model_sentiment_encoder, model_emotion_encoder, \n",
    "                                        data_sentiment_encoder, data_emotion_encoder, \n",
    "                                        model_type=\"multitask\", max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 8):\n",
    "                batch_texts = texts[i:i+8]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': inputs['input_ids'].to(device),\n",
    "                    'attention_mask': inputs['attention_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                outputs = model(**filtered_inputs)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                for j in range(len(batch_texts)):\n",
    "                    sent_id = sentiment_preds[j].item()\n",
    "                    emot_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    if sent_id >= len(model_sentiment_encoder.classes_):\n",
    "                        sent_id = 0\n",
    "                    if emot_id >= len(model_emotion_encoder.classes_):\n",
    "                        emot_id = 0\n",
    "                    \n",
    "                    sentiment_predictions.append(sent_id)\n",
    "                    emotion_predictions.append(emot_id)\n",
    "        \n",
    "        # Map predictions to data label space\n",
    "        mapped_sentiment_preds = []\n",
    "        mapped_emotion_preds = []\n",
    "        \n",
    "        for sent_pred, emot_pred in zip(sentiment_predictions, emotion_predictions):\n",
    "            sent_class = model_sentiment_encoder.classes_[sent_pred]\n",
    "            emot_class = model_emotion_encoder.classes_[emot_pred]\n",
    "            \n",
    "            try:\n",
    "                mapped_sent = data_sentiment_encoder.transform([sent_class])[0]\n",
    "                mapped_emot = data_emotion_encoder.transform([emot_class])[0]\n",
    "            except ValueError:\n",
    "                mapped_sent = 0\n",
    "                mapped_emot = 0\n",
    "            \n",
    "            mapped_sentiment_preds.append(mapped_sent)\n",
    "            mapped_emotion_preds.append(mapped_emot)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, mapped_sentiment_preds)\n",
    "        sentiment_f1 = f1_score(sentiment_labels, mapped_sentiment_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        emotion_accuracy = accuracy_score(emotion_labels, mapped_emotion_preds)\n",
    "        emotion_f1 = f1_score(emotion_labels, mapped_emotion_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1': emotion_f1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # Single task evaluation logic here\n",
    "        pass\n",
    "\n",
    "def bootstrap_evaluation_bertweet(model, tokenizer, data, model_sentiment_encoder, model_emotion_encoder,\n",
    "                                data_sentiment_encoder, data_emotion_encoder, \n",
    "                                n_iterations=1000, sample_size=95):\n",
    "    print(f\"🔄 Starting BERTweet bootstrap evaluation...\")\n",
    "    print(f\"   Iterations: {n_iterations}\")\n",
    "    print(f\"   Sample size: {sample_size}\")\n",
    "    \n",
    "    results = {\n",
    "        'sentiment_accuracy': [],\n",
    "        'sentiment_f1': [],\n",
    "        'emotion_accuracy': [],\n",
    "        'emotion_f1': []\n",
    "    }\n",
    "    \n",
    "    texts = data['texts']\n",
    "    sentiment_labels = data['sentiment_labels']\n",
    "    emotion_labels = data['emotion_labels']\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Bootstrap sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
    "        \n",
    "        sample_texts = [texts[idx] for idx in indices]\n",
    "        sample_sentiment_labels = [sentiment_labels[idx] for idx in indices]\n",
    "        sample_emotion_labels = [emotion_labels[idx] for idx in indices]\n",
    "        \n",
    "        # Evaluate on bootstrap sample\n",
    "        metrics = evaluate_bertweet_on_bootstrap_sample(\n",
    "            model, tokenizer, sample_texts, sample_sentiment_labels, sample_emotion_labels,\n",
    "            model_sentiment_encoder, model_emotion_encoder,\n",
    "            data_sentiment_encoder, data_emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results['sentiment_accuracy'].append(metrics['sentiment_accuracy'])\n",
    "        results['sentiment_f1'].append(metrics['sentiment_f1'])\n",
    "        results['emotion_accuracy'].append(metrics['emotion_accuracy'])\n",
    "        results['emotion_f1'].append(metrics['emotion_f1'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"BERTweet bootstrap analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05733328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bertweet_bootstrap_analysis():\n",
    "    print(\"🚀 Running BERTweet Bootstrap Analysis on General Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load external datasets\n",
    "    print(\"\\n📂 Loading general evaluation datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # 2. Prepare evaluation data\n",
    "    print(\"Preparing SST-2 evaluation data...\")\n",
    "    sst2_eval_data = prepare_sst2_evaluation_data(sentiment_data)\n",
    "    print(\"Preparing GoEmotions evaluation data...\")\n",
    "    goemotions_eval_data = prepare_goemotions_evaluation_data(emotion_data)\n",
    "    print(\"Preparing multitask evaluation data...\")\n",
    "    multitask_eval_data = prepare_multitask_evaluation_data(sst2_eval_data, goemotions_eval_data)\n",
    "    \n",
    "    # 3. Load models\n",
    "    # Load single task models\n",
    "    sentiment_model_path = \"./bertweet_trained_models_seeds/bertweet_sentiment_seed_42\"\n",
    "    sentiment_model, sentiment_tokenizer, sentiment_encoder = load_bertweet_model_for_bootstrap(\n",
    "        sentiment_model_path, \"sentiment\"\n",
    "    )\n",
    "    \n",
    "    emotion_model_path = \"./bertweet_trained_models_seeds/bertweet_emotion_seed_42\"\n",
    "    emotion_model, emotion_tokenizer, emotion_encoder = load_bertweet_model_for_bootstrap(\n",
    "        emotion_model_path, \"emotion\"\n",
    "    )\n",
    "    \n",
    "    # Load multitask model\n",
    "    multitask_model_path = \"./bertweet_trained_models_seeds/bertweet_multitask_seed_42\"\n",
    "    multitask_model, multitask_tokenizer, multitask_sent_encoder, multitask_emot_encoder = load_bertweet_model_for_bootstrap(\n",
    "        multitask_model_path, \"multitask\"\n",
    "    )\n",
    "    \n",
    "    # 4. Run bootstrap evaluation\n",
    "    print(\"\\n🔄 Starting bootstrap evaluation...\")\n",
    "    n_iterations = 1000\n",
    "    sample_size = 95\n",
    "    \n",
    "    # Initialize results dictionary for F1 scores\n",
    "    f1_results = {\n",
    "        'sentiment_single': [],\n",
    "        'emotion_single': [],\n",
    "        'multitask_sentiment': [],\n",
    "        'multitask_emotion': []\n",
    "    }\n",
    "    \n",
    "    # Run bootstrap iterations\n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Sample indices with replacement for each dataset\n",
    "        sst2_indices = np.random.choice(len(sst2_eval_data['texts']), size=sample_size, replace=True)\n",
    "        goemotions_indices = np.random.choice(len(goemotions_eval_data['texts']), size=sample_size, replace=True)\n",
    "        \n",
    "        # Prepare bootstrap samples\n",
    "        sst2_sample = {\n",
    "            'texts': [sst2_eval_data['texts'][i] for i in sst2_indices],\n",
    "            'sentiment_labels': [sst2_eval_data['sentiment_labels'][i] for i in sst2_indices]\n",
    "        }\n",
    "        \n",
    "        goemotions_sample = {\n",
    "            'texts': [goemotions_eval_data['texts'][i] for i in goemotions_indices],\n",
    "            'emotion_labels': [goemotions_eval_data['emotion_labels'][i] for i in goemotions_indices]\n",
    "        }\n",
    "        \n",
    "        multitask_sample = {\n",
    "            'texts': sst2_sample['texts'],  # Use SST-2 texts for multitask\n",
    "            'sentiment_labels': sst2_sample['sentiment_labels'],\n",
    "            'emotion_labels': [goemotions_eval_data['emotion_labels'][i] for i in sst2_indices]\n",
    "        }\n",
    "        \n",
    "        # Evaluate single task models\n",
    "        sentiment_results = evaluate_bertweet_single_task(\n",
    "            sentiment_model, sentiment_tokenizer, sentiment_encoder, \n",
    "            sst2_sample, 'sentiment'\n",
    "        )\n",
    "        f1_results['sentiment_single'].append(sentiment_results['macro_f1'])\n",
    "        \n",
    "        emotion_results = evaluate_bertweet_single_task(\n",
    "            emotion_model, emotion_tokenizer, emotion_encoder, \n",
    "            goemotions_sample, 'emotion'\n",
    "        )\n",
    "        f1_results['emotion_single'].append(emotion_results['macro_f1'])\n",
    "        \n",
    "        # Evaluate multitask model\n",
    "        multitask_results = evaluate_bertweet_multitask(\n",
    "            multitask_model, multitask_tokenizer, \n",
    "            multitask_sent_encoder, multitask_emot_encoder,\n",
    "            multitask_sample\n",
    "        )\n",
    "        f1_results['multitask_sentiment'].append(multitask_results['sentiment']['macro_f1'])\n",
    "        f1_results['multitask_emotion'].append(multitask_results['emotion']['macro_f1'])\n",
    "    \n",
    "    # 5. Calculate and display statistics\n",
    "    print(\"\\n📊 BERTweet Bootstrap Analysis Results (General Datasets)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, f1_scores in f1_results.items():\n",
    "        values = np.array(f1_scores)\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        ci_lower = np.percentile(values, 2.5)\n",
    "        ci_upper = np.percentile(values, 97.5)\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name.replace('_', ' ').upper()} - F1\")\n",
    "        print(f\"   Mean: {mean:.4f}\")\n",
    "        print(f\"   Std:  {std:.4f}\")\n",
    "        print(f\"   95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # 6. Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_bootstrap_general_datasets_{timestamp}.json\"\n",
    "    \n",
    "    results_to_save = {\n",
    "        model_name: {\n",
    "            'values': [float(x) for x in values],\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)),\n",
    "            'ci_lower': float(np.percentile(values, 2.5)),\n",
    "            'ci_upper': float(np.percentile(values, 97.5))\n",
    "        }\n",
    "        for model_name, values in f1_results.items()\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Bootstrap results saved to: {results_file}\")\n",
    "    \n",
    "    return results_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5dec111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BERTweet Bootstrap Analysis on General Datasets\n",
      "============================================================\n",
      "\n",
      "📂 Loading general evaluation datasets...\n",
      "Loading external datasets...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 validation samples\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 validation, 5427 test samples\n",
      "Preparing SST-2 evaluation data...\n",
      "Preparing SST-2 evaluation data...\n",
      "✅ SST-2 evaluation data prepared: 872 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "Preparing GoEmotions evaluation data...\n",
      "Preparing GoEmotions evaluation data...\n",
      "✅ GoEmotions evaluation data prepared: 1000 samples\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "Preparing multitask evaluation data...\n",
      "Preparing multitask evaluation data...\n",
      "✅ Multitask evaluation data prepared: 872 samples\n",
      "📥 Loading BERTweet sentiment model from ./bertweet_trained_models_seeds/bertweet_sentiment_seed_42...\n",
      "📥 Loading BERTweet emotion model from ./bertweet_trained_models_seeds/bertweet_emotion_seed_42...\n",
      "📥 Loading BERTweet multitask model from ./bertweet_trained_models_seeds/bertweet_multitask_seed_42...\n",
      "\n",
      "🔄 Starting bootstrap evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|██████████| 1000/1000 [17:36<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 BERTweet Bootstrap Analysis Results (General Datasets)\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT SINGLE - F1\n",
      "   Mean: 0.3786\n",
      "   Std:  0.0280\n",
      "   95% CI: [0.3267, 0.4359]\n",
      "\n",
      "🎯 EMOTION SINGLE - F1\n",
      "   Mean: 0.7252\n",
      "   Std:  0.0510\n",
      "   95% CI: [0.6200, 0.8191]\n",
      "\n",
      "🎯 MULTITASK SENTIMENT - F1\n",
      "   Mean: 0.3767\n",
      "   Std:  0.0280\n",
      "   95% CI: [0.3238, 0.4310]\n",
      "\n",
      "🎯 MULTITASK EMOTION - F1\n",
      "   Mean: 0.1071\n",
      "   Std:  0.0187\n",
      "   95% CI: [0.0719, 0.1421]\n",
      "\n",
      "💾 Bootstrap results saved to: ./bertweet_seed_analysis_results/bertweet_bootstrap_general_datasets_20250725_162259.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Run BERTweet Bootstrap Analysis\n",
    "\n",
    "# Run bootstrap analysis\n",
    "bootstrap_stats = run_bertweet_bootstrap_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ead5e",
   "metadata": {},
   "source": [
    "# Reddit specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43b40f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Libraries imported and setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports for BERTweet Seed & Bootstrap Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import random\n",
    "from collections import Counter\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./bertweet_seed_analysis_results\", exist_ok=True)\n",
    "os.makedirs(\"./bertweet_trained_models_seeds\", exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31672e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Utility Functions for Analysis\n",
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e948c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet model architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: BERTweet Model Architectures\n",
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.bertweet.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERTweet outputs\n",
    "        outputs = self.bertweet(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.bertweet.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.bertweet(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        shared_pooled = shared_attended[:, 0, :]\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "        outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "        \n",
    "        # Emotion branch\n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "        emotion_logits = self.emotion_classifier(emotion_features)\n",
    "        outputs[\"emotion_logits\"] = emotion_logits\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"BERTweet model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ec8a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Classes for BERTweet\n",
    "class BERTweetDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BERTweetMultiTaskDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], sentiment_labels: List[int], \n",
    "                 emotion_labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"BERTweet dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf64a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data Loading Functions for BERTweet Analysis\n",
    "def load_external_datasets() -> Tuple[Dict, Dict]:\n",
    "    print(\"Loading external datasets...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        sentiment_data = {\n",
    "            'train': sst2_dataset['train'],\n",
    "            'validation': sst2_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sentiment_data['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        emotion_data = {\n",
    "            'train': emotions_dataset['train'],\n",
    "            'validation': emotions_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotion_data['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def prepare_reddit_evaluation_data(reddit_data_path: str) -> Dict:\n",
    "    print(f\"Loading Reddit evaluation data from {reddit_data_path}...\")\n",
    "    \n",
    "    df = pd.read_csv(reddit_data_path)\n",
    "    \n",
    "    # Create label encoders that match BERTweet models\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # Fit encoders\n",
    "    sentiment_encoder.fit(df['sentiment'].tolist())\n",
    "    emotion_encoder.fit(df['emotion'].tolist())\n",
    "    \n",
    "    reddit_data = {\n",
    "        'texts': df['text_content'].tolist(),\n",
    "        'sentiment_labels_text': df['sentiment'].tolist(),\n",
    "        'emotion_labels_text': df['emotion'].tolist(),\n",
    "        'sentiment_labels': sentiment_encoder.transform(df['sentiment'].tolist()),\n",
    "        'emotion_labels': emotion_encoder.transform(df['emotion'].tolist()),\n",
    "        'sentiment_encoder': sentiment_encoder,\n",
    "        'emotion_encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Reddit data prepared: {len(reddit_data['texts'])} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return reddit_data\n",
    "\n",
    "def prepare_bertweet_training_data(sentiment_data: Dict, emotion_data: Dict, max_samples: int = 5000):\n",
    "    \"\"\"Prepare training data for BERTweet models\"\"\"\n",
    "    \n",
    "    # Process sentiment data (SST-2 to 3 classes)\n",
    "    sentiment_texts = sentiment_data['train']['sentence'][:max_samples]\n",
    "    sentiment_labels_raw = sentiment_data['train']['label'][:max_samples]\n",
    "    \n",
    "    # Convert SST-2 binary to 3-class sentiment\n",
    "    sentiment_labels = []\n",
    "    for label in sentiment_labels_raw:\n",
    "        if label == 0:  # Negative\n",
    "            sentiment_labels.append(0)\n",
    "        elif label == 1:  # Positive\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                sentiment_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                sentiment_labels.append(2)  # Positive\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in sentiment_labels:\n",
    "        neutral_indices = np.random.choice(len(sentiment_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            sentiment_labels[idx] = 1\n",
    "    \n",
    "    # Process emotion data (filter to first 6 classes)\n",
    "    emotion_texts_all = emotion_data['train']['text']\n",
    "    emotion_labels_all = emotion_data['train']['labels']\n",
    "    \n",
    "    emotion_texts = []\n",
    "    emotion_labels = []\n",
    "    count = 0\n",
    "    for i, label in enumerate(emotion_labels_all):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):\n",
    "                emotion_texts.append(emotion_texts_all[i])\n",
    "                emotion_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                emotion_texts.append(emotion_texts_all[i])\n",
    "                emotion_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoders\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    return {\n",
    "        'sentiment_data': {\n",
    "            'texts': sentiment_texts,\n",
    "            'labels': sentiment_labels,\n",
    "            'encoder': sentiment_encoder\n",
    "        },\n",
    "        'emotion_data': {\n",
    "            'texts': emotion_texts,\n",
    "            'labels': emotion_labels,\n",
    "            'encoder': emotion_encoder\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87b499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet training functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: BERTweet Training Functions with Best Parameters\n",
    "def train_bertweet_single_task(\n",
    "    task_type: str,  # 'sentiment' or 'emotion'\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 5000\n",
    ") -> Tuple[any, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet {task_type} model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Get appropriate data\n",
    "    if task_type == 'sentiment':\n",
    "        texts = training_data['sentiment_data']['texts'][:max_samples]\n",
    "        labels = training_data['sentiment_data']['labels'][:max_samples]\n",
    "        encoder = training_data['sentiment_data']['encoder']\n",
    "        num_classes = 3\n",
    "    else:  # emotion\n",
    "        texts = training_data['emotion_data']['texts'][:max_samples]\n",
    "        labels = training_data['emotion_data']['labels'][:max_samples]\n",
    "        encoder = training_data['emotion_data']['encoder']\n",
    "        num_classes = 6\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetSingleTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        num_classes=num_classes,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetDataset(texts, labels, tokenizer, max_length=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs['logits'], labels_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_{task_type}_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"task_type\": task_type,\n",
    "        \"model_type\": \"BERTweetSingleTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoder\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(encoder, os.path.join(output_dir, f'{task_type}_encoder.pkl'))\n",
    "    \n",
    "    print(f\"✅ BERTweet {task_type} model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "def train_bertweet_multitask(\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 2000\n",
    ") -> Tuple[any, LabelEncoder, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet multitask model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Prepare multitask data (combine sentiment and emotion data)\n",
    "    min_length = min(len(training_data['sentiment_data']['texts']), \n",
    "                     len(training_data['emotion_data']['texts']))\n",
    "    min_length = min(min_length, max_samples)\n",
    "    \n",
    "    combined_texts = training_data['sentiment_data']['texts'][:min_length]\n",
    "    combined_sentiment_labels = training_data['sentiment_data']['labels'][:min_length]\n",
    "    combined_emotion_labels = training_data['emotion_data']['labels'][:min_length]\n",
    "    \n",
    "    sentiment_encoder = training_data['sentiment_data']['encoder']\n",
    "    emotion_encoder = training_data['emotion_data']['encoder']\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetMultiTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        sentiment_num_classes=3,\n",
    "        emotion_num_classes=6,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetMultiTaskDataset(\n",
    "        combined_texts, combined_sentiment_labels, combined_emotion_labels, \n",
    "        tokenizer, max_length=128\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    emotion_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    alpha = best_params['alpha']\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = sentiment_criterion(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = emotion_criterion(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss_batch = alpha * sentiment_loss + (1 - alpha) * emotion_loss\n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"sentiment_num_classes\": 3,\n",
    "        \"emotion_num_classes\": 6,\n",
    "        \"model_type\": \"BERTweetMultiTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoders\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    print(f\"BERTweet multitask model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, sentiment_encoder, emotion_encoder\n",
    "\n",
    "print(\"BERTweet training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81d537cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions for BERTweet Models\n",
    "def evaluate_bertweet_single_task(model, tokenizer, label_encoder, reddit_data: Dict, task_type: str) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_labels = reddit_data[f'{task_type}_labels']\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 16):  # Batch size 16\n",
    "            batch_texts = texts[i:i+16]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs['logits']\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                pred_id = preds[j].item()\n",
    "                confidence = probs[j][pred_id].item()\n",
    "                \n",
    "                # Handle out of range predictions\n",
    "                if pred_id >= len(label_encoder.classes_):\n",
    "                    pred_id = 0\n",
    "                \n",
    "                predictions.append(pred_id)\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'predictions': predictions,\n",
    "        'confidences': confidences,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "def evaluate_bertweet_multitask(model, tokenizer, sentiment_encoder, emotion_encoder, \n",
    "                               reddit_data: Dict, max_length: int = 128) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_sentiment_labels = reddit_data['sentiment_labels']\n",
    "    true_emotion_labels = reddit_data['emotion_labels']\n",
    "    \n",
    "    sentiment_predictions = []\n",
    "    emotion_predictions = []\n",
    "    sentiment_confidences = []\n",
    "    emotion_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 8):  # Smaller batch size for multitask\n",
    "            batch_texts = texts[i:i+8]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Process sentiment\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "            \n",
    "            # Process emotion\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                # Sentiment\n",
    "                sent_id = sentiment_preds[j].item()\n",
    "                sent_conf = sentiment_probs[j][sent_id].item()\n",
    "                if sent_id >= len(sentiment_encoder.classes_):\n",
    "                    sent_id = 0\n",
    "                sentiment_predictions.append(sent_id)\n",
    "                sentiment_confidences.append(sent_conf)\n",
    "                \n",
    "                # Emotion\n",
    "                emot_id = emotion_preds[j].item()\n",
    "                emot_conf = emotion_probs[j][emot_id].item()\n",
    "                if emot_id >= len(emotion_encoder.classes_):\n",
    "                    emot_id = 0\n",
    "                emotion_predictions.append(emot_id)\n",
    "                emotion_confidences.append(emot_conf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sentiment_accuracy = accuracy_score(true_sentiment_labels, sentiment_predictions)\n",
    "    sentiment_f1 = f1_score(true_sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    emotion_accuracy = accuracy_score(true_emotion_labels, emotion_predictions)\n",
    "    emotion_f1 = f1_score(true_emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'sentiment': {\n",
    "            'accuracy': sentiment_accuracy,\n",
    "            'macro_f1': sentiment_f1,\n",
    "            'predictions': sentiment_predictions,\n",
    "            'confidences': sentiment_confidences\n",
    "        },\n",
    "        'emotion': {\n",
    "            'accuracy': emotion_accuracy,\n",
    "            'macro_f1': emotion_f1,\n",
    "            'predictions': emotion_predictions,\n",
    "            'confidences': emotion_confidences\n",
    "        },\n",
    "        'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "        'combined_f1': (sentiment_f1 + emotion_f1) / 2\n",
    "    }\n",
    "\n",
    "print(\"BERTweet evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7906c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet random seed analysis function defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: BERTweet Random Seed Analysis Function\n",
    "def run_bertweet_seed_analysis(\n",
    "    reddit_data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    seeds: List[int] = [42, 123, 456, 789, 999],\n",
    "    max_training_samples: int = 3000\n",
    "):\n",
    "    \n",
    "    print(\"🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Seeds to test: {seeds}\")\n",
    "    print(f\"Max training samples per dataset: {max_training_samples}\")\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(\"\\n🔄 Preparing BERTweet training data...\")\n",
    "    training_data = prepare_bertweet_training_data(sentiment_data, emotion_data, max_training_samples)\n",
    "    \n",
    "    # Load Reddit evaluation data\n",
    "    print(\"\\n📂 Loading Reddit evaluation data...\")\n",
    "    reddit_data = prepare_reddit_evaluation_data(reddit_data_path)\n",
    "    \n",
    "    # Define best parameters for each BERTweet model\n",
    "    best_params = {\n",
    "        'sentiment': {\n",
    "            'learning_rate': 3.65445235521325e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'emotion': {\n",
    "            'learning_rate': 3.65445235521325e-05, \n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'multitask': {\n",
    "            'learning_rate': 4.166863122305896e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15142344384136117,\n",
    "            'weight_decay': 0.06331731119758383,\n",
    "            'hidden_dropout_prob': 0.10929008254399955,\n",
    "            'classifier_dropout': 0.22150897038028766,\n",
    "            'alpha': 0.4341048247374583\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    # Store results for each seed\n",
    "    all_results = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n🌱 TRAINING AND EVALUATING BERTWEET WITH SEED {seed}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        seed_results = {}\n",
    "        \n",
    "        # 1. Train and evaluate BERTweet Sentiment\n",
    "        print(f\"\\n1️⃣ BERTweet Sentiment (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'sentiment', best_params['sentiment'], seed, \n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_sentiment_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, reddit_data, 'sentiment')\n",
    "        seed_results['bertweet_sentiment'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 2. Train and evaluate BERTweet Emotion\n",
    "        print(f\"\\n2️⃣ BERTweet Emotion (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'emotion', best_params['emotion'], seed,\n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_emotion_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, reddit_data, 'emotion')\n",
    "        seed_results['bertweet_emotion'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 3. Train and evaluate BERTweet Multitask\n",
    "        print(f\"\\n3️⃣ BERTweet Multitask (Seed {seed})\")\n",
    "        model, sent_enc, emot_enc = train_bertweet_multitask(\n",
    "            best_params['multitask'], seed, training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_bertweet_multitask(\n",
    "            model, tokenizer, sent_enc, emot_enc, reddit_data, 128\n",
    "        )\n",
    "        seed_results['bertweet_multitask'] = results\n",
    "        print(f\"   Sentiment - Accuracy: {results['sentiment']['accuracy']:.4f}, F1: {results['sentiment']['macro_f1']:.4f}\")\n",
    "        print(f\"   Emotion - Accuracy: {results['emotion']['accuracy']:.4f}, F1: {results['emotion']['macro_f1']:.4f}\")\n",
    "        print(f\"   Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        all_results[seed] = seed_results\n",
    "        \n",
    "        print(f\"\\n✅ Completed evaluation for seed {seed}\")\n",
    "    \n",
    "    # Analyze stability across seeds\n",
    "    print(f\"\\n📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    stability_analysis = analyze_bertweet_seed_stability(all_results, seeds)\n",
    "    \n",
    "    # Save results\n",
    "    save_bertweet_results(all_results, stability_analysis, seeds)\n",
    "    \n",
    "    return all_results, stability_analysis\n",
    "\n",
    "print(\"✅ BERTweet random seed analysis function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc3da3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet stability analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: BERTweet Stability Analysis Functions\n",
    "def analyze_bertweet_seed_stability(all_results: Dict, seeds: List[int]) -> Dict:\n",
    "    \n",
    "    stability_stats = {}\n",
    "    \n",
    "    # Define model-task combinations\n",
    "    evaluations = [\n",
    "        ('bertweet_sentiment', 'sentiment'),\n",
    "        ('bertweet_emotion', 'emotion'),\n",
    "        ('bertweet_multitask', 'sentiment'),\n",
    "        ('bertweet_multitask', 'emotion')\n",
    "    ]\n",
    "    \n",
    "    for model_name, task in evaluations:\n",
    "        print(f\"\\n🔍 {model_name.upper()} - {task.upper()}\")\n",
    "        \n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            if model_name in all_results[seed]:\n",
    "                result = all_results[seed][model_name]\n",
    "                \n",
    "                if model_name.endswith('_multitask'):\n",
    "                    acc = result[task]['accuracy']\n",
    "                    f1 = result[task]['macro_f1']\n",
    "                else:\n",
    "                    acc = result['accuracy']\n",
    "                    f1 = result['macro_f1']\n",
    "                \n",
    "                accuracies.append(acc)\n",
    "                f1_scores.append(f1)\n",
    "        \n",
    "        if accuracies:\n",
    "            acc_mean = np.mean(accuracies)\n",
    "            acc_std = np.std(accuracies)\n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            \n",
    "            stability_stats[f\"{model_name}_{task}\"] = {\n",
    "                'accuracy_mean': acc_mean,\n",
    "                'accuracy_std': acc_std,\n",
    "                'f1_mean': f1_mean,\n",
    "                'f1_std': f1_std,\n",
    "                'accuracy_values': accuracies,\n",
    "                'f1_values': f1_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "            print(f\"   Macro F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    return stability_stats\n",
    "\n",
    "def save_bertweet_results(all_results: Dict, stability_analysis: Dict, seeds: List[int]):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_raw_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        serializable_results = {}\n",
    "        for seed, seed_results in all_results.items():\n",
    "            serializable_results[str(seed)] = {}\n",
    "            for model, results in seed_results.items():\n",
    "                if isinstance(results, dict):\n",
    "                    serializable_results[str(seed)][model] = {}\n",
    "                    for key, value in results.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            serializable_results[str(seed)][model][key] = {\n",
    "                                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                                   [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in v] if isinstance(v, list) else v\n",
    "                                for k, v in value.items()\n",
    "                            }\n",
    "                        else:\n",
    "                            serializable_results[str(seed)][model][key] = float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # Save stability analysis\n",
    "    stability_file = f\"./bertweet_seed_analysis_results/bertweet_stability_analysis_{timestamp}.json\"\n",
    "    with open(stability_file, 'w') as f:\n",
    "        serializable_stability = {}\n",
    "        for key, stats in stability_analysis.items():\n",
    "            serializable_stability[key] = {\n",
    "                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                   [float(x) for x in v] if isinstance(v, list) else v\n",
    "                for k, v in stats.items()\n",
    "            }\n",
    "        json.dump(serializable_stability, f, indent=2)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = f\"./bertweet_seed_analysis_results/bertweet_summary_report_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"BERTWEET RANDOM SEED ANALYSIS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Seeds tested: {seeds}\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"STABILITY ANALYSIS (Mean ± Std)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        for key, stats in stability_analysis.items():\n",
    "            model_task = key.replace('_', ' ').title()\n",
    "            f.write(f\"\\n{model_task}:\\n\")\n",
    "            f.write(f\"  Accuracy: {stats['accuracy_mean']:.4f} ± {stats['accuracy_std']:.4f}\\n\")\n",
    "            f.write(f\"  Macro F1: {stats['f1_mean']:.4f} ± {stats['f1_std']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nBest Performers (by mean F1 score):\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Find best performers\n",
    "        sentiment_best = max([k for k in stability_analysis.keys() if 'sentiment' in k], \n",
    "                           key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        emotion_best = max([k for k in stability_analysis.keys() if 'emotion' in k], \n",
    "                         key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        \n",
    "        f.write(f\"Sentiment: {sentiment_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[sentiment_best]['f1_mean']:.4f})\\n\")\n",
    "        f.write(f\"Emotion: {emotion_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[emotion_best]['f1_mean']:.4f})\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 BERTweet results saved:\")\n",
    "    print(f\"   Raw results: {results_file}\")\n",
    "    print(f\"   Stability analysis: {stability_file}\")\n",
    "    print(f\"   Summary report: {summary_file}\")\n",
    "\n",
    "print(\"BERTweet stability analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95302a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\n",
      "======================================================================\n",
      "Seeds to test: [42, 123, 456, 789, 999]\n",
      "Max training samples per dataset: 3000\n",
      "\n",
      "📂 Loading external datasets...\n",
      "Loading external datasets...\n",
      "✅ SST-2 dataset loaded: 67349 train samples\n",
      "✅ GoEmotions dataset loaded: 43410 train samples\n",
      "\n",
      "🔄 Preparing BERTweet training data...\n",
      "\n",
      "📂 Loading Reddit evaluation data...\n",
      "Loading Reddit evaluation data from annotated_reddit_posts.csv...\n",
      "✅ Reddit data prepared: 95 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 42\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 42)\n",
      "🚀 Training BERTweet sentiment model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.9036\n",
      "Epoch 2/3, Average Loss: 0.5374\n",
      "Epoch 3/3, Average Loss: 0.4057\n",
      "✅ BERTweet sentiment model trained and saved with seed 42\n",
      "   Accuracy: 0.6105, Macro F1: 0.4038\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 42)\n",
      "🚀 Training BERTweet emotion model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3897\n",
      "Epoch 2/3, Average Loss: 0.6943\n",
      "Epoch 3/3, Average Loss: 0.4847\n",
      "✅ BERTweet emotion model trained and saved with seed 42\n",
      "   Accuracy: 0.1579, Macro F1: 0.0941\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 42)\n",
      "🚀 Training BERTweet multitask model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5970\n",
      "Epoch 2/3, Average Loss: 1.2914\n",
      "Epoch 3/3, Average Loss: 1.1998\n",
      "BERTweet multitask model trained and saved with seed 42\n",
      "   Sentiment - Accuracy: 0.6105, F1: 0.4152\n",
      "   Emotion - Accuracy: 0.2842, F1: 0.1249\n",
      "   Combined - Accuracy: 0.4474, F1: 0.2700\n",
      "\n",
      "✅ Completed evaluation for seed 42\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 123\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 123)\n",
      "🚀 Training BERTweet sentiment model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8028\n",
      "Epoch 2/3, Average Loss: 0.4705\n",
      "Epoch 3/3, Average Loss: 0.3504\n",
      "✅ BERTweet sentiment model trained and saved with seed 123\n",
      "   Accuracy: 0.6000, Macro F1: 0.4073\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 123)\n",
      "🚀 Training BERTweet emotion model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3615\n",
      "Epoch 2/3, Average Loss: 0.6956\n",
      "Epoch 3/3, Average Loss: 0.4921\n",
      "✅ BERTweet emotion model trained and saved with seed 123\n",
      "   Accuracy: 0.1684, Macro F1: 0.0996\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 123)\n",
      "🚀 Training BERTweet multitask model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5860\n",
      "Epoch 2/3, Average Loss: 1.3214\n",
      "Epoch 3/3, Average Loss: 1.1987\n",
      "BERTweet multitask model trained and saved with seed 123\n",
      "   Sentiment - Accuracy: 0.6000, F1: 0.3988\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0702\n",
      "   Combined - Accuracy: 0.4263, F1: 0.2345\n",
      "\n",
      "✅ Completed evaluation for seed 123\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 456\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 456)\n",
      "🚀 Training BERTweet sentiment model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8234\n",
      "Epoch 2/3, Average Loss: 0.4725\n",
      "Epoch 3/3, Average Loss: 0.3779\n",
      "✅ BERTweet sentiment model trained and saved with seed 456\n",
      "   Accuracy: 0.6105, Macro F1: 0.4123\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 456)\n",
      "🚀 Training BERTweet emotion model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3945\n",
      "Epoch 2/3, Average Loss: 0.7019\n",
      "Epoch 3/3, Average Loss: 0.4818\n",
      "✅ BERTweet emotion model trained and saved with seed 456\n",
      "   Accuracy: 0.1579, Macro F1: 0.0937\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 456)\n",
      "🚀 Training BERTweet multitask model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6678\n",
      "Epoch 2/3, Average Loss: 1.5130\n",
      "Epoch 3/3, Average Loss: 1.4590\n",
      "BERTweet multitask model trained and saved with seed 456\n",
      "   Sentiment - Accuracy: 0.5474, F1: 0.2358\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "   Combined - Accuracy: 0.4000, F1: 0.1515\n",
      "\n",
      "✅ Completed evaluation for seed 456\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 789\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 789)\n",
      "🚀 Training BERTweet sentiment model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8127\n",
      "Epoch 2/3, Average Loss: 0.4849\n",
      "Epoch 3/3, Average Loss: 0.3785\n",
      "✅ BERTweet sentiment model trained and saved with seed 789\n",
      "   Accuracy: 0.5895, Macro F1: 0.3846\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 789)\n",
      "🚀 Training BERTweet emotion model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3752\n",
      "Epoch 2/3, Average Loss: 0.6841\n",
      "Epoch 3/3, Average Loss: 0.4946\n",
      "✅ BERTweet emotion model trained and saved with seed 789\n",
      "   Accuracy: 0.1579, Macro F1: 0.0944\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 789)\n",
      "🚀 Training BERTweet multitask model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6214\n",
      "Epoch 2/3, Average Loss: 1.2824\n",
      "Epoch 3/3, Average Loss: 1.1796\n",
      "BERTweet multitask model trained and saved with seed 789\n",
      "   Sentiment - Accuracy: 0.5579, F1: 0.3885\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "   Combined - Accuracy: 0.4053, F1: 0.2279\n",
      "\n",
      "✅ Completed evaluation for seed 789\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 999\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 999)\n",
      "🚀 Training BERTweet sentiment model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.9292\n",
      "Epoch 2/3, Average Loss: 0.9331\n",
      "Epoch 3/3, Average Loss: 0.9326\n",
      "✅ BERTweet sentiment model trained and saved with seed 999\n",
      "   Accuracy: 0.2632, Macro F1: 0.1725\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 999)\n",
      "🚀 Training BERTweet emotion model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3259\n",
      "Epoch 2/3, Average Loss: 0.6866\n",
      "Epoch 3/3, Average Loss: 0.4806\n",
      "✅ BERTweet emotion model trained and saved with seed 999\n",
      "   Accuracy: 0.1789, Macro F1: 0.1019\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 999)\n",
      "🚀 Training BERTweet multitask model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5872\n",
      "Epoch 2/3, Average Loss: 1.3075\n",
      "Epoch 3/3, Average Loss: 1.1884\n",
      "BERTweet multitask model trained and saved with seed 999\n",
      "   Sentiment - Accuracy: 0.5684, F1: 0.3300\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.1043\n",
      "   Combined - Accuracy: 0.4105, F1: 0.2172\n",
      "\n",
      "✅ Completed evaluation for seed 999\n",
      "\n",
      "📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\n",
      "======================================================================\n",
      "\n",
      "🔍 BERTWEET_SENTIMENT - SENTIMENT\n",
      "   Accuracy: 0.5347 ± 0.1360\n",
      "   Macro F1: 0.3561 ± 0.0923\n",
      "\n",
      "🔍 BERTWEET_EMOTION - EMOTION\n",
      "   Accuracy: 0.1642 ± 0.0084\n",
      "   Macro F1: 0.0967 ± 0.0034\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - SENTIMENT\n",
      "   Accuracy: 0.5768 ± 0.0244\n",
      "   Macro F1: 0.3537 ± 0.0656\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - EMOTION\n",
      "   Accuracy: 0.2589 ± 0.0126\n",
      "   Macro F1: 0.0868 ± 0.0237\n",
      "❌ Error during analysis: Object of type ndarray is not JSON serializable\n",
      "🔧 Try restarting the kernel and running cells 1-9 again.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Run BERTweet Random Seed Analysis (Fixed)\n",
    "\n",
    "# Clear any previous results\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Run BERTweet random seed analysis with the fixed saving function\n",
    "try:\n",
    "    all_results, stability_analysis = run_bertweet_seed_analysis(\n",
    "        reddit_data_path=\"annotated_reddit_posts.csv\",\n",
    "        seeds=[42, 123, 456, 789, 999],  # 5 different seeds\n",
    "        max_training_samples=3000  # Reduced for faster training\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🎉 BERTWEET RANDOM SEED ANALYSIS COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Check the './bertweet_seed_analysis_results/' directory for detailed results.\")\n",
    "    \n",
    "    # Display quick summary\n",
    "    print(\"\\n📊 QUICK STABILITY SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name in ['BERTWEET_SENTIMENT', 'BERTWEET_EMOTION', 'BERTWEET_MULTITASK']:\n",
    "        if model_name in stability_analysis:\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            for task in ['sentiment', 'emotion']:\n",
    "                if task in stability_analysis[model_name]:\n",
    "                    metrics = stability_analysis[model_name][task]\n",
    "                    print(f\"  {task.title()}:\")\n",
    "                    print(f\"    Accuracy: {metrics.get('accuracy_mean', 0):.3f} ± {metrics.get('accuracy_std', 0):.3f}\")\n",
    "                    print(f\"    F1 Score: {metrics.get('f1_mean', 0):.3f} ± {metrics.get('f1_std', 0):.3f}\")\n",
    "                    print(f\"    Stability: {metrics.get('stability_score', 0):.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during analysis: {str(e)}\")\n",
    "    print(\"🔧 Try restarting the kernel and running cells 1-9 again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4886efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet bootstrap analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: BERTweet Bootstrap Analysis Functions\n",
    "def load_bertweet_model_for_bootstrap(model_path: str, model_type: str):\n",
    "    print(f\"📥 Loading BERTweet {model_type} model from {model_path}...\")\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_path, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        # Load multitask model\n",
    "        model = BERTweetMultiTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            sentiment_num_classes=config['sentiment_num_classes'],\n",
    "            emotion_num_classes=config['emotion_num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoders\n",
    "        sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "        emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, sentiment_encoder, emotion_encoder\n",
    "        \n",
    "    else:\n",
    "        # Load single-task model\n",
    "        model = BERTweetSingleTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            num_classes=config['num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoder\n",
    "        encoder = joblib.load(os.path.join(model_path, f'{config[\"task_type\"]}_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, encoder\n",
    "\n",
    "def evaluate_bertweet_on_bootstrap_sample(model, tokenizer, texts, sentiment_labels, emotion_labels, \n",
    "                                        model_sentiment_encoder, model_emotion_encoder, \n",
    "                                        data_sentiment_encoder, data_emotion_encoder, \n",
    "                                        model_type=\"multitask\", max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 8):\n",
    "                batch_texts = texts[i:i+8]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': inputs['input_ids'].to(device),\n",
    "                    'attention_mask': inputs['attention_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                outputs = model(**filtered_inputs)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                for j in range(len(batch_texts)):\n",
    "                    sent_id = sentiment_preds[j].item()\n",
    "                    emot_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    if sent_id >= len(model_sentiment_encoder.classes_):\n",
    "                        sent_id = 0\n",
    "                    if emot_id >= len(model_emotion_encoder.classes_):\n",
    "                        emot_id = 0\n",
    "                    \n",
    "                    sentiment_predictions.append(sent_id)\n",
    "                    emotion_predictions.append(emot_id)\n",
    "        \n",
    "        # Map predictions to data label space\n",
    "        mapped_sentiment_preds = []\n",
    "        mapped_emotion_preds = []\n",
    "        \n",
    "        for sent_pred, emot_pred in zip(sentiment_predictions, emotion_predictions):\n",
    "            sent_class = model_sentiment_encoder.classes_[sent_pred]\n",
    "            emot_class = model_emotion_encoder.classes_[emot_pred]\n",
    "            \n",
    "            try:\n",
    "                mapped_sent = data_sentiment_encoder.transform([sent_class])[0]\n",
    "                mapped_emot = data_emotion_encoder.transform([emot_class])[0]\n",
    "            except ValueError:\n",
    "                mapped_sent = 0\n",
    "                mapped_emot = 0\n",
    "            \n",
    "            mapped_sentiment_preds.append(mapped_sent)\n",
    "            mapped_emotion_preds.append(mapped_emot)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, mapped_sentiment_preds)\n",
    "        sentiment_f1 = f1_score(sentiment_labels, mapped_sentiment_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        emotion_accuracy = accuracy_score(emotion_labels, mapped_emotion_preds)\n",
    "        emotion_f1 = f1_score(emotion_labels, mapped_emotion_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1': emotion_f1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # Single task evaluation logic here\n",
    "        pass\n",
    "\n",
    "def bootstrap_evaluation_bertweet(model, tokenizer, data, model_sentiment_encoder, model_emotion_encoder,\n",
    "                                data_sentiment_encoder, data_emotion_encoder, \n",
    "                                n_iterations=1000, sample_size=95):\n",
    "    print(f\"🔄 Starting BERTweet bootstrap evaluation...\")\n",
    "    print(f\"   Iterations: {n_iterations}\")\n",
    "    print(f\"   Sample size: {sample_size}\")\n",
    "    \n",
    "    results = {\n",
    "        'sentiment_accuracy': [],\n",
    "        'sentiment_f1': [],\n",
    "        'emotion_accuracy': [],\n",
    "        'emotion_f1': []\n",
    "    }\n",
    "    \n",
    "    texts = data['texts']\n",
    "    sentiment_labels = data['sentiment_labels']\n",
    "    emotion_labels = data['emotion_labels']\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Bootstrap sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
    "        \n",
    "        sample_texts = [texts[idx] for idx in indices]\n",
    "        sample_sentiment_labels = [sentiment_labels[idx] for idx in indices]\n",
    "        sample_emotion_labels = [emotion_labels[idx] for idx in indices]\n",
    "        \n",
    "        # Evaluate on bootstrap sample\n",
    "        metrics = evaluate_bertweet_on_bootstrap_sample(\n",
    "            model, tokenizer, sample_texts, sample_sentiment_labels, sample_emotion_labels,\n",
    "            model_sentiment_encoder, model_emotion_encoder,\n",
    "            data_sentiment_encoder, data_emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results['sentiment_accuracy'].append(metrics['sentiment_accuracy'])\n",
    "        results['sentiment_f1'].append(metrics['sentiment_f1'])\n",
    "        results['emotion_accuracy'].append(metrics['emotion_accuracy'])\n",
    "        results['emotion_f1'].append(metrics['emotion_f1'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"BERTweet bootstrap analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75296fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bertweet_bootstrap_analysis():\n",
    "    print(\"🚀 Running BERTweet Bootstrap Analysis on General Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load all models (single task and multitask)\n",
    "    print(\"\\n📥 Loading models...\")\n",
    "    \n",
    "    # Load single task models\n",
    "    sentiment_model_path = \"./bertweet_trained_models_seeds/bertweet_sentiment_seed_42\"\n",
    "    sentiment_model, sentiment_tokenizer, sentiment_encoder = load_bertweet_model_for_bootstrap(\n",
    "        sentiment_model_path, \"sentiment\"\n",
    "    )\n",
    "    \n",
    "    emotion_model_path = \"./bertweet_trained_models_seeds/bertweet_emotion_seed_42\"\n",
    "    emotion_model, emotion_tokenizer, emotion_encoder = load_bertweet_model_for_bootstrap(\n",
    "        emotion_model_path, \"emotion\"\n",
    "    )\n",
    "    \n",
    "    # Load multitask model\n",
    "    multitask_model_path = \"./bertweet_trained_models_seeds/bertweet_multitask_seed_42\"\n",
    "    multitask_model, multitask_tokenizer, multitask_sent_encoder, multitask_emot_encoder = load_bertweet_model_for_bootstrap(\n",
    "        multitask_model_path, \"multitask\"\n",
    "    )\n",
    "    \n",
    "    # 2. Load evaluation data\n",
    "    print(\"\\n📂 Loading evaluation datasets...\")\n",
    "    reddit_data = prepare_reddit_evaluation_data(\"annotated_reddit_posts.csv\")\n",
    "    \n",
    "    # 3. Run bootstrap evaluation for each model\n",
    "    print(\"\\n🔄 Starting bootstrap evaluation...\")\n",
    "    n_iterations = 1000\n",
    "    sample_size = 95\n",
    "    \n",
    "    # Initialize results dictionary for F1 scores\n",
    "    f1_results = {\n",
    "        'sentiment_single': [],\n",
    "        'emotion_single': [],\n",
    "        'multitask_sentiment': [],\n",
    "        'multitask_emotion': []\n",
    "    }\n",
    "    \n",
    "    # Run bootstrap iterations\n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Sample indices with replacement\n",
    "        indices = np.random.choice(len(reddit_data['texts']), size=sample_size, replace=True)\n",
    "        \n",
    "        # Prepare bootstrap sample\n",
    "        sample_data = {\n",
    "            'texts': [reddit_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [reddit_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [reddit_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "        \n",
    "        # Evaluate single task models\n",
    "        sentiment_results = evaluate_bertweet_single_task(\n",
    "            sentiment_model, sentiment_tokenizer, sentiment_encoder, \n",
    "            sample_data, 'sentiment'\n",
    "        )\n",
    "        f1_results['sentiment_single'].append(sentiment_results['macro_f1'])\n",
    "        \n",
    "        emotion_results = evaluate_bertweet_single_task(\n",
    "            emotion_model, emotion_tokenizer, emotion_encoder, \n",
    "            sample_data, 'emotion'\n",
    "        )\n",
    "        f1_results['emotion_single'].append(emotion_results['macro_f1'])\n",
    "        \n",
    "        # Evaluate multitask model\n",
    "        multitask_results = evaluate_bertweet_multitask(\n",
    "            multitask_model, multitask_tokenizer, \n",
    "            multitask_sent_encoder, multitask_emot_encoder,\n",
    "            sample_data\n",
    "        )\n",
    "        f1_results['multitask_sentiment'].append(multitask_results['sentiment']['macro_f1'])\n",
    "        f1_results['multitask_emotion'].append(multitask_results['emotion']['macro_f1'])\n",
    "    \n",
    "    # 4. Calculate and display statistics\n",
    "    print(\"\\n📊 BERTweet Bootstrap Analysis Results\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, f1_scores in f1_results.items():\n",
    "        values = np.array(f1_scores)\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        ci_lower = np.percentile(values, 2.5)\n",
    "        ci_upper = np.percentile(values, 97.5)\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name.replace('_', ' ').upper()} - F1\")\n",
    "        print(f\"   Mean: {mean:.4f}\")\n",
    "        print(f\"   Std:  {std:.4f}\")\n",
    "        print(f\"   95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # 5. Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_bootstrap_results_{timestamp}.json\"\n",
    "    \n",
    "    results_to_save = {\n",
    "        model_name: {\n",
    "            'values': [float(x) for x in values],\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)),\n",
    "            'ci_lower': float(np.percentile(values, 2.5)),\n",
    "            'ci_upper': float(np.percentile(values, 97.5))\n",
    "        }\n",
    "        for model_name, values in f1_results.items()\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Bootstrap results saved to: {results_file}\")\n",
    "    \n",
    "    return results_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a3198c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BERTweet Bootstrap Analysis on General Datasets\n",
      "============================================================\n",
      "\n",
      "📥 Loading models...\n",
      "📥 Loading BERTweet sentiment model from ./bertweet_trained_models_seeds/bertweet_sentiment_seed_42...\n",
      "📥 Loading BERTweet emotion model from ./bertweet_trained_models_seeds/bertweet_emotion_seed_42...\n",
      "📥 Loading BERTweet multitask model from ./bertweet_trained_models_seeds/bertweet_multitask_seed_42...\n",
      "\n",
      "📂 Loading evaluation datasets...\n",
      "Loading Reddit evaluation data from annotated_reddit_posts.csv...\n",
      "✅ Reddit data prepared: 95 samples\n",
      "   Sentiment classes: [np.str_('Negative'), np.str_('Neutral'), np.str_('Positive')]\n",
      "   Emotion classes: [np.str_('Anger'), np.str_('Fear'), np.str_('Joy'), np.str_('No Emotion'), np.str_('Sadness'), np.str_('Surprise')]\n",
      "\n",
      "🔄 Starting bootstrap evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|██████████| 1000/1000 [18:54<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 BERTweet Bootstrap Analysis Results\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT SINGLE - F1\n",
      "   Mean: 0.4033\n",
      "   Std:  0.0419\n",
      "   95% CI: [0.3193, 0.4846]\n",
      "\n",
      "🎯 EMOTION SINGLE - F1\n",
      "   Mean: 0.0939\n",
      "   Std:  0.0222\n",
      "   95% CI: [0.0522, 0.1375]\n",
      "\n",
      "🎯 MULTITASK SENTIMENT - F1\n",
      "   Mean: 0.4145\n",
      "   Std:  0.0419\n",
      "   95% CI: [0.3362, 0.4964]\n",
      "\n",
      "🎯 MULTITASK EMOTION - F1\n",
      "   Mean: 0.1232\n",
      "   Std:  0.0220\n",
      "   95% CI: [0.0793, 0.1659]\n",
      "\n",
      "💾 Bootstrap results saved to: ./bertweet_seed_analysis_results/bertweet_bootstrap_results_20250725_172644.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment_single': {'values': [0.4153475048997437,\n",
       "   0.31642976053271865,\n",
       "   0.3841684822076979,\n",
       "   0.3710622710622711,\n",
       "   0.3605935127674258,\n",
       "   0.3465027802165643,\n",
       "   0.38111407551135906,\n",
       "   0.47117794486215536,\n",
       "   0.4133839120708102,\n",
       "   0.3548922056384743,\n",
       "   0.3992248062015504,\n",
       "   0.354727745616243,\n",
       "   0.3843843843843844,\n",
       "   0.41063627730294394,\n",
       "   0.4066666666666667,\n",
       "   0.47222222222222215,\n",
       "   0.2865424430641822,\n",
       "   0.4623875968992248,\n",
       "   0.3812400369158486,\n",
       "   0.4000000000000001,\n",
       "   0.351847910903029,\n",
       "   0.39705323915850227,\n",
       "   0.48779260005033986,\n",
       "   0.49715099715099714,\n",
       "   0.4030075187969924,\n",
       "   0.41924759405074363,\n",
       "   0.39963010385545594,\n",
       "   0.3843843843843844,\n",
       "   0.39154929577464787,\n",
       "   0.35993740219092335,\n",
       "   0.3383838383838384,\n",
       "   0.3971223021582733,\n",
       "   0.41755454084221205,\n",
       "   0.38053323127949995,\n",
       "   0.398382634603107,\n",
       "   0.4458225667527993,\n",
       "   0.3877909530083443,\n",
       "   0.4754521963824289,\n",
       "   0.4451774112943528,\n",
       "   0.37939335580163697,\n",
       "   0.4153520131857782,\n",
       "   0.42905982905982903,\n",
       "   0.3452078032230704,\n",
       "   0.369751021924935,\n",
       "   0.43266426488248416,\n",
       "   0.4438095238095238,\n",
       "   0.4392271949131817,\n",
       "   0.35848535102266443,\n",
       "   0.442369419317187,\n",
       "   0.48235294117647065,\n",
       "   0.34411610421402344,\n",
       "   0.35376344086021505,\n",
       "   0.3923853923853924,\n",
       "   0.4155489319423746,\n",
       "   0.39909297052154197,\n",
       "   0.4013209013209013,\n",
       "   0.3975507765830346,\n",
       "   0.48112823021113554,\n",
       "   0.40953044016399787,\n",
       "   0.4339080459770115,\n",
       "   0.46813725490196073,\n",
       "   0.3820156451735399,\n",
       "   0.432183908045977,\n",
       "   0.3752913752913753,\n",
       "   0.32605042016806723,\n",
       "   0.43950617283950616,\n",
       "   0.3264957264957265,\n",
       "   0.40049751243781095,\n",
       "   0.4026023727516265,\n",
       "   0.375,\n",
       "   0.46868686868686876,\n",
       "   0.47794461127794463,\n",
       "   0.3887496519075466,\n",
       "   0.3628593628593628,\n",
       "   0.4742079078292747,\n",
       "   0.3765312701482914,\n",
       "   0.3703703703703704,\n",
       "   0.4006535947712419,\n",
       "   0.4468468468468469,\n",
       "   0.36870155038759694,\n",
       "   0.41804753925401855,\n",
       "   0.3764097744360902,\n",
       "   0.37572178477690293,\n",
       "   0.32888888888888884,\n",
       "   0.4671201814058957,\n",
       "   0.33857358557598366,\n",
       "   0.36764705882352944,\n",
       "   0.36758893280632404,\n",
       "   0.411764705882353,\n",
       "   0.46956521739130436,\n",
       "   0.4253930971488223,\n",
       "   0.3618865823590233,\n",
       "   0.39145715058123814,\n",
       "   0.48287385129490384,\n",
       "   0.34530439792586903,\n",
       "   0.41999141999142003,\n",
       "   0.38544891640866874,\n",
       "   0.48590381426202317,\n",
       "   0.395995995995996,\n",
       "   0.41330049261083746,\n",
       "   0.36531187676225846,\n",
       "   0.45304136253041366,\n",
       "   0.4030075187969924,\n",
       "   0.43712905452035883,\n",
       "   0.36202356202356206,\n",
       "   0.41269841269841273,\n",
       "   0.356927361336737,\n",
       "   0.40585241730279903,\n",
       "   0.4274672187715665,\n",
       "   0.39797979797979793,\n",
       "   0.3837371205792259,\n",
       "   0.4027636659215606,\n",
       "   0.33285233285233284,\n",
       "   0.351981351981352,\n",
       "   0.4248826291079812,\n",
       "   0.3839342188488299,\n",
       "   0.41919191919191917,\n",
       "   0.46049102537652153,\n",
       "   0.39337085678549094,\n",
       "   0.4891807379612258,\n",
       "   0.46874146874146877,\n",
       "   0.40317460317460324,\n",
       "   0.24367816091954023,\n",
       "   0.3975486277644551,\n",
       "   0.3466666666666667,\n",
       "   0.4676056338028169,\n",
       "   0.44949494949494956,\n",
       "   0.3969043670536208,\n",
       "   0.3485191710713986,\n",
       "   0.30158730158730157,\n",
       "   0.46230158730158727,\n",
       "   0.4517275068833822,\n",
       "   0.4379259259259259,\n",
       "   0.4294599943454906,\n",
       "   0.45283243887895047,\n",
       "   0.3820736205949298,\n",
       "   0.5172101449275363,\n",
       "   0.42128603104212864,\n",
       "   0.37341072415699283,\n",
       "   0.4326972010178117,\n",
       "   0.3538489469862019,\n",
       "   0.3972222222222222,\n",
       "   0.4409321175278622,\n",
       "   0.4121272696984841,\n",
       "   0.40907668231611893,\n",
       "   0.42671394799054374,\n",
       "   0.3988380537400145,\n",
       "   0.38961442786069655,\n",
       "   0.3732251521298175,\n",
       "   0.3784511784511784,\n",
       "   0.4164570230607967,\n",
       "   0.37462863933452173,\n",
       "   0.39926739926739924,\n",
       "   0.32237442922374426,\n",
       "   0.40723981900452494,\n",
       "   0.4271284271284272,\n",
       "   0.40547263681592033,\n",
       "   0.4085914085914086,\n",
       "   0.41304347826086957,\n",
       "   0.37425348374253486,\n",
       "   0.40864197530864194,\n",
       "   0.41264451451208667,\n",
       "   0.3416225749559083,\n",
       "   0.43114909781576444,\n",
       "   0.40852974186307517,\n",
       "   0.40581818181818186,\n",
       "   0.3227699530516432,\n",
       "   0.37115839243498816,\n",
       "   0.41105938778389056,\n",
       "   0.43780193236714976,\n",
       "   0.4013333333333333,\n",
       "   0.4246913580246914,\n",
       "   0.36139522685549436,\n",
       "   0.3362794612794613,\n",
       "   0.3536833536833537,\n",
       "   0.4021822149481724,\n",
       "   0.3088819226750261,\n",
       "   0.4495748774281636,\n",
       "   0.4465897496271878,\n",
       "   0.3975486277644551,\n",
       "   0.38082882882882885,\n",
       "   0.35873015873015873,\n",
       "   0.42240918803418803,\n",
       "   0.3817262549656916,\n",
       "   0.4116161616161616,\n",
       "   0.3547506822128504,\n",
       "   0.3990990990990991,\n",
       "   0.41059408838162126,\n",
       "   0.4599797365754812,\n",
       "   0.3602228182380854,\n",
       "   0.3217761557177616,\n",
       "   0.36884041154173436,\n",
       "   0.2992149758454106,\n",
       "   0.4024654339496918,\n",
       "   0.4378250591016548,\n",
       "   0.37745098039215685,\n",
       "   0.4235294117647059,\n",
       "   0.372283081203269,\n",
       "   0.44580129654756523,\n",
       "   0.39704106280193235,\n",
       "   0.35897435897435903,\n",
       "   0.39562083503331974,\n",
       "   0.3764673764673765,\n",
       "   0.3408182544873192,\n",
       "   0.43620398514015535,\n",
       "   0.3925407925407925,\n",
       "   0.31473196766344874,\n",
       "   0.45567765567765567,\n",
       "   0.3542857142857143,\n",
       "   0.4408242829295461,\n",
       "   0.4484953703703704,\n",
       "   0.3760454002389486,\n",
       "   0.41304347826086957,\n",
       "   0.4090947227116369,\n",
       "   0.43913630229419703,\n",
       "   0.4035768153415213,\n",
       "   0.40433749257278667,\n",
       "   0.4837837837837838,\n",
       "   0.5214586951128752,\n",
       "   0.5166559209112401,\n",
       "   0.3550216883550217,\n",
       "   0.3634392141854828,\n",
       "   0.4233841684822077,\n",
       "   0.3752449871852857,\n",
       "   0.3803661616161616,\n",
       "   0.3661698776202593,\n",
       "   0.4061397028646474,\n",
       "   0.39762170196952806,\n",
       "   0.403604966282221,\n",
       "   0.40531517094017094,\n",
       "   0.4149184149184149,\n",
       "   0.3840549250385316,\n",
       "   0.3810359964881475,\n",
       "   0.3925606641123882,\n",
       "   0.350236646382691,\n",
       "   0.5190575190575191,\n",
       "   0.2975369458128079,\n",
       "   0.3973021832846613,\n",
       "   0.4284932363750098,\n",
       "   0.4306878306878306,\n",
       "   0.4036272670419012,\n",
       "   0.4522144522144522,\n",
       "   0.47436143652775115,\n",
       "   0.41282051282051285,\n",
       "   0.4039867109634551,\n",
       "   0.3333333333333333,\n",
       "   0.4483695652173913,\n",
       "   0.4202516827626573,\n",
       "   0.44574851763555223,\n",
       "   0.43427230046948356,\n",
       "   0.3380471380471381,\n",
       "   0.3815085158150852,\n",
       "   0.40726817042606517,\n",
       "   0.39324618736383443,\n",
       "   0.3682180087006484,\n",
       "   0.32689493707930933,\n",
       "   0.4215966921119592,\n",
       "   0.4006535947712419,\n",
       "   0.4486834596885954,\n",
       "   0.4460156134924142,\n",
       "   0.4014580191050779,\n",
       "   0.4329004329004329,\n",
       "   0.4166666666666667,\n",
       "   0.31993695823483054,\n",
       "   0.44283663508469706,\n",
       "   0.45882507212168894,\n",
       "   0.3152403152403152,\n",
       "   0.36487673083417765,\n",
       "   0.4377880184331797,\n",
       "   0.40783312047679865,\n",
       "   0.4504444444444444,\n",
       "   0.3248906980250264,\n",
       "   0.4259259259259259,\n",
       "   0.3333333333333333,\n",
       "   0.39610808276185444,\n",
       "   0.353968253968254,\n",
       "   0.41304347826086957,\n",
       "   0.3947203016970459,\n",
       "   0.4800524934383202,\n",
       "   0.31651116765620585,\n",
       "   0.37806444407215056,\n",
       "   0.4224857268335529,\n",
       "   0.41560283687943267,\n",
       "   0.4423280423280423,\n",
       "   0.4089195578557281,\n",
       "   0.43015873015873013,\n",
       "   0.30997983870967744,\n",
       "   0.3761904761904762,\n",
       "   0.42647789369100847,\n",
       "   0.45518207282913165,\n",
       "   0.447399011915141,\n",
       "   0.37711442786069654,\n",
       "   0.35004470118973935,\n",
       "   0.4225589225589226,\n",
       "   0.44169664268585135,\n",
       "   0.3925770308123249,\n",
       "   0.4724763979665941,\n",
       "   0.4260465116279069,\n",
       "   0.43123249299719885,\n",
       "   0.3539476396619254,\n",
       "   0.41904761904761906,\n",
       "   0.36970521541950113,\n",
       "   0.39226973684210525,\n",
       "   0.47338935574229696,\n",
       "   0.35038402457757295,\n",
       "   0.44700198920147766,\n",
       "   0.39864689045016916,\n",
       "   0.4373839488343305,\n",
       "   0.43672014260249553,\n",
       "   0.37780481211138145,\n",
       "   0.38154269972451793,\n",
       "   0.44579533941236066,\n",
       "   0.42430665960077724,\n",
       "   0.42346542346542343,\n",
       "   0.4191355026711996,\n",
       "   0.41058491573247924,\n",
       "   0.31717234907725994,\n",
       "   0.3446778711484593,\n",
       "   0.46694383304801496,\n",
       "   0.5130295763389289,\n",
       "   0.3981566820276498,\n",
       "   0.4314420803782506,\n",
       "   0.4493638676844784,\n",
       "   0.39691444600280507,\n",
       "   0.37540906965871895,\n",
       "   0.4396135265700483,\n",
       "   0.45322997416020677,\n",
       "   0.34242424242424246,\n",
       "   0.4241545893719807,\n",
       "   0.39326599326599326,\n",
       "   0.37219186399514265,\n",
       "   0.2972636815920398,\n",
       "   0.45832380080835816,\n",
       "   0.3909727431857964,\n",
       "   0.4299933642999336,\n",
       "   0.4153130481201861,\n",
       "   0.41044776119402987,\n",
       "   0.39379528985507245,\n",
       "   0.39576719576719577,\n",
       "   0.4035789969390158,\n",
       "   0.38946638946638945,\n",
       "   0.41292356185973206,\n",
       "   0.3739170086639307,\n",
       "   0.33001773459788725,\n",
       "   0.4683760683760683,\n",
       "   0.4233841684822077,\n",
       "   0.4375635311595903,\n",
       "   0.4461152882205514,\n",
       "   0.43240093240093236,\n",
       "   0.43847998524257514,\n",
       "   0.4076319831837658,\n",
       "   0.4617316784869976,\n",
       "   0.324468085106383,\n",
       "   0.4587484035759897,\n",
       "   0.3922101449275362,\n",
       "   0.44665836446658364,\n",
       "   0.3702473237356958,\n",
       "   0.45075239398084815,\n",
       "   0.45008221752407795,\n",
       "   0.38852205103122195,\n",
       "   0.38215962441314555,\n",
       "   0.3872482205058307,\n",
       "   0.3647683200653195,\n",
       "   0.3955656336608717,\n",
       "   0.32665112665112667,\n",
       "   0.3618233618233619,\n",
       "   0.3961352657004831,\n",
       "   0.3981762917933131,\n",
       "   0.38131414101026256,\n",
       "   0.4469135802469135,\n",
       "   0.4295715778474399,\n",
       "   0.40492610837438425,\n",
       "   0.3874531465772342,\n",
       "   0.3193548387096774,\n",
       "   0.4568921011874032,\n",
       "   0.41336989163076127,\n",
       "   0.4216205000518726,\n",
       "   0.4348921429696111,\n",
       "   0.4148418491484185,\n",
       "   0.3700564971751413,\n",
       "   0.4108236470441195,\n",
       "   0.4111623143881209,\n",
       "   0.4153520131857782,\n",
       "   0.40740740740740744,\n",
       "   0.41290139244103435,\n",
       "   0.4170641229464758,\n",
       "   0.4570035460992908,\n",
       "   0.32063492063492066,\n",
       "   0.36764705882352944,\n",
       "   0.42018479033404404,\n",
       "   0.42345679012345677,\n",
       "   0.3675741630443072,\n",
       "   0.40601180891035965,\n",
       "   0.45740740740740743,\n",
       "   0.29180695847362514,\n",
       "   0.42017879948914433,\n",
       "   0.35897435897435903,\n",
       "   0.4083080040526849,\n",
       "   0.3901234567901235,\n",
       "   0.3957516339869281,\n",
       "   0.41960375391032323,\n",
       "   0.3249466950959488,\n",
       "   0.3719905825168983,\n",
       "   0.48604020923930175,\n",
       "   0.42062415196743547,\n",
       "   0.3449419568822554,\n",
       "   0.4162381175003896,\n",
       "   0.3915667030170847,\n",
       "   0.46180836707152495,\n",
       "   0.42017879948914433,\n",
       "   0.3603978718482535,\n",
       "   0.412738319715064,\n",
       "   0.3725972143442365,\n",
       "   0.443673285778549,\n",
       "   0.3861062169645104,\n",
       "   0.3494252873563218,\n",
       "   0.4181818181818182,\n",
       "   0.40137812230835485,\n",
       "   0.48674242424242425,\n",
       "   0.4005376344086022,\n",
       "   0.42637362637362636,\n",
       "   0.40740740740740744,\n",
       "   0.3958333333333333,\n",
       "   0.4229885057471265,\n",
       "   0.44398496240601504,\n",
       "   0.4261294261294261,\n",
       "   0.410941475826972,\n",
       "   0.39747899159663863,\n",
       "   0.42943275501415035,\n",
       "   0.45055555555555554,\n",
       "   0.4385737719071052,\n",
       "   0.41043083900226757,\n",
       "   0.41570338058887674,\n",
       "   0.4419678036699313,\n",
       "   0.4664774263314409,\n",
       "   0.4018186984938903,\n",
       "   0.4028871391076116,\n",
       "   0.2949082858950032,\n",
       "   0.3828689370485036,\n",
       "   0.4271284271284272,\n",
       "   0.411965811965812,\n",
       "   0.3879310344827586,\n",
       "   0.3805175038051751,\n",
       "   0.3946053946053946,\n",
       "   0.33639684222929184,\n",
       "   0.38095238095238093,\n",
       "   0.40953044016399787,\n",
       "   0.362512077294686,\n",
       "   0.4287317620650954,\n",
       "   0.44996059889676915,\n",
       "   0.4543302701197438,\n",
       "   0.38434022257551675,\n",
       "   0.3956687152472929,\n",
       "   0.4153520131857782,\n",
       "   0.41904761904761906,\n",
       "   0.3952380952380952,\n",
       "   0.4274672187715665,\n",
       "   0.34618393116549334,\n",
       "   0.34793447293447294,\n",
       "   0.3693585107807687,\n",
       "   0.4313953488372093,\n",
       "   0.413035584604212,\n",
       "   0.42578668894458366,\n",
       "   0.4127887285782023,\n",
       "   0.37670825906120026,\n",
       "   0.3805639043938743,\n",
       "   0.40264900662251657,\n",
       "   0.41868633649455567,\n",
       "   0.3927469135802469,\n",
       "   0.39303190448228614,\n",
       "   0.3628380751668423,\n",
       "   0.38367064521709054,\n",
       "   0.3942748091603054,\n",
       "   0.42671394799054374,\n",
       "   0.3826652963343611,\n",
       "   0.37703141928494044,\n",
       "   0.38095238095238093,\n",
       "   0.3663139329805996,\n",
       "   0.36060075823855353,\n",
       "   0.4262273901808786,\n",
       "   0.4185185185185185,\n",
       "   0.41044776119402987,\n",
       "   0.3603988603988604,\n",
       "   0.3563103563103563,\n",
       "   0.3140096618357488,\n",
       "   0.3328735632183908,\n",
       "   0.36493209627537987,\n",
       "   0.41465752414657525,\n",
       "   0.4018214936247723,\n",
       "   0.38888888888888884,\n",
       "   0.3893436045578558,\n",
       "   0.46813725490196073,\n",
       "   0.40956072351421186,\n",
       "   0.3870625662778367,\n",
       "   0.3659919028340081,\n",
       "   0.4354520817935452,\n",
       "   0.45171752911690993,\n",
       "   0.3879428459581131,\n",
       "   0.42334969994544464,\n",
       "   0.4006734006734007,\n",
       "   0.353968253968254,\n",
       "   0.381486093814861,\n",
       "   0.3605935127674258,\n",
       "   0.39873916469661147,\n",
       "   0.4542723004694836,\n",
       "   0.45075154730327144,\n",
       "   0.41658788680910064,\n",
       "   0.40367231638418083,\n",
       "   0.38126361655773416,\n",
       "   0.44110275689223055,\n",
       "   0.3450904392764858,\n",
       "   0.41904761904761906,\n",
       "   0.31820552947313513,\n",
       "   0.40134791279829446,\n",
       "   0.41391391391391386,\n",
       "   0.4245933278191343,\n",
       "   0.3750932140193885,\n",
       "   0.45245992260917633,\n",
       "   0.36170212765957444,\n",
       "   0.361359126984127,\n",
       "   0.4549883216549883,\n",
       "   0.43672014260249553,\n",
       "   0.3705781517843229,\n",
       "   0.36459154189650644,\n",
       "   0.45621045621045625,\n",
       "   0.41736111111111107,\n",
       "   0.3940568475452196,\n",
       "   0.4005561348627042,\n",
       "   0.41282051282051285,\n",
       "   0.44946899623158615,\n",
       "   0.40956072351421186,\n",
       "   0.41614850860592956,\n",
       "   0.400907029478458,\n",
       "   0.4326599326599327,\n",
       "   0.35176715176715173,\n",
       "   0.4652421652421652,\n",
       "   0.3958333333333333,\n",
       "   0.2991962174940898,\n",
       "   0.41330049261083746,\n",
       "   0.4106143220647038,\n",
       "   0.48453996983408754,\n",
       "   0.3645211236452113,\n",
       "   0.3960484957341715,\n",
       "   0.42328042328042326,\n",
       "   0.36960431654676257,\n",
       "   0.41304347826086957,\n",
       "   0.34772296015180265,\n",
       "   0.3922576224734498,\n",
       "   0.42216666666666663,\n",
       "   0.4781089414182939,\n",
       "   0.3587509077705156,\n",
       "   0.4006438396254024,\n",
       "   0.4058618976651764,\n",
       "   0.4219858156028369,\n",
       "   0.3907342657342657,\n",
       "   0.4324116743471582,\n",
       "   0.37701149425287356,\n",
       "   0.4614197530864197,\n",
       "   0.4515068493150685,\n",
       "   0.3875746714456392,\n",
       "   0.45517241379310347,\n",
       "   0.43030303030303035,\n",
       "   0.4517275068833822,\n",
       "   0.4347509996364958,\n",
       "   0.43194600674915634,\n",
       "   0.4101259215763033,\n",
       "   0.38095238095238093,\n",
       "   0.3988380537400145,\n",
       "   0.3607235142118863,\n",
       "   0.3936543073233721,\n",
       "   0.46290408055113935,\n",
       "   0.3506874013973406,\n",
       "   0.40105820105820106,\n",
       "   0.4326599326599327,\n",
       "   0.4335542378793164,\n",
       "   0.48034825870646763,\n",
       "   0.40404040404040403,\n",
       "   0.4509852015370779,\n",
       "   0.4178743961352657,\n",
       "   0.42896627971254836,\n",
       "   0.37068842135394303,\n",
       "   0.44880019006890004,\n",
       "   0.4373839488343305,\n",
       "   0.45714285714285713,\n",
       "   0.3780864197530864,\n",
       "   0.4323671497584541,\n",
       "   0.399783241888505,\n",
       "   0.3987562189054727,\n",
       "   0.4034188034188035,\n",
       "   0.3368544600938967,\n",
       "   0.4473039215686274,\n",
       "   0.4020303156723683,\n",
       "   0.4276081424936387,\n",
       "   0.37645847632120794,\n",
       "   0.3883597883597883,\n",
       "   0.36135549038774845,\n",
       "   0.4444444444444444,\n",
       "   0.33686067019400356,\n",
       "   0.5073441247002398,\n",
       "   0.4715071897745429,\n",
       "   0.4289405684754522,\n",
       "   0.371551117085974,\n",
       "   0.4515519568151147,\n",
       "   0.3660130718954248,\n",
       "   0.41798941798941797,\n",
       "   0.3827338129496403,\n",
       "   0.37086288416075647,\n",
       "   0.4575757575757575,\n",
       "   0.3797331165752218,\n",
       "   0.3922027290448343,\n",
       "   0.37160493827160496,\n",
       "   0.41308555942702285,\n",
       "   0.3924826204783787,\n",
       "   0.39844282238442824,\n",
       "   0.3988187523071243,\n",
       "   0.30158730158730157,\n",
       "   0.3748792270531401,\n",
       "   0.39987596899224803,\n",
       "   0.40864197530864194,\n",
       "   0.40590196495708303,\n",
       "   0.2984126984126984,\n",
       "   0.37971014492753624,\n",
       "   0.4348595443485954,\n",
       "   0.35127098321342926,\n",
       "   0.4134484325306678,\n",
       "   0.4121621621621621,\n",
       "   0.3700384122919334,\n",
       "   0.37733385559472515,\n",
       "   0.37436676798378926,\n",
       "   0.3771043771043771,\n",
       "   0.42270531400966177,\n",
       "   0.3973084886128364,\n",
       "   0.48499594484995945,\n",
       "   0.4369703036369703,\n",
       "   0.40049751243781095,\n",
       "   0.3442374678176587,\n",
       "   0.439859649122807,\n",
       "   0.44083986663062086,\n",
       "   0.39197530864197533,\n",
       "   0.3944055944055944,\n",
       "   0.30254215957714575,\n",
       "   0.3861751152073733,\n",
       "   0.42140151515151514,\n",
       "   0.448062015503876,\n",
       "   0.41520352469257576,\n",
       "   0.4456469456469456,\n",
       "   0.4358095238095238,\n",
       "   0.44976258309591644,\n",
       "   0.42784992784992787,\n",
       "   0.3579270970575319,\n",
       "   0.3796296296296296,\n",
       "   0.3841684822076979,\n",
       "   0.42578668894458366,\n",
       "   0.3563531728002423,\n",
       "   0.4486803519061584,\n",
       "   0.4520884520884521,\n",
       "   0.4429008517324072,\n",
       "   0.320048309178744,\n",
       "   0.38437209302325576,\n",
       "   0.44175824175824174,\n",
       "   0.4294599943454906,\n",
       "   0.41322819243230025,\n",
       "   0.34110352405619654,\n",
       "   0.42818285513731685,\n",
       "   0.39052095573834705,\n",
       "   0.42808165809568327,\n",
       "   0.44351755300660406,\n",
       "   0.4818803766172188,\n",
       "   0.4274322169059011,\n",
       "   0.40531517094017094,\n",
       "   0.39561078409999273,\n",
       "   0.46810207336523124,\n",
       "   0.49585406301824214,\n",
       "   0.36552706552706554,\n",
       "   0.2968824940047962,\n",
       "   0.42142857142857143,\n",
       "   0.4267206477732793,\n",
       "   0.3621890547263682,\n",
       "   0.41059691482226696,\n",
       "   0.3733333333333333,\n",
       "   0.3519752338649977,\n",
       "   0.4127893104271057,\n",
       "   0.38528491772065826,\n",
       "   0.2928097994798221,\n",
       "   0.36752136752136755,\n",
       "   0.4171410499683745,\n",
       "   0.3742017879948915,\n",
       "   0.3877260981912145,\n",
       "   0.41282389108476064,\n",
       "   0.36316316316316316,\n",
       "   0.35221625919300337,\n",
       "   0.398712816890354,\n",
       "   0.3837371205792259,\n",
       "   0.361052429421932,\n",
       "   0.403584229390681,\n",
       "   0.44126984126984126,\n",
       "   0.34997029114676176,\n",
       "   0.3662625355485705,\n",
       "   0.35125448028673834,\n",
       "   0.4168707482993197,\n",
       "   0.4040485829959514,\n",
       "   0.3924953095684803,\n",
       "   0.41101664123246856,\n",
       "   0.4081951219512195,\n",
       "   0.3780769753841969,\n",
       "   0.41582410528005137,\n",
       "   0.38555008210180625,\n",
       "   0.4351851851851852,\n",
       "   0.4860111910471622,\n",
       "   0.39747899159663863,\n",
       "   0.41646423751686906,\n",
       "   0.3625,\n",
       "   0.5122685185185185,\n",
       "   0.4625902992776058,\n",
       "   0.3887496519075466,\n",
       "   0.4430927277642606,\n",
       "   0.41827541827541825,\n",
       "   0.41675695196821955,\n",
       "   0.4007633587786259,\n",
       "   0.3432933478735006,\n",
       "   0.4739583333333333,\n",
       "   0.4048083170890188,\n",
       "   0.3468265867066467,\n",
       "   0.3784511784511784,\n",
       "   0.46536574232568756,\n",
       "   0.3436073059360731,\n",
       "   0.4843065693430657,\n",
       "   0.3671497584541063,\n",
       "   0.4220430107526882,\n",
       "   0.3437046949927512,\n",
       "   0.4444444444444445,\n",
       "   0.4369156041287188,\n",
       "   0.3283255086071988,\n",
       "   0.38447488584474887,\n",
       "   0.3786646957378665,\n",
       "   0.3569023569023569,\n",
       "   0.3634920634920635,\n",
       "   0.34585167918501253,\n",
       "   0.39344815940560623,\n",
       "   0.3862215553888472,\n",
       "   0.41032001984619204,\n",
       "   0.43777086003784405,\n",
       "   0.3912198912198912,\n",
       "   0.3702997967479675,\n",
       "   0.36717716014369445,\n",
       "   0.3899150949510662,\n",
       "   0.4101259215763033,\n",
       "   0.3340961098398169,\n",
       "   0.3636660943504211,\n",
       "   0.3854166666666667,\n",
       "   0.3666551366309236,\n",
       "   0.42845637583892615,\n",
       "   0.4280792420327304,\n",
       "   0.3523125115164916,\n",
       "   0.4701923780264794,\n",
       "   0.43677058353317344,\n",
       "   0.4188948306595366,\n",
       "   0.4342572850035536,\n",
       "   0.3752449871852857,\n",
       "   0.4264705882352941,\n",
       "   0.41358024691358025,\n",
       "   0.41277372262773726,\n",
       "   0.3971076339497392,\n",
       "   0.41781548250265105,\n",
       "   0.4161931818181818,\n",
       "   0.44588979223125563,\n",
       "   0.41389343517003097,\n",
       "   0.4282125603864735,\n",
       "   0.3594244149272612,\n",
       "   0.38725490196078427,\n",
       "   0.44113907958301546,\n",
       "   0.37711442786069654,\n",
       "   0.4314888010540184,\n",
       "   0.4430927277642606,\n",
       "   0.4266666666666667,\n",
       "   0.32554200542005424,\n",
       "   0.4040492957746478,\n",
       "   0.42079727248885396,\n",
       "   0.46288976723759334,\n",
       "   0.4108236470441195,\n",
       "   0.4276891405043746,\n",
       "   0.40657628175159727,\n",
       "   0.34351395730706075,\n",
       "   0.3987747145641882,\n",
       "   0.4619670542635659,\n",
       "   0.4404619673436878,\n",
       "   0.349954519143306,\n",
       "   0.46868686868686876,\n",
       "   0.34188034188034183,\n",
       "   0.38474564790354265,\n",
       "   0.4207992296581608,\n",
       "   0.44612794612794615,\n",
       "   0.404040404040404,\n",
       "   0.4475839475839476,\n",
       "   0.33213590779823376,\n",
       "   0.40310077519379844,\n",
       "   0.45646215475437485,\n",
       "   0.3927469135802469,\n",
       "   0.4240669240669241,\n",
       "   0.3432098765432099,\n",
       "   0.44162210338680924,\n",
       "   0.46817164937970307,\n",
       "   0.37654320987654316,\n",
       "   0.42808165809568327,\n",
       "   0.49523809523809526,\n",
       "   0.4416839199447895,\n",
       "   0.37993421052631576,\n",
       "   0.440551500405515,\n",
       "   0.505700871898055,\n",
       "   0.4351407000686342,\n",
       "   0.3812400369158486,\n",
       "   0.41477028767711466,\n",
       "   0.43481673338042537,\n",
       "   0.3541666666666667,\n",
       "   0.3968253968253968,\n",
       "   0.4456845238095238,\n",
       "   0.47433299560959136,\n",
       "   0.3606837606837607,\n",
       "   0.32793764988009594,\n",
       "   0.4151978065021544,\n",
       "   0.4450317124735729,\n",
       "   0.4356519120746763,\n",
       "   0.4714285714285715,\n",
       "   0.3667657925565468,\n",
       "   0.3869429671740326,\n",
       "   0.3937810945273632,\n",
       "   0.4101259215763033,\n",
       "   0.39691444600280507,\n",
       "   0.4002735042735042,\n",
       "   0.48968707482993196,\n",
       "   0.37543231961836615,\n",
       "   0.43843951324266284,\n",
       "   0.37923021060275963,\n",
       "   0.3436465495289025,\n",
       "   0.4826635145784082,\n",
       "   0.37547348484848486,\n",
       "   0.36787391012743126,\n",
       "   0.42335939350864726,\n",
       "   0.38472314384723144,\n",
       "   0.4295119507657077,\n",
       "   0.49466047118284834,\n",
       "   0.423967042287653,\n",
       "   0.38798884339208356,\n",
       "   0.4170940170940171,\n",
       "   0.35014245014245016,\n",
       "   0.4281586551090097,\n",
       "   0.4696969696969697,\n",
       "   0.3813852813852814,\n",
       "   0.36430921052631576,\n",
       "   0.3799300254452926,\n",
       "   0.4473604826546003,\n",
       "   0.43793103448275855,\n",
       "   0.3414462081128748,\n",
       "   0.37158844353088955,\n",
       "   0.40113717128642506,\n",
       "   0.4299329766955666,\n",
       "   0.48012976480129765,\n",
       "   0.48138675760159133,\n",
       "   0.36051159072741806,\n",
       "   0.34366925064599485,\n",
       "   0.3950617283950617,\n",
       "   0.3864042933810376,\n",
       "   0.39444444444444443,\n",
       "   0.39845339845339844,\n",
       "   0.4140056022408964,\n",
       "   0.4332171893147503,\n",
       "   0.3926218708827405,\n",
       "   0.3274689186019235,\n",
       "   0.41105938778389056,\n",
       "   0.3602967149417167,\n",
       "   0.36286310383566045,\n",
       "   0.38544891640866874,\n",
       "   0.4537986186841148,\n",
       "   0.36452762923351156,\n",
       "   0.34188034188034183,\n",
       "   0.41990650334339313,\n",
       "   0.3961469969240732,\n",
       "   0.3169099756690998,\n",
       "   0.3523125115164916,\n",
       "   0.349954519143306,\n",
       "   0.42623965343853953,\n",
       "   0.3904761904761905,\n",
       "   0.4673295454545454,\n",
       "   0.36063492063492064,\n",
       "   0.4865861837692824,\n",
       "   0.4192592592592592,\n",
       "   0.37711442786069654,\n",
       "   0.4291719209751997,\n",
       "   0.44035324053285435,\n",
       "   0.30994152046783624,\n",
       "   0.46874146874146877,\n",
       "   0.35936546674801706,\n",
       "   0.4017448200654308,\n",
       "   0.3620370370370371,\n",
       "   0.3992248062015504,\n",
       "   0.4080704745666382,\n",
       "   0.37359307359307364,\n",
       "   0.38519523594150457,\n",
       "   0.418342151675485,\n",
       "   0.3909594372099985,\n",
       "   0.37843273768082303,\n",
       "   0.4253930971488223,\n",
       "   0.41854636591478694,\n",
       "   0.40697537410156714,\n",
       "   0.37700311384521906,\n",
       "   0.49074074074074076,\n",
       "   0.40844517698767757,\n",
       "   0.46144144144144145,\n",
       "   0.340625,\n",
       "   0.34005505409384806,\n",
       "   0.333000333000333,\n",
       "   0.37468905472636815,\n",
       "   0.4407096171802054,\n",
       "   0.4476472037447647,\n",
       "   0.39495063117110357,\n",
       "   0.3875746714456392,\n",
       "   0.353395061728395,\n",
       "   0.35475768321513,\n",
       "   0.4576420311714429,\n",
       "   0.3833298947802765,\n",
       "   0.3502197547999075,\n",
       "   0.4418411113244991,\n",
       "   0.37843273768082303,\n",
       "   0.4544529262086514,\n",
       "   0.4213080936779517,\n",
       "   0.4264705882352941,\n",
       "   0.39926739926739924,\n",
       "   0.3271144278606965,\n",
       "   0.41228070175438597,\n",
       "   0.49590163934426235,\n",
       "   0.33785822021116135,\n",
       "   0.39762724837351704,\n",
       "   0.35358796296296297,\n",
       "   0.4203309692671395,\n",
       "   0.4435120435120435,\n",
       "   0.3363826031134649,\n",
       "   0.3819789939192924,\n",
       "   0.3501831501831502,\n",
       "   0.4007619047619047,\n",
       "   0.425599276345545,\n",
       "   0.44395532730088855,\n",
       "   0.5166439909297053,\n",
       "   0.4377434938444756,\n",
       "   0.4346153846153846,\n",
       "   0.3642178910544727,\n",
       "   0.44087009803921573,\n",
       "   0.37721592560302236,\n",
       "   0.3846153846153846,\n",
       "   0.4648040943032134,\n",
       "   0.47356051703877794,\n",
       "   0.4238905572238905,\n",
       "   0.4248826291079812,\n",
       "   0.39718550667455776,\n",
       "   0.4555488661448926,\n",
       "   0.42777777777777776,\n",
       "   0.433096926713948,\n",
       "   0.38434892142969607,\n",
       "   0.3571937321937322,\n",
       "   0.44169246646026833,\n",
       "   0.4569336778639104,\n",
       "   0.4254051304411017,\n",
       "   0.43972572439725727,\n",
       "   0.43706394510753616,\n",
       "   0.35856052344601963,\n",
       "   0.3494623655913978,\n",
       "   0.40594325668952536,\n",
       "   0.421119592875318,\n",
       "   0.4360902255639097,\n",
       "   0.4165266106442577,\n",
       "   0.36491050320837554,\n",
       "   0.44298245614035087,\n",
       "   0.3782408362561034,\n",
       "   0.3880952380952381,\n",
       "   0.3557625948930297,\n",
       "   0.42633402633402634,\n",
       "   0.3886524822695036,\n",
       "   0.372053872053872,\n",
       "   0.4472588089609366,\n",
       "   0.441358024691358,\n",
       "   0.48427140640428834,\n",
       "   0.40488400488400494,\n",
       "   0.42283950617283955,\n",
       "   0.376216001588247,\n",
       "   0.4444444444444445,\n",
       "   0.3733031674208145,\n",
       "   0.49282452707110247,\n",
       "   0.4359242963894127,\n",
       "   0.3847041847041847,\n",
       "   0.41006375227686703,\n",
       "   0.3421717171717171,\n",
       "   0.38230127360562144,\n",
       "   0.41393338418510045,\n",
       "   0.3665401644528779,\n",
       "   0.40374440374440373,\n",
       "   0.4650537634408602,\n",
       "   0.47522429261559695,\n",
       "   0.39338061465721036,\n",
       "   0.3682983682983683,\n",
       "   0.3487592219986586,\n",
       "   0.40379497859539515],\n",
       "  'mean': 0.4033255478677471,\n",
       "  'std': 0.04193815280223178,\n",
       "  'ci_lower': 0.3193261059787639,\n",
       "  'ci_upper': 0.48455136920948433},\n",
       " 'emotion_single': {'values': [0.12037037037037039,\n",
       "   0.08414637996905687,\n",
       "   0.058407833120476794,\n",
       "   0.07648489503328212,\n",
       "   0.09338913430985042,\n",
       "   0.09952321724709784,\n",
       "   0.12978446674098848,\n",
       "   0.06832795264998655,\n",
       "   0.055813236618646477,\n",
       "   0.07017543859649122,\n",
       "   0.12613875466167487,\n",
       "   0.08178645625454135,\n",
       "   0.08319611477506214,\n",
       "   0.04450716456795484,\n",
       "   0.09676635421316272,\n",
       "   0.08184936505619618,\n",
       "   0.10278304870335231,\n",
       "   0.1254892905836302,\n",
       "   0.08782033163745996,\n",
       "   0.0974333498923663,\n",
       "   0.12503480924533555,\n",
       "   0.13869281045751633,\n",
       "   0.05701058201058201,\n",
       "   0.08571004159239452,\n",
       "   0.11001054245735097,\n",
       "   0.10120340875057855,\n",
       "   0.10761526832955404,\n",
       "   0.09595959595959597,\n",
       "   0.07741428281598701,\n",
       "   0.07597391151066456,\n",
       "   0.11527955612462655,\n",
       "   0.07297970536096338,\n",
       "   0.08515590100955954,\n",
       "   0.11356877721082977,\n",
       "   0.06341189674523008,\n",
       "   0.0943474462067025,\n",
       "   0.07450980392156863,\n",
       "   0.07792207792207792,\n",
       "   0.13846653671215073,\n",
       "   0.08496348686102007,\n",
       "   0.1511779571207304,\n",
       "   0.07436301549776726,\n",
       "   0.0879384003974168,\n",
       "   0.10581725026265416,\n",
       "   0.09947786131996657,\n",
       "   0.08024691358024692,\n",
       "   0.09232677653730285,\n",
       "   0.09297719166633762,\n",
       "   0.06921768707482993,\n",
       "   0.13511550479421672,\n",
       "   0.11284297568450574,\n",
       "   0.1304821445486407,\n",
       "   0.06039747724980326,\n",
       "   0.10365695288556438,\n",
       "   0.11957754961971061,\n",
       "   0.12362373795056876,\n",
       "   0.11507936507936507,\n",
       "   0.0915119297194769,\n",
       "   0.10337690631808279,\n",
       "   0.10510124795839082,\n",
       "   0.11882716049382715,\n",
       "   0.12923446062980948,\n",
       "   0.0743413745612866,\n",
       "   0.09802442528735633,\n",
       "   0.06133118625872249,\n",
       "   0.1047008547008547,\n",
       "   0.07541141456582634,\n",
       "   0.12896825396825395,\n",
       "   0.049407796689936466,\n",
       "   0.049883449883449886,\n",
       "   0.07532795953848585,\n",
       "   0.08221273005326162,\n",
       "   0.08521505376344087,\n",
       "   0.10241452991452993,\n",
       "   0.044489383215369056,\n",
       "   0.06993727598566307,\n",
       "   0.09152927450799792,\n",
       "   0.08199112978524743,\n",
       "   0.10092368397453144,\n",
       "   0.12634408602150538,\n",
       "   0.060786914235190094,\n",
       "   0.0893939393939394,\n",
       "   0.11841560388524582,\n",
       "   0.09894179894179893,\n",
       "   0.08779264214046822,\n",
       "   0.0855574717363335,\n",
       "   0.1333766877245138,\n",
       "   0.14043039086517348,\n",
       "   0.10711725550435229,\n",
       "   0.12417148069321982,\n",
       "   0.13504594820384294,\n",
       "   0.09227994227994228,\n",
       "   0.096847810583369,\n",
       "   0.07807334941481282,\n",
       "   0.11219806763285023,\n",
       "   0.0843220941812491,\n",
       "   0.10430839002267574,\n",
       "   0.09819738669326084,\n",
       "   0.09105955772622439,\n",
       "   0.055803571428571425,\n",
       "   0.06540861958818615,\n",
       "   0.09786517116850095,\n",
       "   0.0911974926422811,\n",
       "   0.10063131313131313,\n",
       "   0.08303221921108099,\n",
       "   0.09848484848484847,\n",
       "   0.1097114556416882,\n",
       "   0.08285233285233284,\n",
       "   0.09249874874874875,\n",
       "   0.06828165374677002,\n",
       "   0.05694533992406333,\n",
       "   0.07906637707893249,\n",
       "   0.12888589398023362,\n",
       "   0.12561997807899447,\n",
       "   0.05624461670973299,\n",
       "   0.09895680760342414,\n",
       "   0.08337224037884346,\n",
       "   0.09488680996929229,\n",
       "   0.07573532362264757,\n",
       "   0.10829931972789116,\n",
       "   0.07943951691320734,\n",
       "   0.05982905982905983,\n",
       "   0.11376984126984128,\n",
       "   0.0970409592230757,\n",
       "   0.10785438171700439,\n",
       "   0.06398166197905802,\n",
       "   0.1224404388246196,\n",
       "   0.10276939414263357,\n",
       "   0.05249266862170088,\n",
       "   0.11981156098803157,\n",
       "   0.08798650988970924,\n",
       "   0.09438280859570215,\n",
       "   0.130242656449553,\n",
       "   0.08128192681968426,\n",
       "   0.09442014969304517,\n",
       "   0.0507936507936508,\n",
       "   0.10096234209709387,\n",
       "   0.07609989648033126,\n",
       "   0.11104955555222823,\n",
       "   0.10419326814877006,\n",
       "   0.08562091503267973,\n",
       "   0.11269306221938302,\n",
       "   0.07431548186265167,\n",
       "   0.060690845253057235,\n",
       "   0.12037037037037036,\n",
       "   0.07711610568380438,\n",
       "   0.10606738223017292,\n",
       "   0.09728663933140642,\n",
       "   0.11488346735723048,\n",
       "   0.10467886411282638,\n",
       "   0.0931225273361655,\n",
       "   0.08442113442113443,\n",
       "   0.09213326356183499,\n",
       "   0.11563936973773041,\n",
       "   0.07692307692307693,\n",
       "   0.0698595146871009,\n",
       "   0.07948519948519948,\n",
       "   0.062235506799786344,\n",
       "   0.09378614494893563,\n",
       "   0.06662319218618672,\n",
       "   0.08461961961961963,\n",
       "   0.08618524332810047,\n",
       "   0.1182653241476771,\n",
       "   0.08723503377591428,\n",
       "   0.09916722599249261,\n",
       "   0.08700550469182282,\n",
       "   0.09448171337706222,\n",
       "   0.06441745389113811,\n",
       "   0.10538137797323498,\n",
       "   0.06641414141414141,\n",
       "   0.06851962278318867,\n",
       "   0.052220462824971024,\n",
       "   0.12211772901428074,\n",
       "   0.046875446875446876,\n",
       "   0.06174957118353345,\n",
       "   0.09508070373060222,\n",
       "   0.1055021612001017,\n",
       "   0.0862802430913886,\n",
       "   0.09345711759504864,\n",
       "   0.14744548571893729,\n",
       "   0.1105510752688172,\n",
       "   0.08508898508898509,\n",
       "   0.06785984848484848,\n",
       "   0.09012515262515264,\n",
       "   0.09989668617214903,\n",
       "   0.09728398822215452,\n",
       "   0.06747660742392964,\n",
       "   0.10513841226264013,\n",
       "   0.11034282133906193,\n",
       "   0.04259887005649717,\n",
       "   0.11660763385393143,\n",
       "   0.18862284732687143,\n",
       "   0.06307445941592282,\n",
       "   0.1351288129249579,\n",
       "   0.06842180526391052,\n",
       "   0.09003618261091743,\n",
       "   0.10832594562647753,\n",
       "   0.09088827838827838,\n",
       "   0.046645702306079666,\n",
       "   0.10614713829525914,\n",
       "   0.10005900508351488,\n",
       "   0.10710108604845447,\n",
       "   0.09101654846335698,\n",
       "   0.11563956876456877,\n",
       "   0.09711751662971174,\n",
       "   0.14125360608444795,\n",
       "   0.12773222138547216,\n",
       "   0.11695906432748537,\n",
       "   0.10358284359212289,\n",
       "   0.07380843958712811,\n",
       "   0.09333074333074332,\n",
       "   0.06737138830162086,\n",
       "   0.1529241999830235,\n",
       "   0.10321879598001445,\n",
       "   0.0657081908769469,\n",
       "   0.10386736982481663,\n",
       "   0.096293376545643,\n",
       "   0.11053033332121696,\n",
       "   0.0734567901234568,\n",
       "   0.07477868112014453,\n",
       "   0.1293168168168168,\n",
       "   0.08793353406936544,\n",
       "   0.08724881796690308,\n",
       "   0.13518518518518519,\n",
       "   0.12006405027342215,\n",
       "   0.10623482208848062,\n",
       "   0.08077133364175464,\n",
       "   0.07677058111380146,\n",
       "   0.08813588328413126,\n",
       "   0.10701351379317481,\n",
       "   0.11064935064935065,\n",
       "   0.1027748858257333,\n",
       "   0.09064893770776124,\n",
       "   0.08995670995670996,\n",
       "   0.10915501206456298,\n",
       "   0.06639962846859399,\n",
       "   0.06370650953984287,\n",
       "   0.0654320987654321,\n",
       "   0.06431708388230127,\n",
       "   0.09205227955227956,\n",
       "   0.12954181672669066,\n",
       "   0.09875541125541125,\n",
       "   0.09068877337834902,\n",
       "   0.09235209235209234,\n",
       "   0.0940062601857361,\n",
       "   0.04714640198511166,\n",
       "   0.10622934925434065,\n",
       "   0.08069171138938581,\n",
       "   0.13649891774891776,\n",
       "   0.0996925779534475,\n",
       "   0.12852380009242756,\n",
       "   0.10382161697951171,\n",
       "   0.10501711265179463,\n",
       "   0.08893557422969188,\n",
       "   0.10530923551756884,\n",
       "   0.09584520417853752,\n",
       "   0.11236424394319129,\n",
       "   0.10347985347985349,\n",
       "   0.10828877005347592,\n",
       "   0.10623523369286081,\n",
       "   0.08333333333333333,\n",
       "   0.09325632907722459,\n",
       "   0.09072039072039073,\n",
       "   0.11041351406462968,\n",
       "   0.1495330665141986,\n",
       "   0.09221696341371798,\n",
       "   0.10165716322166785,\n",
       "   0.0763888888888889,\n",
       "   0.10129068462401795,\n",
       "   0.09717194570135747,\n",
       "   0.08725490196078432,\n",
       "   0.08400443202135428,\n",
       "   0.14224571294338736,\n",
       "   0.10880756532930447,\n",
       "   0.08305069775658011,\n",
       "   0.11213235294117647,\n",
       "   0.13088235294117648,\n",
       "   0.11717365691649735,\n",
       "   0.12021165173386428,\n",
       "   0.08381410256410256,\n",
       "   0.102943412620832,\n",
       "   0.06363636363636364,\n",
       "   0.05714285714285714,\n",
       "   0.08394642520279193,\n",
       "   0.1200034475965327,\n",
       "   0.09934020312847504,\n",
       "   0.06397058823529413,\n",
       "   0.10872278736055825,\n",
       "   0.0955157134862324,\n",
       "   0.1093358395989975,\n",
       "   0.10473769525493663,\n",
       "   0.11399523263930043,\n",
       "   0.0661547726588377,\n",
       "   0.11956815114709851,\n",
       "   0.10168492989277718,\n",
       "   0.12835941934960668,\n",
       "   0.07065527065527066,\n",
       "   0.06628006393814356,\n",
       "   0.08800186741363213,\n",
       "   0.0980859010270775,\n",
       "   0.1401352115125429,\n",
       "   0.07753623188405796,\n",
       "   0.1018789033555626,\n",
       "   0.11242929728693318,\n",
       "   0.09162393162393162,\n",
       "   0.05523403391207229,\n",
       "   0.11904761904761903,\n",
       "   0.07990543735224587,\n",
       "   0.11141016793190706,\n",
       "   0.13686252771618626,\n",
       "   0.0850374183707517,\n",
       "   0.09356978382651132,\n",
       "   0.0903183885640026,\n",
       "   0.11036364900570156,\n",
       "   0.0842156960676589,\n",
       "   0.09851290684624019,\n",
       "   0.09322644568546208,\n",
       "   0.10756843800322062,\n",
       "   0.10303536619326092,\n",
       "   0.06974681020733652,\n",
       "   0.10823408528326561,\n",
       "   0.050170068027210885,\n",
       "   0.12440395861448493,\n",
       "   0.09207837480466402,\n",
       "   0.08772093643700395,\n",
       "   0.11666290478539165,\n",
       "   0.11646811254794148,\n",
       "   0.10135135135135136,\n",
       "   0.1044031647746818,\n",
       "   0.11447191658880569,\n",
       "   0.10193914522330672,\n",
       "   0.08536000163358654,\n",
       "   0.10258404318206045,\n",
       "   0.09168184578020644,\n",
       "   0.09702721782100558,\n",
       "   0.0951058201058201,\n",
       "   0.09532671629445823,\n",
       "   0.086194361829228,\n",
       "   0.07587954680977937,\n",
       "   0.10269360269360268,\n",
       "   0.12239824743456705,\n",
       "   0.062365893029390336,\n",
       "   0.11363642307038535,\n",
       "   0.11654521323216387,\n",
       "   0.06765492542726324,\n",
       "   0.13032581453634084,\n",
       "   0.10496812092556773,\n",
       "   0.08224461863879556,\n",
       "   0.05312587023113339,\n",
       "   0.09892466208255683,\n",
       "   0.056298892704422655,\n",
       "   0.05903250188964474,\n",
       "   0.11344144303592103,\n",
       "   0.07956259426847662,\n",
       "   0.09466230936819171,\n",
       "   0.0782010582010582,\n",
       "   0.10843214756258235,\n",
       "   0.07747543461829175,\n",
       "   0.12549611734253666,\n",
       "   0.08501148207030561,\n",
       "   0.0886848139736458,\n",
       "   0.09035425904726817,\n",
       "   0.11844248941023133,\n",
       "   0.11831950955643707,\n",
       "   0.0659942680776014,\n",
       "   0.07841269841269842,\n",
       "   0.09839535085436724,\n",
       "   0.11543615984405457,\n",
       "   0.0688020556441609,\n",
       "   0.10121765601217654,\n",
       "   0.07524366471734893,\n",
       "   0.0865315687896333,\n",
       "   0.06377953886352543,\n",
       "   0.12149399783808386,\n",
       "   0.05861581920903955,\n",
       "   0.11189108411726273,\n",
       "   0.09440504334121354,\n",
       "   0.10975425340185381,\n",
       "   0.09286672629695886,\n",
       "   0.06486042692939244,\n",
       "   0.13784461152882205,\n",
       "   0.03988154348134487,\n",
       "   0.09544367795087334,\n",
       "   0.08666449501295236,\n",
       "   0.07602399450346177,\n",
       "   0.09798355766097701,\n",
       "   0.07299880525686978,\n",
       "   0.11268518518518518,\n",
       "   0.08785853522695626,\n",
       "   0.09027544351073763,\n",
       "   0.12018807980683695,\n",
       "   0.06476536042573779,\n",
       "   0.061879645052258354,\n",
       "   0.06293669556080904,\n",
       "   0.08048300453829768,\n",
       "   0.0903720462543992,\n",
       "   0.09609729132117191,\n",
       "   0.05994888706608412,\n",
       "   0.08584953477426595,\n",
       "   0.1533492347445836,\n",
       "   0.0748687959214275,\n",
       "   0.06509102509922854,\n",
       "   0.10525487159633501,\n",
       "   0.08217117856166256,\n",
       "   0.09575099292841228,\n",
       "   0.08889871303664408,\n",
       "   0.0797674883089871,\n",
       "   0.12333333333333334,\n",
       "   0.09566531014921692,\n",
       "   0.08428912783751492,\n",
       "   0.10402628032345013,\n",
       "   0.10280762198346856,\n",
       "   0.06729473436939777,\n",
       "   0.10175010175010174,\n",
       "   0.07917908964420593,\n",
       "   0.10610599078341014,\n",
       "   0.07354052732203993,\n",
       "   0.09146341463414635,\n",
       "   0.10273976093992915,\n",
       "   0.10552825552825555,\n",
       "   0.08477246646725738,\n",
       "   0.07415343915343915,\n",
       "   0.08857826701573353,\n",
       "   0.1544101545492721,\n",
       "   0.05022585750759597,\n",
       "   0.12896825396825395,\n",
       "   0.083016441961876,\n",
       "   0.0767486481772196,\n",
       "   0.10052273604799666,\n",
       "   0.1053377335693179,\n",
       "   0.08520499108734403,\n",
       "   0.06561843495330581,\n",
       "   0.1503968253968254,\n",
       "   0.06087962962962964,\n",
       "   0.06945054945054945,\n",
       "   0.07080920155980688,\n",
       "   0.06263136101807737,\n",
       "   0.0647911338448423,\n",
       "   0.10916331580575317,\n",
       "   0.10249554367201426,\n",
       "   0.09493055555555556,\n",
       "   0.0791170634920635,\n",
       "   0.085239651416122,\n",
       "   0.07009814128458196,\n",
       "   0.1245686680469289,\n",
       "   0.09803000483252206,\n",
       "   0.114523426117629,\n",
       "   0.11192511192511194,\n",
       "   0.1088458110516934,\n",
       "   0.09158344188739326,\n",
       "   0.07319410815173527,\n",
       "   0.09020042125377283,\n",
       "   0.09609448522491999,\n",
       "   0.0699398162812797,\n",
       "   0.10324947589098532,\n",
       "   0.09735385328811987,\n",
       "   0.08238826571078356,\n",
       "   0.10039682539682539,\n",
       "   0.09249507267799952,\n",
       "   0.155394135055152,\n",
       "   0.08522297808012093,\n",
       "   0.07708959644781062,\n",
       "   0.11662277814351547,\n",
       "   0.12966568741317666,\n",
       "   0.1334040296924708,\n",
       "   0.06477035435282645,\n",
       "   0.09969696969696969,\n",
       "   0.12606343593695696,\n",
       "   0.08718766613503455,\n",
       "   0.12695017400899755,\n",
       "   0.08717210914882571,\n",
       "   0.09790429281954705,\n",
       "   0.12386621315192743,\n",
       "   0.0952861952861953,\n",
       "   0.06688998136366557,\n",
       "   0.122586408300694,\n",
       "   0.10839892478310557,\n",
       "   0.0995380736760047,\n",
       "   0.06389452332657201,\n",
       "   0.08983206809293764,\n",
       "   0.07888540031397175,\n",
       "   0.08595352016404649,\n",
       "   0.11785198928056072,\n",
       "   0.11403630550472225,\n",
       "   0.08029751062537949,\n",
       "   0.06851497015431442,\n",
       "   0.11433554859572735,\n",
       "   0.056460595934280135,\n",
       "   0.1337371854613234,\n",
       "   0.09578544061302681,\n",
       "   0.09483294483294484,\n",
       "   0.10231935771632472,\n",
       "   0.10235690235690235,\n",
       "   0.10056883316645626,\n",
       "   0.10234432234432235,\n",
       "   0.08558683621152385,\n",
       "   0.07620899985445694,\n",
       "   0.1007655814786148,\n",
       "   0.11666666666666665,\n",
       "   0.11909753419187381,\n",
       "   0.07013227513227514,\n",
       "   0.10773172905525846,\n",
       "   0.06674603174603173,\n",
       "   0.07737084647101625,\n",
       "   0.08498528513521016,\n",
       "   0.10178424975039224,\n",
       "   0.062473572938689215,\n",
       "   0.0657707013639217,\n",
       "   0.0865291209972061,\n",
       "   0.09879599293505374,\n",
       "   0.09469685317911963,\n",
       "   0.0907044299201162,\n",
       "   0.15574552484245938,\n",
       "   0.06408065618591934,\n",
       "   0.0957070707070707,\n",
       "   0.06943426943426943,\n",
       "   0.10531309297912712,\n",
       "   0.12345212836096427,\n",
       "   0.02649132837812083,\n",
       "   0.1069069069069069,\n",
       "   0.10178223336118071,\n",
       "   0.1065291394335512,\n",
       "   0.11542006380716059,\n",
       "   0.08420363450544537,\n",
       "   0.1011579206701158,\n",
       "   0.06389239176124423,\n",
       "   0.09781959378733573,\n",
       "   0.12469220338072796,\n",
       "   0.09550157444894287,\n",
       "   0.09128436507435687,\n",
       "   0.09178999295278366,\n",
       "   0.09624338624338624,\n",
       "   0.07409551962860815,\n",
       "   0.12853107344632766,\n",
       "   0.1078930158835722,\n",
       "   0.08824672059966177,\n",
       "   0.08700172630257376,\n",
       "   0.1005209479785751,\n",
       "   0.10555555555555556,\n",
       "   0.11131533918419163,\n",
       "   0.06330128205128205,\n",
       "   0.07428631213941948,\n",
       "   0.08136885255529323,\n",
       "   0.07675438596491227,\n",
       "   0.07196443456515698,\n",
       "   0.08926959858636878,\n",
       "   0.12987782269368855,\n",
       "   0.11637137597791074,\n",
       "   0.06644308474778765,\n",
       "   0.09224055475848876,\n",
       "   0.1191550282206567,\n",
       "   0.08982240437158469,\n",
       "   0.10239691674146033,\n",
       "   0.07498057498057498,\n",
       "   0.09633378317588843,\n",
       "   0.07855107855107855,\n",
       "   0.13747698781449988,\n",
       "   0.09232210024548455,\n",
       "   0.11955756661639015,\n",
       "   0.07857813547954394,\n",
       "   0.12653793574846206,\n",
       "   0.06977738024249652,\n",
       "   0.09363370547581074,\n",
       "   0.12085803313573491,\n",
       "   0.05956066637181188,\n",
       "   0.1491269841269841,\n",
       "   0.10299483125570082,\n",
       "   0.11071050041638276,\n",
       "   0.09726704733531943,\n",
       "   0.08727645305514158,\n",
       "   0.11891451199627301,\n",
       "   0.13554723392451587,\n",
       "   0.09808304842056048,\n",
       "   0.08229051650104281,\n",
       "   0.08518035636679705,\n",
       "   0.08888888888888889,\n",
       "   0.11827170005136108,\n",
       "   0.05018895122331307,\n",
       "   0.09393603067298961,\n",
       "   0.06526910496580433,\n",
       "   0.07132183908045976,\n",
       "   0.11167133520074697,\n",
       "   0.09859615550942742,\n",
       "   0.09511104837191793,\n",
       "   0.09106066974919434,\n",
       "   0.09438448624387569,\n",
       "   0.12796846011131724,\n",
       "   0.05607155142758857,\n",
       "   0.105,\n",
       "   0.10937102113572701,\n",
       "   0.0824528944934028,\n",
       "   0.10776088213063002,\n",
       "   0.06533859605649757,\n",
       "   0.06825033598488976,\n",
       "   0.045075757575757575,\n",
       "   0.07785287930721509,\n",
       "   0.06329923273657288,\n",
       "   0.10897435897435898,\n",
       "   0.07234531587787608,\n",
       "   0.08716895563256749,\n",
       "   0.06641737891737891,\n",
       "   0.08445866750951497,\n",
       "   0.08327759197324415,\n",
       "   0.12947547440028642,\n",
       "   0.06399855620468127,\n",
       "   0.13690680388793597,\n",
       "   0.12393059458495026,\n",
       "   0.07560551318078416,\n",
       "   0.12054341322634006,\n",
       "   0.06045634920634921,\n",
       "   0.10757575757575759,\n",
       "   0.07878787878787878,\n",
       "   0.0866161616161616,\n",
       "   0.08904249871991805,\n",
       "   0.07652329749103942,\n",
       "   0.11186922530206111,\n",
       "   0.12241708793432932,\n",
       "   0.08886171733178884,\n",
       "   0.10803211006833341,\n",
       "   0.06761786600496278,\n",
       "   0.08983382209188662,\n",
       "   0.11535390671420083,\n",
       "   0.1301566842550449,\n",
       "   0.09873231724644611,\n",
       "   0.06581628133581363,\n",
       "   0.12727765359344306,\n",
       "   0.08540578393519571,\n",
       "   0.08564841351726599,\n",
       "   0.10369749300070115,\n",
       "   0.10632607627143147,\n",
       "   0.08378052975576196,\n",
       "   0.08962738457744007,\n",
       "   0.07039081182990578,\n",
       "   0.10073343531990149,\n",
       "   0.10412427079093745,\n",
       "   0.07652088589851416,\n",
       "   0.12384133863007102,\n",
       "   0.08627569470422913,\n",
       "   0.09375,\n",
       "   0.12567819148936168,\n",
       "   0.06770096934031361,\n",
       "   0.12458117575388362,\n",
       "   0.06495693103384123,\n",
       "   0.09372877084565993,\n",
       "   0.0665380906460945,\n",
       "   0.08235431235431236,\n",
       "   0.06349100810536981,\n",
       "   0.07273819370593564,\n",
       "   0.1418178865749198,\n",
       "   0.0553030303030303,\n",
       "   0.14019072263753116,\n",
       "   0.08562610229276896,\n",
       "   0.09189296823705424,\n",
       "   0.13198380566801618,\n",
       "   0.09227603396806076,\n",
       "   0.09899056881815503,\n",
       "   0.07047239547239546,\n",
       "   0.09085283831046542,\n",
       "   0.08856321839080461,\n",
       "   0.12211538461538463,\n",
       "   0.08770053475935828,\n",
       "   0.07361722278045343,\n",
       "   0.09174065106268496,\n",
       "   0.11252514450131133,\n",
       "   0.1015478655230208,\n",
       "   0.09466247171165204,\n",
       "   0.07962601508470084,\n",
       "   0.07126927006974987,\n",
       "   0.0896780303030303,\n",
       "   0.08593352791639204,\n",
       "   0.12497556207233625,\n",
       "   0.10176100628930818,\n",
       "   0.08028375018282873,\n",
       "   0.10999472252669183,\n",
       "   0.10919045980965485,\n",
       "   0.09406154860096695,\n",
       "   0.0857059607059607,\n",
       "   0.1068881553477189,\n",
       "   0.09031772367468954,\n",
       "   0.1031648705561749,\n",
       "   0.09064701395432313,\n",
       "   0.07660768112745514,\n",
       "   0.09136739445269391,\n",
       "   0.08130779796362099,\n",
       "   0.08158508158508158,\n",
       "   0.07030296092796091,\n",
       "   0.1001303899407367,\n",
       "   0.09439374185136899,\n",
       "   0.10253200568990044,\n",
       "   0.13881185476337785,\n",
       "   0.067816091954023,\n",
       "   0.09253432494279175,\n",
       "   0.10999668684678171,\n",
       "   0.11216062460765851,\n",
       "   0.09519334049409238,\n",
       "   0.11062091503267973,\n",
       "   0.08351139601139601,\n",
       "   0.0975649088856636,\n",
       "   0.07949232332958861,\n",
       "   0.1328499369482976,\n",
       "   0.11289951878187172,\n",
       "   0.1017651430694909,\n",
       "   0.0767370926945395,\n",
       "   0.05922252621979024,\n",
       "   0.11350253563102362,\n",
       "   0.0927551375827238,\n",
       "   0.09369501466275659,\n",
       "   0.10053617406558583,\n",
       "   0.07538103171664816,\n",
       "   0.06473473046408701,\n",
       "   0.09444444444444444,\n",
       "   0.09503281579784312,\n",
       "   0.09013813603776884,\n",
       "   0.07695810564663023,\n",
       "   0.12633689839572193,\n",
       "   0.07965337132003798,\n",
       "   0.11289662063655871,\n",
       "   0.061114130434782615,\n",
       "   0.12283874540063588,\n",
       "   0.0578323470309533,\n",
       "   0.06962205171923842,\n",
       "   0.08052292535051156,\n",
       "   0.06885521885521885,\n",
       "   0.10420819490586931,\n",
       "   0.10772108843537416,\n",
       "   0.09804404628592757,\n",
       "   0.06270396270396271,\n",
       "   0.08668650793650794,\n",
       "   0.1253846153846154,\n",
       "   0.08827404479578393,\n",
       "   0.058946902727369556,\n",
       "   0.1040401829741129,\n",
       "   0.09753086419753088,\n",
       "   0.0902184235517569,\n",
       "   0.08900820985277409,\n",
       "   0.09093735866123927,\n",
       "   0.11962119228419306,\n",
       "   0.090958605664488,\n",
       "   0.052821869488536156,\n",
       "   0.10165143498476832,\n",
       "   0.10782163742690058,\n",
       "   0.09584187720651403,\n",
       "   0.05862884160756501,\n",
       "   0.0915716536892844,\n",
       "   0.08502605877458065,\n",
       "   0.11275959074701214,\n",
       "   0.11258503401360544,\n",
       "   0.10907383708672265,\n",
       "   0.1316776460254721,\n",
       "   0.1011999135666513,\n",
       "   0.09863131855613057,\n",
       "   0.0999581252361839,\n",
       "   0.09952959730266199,\n",
       "   0.07946613846444067,\n",
       "   0.09260341707150217,\n",
       "   0.09539711452279757,\n",
       "   0.06597568899665605,\n",
       "   0.07253086419753087,\n",
       "   0.13926799007444168,\n",
       "   0.07583774250440917,\n",
       "   0.10886693755546213,\n",
       "   0.11479468599033815,\n",
       "   0.08461655508216677,\n",
       "   0.13088542910704334,\n",
       "   0.0793962029256147,\n",
       "   0.08060349609344046,\n",
       "   0.0909676643827383,\n",
       "   0.06069019996227126,\n",
       "   0.10774216001655473,\n",
       "   0.10206475370409795,\n",
       "   0.13445876026100897,\n",
       "   0.080722939582337,\n",
       "   0.08235431235431236,\n",
       "   0.06968607912004139,\n",
       "   0.0962171456644559,\n",
       "   0.11124752266867777,\n",
       "   0.08150718007370696,\n",
       "   0.06391647568118157,\n",
       "   0.10774857460714782,\n",
       "   0.0838807267378696,\n",
       "   0.07989459815546772,\n",
       "   0.14565141285321329,\n",
       "   0.08572124756335282,\n",
       "   0.08589743589743588,\n",
       "   0.09611069839340251,\n",
       "   0.10328965328965328,\n",
       "   0.09370429252782193,\n",
       "   0.08024124220932731,\n",
       "   0.08066612169611603,\n",
       "   0.10562347950845237,\n",
       "   0.08111111111111112,\n",
       "   0.06730924808728013,\n",
       "   0.12620173364854217,\n",
       "   0.09836383374689826,\n",
       "   0.09389140271493213,\n",
       "   0.07594334146058285,\n",
       "   0.08125436757512229,\n",
       "   0.08848905723905724,\n",
       "   0.11054994388327721,\n",
       "   0.09895164741121094,\n",
       "   0.09134185546614926,\n",
       "   0.08922928922928923,\n",
       "   0.06278504104591061,\n",
       "   0.09380106450612906,\n",
       "   0.08127846116055402,\n",
       "   0.08298604380693933,\n",
       "   0.10641112236856919,\n",
       "   0.04311490978157645,\n",
       "   0.10723477715003138,\n",
       "   0.12028904054596547,\n",
       "   0.09424816521590713,\n",
       "   0.07093554691629889,\n",
       "   0.08904962446564674,\n",
       "   0.07659206289178443,\n",
       "   0.06725806451612903,\n",
       "   0.10012981714328102,\n",
       "   0.09459433015484307,\n",
       "   0.04377205587651642,\n",
       "   0.0832637148426622,\n",
       "   0.10185185185185186,\n",
       "   0.10872912127814088,\n",
       "   0.09844070212955537,\n",
       "   0.09179470763977805,\n",
       "   0.10762001894077367,\n",
       "   0.09730359406585125,\n",
       "   0.06470075898967276,\n",
       "   0.06731568736141906,\n",
       "   0.11215007215007215,\n",
       "   0.08182081343175568,\n",
       "   0.12212984416882466,\n",
       "   0.09325396825396826,\n",
       "   0.06769671431325566,\n",
       "   0.07473825475853874,\n",
       "   0.028203616352201255,\n",
       "   0.1009371833839919,\n",
       "   0.12124268311975954,\n",
       "   0.06563958916900094,\n",
       "   0.12728856038401087,\n",
       "   0.11127531046641259,\n",
       "   0.08760170006071645,\n",
       "   0.11500725385501263,\n",
       "   0.09186535764375876,\n",
       "   0.08084656084656085,\n",
       "   0.11316860465116278,\n",
       "   0.13737373737373737,\n",
       "   0.0731152676124213,\n",
       "   0.04497354497354497,\n",
       "   0.10178149273893955,\n",
       "   0.17299717321945382,\n",
       "   0.06720372359470103,\n",
       "   0.08413461538461538,\n",
       "   0.11018826135105204,\n",
       "   0.03748792270531401,\n",
       "   0.09579020013802624,\n",
       "   0.11377289377289378,\n",
       "   0.06140460235831905,\n",
       "   0.05497685185185185,\n",
       "   0.0788023679417122,\n",
       "   0.08468104222821203,\n",
       "   0.1253968253968254,\n",
       "   0.09483470099981135,\n",
       "   0.10974293563579278,\n",
       "   0.07388609507253575,\n",
       "   0.11366865586814436,\n",
       "   0.09827965572646423,\n",
       "   0.07099232217876285,\n",
       "   0.09146516744612866,\n",
       "   0.07580645161290323,\n",
       "   0.11239675860994235,\n",
       "   0.0934973434973435,\n",
       "   0.1007615405260333,\n",
       "   0.11397202125439132,\n",
       "   0.08786924445222057,\n",
       "   0.13204435739828363,\n",
       "   0.121763577615517,\n",
       "   0.09340449866565002,\n",
       "   0.08914150292329127,\n",
       "   0.10665317790602319,\n",
       "   0.12550077041602467,\n",
       "   0.11278264497288278,\n",
       "   0.11078565959432583,\n",
       "   0.058235415378272516,\n",
       "   0.10441745242430006,\n",
       "   0.09271906254664876,\n",
       "   0.06114107173429207,\n",
       "   0.08616427432216905,\n",
       "   0.11373878364905284,\n",
       "   0.10740634765679791,\n",
       "   0.12492932056543549,\n",
       "   0.11761779338601112,\n",
       "   0.0947465127151353,\n",
       "   0.09339033760865399,\n",
       "   0.09188799772094756,\n",
       "   0.12024985012310048,\n",
       "   0.09070048309178745,\n",
       "   0.12012559007094525,\n",
       "   0.06188001889466225,\n",
       "   0.08931969498007235,\n",
       "   0.0989244190065392,\n",
       "   0.03753930817610063,\n",
       "   0.09878618113912234,\n",
       "   0.059005580923389145,\n",
       "   0.0748206425674973,\n",
       "   0.12181477486899588,\n",
       "   0.06734006734006734,\n",
       "   0.08762280337622803,\n",
       "   0.06689016602809705,\n",
       "   0.09095300834431269,\n",
       "   0.05390648567119156,\n",
       "   0.12872835046772252,\n",
       "   0.0892334261264048,\n",
       "   0.11923762511997806,\n",
       "   0.10347086898811036,\n",
       "   0.09904169904169903,\n",
       "   0.06946965113934261,\n",
       "   0.08906786122760198,\n",
       "   0.10793905285430709,\n",
       "   0.06247401738473168,\n",
       "   0.07301066447908121,\n",
       "   0.09666173934466617,\n",
       "   0.07934223282869714,\n",
       "   0.11580579408448262,\n",
       "   0.09145299145299146,\n",
       "   0.08275058275058274,\n",
       "   0.10281004544416476,\n",
       "   0.08859300511586965,\n",
       "   0.14316308826718716,\n",
       "   0.08525219298245613,\n",
       "   0.07442463374666765,\n",
       "   0.10013227513227513,\n",
       "   0.137511215314389,\n",
       "   0.07006802721088434,\n",
       "   0.07652901085104474,\n",
       "   0.1010365353037767,\n",
       "   0.10652337858220211,\n",
       "   0.10577415416125095,\n",
       "   0.12050720671410327,\n",
       "   0.12630096181452768,\n",
       "   0.1028401143343672,\n",
       "   0.11476139601139601,\n",
       "   0.10709470845972363,\n",
       "   0.07880745994221171,\n",
       "   0.10295326066510242,\n",
       "   0.09635166728189985,\n",
       "   0.12573202313599163,\n",
       "   0.08721588763875254,\n",
       "   0.08608632746563782,\n",
       "   0.09983296487349547,\n",
       "   0.08238838851811055,\n",
       "   0.04102483164983165,\n",
       "   0.12196048632218844,\n",
       "   0.08791005742225255,\n",
       "   0.0903754686363382,\n",
       "   0.04892154411151808,\n",
       "   0.12937767699672462,\n",
       "   0.08377659574468084,\n",
       "   0.128438309067849,\n",
       "   0.08242650595591772,\n",
       "   0.10794871794871795,\n",
       "   0.09350787569178372,\n",
       "   0.08928571428571429,\n",
       "   0.08454766241651489,\n",
       "   0.08145687215454657,\n",
       "   0.08882987483530962,\n",
       "   0.11334863442956256,\n",
       "   0.07255949769113437,\n",
       "   0.10399760361636075,\n",
       "   0.09094937409992244,\n",
       "   0.09907476403378042,\n",
       "   0.10788564427982121,\n",
       "   0.07850506980941764,\n",
       "   0.06507575757575758,\n",
       "   0.10022111663902707,\n",
       "   0.10392458418774209,\n",
       "   0.10451559934318555,\n",
       "   0.11435786435786437,\n",
       "   0.0690274414850686,\n",
       "   0.05855943152454781,\n",
       "   0.0760755048287972,\n",
       "   0.1045673076923077,\n",
       "   0.09849056603773586,\n",
       "   0.10633846272513402,\n",
       "   0.09735210323445619,\n",
       "   0.09168946995033951,\n",
       "   0.07391774891774892,\n",
       "   0.07759969028261711,\n",
       "   0.12676747938149494,\n",
       "   0.11520181011706436,\n",
       "   0.05224202189610994,\n",
       "   0.12535227535227536,\n",
       "   0.07911303433691494,\n",
       "   0.1011446298673513,\n",
       "   0.12175438596491228,\n",
       "   0.08668606973691718,\n",
       "   0.06220419202163624,\n",
       "   0.05152534602183314,\n",
       "   0.10405928557650468,\n",
       "   0.10118353647765412,\n",
       "   0.08236208236208237,\n",
       "   0.11124465811965811],\n",
       "  'mean': 0.09392340196666149,\n",
       "  'std': 0.022187774311451035,\n",
       "  'ci_lower': 0.05224148291933147,\n",
       "  'ci_upper': 0.13751955021974982},\n",
       " 'multitask_sentiment': {'values': [0.40539725614352484,\n",
       "   0.3487654320987654,\n",
       "   0.3988380537400145,\n",
       "   0.37637385686718683,\n",
       "   0.37957211870255353,\n",
       "   0.35185185185185186,\n",
       "   0.38553473667977484,\n",
       "   0.4627039627039627,\n",
       "   0.43243243243243246,\n",
       "   0.4028948488718604,\n",
       "   0.3913288288288288,\n",
       "   0.3940648723257419,\n",
       "   0.37664473684210525,\n",
       "   0.42039800995024873,\n",
       "   0.40896057347670256,\n",
       "   0.47631952510001296,\n",
       "   0.3312745648512072,\n",
       "   0.4691120926957302,\n",
       "   0.4006535947712419,\n",
       "   0.40458640458640466,\n",
       "   0.394940170940171,\n",
       "   0.346765350877193,\n",
       "   0.48779260005033986,\n",
       "   0.5286682030868078,\n",
       "   0.4090909090909091,\n",
       "   0.4289585753000387,\n",
       "   0.39493526817470476,\n",
       "   0.37656140350877193,\n",
       "   0.40907668231611893,\n",
       "   0.3877151799687011,\n",
       "   0.3659919028340081,\n",
       "   0.4140056022408964,\n",
       "   0.45242656449553004,\n",
       "   0.3855889724310777,\n",
       "   0.4010416666666667,\n",
       "   0.48709239130434784,\n",
       "   0.4165266106442577,\n",
       "   0.5023768115942029,\n",
       "   0.40736728060671723,\n",
       "   0.37939335580163697,\n",
       "   0.43107769423558895,\n",
       "   0.4561285248848444,\n",
       "   0.36565656565656574,\n",
       "   0.3683453237410072,\n",
       "   0.4611992945326279,\n",
       "   0.4336838221730308,\n",
       "   0.42144349036899514,\n",
       "   0.3676236044657097,\n",
       "   0.4542403628117914,\n",
       "   0.47033023735810114,\n",
       "   0.35353535353535354,\n",
       "   0.3724253888188314,\n",
       "   0.40902872777017785,\n",
       "   0.3995169082125603,\n",
       "   0.41478342749529196,\n",
       "   0.4184149184149184,\n",
       "   0.40690559440559443,\n",
       "   0.48189134808853124,\n",
       "   0.39993811402490914,\n",
       "   0.46380998234633286,\n",
       "   0.47901234567901235,\n",
       "   0.3789173789173789,\n",
       "   0.4781609195402299,\n",
       "   0.3752913752913753,\n",
       "   0.3421448527831507,\n",
       "   0.4453242835595777,\n",
       "   0.3363486842105263,\n",
       "   0.41740715424925945,\n",
       "   0.41969638984564356,\n",
       "   0.3772074405462679,\n",
       "   0.4664774263314409,\n",
       "   0.47300633967300637,\n",
       "   0.4683760683760683,\n",
       "   0.3693398799781779,\n",
       "   0.4692327834511652,\n",
       "   0.3700280112044818,\n",
       "   0.3732251521298175,\n",
       "   0.3957516339869281,\n",
       "   0.4468468468468469,\n",
       "   0.4209563029055136,\n",
       "   0.4444444444444444,\n",
       "   0.42637362637362636,\n",
       "   0.3871391076115486,\n",
       "   0.3143042792165599,\n",
       "   0.47586206896551725,\n",
       "   0.3957516339869281,\n",
       "   0.35784313725490197,\n",
       "   0.3805639043938743,\n",
       "   0.411764705882353,\n",
       "   0.4281586551090097,\n",
       "   0.41521498010047625,\n",
       "   0.3657142857142857,\n",
       "   0.39436152570480926,\n",
       "   0.5034444237572147,\n",
       "   0.3386243386243386,\n",
       "   0.43418701608100063,\n",
       "   0.40075973409306737,\n",
       "   0.48590381426202317,\n",
       "   0.3841319717203457,\n",
       "   0.4319744204636291,\n",
       "   0.3864042933810376,\n",
       "   0.4752635847526358,\n",
       "   0.411764705882353,\n",
       "   0.45171578007398905,\n",
       "   0.3882211538461539,\n",
       "   0.4392271949131817,\n",
       "   0.38472314384723144,\n",
       "   0.41316469468244255,\n",
       "   0.4197080291970803,\n",
       "   0.3965085126609627,\n",
       "   0.378724589250905,\n",
       "   0.3952684742158426,\n",
       "   0.3852459016393442,\n",
       "   0.3543743078626799,\n",
       "   0.4392271949131817,\n",
       "   0.3815085158150852,\n",
       "   0.4495748774281636,\n",
       "   0.4651162790697674,\n",
       "   0.40716131413805834,\n",
       "   0.4840525328330207,\n",
       "   0.4512820512820513,\n",
       "   0.40317460317460324,\n",
       "   0.2377622377622378,\n",
       "   0.4469313438138378,\n",
       "   0.3701122725512969,\n",
       "   0.4408109875735775,\n",
       "   0.47472527472527476,\n",
       "   0.4272199386703203,\n",
       "   0.37150767681852503,\n",
       "   0.30988649940262847,\n",
       "   0.48729251700680276,\n",
       "   0.47641886490807356,\n",
       "   0.419588592800374,\n",
       "   0.4358765129878961,\n",
       "   0.46833631484794275,\n",
       "   0.400735294117647,\n",
       "   0.5120018115942029,\n",
       "   0.42128603104212864,\n",
       "   0.3737316442931871,\n",
       "   0.4493638676844784,\n",
       "   0.3538847117794486,\n",
       "   0.37339803409232303,\n",
       "   0.44396914900512024,\n",
       "   0.45304136253041366,\n",
       "   0.41960375391032323,\n",
       "   0.4302589731940027,\n",
       "   0.4406224406224406,\n",
       "   0.38647764449291166,\n",
       "   0.40587385652383245,\n",
       "   0.37351290684624017,\n",
       "   0.4216205000518726,\n",
       "   0.412738319715064,\n",
       "   0.40291858678955456,\n",
       "   0.3132420091324201,\n",
       "   0.41435185185185186,\n",
       "   0.44825174825174824,\n",
       "   0.4474980554835364,\n",
       "   0.42027397260273974,\n",
       "   0.4215686274509804,\n",
       "   0.38095238095238093,\n",
       "   0.42271401612626525,\n",
       "   0.4209450830140485,\n",
       "   0.33686067019400356,\n",
       "   0.43114909781576444,\n",
       "   0.4090909090909091,\n",
       "   0.4206868686868687,\n",
       "   0.3485191710713986,\n",
       "   0.4600472813238771,\n",
       "   0.42263630089717047,\n",
       "   0.44388714733542317,\n",
       "   0.43973829201101927,\n",
       "   0.43235435724602794,\n",
       "   0.36548636548636554,\n",
       "   0.3389162561576355,\n",
       "   0.3731178339549661,\n",
       "   0.39567183462532296,\n",
       "   0.34181343770384864,\n",
       "   0.4444858189039906,\n",
       "   0.4375,\n",
       "   0.3769482151835093,\n",
       "   0.38082882882882885,\n",
       "   0.3736365746112787,\n",
       "   0.42240918803418803,\n",
       "   0.3666666666666667,\n",
       "   0.3911869225302061,\n",
       "   0.3860294117647059,\n",
       "   0.4422458988669034,\n",
       "   0.4325980392156863,\n",
       "   0.44396914900512024,\n",
       "   0.37112010796221323,\n",
       "   0.29462545252018935,\n",
       "   0.3547887635478877,\n",
       "   0.37341072415699283,\n",
       "   0.37793209876543205,\n",
       "   0.4721223021582734,\n",
       "   0.37985696379856965,\n",
       "   0.42507296028422786,\n",
       "   0.3752913752913753,\n",
       "   0.4613003095975232,\n",
       "   0.4339414040906579,\n",
       "   0.390625,\n",
       "   0.4079967360261118,\n",
       "   0.3846153846153846,\n",
       "   0.3861194527861194,\n",
       "   0.4238095238095238,\n",
       "   0.40256410256410263,\n",
       "   0.36611062335381916,\n",
       "   0.45054945054945056,\n",
       "   0.3600734843845683,\n",
       "   0.45934280144806455,\n",
       "   0.4618055555555555,\n",
       "   0.3907342657342657,\n",
       "   0.3957516339869281,\n",
       "   0.3957516339869281,\n",
       "   0.4531902206320811,\n",
       "   0.41687370600414075,\n",
       "   0.4188948306595366,\n",
       "   0.5026819923371647,\n",
       "   0.5101449275362319,\n",
       "   0.5067226890756302,\n",
       "   0.3861194527861194,\n",
       "   0.39060150375939845,\n",
       "   0.4022556390977443,\n",
       "   0.39567430025445294,\n",
       "   0.3751578282828283,\n",
       "   0.42055555555555557,\n",
       "   0.41010101010101013,\n",
       "   0.40614538498326164,\n",
       "   0.4306763285024155,\n",
       "   0.42554059567523456,\n",
       "   0.4102564102564103,\n",
       "   0.38441734417344176,\n",
       "   0.3860294117647059,\n",
       "   0.4219858156028369,\n",
       "   0.34533468559837727,\n",
       "   0.5145530145530145,\n",
       "   0.3448075526506899,\n",
       "   0.39052095573834705,\n",
       "   0.43870967741935485,\n",
       "   0.44550264550264557,\n",
       "   0.41988742964352727,\n",
       "   0.4475524475524475,\n",
       "   0.49062159912937714,\n",
       "   0.4294871794871795,\n",
       "   0.40590196495708303,\n",
       "   0.33523238380809595,\n",
       "   0.4348595443485954,\n",
       "   0.4120300751879699,\n",
       "   0.4715071897745429,\n",
       "   0.43734335839599,\n",
       "   0.3626089004002826,\n",
       "   0.3956356736242885,\n",
       "   0.4512820512820513,\n",
       "   0.41655420602789023,\n",
       "   0.36565656565656574,\n",
       "   0.38047138047138046,\n",
       "   0.42863657090743273,\n",
       "   0.40547263681592033,\n",
       "   0.44395532730088855,\n",
       "   0.4818242790073776,\n",
       "   0.4206684594698425,\n",
       "   0.4331198366286085,\n",
       "   0.43082162622392506,\n",
       "   0.3365583395352683,\n",
       "   0.40769230769230774,\n",
       "   0.4531680440771349,\n",
       "   0.31203703703703706,\n",
       "   0.35873015873015873,\n",
       "   0.4380952380952381,\n",
       "   0.4028948488718604,\n",
       "   0.469359808284834,\n",
       "   0.32994062765055127,\n",
       "   0.42857142857142855,\n",
       "   0.3560844185002245,\n",
       "   0.4047828380516969,\n",
       "   0.3943562610229277,\n",
       "   0.4438122332859175,\n",
       "   0.3791602662570404,\n",
       "   0.4748031496062992,\n",
       "   0.35380577427821525,\n",
       "   0.38175015357122916,\n",
       "   0.4291858678955453,\n",
       "   0.4600472813238771,\n",
       "   0.5031897926634769,\n",
       "   0.45182315685912805,\n",
       "   0.42144349036899514,\n",
       "   0.3333333333333333,\n",
       "   0.3936543073233721,\n",
       "   0.4416294088425236,\n",
       "   0.4569046658598898,\n",
       "   0.4366463237430979,\n",
       "   0.39113913816800067,\n",
       "   0.37395833333333334,\n",
       "   0.4373839488343305,\n",
       "   0.44169664268585135,\n",
       "   0.43045843045843046,\n",
       "   0.49637983848510164,\n",
       "   0.43134242366045705,\n",
       "   0.4289363990856529,\n",
       "   0.3837408837408837,\n",
       "   0.411764705882353,\n",
       "   0.41868633649455567,\n",
       "   0.409813596491228,\n",
       "   0.47729988052568695,\n",
       "   0.42142857142857143,\n",
       "   0.45416666666666666,\n",
       "   0.4064321865590365,\n",
       "   0.4515519568151147,\n",
       "   0.4405413625304136,\n",
       "   0.39632333062989994,\n",
       "   0.4117886178861789,\n",
       "   0.44579533941236066,\n",
       "   0.42023809523809524,\n",
       "   0.41988742964352727,\n",
       "   0.4144406669904484,\n",
       "   0.4238095238095238,\n",
       "   0.3429951690821256,\n",
       "   0.3446778711484593,\n",
       "   0.4900362318840579,\n",
       "   0.5352517985611511,\n",
       "   0.4079929732103645,\n",
       "   0.46044166231959655,\n",
       "   0.46603053435114505,\n",
       "   0.3842761557177616,\n",
       "   0.3585766423357664,\n",
       "   0.43391552087204266,\n",
       "   0.4698966408268734,\n",
       "   0.357618740597464,\n",
       "   0.4153520131857782,\n",
       "   0.3956687152472929,\n",
       "   0.35295508274231674,\n",
       "   0.2922885572139304,\n",
       "   0.45832380080835816,\n",
       "   0.41663630843958704,\n",
       "   0.44607843137254904,\n",
       "   0.41290139244103435,\n",
       "   0.4379509379509379,\n",
       "   0.3885869565217391,\n",
       "   0.4204585537918872,\n",
       "   0.398712816890354,\n",
       "   0.35461312880667717,\n",
       "   0.4376149198844234,\n",
       "   0.3806175902950096,\n",
       "   0.3082956259426848,\n",
       "   0.4728618421052631,\n",
       "   0.42578668894458366,\n",
       "   0.4295281582952815,\n",
       "   0.44633356465417534,\n",
       "   0.4198321240574761,\n",
       "   0.4336838221730308,\n",
       "   0.398989898989899,\n",
       "   0.4873542329244283,\n",
       "   0.34476190476190477,\n",
       "   0.48169191919191917,\n",
       "   0.3922101449275362,\n",
       "   0.4430769230769231,\n",
       "   0.38612033960871167,\n",
       "   0.4364842454394693,\n",
       "   0.44423076923076926,\n",
       "   0.39168343393695504,\n",
       "   0.3748951422252726,\n",
       "   0.411965811965812,\n",
       "   0.3600734843845683,\n",
       "   0.3925925925925926,\n",
       "   0.34891857506361323,\n",
       "   0.4045584045584046,\n",
       "   0.3760330578512397,\n",
       "   0.390311986863711,\n",
       "   0.3818947368421053,\n",
       "   0.4419753086419753,\n",
       "   0.4301985370950889,\n",
       "   0.4166278166278166,\n",
       "   0.3874531465772342,\n",
       "   0.3446778711484593,\n",
       "   0.4524178282567544,\n",
       "   0.42094017094017094,\n",
       "   0.4346642468239564,\n",
       "   0.4533144386710754,\n",
       "   0.39389978213507626,\n",
       "   0.3977207977207977,\n",
       "   0.4015553199010251,\n",
       "   0.41613756613756614,\n",
       "   0.40686274509803927,\n",
       "   0.4010416666666667,\n",
       "   0.4293361140076469,\n",
       "   0.424671514223753,\n",
       "   0.49472830494728304,\n",
       "   0.33042720139494336,\n",
       "   0.38472314384723144,\n",
       "   0.41844589687726946,\n",
       "   0.4351851851851852,\n",
       "   0.3646322378716745,\n",
       "   0.4122305764411028,\n",
       "   0.46499890278692124,\n",
       "   0.292022792022792,\n",
       "   0.4639249639249639,\n",
       "   0.37697791688402016,\n",
       "   0.4101335159192057,\n",
       "   0.41868633649455567,\n",
       "   0.40547263681592033,\n",
       "   0.42709376042709374,\n",
       "   0.3390191897654584,\n",
       "   0.4004362050163577,\n",
       "   0.4812409812409812,\n",
       "   0.46649703138252757,\n",
       "   0.3449419568822554,\n",
       "   0.45899280575539575,\n",
       "   0.42735745614035087,\n",
       "   0.4447916666666667,\n",
       "   0.43885003885003887,\n",
       "   0.4013605442176871,\n",
       "   0.412738319715064,\n",
       "   0.3563370517426105,\n",
       "   0.4495748774281636,\n",
       "   0.4006535947712419,\n",
       "   0.3954022988505747,\n",
       "   0.4285426731078905,\n",
       "   0.4384057971014492,\n",
       "   0.48657616892911015,\n",
       "   0.40373073803730736,\n",
       "   0.40413851351351354,\n",
       "   0.4227761485826002,\n",
       "   0.4202516827626573,\n",
       "   0.40694789081885857,\n",
       "   0.4389724310776943,\n",
       "   0.4126666666666667,\n",
       "   0.42562984496124034,\n",
       "   0.41358024691358025,\n",
       "   0.41648484848484846,\n",
       "   0.4565134099616858,\n",
       "   0.42924976258309594,\n",
       "   0.4342403628117914,\n",
       "   0.42966194111232275,\n",
       "   0.45615220083305186,\n",
       "   0.47602013778484364,\n",
       "   0.4084867989749091,\n",
       "   0.40491763565891475,\n",
       "   0.3362654320987654,\n",
       "   0.4031878031878032,\n",
       "   0.44825174825174824,\n",
       "   0.4040587611691655,\n",
       "   0.4011156186612576,\n",
       "   0.39817351598173517,\n",
       "   0.3946053946053946,\n",
       "   0.3298453668178119,\n",
       "   0.3922101449275362,\n",
       "   0.407936507936508,\n",
       "   0.4347747747747748,\n",
       "   0.4287317620650954,\n",
       "   0.46486486486486484,\n",
       "   0.4683760683760683,\n",
       "   0.41378892523930694,\n",
       "   0.3986531986531987,\n",
       "   0.40686274509803927,\n",
       "   0.42857142857142855,\n",
       "   0.41093609937251885,\n",
       "   0.42263630089717047,\n",
       "   0.4237946386283982,\n",
       "   0.34118967452300786,\n",
       "   0.3642694522565957,\n",
       "   0.4399136178861789,\n",
       "   0.42751322751322746,\n",
       "   0.4406224406224406,\n",
       "   0.42896627971254836,\n",
       "   0.3621890547263682,\n",
       "   0.4001717475311293,\n",
       "   0.424957264957265,\n",
       "   0.4259259259259259,\n",
       "   0.46204906204906204,\n",
       "   0.38600988829884636,\n",
       "   0.3452380952380952,\n",
       "   0.3816296296296296,\n",
       "   0.39627039627039623,\n",
       "   0.4437229437229437,\n",
       "   0.37464601032816924,\n",
       "   0.3748792270531401,\n",
       "   0.38095238095238093,\n",
       "   0.4024654339496918,\n",
       "   0.3625,\n",
       "   0.47260606060606064,\n",
       "   0.44047619047619047,\n",
       "   0.4379509379509379,\n",
       "   0.3572519083969466,\n",
       "   0.3780864197530864,\n",
       "   0.31632875517767606,\n",
       "   0.32827586206896553,\n",
       "   0.40585241730279903,\n",
       "   0.4299933642999336,\n",
       "   0.43498817966903075,\n",
       "   0.3952205882352941,\n",
       "   0.3764097744360902,\n",
       "   0.46941761779338603,\n",
       "   0.4043927648578811,\n",
       "   0.4015553199010251,\n",
       "   0.39833035181872384,\n",
       "   0.44101587301587303,\n",
       "   0.4581566299123551,\n",
       "   0.3974358974358974,\n",
       "   0.4071428571428572,\n",
       "   0.38685446009389673,\n",
       "   0.35607728050868187,\n",
       "   0.3657407407407407,\n",
       "   0.3584687805311306,\n",
       "   0.40267244492596604,\n",
       "   0.42086330935251803,\n",
       "   0.4660493827160494,\n",
       "   0.41923076923076924,\n",
       "   0.40756458938277124,\n",
       "   0.44039735099337746,\n",
       "   0.43107769423558895,\n",
       "   0.34759358288770054,\n",
       "   0.4351851851851852,\n",
       "   0.3070607553366174,\n",
       "   0.4257602862254026,\n",
       "   0.4342403628117914,\n",
       "   0.46273684210526317,\n",
       "   0.3750932140193885,\n",
       "   0.45800106326422113,\n",
       "   0.41238095238095235,\n",
       "   0.4177777777777778,\n",
       "   0.48287385129490384,\n",
       "   0.4356751824817518,\n",
       "   0.3988380537400145,\n",
       "   0.3857142857142857,\n",
       "   0.4721223021582734,\n",
       "   0.4321759259259259,\n",
       "   0.446,\n",
       "   0.38834422657952067,\n",
       "   0.44870956015994184,\n",
       "   0.44946899623158615,\n",
       "   0.42166241496598644,\n",
       "   0.41614850860592956,\n",
       "   0.3872580995868667,\n",
       "   0.46558008390069466,\n",
       "   0.3986031746031746,\n",
       "   0.48302435478008005,\n",
       "   0.41798941798941797,\n",
       "   0.31978021978021975,\n",
       "   0.4319744204636291,\n",
       "   0.4334521687462864,\n",
       "   0.5016339869281046,\n",
       "   0.35784313725490197,\n",
       "   0.40587385652383245,\n",
       "   0.4512820512820513,\n",
       "   0.405037927579791,\n",
       "   0.38941798941798944,\n",
       "   0.3379190385831752,\n",
       "   0.39073371283997466,\n",
       "   0.40762463343108496,\n",
       "   0.4733127783487496,\n",
       "   0.3798941798941799,\n",
       "   0.4120300751879699,\n",
       "   0.4129782675237221,\n",
       "   0.4382265477155988,\n",
       "   0.4245269597382273,\n",
       "   0.4503529411764706,\n",
       "   0.38544251447477257,\n",
       "   0.4830917874396135,\n",
       "   0.4515068493150685,\n",
       "   0.4263059701492537,\n",
       "   0.4504107981220657,\n",
       "   0.43902439024390244,\n",
       "   0.4599379142545515,\n",
       "   0.4771282910817794,\n",
       "   0.46044050695213484,\n",
       "   0.40503686305213016,\n",
       "   0.39052095573834705,\n",
       "   0.42905982905982903,\n",
       "   0.37739018087855297,\n",
       "   0.4051094890510949,\n",
       "   0.44483430799220275,\n",
       "   0.37846153846153846,\n",
       "   0.4428985507246377,\n",
       "   0.4511784511784512,\n",
       "   0.4385902031063322,\n",
       "   0.47203374289381994,\n",
       "   0.4229112833763997,\n",
       "   0.4704761904761905,\n",
       "   0.39705882352941174,\n",
       "   0.4354066985645933,\n",
       "   0.40852130325814534,\n",
       "   0.4486803519061584,\n",
       "   0.4515519568151147,\n",
       "   0.4639249639249639,\n",
       "   0.3780864197530864,\n",
       "   0.44774502978262554,\n",
       "   0.42328042328042326,\n",
       "   0.4271284271284272,\n",
       "   0.39865689865689863,\n",
       "   0.35768779342723,\n",
       "   0.45145592967584963,\n",
       "   0.42163815880962313,\n",
       "   0.4442748091603053,\n",
       "   0.38888888888888884,\n",
       "   0.4366463237430979,\n",
       "   0.3828608667318345,\n",
       "   0.4502262443438914,\n",
       "   0.35709876543209873,\n",
       "   0.5342549923195085,\n",
       "   0.4836829836829837,\n",
       "   0.4365530303030303,\n",
       "   0.38437499999999997,\n",
       "   0.4428543033194196,\n",
       "   0.36246089676746607,\n",
       "   0.4227761485826002,\n",
       "   0.40299277605779155,\n",
       "   0.35100069013112495,\n",
       "   0.4627594627594627,\n",
       "   0.3974358974358974,\n",
       "   0.4389724310776943,\n",
       "   0.3901234567901235,\n",
       "   0.418,\n",
       "   0.40896057347670256,\n",
       "   0.4033816425120773,\n",
       "   0.3988187523071243,\n",
       "   0.3426976744186046,\n",
       "   0.3860294117647059,\n",
       "   0.410046511627907,\n",
       "   0.43045843045843046,\n",
       "   0.40590196495708303,\n",
       "   0.322434875066454,\n",
       "   0.3938466368416922,\n",
       "   0.44607843137254904,\n",
       "   0.3745819397993311,\n",
       "   0.42017879948914433,\n",
       "   0.40726817042606517,\n",
       "   0.39551855278034737,\n",
       "   0.372502937720329,\n",
       "   0.39344815940560623,\n",
       "   0.38552188552188554,\n",
       "   0.4820261437908497,\n",
       "   0.3830227743271222,\n",
       "   0.48499594484995945,\n",
       "   0.4444858189039906,\n",
       "   0.39552238805970147,\n",
       "   0.3465836526181354,\n",
       "   0.4299613487938158,\n",
       "   0.44887186036611326,\n",
       "   0.4202172096908939,\n",
       "   0.3817262549656916,\n",
       "   0.35883084577114427,\n",
       "   0.40291858678955456,\n",
       "   0.4161931818181818,\n",
       "   0.47419072615923,\n",
       "   0.40789473684210525,\n",
       "   0.46079846079846076,\n",
       "   0.4643660915228807,\n",
       "   0.4478844169246646,\n",
       "   0.44288945433983606,\n",
       "   0.3666666666666667,\n",
       "   0.3907760287213477,\n",
       "   0.3988380537400145,\n",
       "   0.4694727400858479,\n",
       "   0.35819735819735815,\n",
       "   0.45528455284552843,\n",
       "   0.4475839475839476,\n",
       "   0.4658893574795336,\n",
       "   0.31521739130434784,\n",
       "   0.41300097751710657,\n",
       "   0.47944006999125105,\n",
       "   0.4329004329004329,\n",
       "   0.432089552238806,\n",
       "   0.3483726150392817,\n",
       "   0.4230937966131438,\n",
       "   0.39197530864197533,\n",
       "   0.4318181818181818,\n",
       "   0.44351755300660406,\n",
       "   0.4818803766172188,\n",
       "   0.4347509996364958,\n",
       "   0.42240918803418803,\n",
       "   0.4011286509570655,\n",
       "   0.4856459330143541,\n",
       "   0.49585406301824214,\n",
       "   0.39226973684210525,\n",
       "   0.37429193899782137,\n",
       "   0.49287531806615775,\n",
       "   0.42954404786465855,\n",
       "   0.3906237952039479,\n",
       "   0.42697881828316603,\n",
       "   0.3661744966442953,\n",
       "   0.3596747967479675,\n",
       "   0.4297643559826127,\n",
       "   0.38528491772065826,\n",
       "   0.29365079365079366,\n",
       "   0.3992248062015504,\n",
       "   0.4386464263124605,\n",
       "   0.38974358974358975,\n",
       "   0.3846153846153846,\n",
       "   0.41606714628297364,\n",
       "   0.35023456076087656,\n",
       "   0.39882209845720507,\n",
       "   0.42522680714076677,\n",
       "   0.4068376068376069,\n",
       "   0.35055374175789006,\n",
       "   0.4226252761306825,\n",
       "   0.48255072463768117,\n",
       "   0.36119840771003564,\n",
       "   0.3992248062015504,\n",
       "   0.3872571872571873,\n",
       "   0.41153741496598634,\n",
       "   0.3864042933810376,\n",
       "   0.38058324104835733,\n",
       "   0.41960375391032323,\n",
       "   0.41084656084656085,\n",
       "   0.38301282051282054,\n",
       "   0.43939393939393945,\n",
       "   0.3920835281284089,\n",
       "   0.4417386750458235,\n",
       "   0.5009946007388463,\n",
       "   0.40594325668952536,\n",
       "   0.40620782726045884,\n",
       "   0.4052093023255814,\n",
       "   0.5233367568184412,\n",
       "   0.48013415892672856,\n",
       "   0.39448539448539455,\n",
       "   0.45310015898251194,\n",
       "   0.453405017921147,\n",
       "   0.40736728060671723,\n",
       "   0.41475922451532216,\n",
       "   0.38138858596873865,\n",
       "   0.5072941176470588,\n",
       "   0.4048083170890188,\n",
       "   0.3669099756690997,\n",
       "   0.41527655838454786,\n",
       "   0.47346451997614797,\n",
       "   0.3618233618233619,\n",
       "   0.49725011956001913,\n",
       "   0.43304843304843305,\n",
       "   0.4294753086419753,\n",
       "   0.36936639118457304,\n",
       "   0.47822222222222227,\n",
       "   0.4432866211522338,\n",
       "   0.3422619047619048,\n",
       "   0.40151515151515155,\n",
       "   0.392826850842118,\n",
       "   0.37542087542087543,\n",
       "   0.3857142857142857,\n",
       "   0.34118967452300786,\n",
       "   0.3807881773399015,\n",
       "   0.403936507936508,\n",
       "   0.4241545893719807,\n",
       "   0.46222222222222226,\n",
       "   0.4007633587786259,\n",
       "   0.38445194350706163,\n",
       "   0.39071038251366125,\n",
       "   0.4264705882352941,\n",
       "   0.4297739297739298,\n",
       "   0.3410069249485308,\n",
       "   0.3574603174603174,\n",
       "   0.37942857142857145,\n",
       "   0.3666551366309236,\n",
       "   0.4196119196119196,\n",
       "   0.45054945054945056,\n",
       "   0.37343358395989973,\n",
       "   0.5082917082917083,\n",
       "   0.4447561165653733,\n",
       "   0.4447293447293448,\n",
       "   0.44931773879142306,\n",
       "   0.4088023088023088,\n",
       "   0.44882476461423826,\n",
       "   0.39849624060150374,\n",
       "   0.4386657101865136,\n",
       "   0.4144379212872364,\n",
       "   0.42146812772475545,\n",
       "   0.40874890638670164,\n",
       "   0.44968055799777273,\n",
       "   0.40916530278232405,\n",
       "   0.43772694262890344,\n",
       "   0.38743918620079604,\n",
       "   0.38958932389589324,\n",
       "   0.4288477135192464,\n",
       "   0.40080191225229395,\n",
       "   0.4388500893292188,\n",
       "   0.45310015898251194,\n",
       "   0.4220689655172414,\n",
       "   0.33824021411843425,\n",
       "   0.3993544600938967,\n",
       "   0.4239535847492748,\n",
       "   0.46049382716049386,\n",
       "   0.4158914728682171,\n",
       "   0.45067764625150114,\n",
       "   0.4017453638772012,\n",
       "   0.3516583371194912,\n",
       "   0.4233841684822077,\n",
       "   0.4544112218530823,\n",
       "   0.4224196855775803,\n",
       "   0.349954519143306,\n",
       "   0.4664774263314409,\n",
       "   0.35055374175789006,\n",
       "   0.3879428459581131,\n",
       "   0.43235435724602794,\n",
       "   0.44612794612794615,\n",
       "   0.4208037825059101,\n",
       "   0.43695159223109536,\n",
       "   0.3363826031134649,\n",
       "   0.42121002990568207,\n",
       "   0.4695340501792115,\n",
       "   0.4171410499683745,\n",
       "   0.44124087591240874,\n",
       "   0.37373737373737376,\n",
       "   0.4569221628045157,\n",
       "   0.466031746031746,\n",
       "   0.3950617283950617,\n",
       "   0.4662408759124088,\n",
       "   0.5179164126532547,\n",
       "   0.42287581699346405,\n",
       "   0.38647764449291166,\n",
       "   0.48499594484995945,\n",
       "   0.5111111111111111,\n",
       "   0.4297643559826127,\n",
       "   0.42287581699346405,\n",
       "   0.4654320987654321,\n",
       "   0.4608695652173913,\n",
       "   0.39258729022508554,\n",
       "   0.4260465116279069,\n",
       "   0.480872250582971,\n",
       "   0.48707420924574213,\n",
       "   0.40739329268292684,\n",
       "   0.36960431654676257,\n",
       "   0.41549725442342894,\n",
       "   0.4756276032871778,\n",
       "   0.43071364046973803,\n",
       "   0.46109319440998925,\n",
       "   0.38795518207282914,\n",
       "   0.44065460809646856,\n",
       "   0.43032132078821234,\n",
       "   0.40503686305213016,\n",
       "   0.4018122400475342,\n",
       "   0.4152046783625731,\n",
       "   0.48677248677248675,\n",
       "   0.36517590936195593,\n",
       "   0.4656084656084656,\n",
       "   0.37429193899782137,\n",
       "   0.360594315245478,\n",
       "   0.5024154589371981,\n",
       "   0.3610419777667164,\n",
       "   0.3922101449275362,\n",
       "   0.41615667074663404,\n",
       "   0.37254901960784315,\n",
       "   0.46049382716049386,\n",
       "   0.49466047118284834,\n",
       "   0.4419678036699313,\n",
       "   0.42361459203564467,\n",
       "   0.44289405684754524,\n",
       "   0.36119840771003564,\n",
       "   0.41904761904761906,\n",
       "   0.4968960863697706,\n",
       "   0.41038961038961047,\n",
       "   0.36430921052631576,\n",
       "   0.3828608667318345,\n",
       "   0.45580808080808083,\n",
       "   0.45242656449553004,\n",
       "   0.35848535102266443,\n",
       "   0.3579270970575319,\n",
       "   0.41228070175438597,\n",
       "   0.4442028985507247,\n",
       "   0.48012976480129765,\n",
       "   0.4692327834511652,\n",
       "   0.3731751824817518,\n",
       "   0.3570198105081826,\n",
       "   0.41358024691358025,\n",
       "   0.36507936507936506,\n",
       "   0.4119418284323147,\n",
       "   0.40466824023078934,\n",
       "   0.4304213771839671,\n",
       "   0.4528871391076115,\n",
       "   0.4419753086419753,\n",
       "   0.32293377120963324,\n",
       "   0.4121621621621621,\n",
       "   0.38095238095238093,\n",
       "   0.4230889724310776,\n",
       "   0.4082156611039795,\n",
       "   0.4447293447293448,\n",
       "   0.3521628498727735,\n",
       "   0.36507936507936506,\n",
       "   0.4228438228438229,\n",
       "   0.3886524822695036,\n",
       "   0.3370588235294118,\n",
       "   0.3371647509578544,\n",
       "   0.3429951690821256,\n",
       "   0.4178743961352657,\n",
       "   0.40018279148713926,\n",
       "   0.47028423772609823,\n",
       "   0.36202877582187926,\n",
       "   0.4764000978234287,\n",
       "   0.42636063293340287,\n",
       "   0.38193043584571185,\n",
       "   0.4117886178861789,\n",
       "   0.42637362637362636,\n",
       "   0.3106414685362054,\n",
       "   0.49059829059829063,\n",
       "   0.3574603174603174,\n",
       "   0.4188948306595366,\n",
       "   0.3954470073873059,\n",
       "   0.40219780219780216,\n",
       "   0.4166666666666667,\n",
       "   0.3914515446466356,\n",
       "   0.4004221317654153,\n",
       "   0.4339414040906579,\n",
       "   0.393939393939394,\n",
       "   0.3922101449275362,\n",
       "   0.4203040386246493,\n",
       "   0.4419753086419753,\n",
       "   0.42771684945164506,\n",
       "   0.4334521687462864,\n",
       "   0.4985507246376812,\n",
       "   0.45145592967584963,\n",
       "   0.456936936936937,\n",
       "   0.33671238322401115,\n",
       "   0.35411020776874436,\n",
       "   0.35234864837085,\n",
       "   0.4245728825881498,\n",
       "   0.45222304102186195,\n",
       "   0.46080213255785774,\n",
       "   0.4108236470441195,\n",
       "   0.40049751243781095,\n",
       "   0.3707958707958708,\n",
       "   0.3937950937950938,\n",
       "   0.46983408748114625,\n",
       "   0.38437499999999997,\n",
       "   0.35358796296296297,\n",
       "   0.4444444444444445,\n",
       "   0.4178743961352657,\n",
       "   0.4754521963824289,\n",
       "   0.4700885814828298,\n",
       "   0.4027636659215606,\n",
       "   0.37188940092165895,\n",
       "   0.3557625948930297,\n",
       "   0.40726817042606517,\n",
       "   0.47530739770207625,\n",
       "   0.3259259259259259,\n",
       "   0.4398400581606688,\n",
       "   0.35555555555555557,\n",
       "   0.44255319148936173,\n",
       "   0.42671394799054374,\n",
       "   0.3555764411027569,\n",
       "   0.37472058524690105,\n",
       "   0.3882783882783883,\n",
       "   0.3970412789310427,\n",
       "   0.42437093582131746,\n",
       "   0.4692028985507246,\n",
       "   0.5047418335089567,\n",
       "   0.45441795231416554,\n",
       "   0.47570429192664027,\n",
       "   0.3642178910544727,\n",
       "   0.45099362579677543,\n",
       "   0.4370523415977961,\n",
       "   0.40802845528455284,\n",
       "   0.47032436162870944,\n",
       "   0.47356051703877794,\n",
       "   0.3974747474747475,\n",
       "   0.41693531055233185,\n",
       "   0.394524959742351,\n",
       "   0.445024154589372,\n",
       "   0.4347509996364958,\n",
       "   0.433096926713948,\n",
       "   0.3814102564102564,\n",
       "   0.3828347578347578,\n",
       "   0.45194805194805193,\n",
       "   0.4599610836701093,\n",
       "   0.4265792686845318,\n",
       "   0.46241017136539525,\n",
       "   0.45395044413277236,\n",
       "   0.38647764449291166,\n",
       "   0.3494623655913978,\n",
       "   0.42396127470754336,\n",
       "   0.4443520443520444,\n",
       "   0.4340080971659919,\n",
       "   0.42396127470754336,\n",
       "   0.37670825906120026,\n",
       "   0.4498575498575499,\n",
       "   0.3864042933810376,\n",
       "   0.38481728245507774,\n",
       "   0.3611310234966149,\n",
       "   0.4294599943454906,\n",
       "   0.3975486277644551,\n",
       "   0.3879699248120301,\n",
       "   0.46499890278692124,\n",
       "   0.441358024691358,\n",
       "   0.48499594484995945,\n",
       "   0.4417613636363636,\n",
       "   0.4406015037593985,\n",
       "   0.40520150883462375,\n",
       "   0.435931899641577,\n",
       "   0.40821256038647347,\n",
       "   0.5,\n",
       "   0.41900531664311186,\n",
       "   0.39567430025445294,\n",
       "   0.4333333333333333,\n",
       "   0.3526395173453997,\n",
       "   0.40769230769230774,\n",
       "   0.38831677205660947,\n",
       "   0.42090592334494775,\n",
       "   0.418,\n",
       "   0.44580129654756523,\n",
       "   0.5029978313560403,\n",
       "   0.41244239631336405,\n",
       "   0.3890589916309451,\n",
       "   0.35051368384701714,\n",
       "   0.40564373897707223],\n",
       "  'mean': 0.414538787814567,\n",
       "  'std': 0.04187856179257706,\n",
       "  'ci_lower': 0.3362396058914987,\n",
       "  'ci_upper': 0.49639274468221833},\n",
       " 'multitask_emotion': {'values': [0.13225172074729596,\n",
       "   0.08354011579818033,\n",
       "   0.11603269936603271,\n",
       "   0.12915992084907357,\n",
       "   0.15158277543598644,\n",
       "   0.1307108692066634,\n",
       "   0.11856033874382499,\n",
       "   0.11057173678532901,\n",
       "   0.12724696054299992,\n",
       "   0.09937209937209938,\n",
       "   0.1404870216751405,\n",
       "   0.13225715094873972,\n",
       "   0.07012138188608777,\n",
       "   0.11895652173913043,\n",
       "   0.10363447559709242,\n",
       "   0.11984912776991985,\n",
       "   0.08839285714285715,\n",
       "   0.1267636684303351,\n",
       "   0.12901046943600133,\n",
       "   0.13973063973063973,\n",
       "   0.11979609175870858,\n",
       "   0.1595959595959596,\n",
       "   0.1096096096096096,\n",
       "   0.11694713581506035,\n",
       "   0.13616721937493717,\n",
       "   0.10403578907291072,\n",
       "   0.1209761163032191,\n",
       "   0.12698412698412698,\n",
       "   0.13465940484647565,\n",
       "   0.1290204678362573,\n",
       "   0.11033411033411034,\n",
       "   0.1798573975044563,\n",
       "   0.10067340067340068,\n",
       "   0.11728395061728396,\n",
       "   0.13477834269913477,\n",
       "   0.13082141547488083,\n",
       "   0.15813492063492063,\n",
       "   0.13886693017127802,\n",
       "   0.1056547619047619,\n",
       "   0.12585669781931463,\n",
       "   0.12031224851118623,\n",
       "   0.0860515688101895,\n",
       "   0.16161616161616163,\n",
       "   0.1452619843924192,\n",
       "   0.1377551020408163,\n",
       "   0.13901458019105079,\n",
       "   0.15488215488215487,\n",
       "   0.1341860465116279,\n",
       "   0.10707070707070708,\n",
       "   0.1190958164642375,\n",
       "   0.1618556701030928,\n",
       "   0.1486046511627907,\n",
       "   0.12448979591836735,\n",
       "   0.07880952380952382,\n",
       "   0.13917763917763917,\n",
       "   0.13652868554829337,\n",
       "   0.12498073663122207,\n",
       "   0.16076555023923444,\n",
       "   0.13620448179271707,\n",
       "   0.12435897435897436,\n",
       "   0.12735935495693604,\n",
       "   0.11363636363636365,\n",
       "   0.15060728744939272,\n",
       "   0.1446808510638298,\n",
       "   0.13381642512077294,\n",
       "   0.11090629800307221,\n",
       "   0.1330128205128205,\n",
       "   0.10240240240240239,\n",
       "   0.11815661815661815,\n",
       "   0.11916867362411916,\n",
       "   0.1021825396825397,\n",
       "   0.17100297914597815,\n",
       "   0.1443001443001443,\n",
       "   0.15256268436578171,\n",
       "   0.0797979797979798,\n",
       "   0.1703185703185703,\n",
       "   0.10626102292768959,\n",
       "   0.10343915343915344,\n",
       "   0.12024390243902439,\n",
       "   0.11401597676107479,\n",
       "   0.08697411003236245,\n",
       "   0.09423118629364563,\n",
       "   0.12635712635712637,\n",
       "   0.10919815080503747,\n",
       "   0.11935599928044612,\n",
       "   0.13853020883150008,\n",
       "   0.14414414414414414,\n",
       "   0.17210329595650697,\n",
       "   0.13468013468013468,\n",
       "   0.1439153439153439,\n",
       "   0.12455397892291097,\n",
       "   0.08782051282051283,\n",
       "   0.10649397373628351,\n",
       "   0.11148867313915857,\n",
       "   0.11477663230240549,\n",
       "   0.14814814814814814,\n",
       "   0.10326766729205754,\n",
       "   0.13071895424836602,\n",
       "   0.1036938139741878,\n",
       "   0.1358024691358025,\n",
       "   0.12104172832198874,\n",
       "   0.18041804180418042,\n",
       "   0.0899024024024024,\n",
       "   0.16177908113391984,\n",
       "   0.11812106918238995,\n",
       "   0.14362745098039217,\n",
       "   0.10773273273273272,\n",
       "   0.10986456438245401,\n",
       "   0.09382366808109383,\n",
       "   0.1264327485380117,\n",
       "   0.11910377358490565,\n",
       "   0.1330749354005168,\n",
       "   0.087941878117545,\n",
       "   0.11062661498708011,\n",
       "   0.15007716049382716,\n",
       "   0.1130952380952381,\n",
       "   0.12345679012345678,\n",
       "   0.10836083608360836,\n",
       "   0.10311986863711002,\n",
       "   0.12833008447043534,\n",
       "   0.11082251082251082,\n",
       "   0.15061162079510704,\n",
       "   0.1339031339031339,\n",
       "   0.09663865546218486,\n",
       "   0.13176638176638175,\n",
       "   0.12112890922959574,\n",
       "   0.11614555732202791,\n",
       "   0.08465959328028294,\n",
       "   0.12386417981826876,\n",
       "   0.1503030303030303,\n",
       "   0.13529411764705881,\n",
       "   0.1425438596491228,\n",
       "   0.12564745196324142,\n",
       "   0.148995148995149,\n",
       "   0.14275793650793653,\n",
       "   0.13584905660377358,\n",
       "   0.11715686274509803,\n",
       "   0.13692307692307693,\n",
       "   0.1295210166177908,\n",
       "   0.11947194719471947,\n",
       "   0.1316045066045066,\n",
       "   0.1465502482451635,\n",
       "   0.10493827160493825,\n",
       "   0.12547782874617738,\n",
       "   0.135202492211838,\n",
       "   0.15441906653426019,\n",
       "   0.1089544513457557,\n",
       "   0.1388028895768834,\n",
       "   0.09935064935064936,\n",
       "   0.1451713395638629,\n",
       "   0.08782874617737003,\n",
       "   0.13369043633523045,\n",
       "   0.11904761904761905,\n",
       "   0.1334084084084084,\n",
       "   0.08534890186266335,\n",
       "   0.13441938178780286,\n",
       "   0.1446341463414634,\n",
       "   0.117003367003367,\n",
       "   0.09577922077922078,\n",
       "   0.1512435400516796,\n",
       "   0.1111111111111111,\n",
       "   0.13803458631044838,\n",
       "   0.09935897435897435,\n",
       "   0.1164994425863991,\n",
       "   0.12592364532019704,\n",
       "   0.12022703818369453,\n",
       "   0.11485890652557318,\n",
       "   0.13820931022765884,\n",
       "   0.12958137919055063,\n",
       "   0.12272214159006611,\n",
       "   0.06535947712418301,\n",
       "   0.12895949789153674,\n",
       "   0.13224545300017,\n",
       "   0.15786468741079074,\n",
       "   0.10793650793650793,\n",
       "   0.1073876618431074,\n",
       "   0.10925799863852963,\n",
       "   0.07458333333333333,\n",
       "   0.09204857030943987,\n",
       "   0.13045549838002668,\n",
       "   0.08476190476190476,\n",
       "   0.14330400782013686,\n",
       "   0.09978582554517135,\n",
       "   0.10795809881175734,\n",
       "   0.15213675213675212,\n",
       "   0.11442939244663382,\n",
       "   0.09802693391794552,\n",
       "   0.15384615384615385,\n",
       "   0.13144709696433834,\n",
       "   0.10390189520624304,\n",
       "   0.16561624649859943,\n",
       "   0.1505034446210917,\n",
       "   0.16979006656426013,\n",
       "   0.11707595659623073,\n",
       "   0.10303030303030303,\n",
       "   0.15327380952380953,\n",
       "   0.14896331738437,\n",
       "   0.10651074589127686,\n",
       "   0.11783893985728848,\n",
       "   0.11871657754010695,\n",
       "   0.13730158730158729,\n",
       "   0.15993135993135996,\n",
       "   0.07777777777777778,\n",
       "   0.12698412698412698,\n",
       "   0.12289800408612289,\n",
       "   0.13756613756613756,\n",
       "   0.1404360415602112,\n",
       "   0.15871754107048225,\n",
       "   0.1219205630970337,\n",
       "   0.10836083608360836,\n",
       "   0.08662486938349008,\n",
       "   0.0880952380952381,\n",
       "   0.11764705882352942,\n",
       "   0.0938822624086187,\n",
       "   0.14780701754385964,\n",
       "   0.09515114627713532,\n",
       "   0.12119741100323624,\n",
       "   0.15804878048780488,\n",
       "   0.115359477124183,\n",
       "   0.14684581665713742,\n",
       "   0.11631340293156174,\n",
       "   0.14047451256753582,\n",
       "   0.16707964601769912,\n",
       "   0.10891812865497075,\n",
       "   0.12431561996779388,\n",
       "   0.09482200647249191,\n",
       "   0.1211524024024024,\n",
       "   0.07841269841269842,\n",
       "   0.09350741002117151,\n",
       "   0.13105702050935317,\n",
       "   0.15683865683865683,\n",
       "   0.07643467643467644,\n",
       "   0.10060060060060061,\n",
       "   0.13333333333333333,\n",
       "   0.11287758346581878,\n",
       "   0.12976437976437977,\n",
       "   0.12256341789052068,\n",
       "   0.11159420289855072,\n",
       "   0.10783349721402818,\n",
       "   0.12807017543859647,\n",
       "   0.10287467700258397,\n",
       "   0.1322179322179322,\n",
       "   0.09634408602150539,\n",
       "   0.12186777623670826,\n",
       "   0.0989247311827957,\n",
       "   0.10072463768115943,\n",
       "   0.14761040532365396,\n",
       "   0.11758563074352547,\n",
       "   0.11031746031746031,\n",
       "   0.13045549838002668,\n",
       "   0.11238599588114151,\n",
       "   0.1312169312169312,\n",
       "   0.14527148507731033,\n",
       "   0.12289800408612289,\n",
       "   0.11904087605022184,\n",
       "   0.10375816993464053,\n",
       "   0.12513227513227512,\n",
       "   0.12735935495693604,\n",
       "   0.14754377018527962,\n",
       "   0.15155677655677655,\n",
       "   0.14676434676434677,\n",
       "   0.14019495412844038,\n",
       "   0.10594248094248093,\n",
       "   0.15873015873015872,\n",
       "   0.1489057239057239,\n",
       "   0.11011904761904762,\n",
       "   0.13873873873873874,\n",
       "   0.11777231777231778,\n",
       "   0.09673481843149223,\n",
       "   0.10863095238095237,\n",
       "   0.12800591934887162,\n",
       "   0.12894906511927787,\n",
       "   0.11492913997282082,\n",
       "   0.13943355119825707,\n",
       "   0.12631975867269984,\n",
       "   0.09883619940941463,\n",
       "   0.1532997928381178,\n",
       "   0.12770789657582113,\n",
       "   0.10042432814710041,\n",
       "   0.10723093617441058,\n",
       "   0.11292871819187607,\n",
       "   0.10955207771181867,\n",
       "   0.10543685947542121,\n",
       "   0.11658704215908139,\n",
       "   0.17655677655677657,\n",
       "   0.13648867313915858,\n",
       "   0.13088685015290522,\n",
       "   0.13226694186446508,\n",
       "   0.11462526356143377,\n",
       "   0.11578947368421051,\n",
       "   0.1345369831100757,\n",
       "   0.09547783077194842,\n",
       "   0.10773169482846902,\n",
       "   0.11710526315789475,\n",
       "   0.10417710944026733,\n",
       "   0.128995756718529,\n",
       "   0.11904761904761903,\n",
       "   0.09904601571268239,\n",
       "   0.10404761904761906,\n",
       "   0.16078285643503035,\n",
       "   0.09312473392933163,\n",
       "   0.12654033343688517,\n",
       "   0.12237762237762238,\n",
       "   0.10130718954248367,\n",
       "   0.11091198725607328,\n",
       "   0.09129129129129128,\n",
       "   0.09312473392933163,\n",
       "   0.13333333333333333,\n",
       "   0.10978835978835978,\n",
       "   0.13247863247863248,\n",
       "   0.09769230769230769,\n",
       "   0.12836488537423116,\n",
       "   0.10816326530612246,\n",
       "   0.12915992084907357,\n",
       "   0.11507936507936507,\n",
       "   0.11990950226244344,\n",
       "   0.12533097969991172,\n",
       "   0.07458333333333333,\n",
       "   0.09084500837078156,\n",
       "   0.1252540650406504,\n",
       "   0.10528175234057587,\n",
       "   0.1263520157325467,\n",
       "   0.13709150326797384,\n",
       "   0.13587223587223587,\n",
       "   0.10341880341880343,\n",
       "   0.0977591036414566,\n",
       "   0.1202020202020202,\n",
       "   0.09784366576819407,\n",
       "   0.08778758000345961,\n",
       "   0.1353724157462475,\n",
       "   0.133442265795207,\n",
       "   0.07700163398692811,\n",
       "   0.10671191553544494,\n",
       "   0.10307477288609364,\n",
       "   0.11699832174538478,\n",
       "   0.10058651026392962,\n",
       "   0.13809523809523808,\n",
       "   0.10269687162891046,\n",
       "   0.1066137566137566,\n",
       "   0.12770789657582113,\n",
       "   0.10393727060393727,\n",
       "   0.095784463708992,\n",
       "   0.09829580036518563,\n",
       "   0.12775651540003644,\n",
       "   0.10922619047619048,\n",
       "   0.13234834271419638,\n",
       "   0.1129619989588756,\n",
       "   0.13717948717948716,\n",
       "   0.14309901124204252,\n",
       "   0.12056737588652482,\n",
       "   0.13004271476032273,\n",
       "   0.13305459911460896,\n",
       "   0.15566037735849056,\n",
       "   0.14372316241475122,\n",
       "   0.12807017543859647,\n",
       "   0.14081790123456792,\n",
       "   0.09991909385113269,\n",
       "   0.1353724157462475,\n",
       "   0.1061624649859944,\n",
       "   0.15010482180293502,\n",
       "   0.1561933036450602,\n",
       "   0.1316644707949056,\n",
       "   0.1584415584415584,\n",
       "   0.16666666666666666,\n",
       "   0.10331033103310332,\n",
       "   0.15130584934167926,\n",
       "   0.09183673469387754,\n",
       "   0.11244413079275466,\n",
       "   0.1085164835164835,\n",
       "   0.12850729517396184,\n",
       "   0.1707419017763845,\n",
       "   0.14025207830517564,\n",
       "   0.12174688057040998,\n",
       "   0.10476405509144276,\n",
       "   0.14545597114404454,\n",
       "   0.16742979242979242,\n",
       "   0.10485810485810486,\n",
       "   0.15975975975975976,\n",
       "   0.1365079365079365,\n",
       "   0.06936813186813187,\n",
       "   0.1367448065561273,\n",
       "   0.11972096530920061,\n",
       "   0.121716611172771,\n",
       "   0.15011501150115011,\n",
       "   0.10933445543165721,\n",
       "   0.14174174174174173,\n",
       "   0.14432551817872918,\n",
       "   0.10409652076318743,\n",
       "   0.12875816993464054,\n",
       "   0.14405113077679452,\n",
       "   0.10397727272727271,\n",
       "   0.161038961038961,\n",
       "   0.1292929292929293,\n",
       "   0.13580246913580246,\n",
       "   0.12998442367601246,\n",
       "   0.1272066458982347,\n",
       "   0.11669613538772416,\n",
       "   0.11833216864663405,\n",
       "   0.1636363636363636,\n",
       "   0.15840840840840842,\n",
       "   0.08217592592592592,\n",
       "   0.14564393939393938,\n",
       "   0.14343434343434344,\n",
       "   0.12097902097902098,\n",
       "   0.06600790513833991,\n",
       "   0.11699832174538478,\n",
       "   0.12202380952380953,\n",
       "   0.12654320987654322,\n",
       "   0.09916543937162493,\n",
       "   0.13271604938271606,\n",
       "   0.13257667230137649,\n",
       "   0.10897435897435898,\n",
       "   0.09634244836737049,\n",
       "   0.10350678733031675,\n",
       "   0.13778467908902692,\n",
       "   0.11900207237366493,\n",
       "   0.13826232247284878,\n",
       "   0.09088685015290521,\n",
       "   0.08888888888888889,\n",
       "   0.14061660758405845,\n",
       "   0.09057444541315508,\n",
       "   0.12481481481481482,\n",
       "   0.14259420711033613,\n",
       "   0.1349206349206349,\n",
       "   0.10962783171521036,\n",
       "   0.1112781954887218,\n",
       "   0.12607260726072608,\n",
       "   0.0964726631393298,\n",
       "   0.12345679012345678,\n",
       "   0.1186217576461479,\n",
       "   0.135,\n",
       "   0.13743517208863743,\n",
       "   0.1374064235691862,\n",
       "   0.09396671289875173,\n",
       "   0.11587301587301586,\n",
       "   0.14153283718501108,\n",
       "   0.15918803418803418,\n",
       "   0.13118279569892474,\n",
       "   0.07400215749730311,\n",
       "   0.12872628726287264,\n",
       "   0.09107059373539035,\n",
       "   0.12461513531032246,\n",
       "   0.10986624978287303,\n",
       "   0.13060428849902533,\n",
       "   0.09943613099110822,\n",
       "   0.12750333778371162,\n",
       "   0.1258296460176991,\n",
       "   0.11334052499101044,\n",
       "   0.0989010989010989,\n",
       "   0.09527709527709527,\n",
       "   0.12542087542087543,\n",
       "   0.12398867313915858,\n",
       "   0.15891670924885026,\n",
       "   0.15531135531135531,\n",
       "   0.1340271877655055,\n",
       "   0.12632524707996406,\n",
       "   0.12080262080262079,\n",
       "   0.13849206349206347,\n",
       "   0.10885642135642136,\n",
       "   0.08561953572505275,\n",
       "   0.09529833438751845,\n",
       "   0.13240365871944818,\n",
       "   0.11929824561403508,\n",
       "   0.16302857745439206,\n",
       "   0.15129449838187703,\n",
       "   0.13333333333333333,\n",
       "   0.13215610385421706,\n",
       "   0.1867924528301887,\n",
       "   0.1367888748419722,\n",
       "   0.09099842767295596,\n",
       "   0.11584150522203619,\n",
       "   0.11923076923076921,\n",
       "   0.11794871794871796,\n",
       "   0.09794871794871796,\n",
       "   0.12103960396039604,\n",
       "   0.12996031746031747,\n",
       "   0.1360032362459547,\n",
       "   0.10943053144888008,\n",
       "   0.11837398373983739,\n",
       "   0.1274412446209864,\n",
       "   0.12390572390572391,\n",
       "   0.11376344086021506,\n",
       "   0.14564393939393938,\n",
       "   0.10779220779220779,\n",
       "   0.1263034879539734,\n",
       "   0.11868686868686869,\n",
       "   0.13522727272727272,\n",
       "   0.12837606837606838,\n",
       "   0.14973262032085563,\n",
       "   0.12469660194174757,\n",
       "   0.15320910973084886,\n",
       "   0.11754611754611755,\n",
       "   0.13593862383121555,\n",
       "   0.0830299352750809,\n",
       "   0.1324575807334428,\n",
       "   0.12357723577235773,\n",
       "   0.13678034470113679,\n",
       "   0.11713286713286714,\n",
       "   0.19145114942528738,\n",
       "   0.15643378028699131,\n",
       "   0.09990253411306042,\n",
       "   0.10591133004926108,\n",
       "   0.10577243910577244,\n",
       "   0.09067460317460317,\n",
       "   0.12184648416532473,\n",
       "   0.12962962962962962,\n",
       "   0.08454564621547545,\n",
       "   0.10268172194777699,\n",
       "   0.14635854341736695,\n",
       "   0.07455281368324847,\n",
       "   0.12602662135372414,\n",
       "   0.10870219813309244,\n",
       "   0.13773148148148148,\n",
       "   0.10195503421309872,\n",
       "   0.1302346985734008,\n",
       "   0.11918328584995252,\n",
       "   0.10493827160493825,\n",
       "   0.11748483177054607,\n",
       "   0.09047619047619049,\n",
       "   0.11904087605022184,\n",
       "   0.1475754104620084,\n",
       "   0.12159329140461217,\n",
       "   0.11419753086419752,\n",
       "   0.14270152505446623,\n",
       "   0.11692084241103849,\n",
       "   0.08835542218888055,\n",
       "   0.14126984126984124,\n",
       "   0.13378684807256236,\n",
       "   0.13971166448230668,\n",
       "   0.1421383647798742,\n",
       "   0.10339506172839506,\n",
       "   0.10114942528735632,\n",
       "   0.13505297898210875,\n",
       "   0.10552168021680218,\n",
       "   0.08926273211987497,\n",
       "   0.16592920353982302,\n",
       "   0.10331033103310332,\n",
       "   0.1263034879539734,\n",
       "   0.10011312217194569,\n",
       "   0.15536231884057972,\n",
       "   0.12172183034906854,\n",
       "   0.10775151181968114,\n",
       "   0.14625647152528873,\n",
       "   0.11544878211544878,\n",
       "   0.14285714285714288,\n",
       "   0.13381410256410256,\n",
       "   0.13983739837398376,\n",
       "   0.11438613307772187,\n",
       "   0.13675749034345966,\n",
       "   0.10652337858220211,\n",
       "   0.12992831541218638,\n",
       "   0.05555555555555555,\n",
       "   0.10598648963428836,\n",
       "   0.11379928315412186,\n",
       "   0.15405674464907915,\n",
       "   0.12378747795414462,\n",
       "   0.164050288820931,\n",
       "   0.12931034482758622,\n",
       "   0.1186080586080586,\n",
       "   0.1471413160733549,\n",
       "   0.1185163720660678,\n",
       "   0.1716316199376947,\n",
       "   0.10263452344624148,\n",
       "   0.15423037716615698,\n",
       "   0.1447811447811448,\n",
       "   0.12009178990311066,\n",
       "   0.1341780376868096,\n",
       "   0.10185185185185186,\n",
       "   0.08679245283018867,\n",
       "   0.13959120410733314,\n",
       "   0.18073336494389128,\n",
       "   0.11195844385499558,\n",
       "   0.13853476117627062,\n",
       "   0.14650907091808685,\n",
       "   0.13285818713450293,\n",
       "   0.12695652173913044,\n",
       "   0.10683760683760683,\n",
       "   0.1562834020461139,\n",
       "   0.12877949852507375,\n",
       "   0.09267399267399268,\n",
       "   0.09757575757575758,\n",
       "   0.12715793627305907,\n",
       "   0.1176470588235294,\n",
       "   0.15061728395061727,\n",
       "   0.09887583035258048,\n",
       "   0.12850729517396184,\n",
       "   0.10096786682152536,\n",
       "   0.1286549707602339,\n",
       "   0.13247863247863248,\n",
       "   0.12986474151522695,\n",
       "   0.14662803798487042,\n",
       "   0.11668811668811668,\n",
       "   0.13452380952380952,\n",
       "   0.07695675971538041,\n",
       "   0.08838383838383838,\n",
       "   0.1096096096096096,\n",
       "   0.13550378335324573,\n",
       "   0.14269841269841269,\n",
       "   0.14103773584905663,\n",
       "   0.13159371492704827,\n",
       "   0.11298076923076923,\n",
       "   0.09090043784504093,\n",
       "   0.09996806132226126,\n",
       "   0.1387585991244528,\n",
       "   0.06784660766961652,\n",
       "   0.09919294294294294,\n",
       "   0.10860159224645205,\n",
       "   0.101984126984127,\n",
       "   0.11248331108144193,\n",
       "   0.14472309299895506,\n",
       "   0.10210622710622712,\n",
       "   0.1106060606060606,\n",
       "   0.14326948307530832,\n",
       "   0.12503304255881575,\n",
       "   0.1255050505050505,\n",
       "   0.11755351681957187,\n",
       "   0.12040263259775456,\n",
       "   0.1056065239551478,\n",
       "   0.1345988845988846,\n",
       "   0.09595959595959595,\n",
       "   0.1349206349206349,\n",
       "   0.10555555555555556,\n",
       "   0.12009178990311066,\n",
       "   0.09047619047619049,\n",
       "   0.156067168148317,\n",
       "   0.12418921056794811,\n",
       "   0.1148982957669049,\n",
       "   0.12571428571428572,\n",
       "   0.10992434356920337,\n",
       "   0.12649071358748779,\n",
       "   0.1408961408961409,\n",
       "   0.10240240240240239,\n",
       "   0.13716108452950557,\n",
       "   0.10696710696710697,\n",
       "   0.13318570072761785,\n",
       "   0.16423948220064724,\n",
       "   0.11468604047742605,\n",
       "   0.12564671101256467,\n",
       "   0.10992907801418439,\n",
       "   0.16753246753246753,\n",
       "   0.1382173382173382,\n",
       "   0.16469201296787506,\n",
       "   0.10172839506172839,\n",
       "   0.13438266903613438,\n",
       "   0.12927634226837317,\n",
       "   0.1425848925848926,\n",
       "   0.1158459595959596,\n",
       "   0.1308712121212121,\n",
       "   0.061237373737373736,\n",
       "   0.135632183908046,\n",
       "   0.09939898289412852,\n",
       "   0.11615487316421895,\n",
       "   0.09572649572649572,\n",
       "   0.16049382716049385,\n",
       "   0.09958391123439668,\n",
       "   0.12883851593529014,\n",
       "   0.14004007170726565,\n",
       "   0.10000000000000002,\n",
       "   0.12222222222222223,\n",
       "   0.05882352941176471,\n",
       "   0.1298602009611184,\n",
       "   0.1242874287428743,\n",
       "   0.15396825396825398,\n",
       "   0.11863334387851947,\n",
       "   0.1261904761904762,\n",
       "   0.16335282651072125,\n",
       "   0.12444659100056348,\n",
       "   0.1054574824045229,\n",
       "   0.09725906277630415,\n",
       "   0.11844660194174757,\n",
       "   0.10126317987263807,\n",
       "   0.12609295904279796,\n",
       "   0.15401709401709401,\n",
       "   0.11334714412548091,\n",
       "   0.07929997265518184,\n",
       "   0.11295347201286303,\n",
       "   0.09775122396481621,\n",
       "   0.16228338430173292,\n",
       "   0.11483516483516483,\n",
       "   0.15714016223354085,\n",
       "   0.12254901960784315,\n",
       "   0.1317094017094017,\n",
       "   0.15065538410486146,\n",
       "   0.12160493827160494,\n",
       "   0.13827614379084965,\n",
       "   0.1198220064724919,\n",
       "   0.1457821457821458,\n",
       "   0.12648809523809523,\n",
       "   0.11399711399711399,\n",
       "   0.08995037220843671,\n",
       "   0.10883838383838385,\n",
       "   0.09335624284077892,\n",
       "   0.138014763014763,\n",
       "   0.11324786324786325,\n",
       "   0.10485810485810486,\n",
       "   0.10303030303030303,\n",
       "   0.11964601769911505,\n",
       "   0.133698518969365,\n",
       "   0.08376214979988565,\n",
       "   0.13074039362699155,\n",
       "   0.17686823992133727,\n",
       "   0.15012493753123438,\n",
       "   0.1527321714237602,\n",
       "   0.118213058419244,\n",
       "   0.13167701863354037,\n",
       "   0.12345679012345678,\n",
       "   0.09000429000429,\n",
       "   0.1360544217687075,\n",
       "   0.10980392156862746,\n",
       "   0.11043153244988108,\n",
       "   0.14547342584725761,\n",
       "   0.0897185845639454,\n",
       "   0.12031220255092329,\n",
       "   0.12602040816326532,\n",
       "   0.1443991160972293,\n",
       "   0.12955974842767295,\n",
       "   0.10792610792610795,\n",
       "   0.10649397373628351,\n",
       "   0.1573890722405574,\n",
       "   0.14303411473222793,\n",
       "   0.11969496021220159,\n",
       "   0.09591853494292518,\n",
       "   0.12105208656932794,\n",
       "   0.11136363636363637,\n",
       "   0.1171280393345457,\n",
       "   0.1389937106918239,\n",
       "   0.11600697037590242,\n",
       "   0.16547619047619047,\n",
       "   0.1526430755788554,\n",
       "   0.12442002442002442,\n",
       "   0.15343125051862916,\n",
       "   0.15557536145771442,\n",
       "   0.1457251644167532,\n",
       "   0.1304725778409989,\n",
       "   0.13797169811320756,\n",
       "   0.10901960784313725,\n",
       "   0.11629789530491097,\n",
       "   0.1308712121212121,\n",
       "   0.0956386292834891,\n",
       "   0.12351190476190477,\n",
       "   0.1883848133848134,\n",
       "   0.13550284978856408,\n",
       "   0.13522727272727272,\n",
       "   0.10347530702900444,\n",
       "   0.14377104377104377,\n",
       "   0.13370007175316911,\n",
       "   0.13060245987075256,\n",
       "   0.12395514780835881,\n",
       "   0.13452380952380952,\n",
       "   0.11616161616161617,\n",
       "   0.09597143175984824,\n",
       "   0.11888888888888889,\n",
       "   0.1447811447811448,\n",
       "   0.10942760942760943,\n",
       "   0.153417645287564,\n",
       "   0.12535885167464114,\n",
       "   0.09135802469135802,\n",
       "   0.13874386964087962,\n",
       "   0.13381410256410256,\n",
       "   0.0803349875930521,\n",
       "   0.11251599307593889,\n",
       "   0.13467920353982302,\n",
       "   0.11207982175724111,\n",
       "   0.11125541125541126,\n",
       "   0.09108653220559532,\n",
       "   0.13049376017362993,\n",
       "   0.1316644707949056,\n",
       "   0.07322264793529161,\n",
       "   0.11672240802675586,\n",
       "   0.1175968992248062,\n",
       "   0.15634920634920635,\n",
       "   0.11717171717171716,\n",
       "   0.16643975493533014,\n",
       "   0.1372100122100122,\n",
       "   0.10637140637140637,\n",
       "   0.11168801199953848,\n",
       "   0.1382001836547291,\n",
       "   0.13309566250742721,\n",
       "   0.1359840232389252,\n",
       "   0.12800819252432155,\n",
       "   0.09728813559322035,\n",
       "   0.11998456790123457,\n",
       "   0.125,\n",
       "   0.1512435400516796,\n",
       "   0.1384245602534688,\n",
       "   0.12186777623670826,\n",
       "   0.12622392507449978,\n",
       "   0.14314242351625528,\n",
       "   0.11461412151067324,\n",
       "   0.14402683239617245,\n",
       "   0.13445566778900112,\n",
       "   0.1365040650406504,\n",
       "   0.13462783171521034,\n",
       "   0.12119487908961595,\n",
       "   0.14768302180685358,\n",
       "   0.1247495518295898,\n",
       "   0.12269412269412268,\n",
       "   0.13824455899198165,\n",
       "   0.16666666666666666,\n",
       "   0.13183940242763773,\n",
       "   0.1364575584759071,\n",
       "   0.12602607436021246,\n",
       "   0.09539817720005976,\n",
       "   0.10040053404539385,\n",
       "   0.12879394748553627,\n",
       "   0.12607260726072608,\n",
       "   0.11447811447811447,\n",
       "   0.13151927437641722,\n",
       "   0.13441380422512497,\n",
       "   0.12792056074766356,\n",
       "   0.10437091503267974,\n",
       "   0.1434692671394799,\n",
       "   0.15396825396825398,\n",
       "   0.1462147451756556,\n",
       "   0.10674974039460021,\n",
       "   0.13484452194129615,\n",
       "   0.09978582554517135,\n",
       "   0.10854123354123353,\n",
       "   0.12475109518120271,\n",
       "   0.15306122448979592,\n",
       "   0.15800865800865802,\n",
       "   0.08958811318693323,\n",
       "   0.1302642206068997,\n",
       "   0.10961337513061652,\n",
       "   0.09290382819794585,\n",
       "   0.11031844499586435,\n",
       "   0.07322264793529161,\n",
       "   0.138662486938349,\n",
       "   0.17311971517578995,\n",
       "   0.15252525252525254,\n",
       "   0.1111111111111111,\n",
       "   0.1355964167845356,\n",
       "   0.16754850088183423,\n",
       "   0.10661066106610662,\n",
       "   0.10648148148148147,\n",
       "   0.10011312217194569,\n",
       "   0.1313131313131313,\n",
       "   0.0803926546500804,\n",
       "   0.1805208043740154,\n",
       "   0.1056547619047619,\n",
       "   0.12564102564102564,\n",
       "   0.07643965627805567,\n",
       "   0.12473794549266248,\n",
       "   0.09635349635349637,\n",
       "   0.13209310227658852,\n",
       "   0.11972332417861865,\n",
       "   0.12281746031746033,\n",
       "   0.13790386130811663,\n",
       "   0.10015246807699636,\n",
       "   0.10833333333333334,\n",
       "   0.12513227513227512,\n",
       "   0.1080464842600765,\n",
       "   0.11381932021466905,\n",
       "   0.12791375291375293,\n",
       "   0.07497287165828155,\n",
       "   0.15570117955439056,\n",
       "   0.11895424836601308,\n",
       "   0.12046783625730995,\n",
       "   0.11352357320099256,\n",
       "   0.13129251700680272,\n",
       "   0.10404674690388976,\n",
       "   0.10933706816059757,\n",
       "   0.11587301587301586,\n",
       "   0.09150326797385622,\n",
       "   0.1631488430268918,\n",
       "   0.14179818268770925,\n",
       "   0.11333333333333334,\n",
       "   0.16149659863945579,\n",
       "   0.11675706537253783,\n",
       "   0.12828282828282828,\n",
       "   0.12483358934971837,\n",
       "   0.1461675579322638,\n",
       "   0.08443978569764732,\n",
       "   0.10782556750298684,\n",
       "   0.09334014648271165,\n",
       "   0.1261904761904762,\n",
       "   0.12713215367447792,\n",
       "   0.09876543209876543,\n",
       "   0.11694713581506035,\n",
       "   0.10114942528735632,\n",
       "   0.10231542158147662,\n",
       "   0.1053735255570118,\n",
       "   0.1162907268170426,\n",
       "   0.12287157287157287,\n",
       "   0.16592920353982302,\n",
       "   0.12101313320825517,\n",
       "   0.11167945439045183,\n",
       "   0.12222222222222223,\n",
       "   0.11587301587301586,\n",
       "   0.13722143195827408,\n",
       "   0.12486126526082131,\n",
       "   0.1762748255501879,\n",
       "   0.1463111620795107,\n",
       "   0.13030303030303028,\n",
       "   0.13445566778900112,\n",
       "   0.16967175219602404,\n",
       "   0.11277072442120988,\n",
       "   0.11381172839506172,\n",
       "   0.1610032362459547,\n",
       "   0.09622641509433964,\n",
       "   0.13348001466813347,\n",
       "   0.07004830917874395,\n",
       "   0.15271512113617378,\n",
       "   0.13668379973348563,\n",
       "   0.10671191553544494,\n",
       "   0.13748597081930414,\n",
       "   0.16408643767134332,\n",
       "   0.1285204991087344,\n",
       "   0.12289182511547965,\n",
       "   0.06186224489795918,\n",
       "   0.11427472104655638,\n",
       "   0.10499222999223,\n",
       "   0.11238599588114151,\n",
       "   0.12195497995683009,\n",
       "   0.11412025208652699,\n",
       "   0.1237458193979933,\n",
       "   0.13211951447245565,\n",
       "   0.08875683833239012,\n",
       "   0.11666666666666665,\n",
       "   0.11019719498049141,\n",
       "   0.12318059299191375,\n",
       "   0.1543729372937294,\n",
       "   0.11717171717171719,\n",
       "   0.09758131776480401,\n",
       "   0.14349816849816852,\n",
       "   0.12222222222222223,\n",
       "   0.11511668408220133,\n",
       "   0.1567610062893082,\n",
       "   0.09787980232743504,\n",
       "   0.10455974842767296,\n",
       "   0.16187134502923975,\n",
       "   0.06867518766484074,\n",
       "   0.13741496598639455,\n",
       "   0.11972096530920061,\n",
       "   0.13314176245210727,\n",
       "   0.1263851452530698,\n",
       "   0.1399446628804427,\n",
       "   0.11675706537253783,\n",
       "   0.14472309299895506,\n",
       "   0.15809968847352027,\n",
       "   0.10656270305393112,\n",
       "   0.12995337995337997,\n",
       "   0.15008246289169871,\n",
       "   0.13723837617642928,\n",
       "   0.12681159420289856,\n",
       "   0.0989010989010989,\n",
       "   0.08840155945419104,\n",
       "   0.09205851619644723,\n",
       "   0.15052667346245327,\n",
       "   0.15490196078431373,\n",
       "   0.15488215488215487,\n",
       "   0.153021978021978,\n",
       "   0.1461260110685398,\n",
       "   0.09986194201564658,\n",
       "   0.12303030303030303,\n",
       "   0.12203065134099617,\n",
       "   0.12242599742599743,\n",
       "   0.14292967758314293,\n",
       "   0.10691391941391941,\n",
       "   0.13512145748987855,\n",
       "   0.12682406951959338,\n",
       "   0.1138047138047138,\n",
       "   0.10386989553656219,\n",
       "   0.1579338535860275,\n",
       "   0.10238225131842153,\n",
       "   0.12014652014652015,\n",
       "   0.15666666666666665,\n",
       "   0.09558733651142892,\n",
       "   0.11492552037091841,\n",
       "   0.13086219336219337,\n",
       "   0.14992826398852224,\n",
       "   0.10272385973320552,\n",
       "   0.10932857991681522,\n",
       "   0.09698275862068965,\n",
       "   0.10868533459895587,\n",
       "   0.09565217391304348,\n",
       "   0.11240079365079365,\n",
       "   0.12875816993464054,\n",
       "   0.13906050799254685,\n",
       "   0.15313359386307715,\n",
       "   0.15259259259259259,\n",
       "   0.11270772238514175,\n",
       "   0.11497641509433963,\n",
       "   0.10909794812233836,\n",
       "   0.15372009177318913,\n",
       "   0.14169278996865203,\n",
       "   0.16320116054158607,\n",
       "   0.11923500042311923,\n",
       "   0.1004040404040404,\n",
       "   0.0886737872719181,\n",
       "   0.11025347817800647,\n",
       "   0.15147783251231528,\n",
       "   0.1069973640067098,\n",
       "   0.09782509782509784,\n",
       "   0.10736747529200359,\n",
       "   0.12307098765432099,\n",
       "   0.13782941820324998,\n",
       "   0.12831615120274914,\n",
       "   0.13853020883150008,\n",
       "   0.1572192513368984],\n",
       "  'mean': 0.12324100545744274,\n",
       "  'std': 0.021965014697464872,\n",
       "  'ci_lower': 0.07928771143404038,\n",
       "  'ci_upper': 0.16594196732471067}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_bertweet_bootstrap_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd06b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
