{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ebee31",
   "metadata": {},
   "source": [
    "# General Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Libraries imported and setup complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import random\n",
    "from collections import Counter\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./bertweet_seed_analysis_results\", exist_ok=True)\n",
    "os.makedirs(\"./bertweet_trained_models_seeds\", exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fc10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee15564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet model architectures defined\n"
     ]
    }
   ],
   "source": [
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.bertweet.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERTweet outputs\n",
    "        outputs = self.bertweet(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.bertweet.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.bertweet(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        shared_pooled = shared_attended[:, 0, :]\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "        outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "        \n",
    "        # Emotion branch\n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "        emotion_logits = self.emotion_classifier(emotion_features)\n",
    "        outputs[\"emotion_logits\"] = emotion_logits\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"BERTweet model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84190c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "class BERTweetDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BERTweetMultiTaskDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], sentiment_labels: List[int], \n",
    "                 emotion_labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"BERTweet dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feded95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modified data loading functions defined!\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def load_external_datasets() -> Tuple[Dict, Dict]:\n",
    "    print(\"Loading external datasets...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        sentiment_data = {\n",
    "            'train': sst2_dataset['train'],\n",
    "            'validation': sst2_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sentiment_data['train'])} train, {len(sentiment_data['validation'])} validation samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        emotion_data = {\n",
    "            'train': emotions_dataset['train'],\n",
    "            'validation': emotions_dataset['validation'],\n",
    "            'test': emotions_dataset['test']  # GoEmotions has a test split\n",
    "        }\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotion_data['train'])} train, {len(emotion_data['validation'])} validation, {len(emotion_data['test'])} test samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def prepare_sst2_evaluation_data(sentiment_data: Dict, max_samples: int = 1000) -> Dict:\n",
    "    print(\"Preparing SST-2 evaluation data...\")\n",
    "    \n",
    "    eval_texts = sentiment_data['validation']['sentence'][:max_samples]\n",
    "    eval_labels_raw = sentiment_data['validation']['label'][:max_samples]\n",
    "    \n",
    "    # Convert SST-2 binary to 3-class sentiment (same as training)\n",
    "    eval_labels = []\n",
    "    for label in eval_labels_raw:\n",
    "        if label == 0:  # Negative\n",
    "            eval_labels.append(0)\n",
    "        elif label == 1:  # Positive\n",
    "            if np.random.random() < 0.15:  \n",
    "                eval_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                eval_labels.append(2)  # Positive\n",
    "    \n",
    "    # Create encoder that matches training\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    sst2_eval_data = {\n",
    "        'texts': eval_texts,\n",
    "        'sentiment_labels': eval_labels,\n",
    "        'sentiment_encoder': sentiment_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ SST-2 evaluation data prepared: {len(sst2_eval_data['texts'])} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    \n",
    "    return sst2_eval_data\n",
    "\n",
    "def prepare_goemotions_evaluation_data(emotion_data: Dict, max_samples: int = 1000) -> Dict:\n",
    "    print(\"Preparing GoEmotions evaluation data...\")\n",
    "    \n",
    "    eval_split = emotion_data.get('test', emotion_data.get('validation'))\n",
    "    eval_texts_all = eval_split['text']\n",
    "    eval_labels_all = eval_split['labels']\n",
    "    \n",
    "    eval_texts = []\n",
    "    eval_labels = []\n",
    "    count = 0\n",
    "    \n",
    "    for i, label in enumerate(eval_labels_all):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):\n",
    "                eval_texts.append(eval_texts_all[i])\n",
    "                eval_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                eval_texts.append(eval_texts_all[i])\n",
    "                eval_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoder that matches training\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    goemotions_eval_data = {\n",
    "        'texts': eval_texts,\n",
    "        'emotion_labels': eval_labels,\n",
    "        'emotion_encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ GoEmotions evaluation data prepared: {len(goemotions_eval_data['texts'])} samples\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return goemotions_eval_data\n",
    "\n",
    "def prepare_multitask_evaluation_data(sst2_eval_data: Dict, goemotions_eval_data: Dict) -> Dict:\n",
    "    print(\"Preparing multitask evaluation data...\")\n",
    "    \n",
    "    # Take minimum length to ensure both tasks have same number of samples\n",
    "    min_length = min(len(sst2_eval_data['texts']), len(goemotions_eval_data['texts']))\n",
    "    \n",
    "    multitask_eval_data = {\n",
    "        'texts': sst2_eval_data['texts'][:min_length],\n",
    "        'sentiment_labels': sst2_eval_data['sentiment_labels'][:min_length],\n",
    "        'emotion_labels': goemotions_eval_data['emotion_labels'][:min_length],\n",
    "        'sentiment_encoder': sst2_eval_data['sentiment_encoder'],\n",
    "        'emotion_encoder': goemotions_eval_data['emotion_encoder']\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Multitask evaluation data prepared: {len(multitask_eval_data['texts'])} samples\")\n",
    "    \n",
    "    return multitask_eval_data\n",
    "\n",
    "def prepare_bertweet_training_data(sentiment_data: Dict, emotion_data: Dict, max_samples: int = 3000) -> Dict:\n",
    "    print(\"Preparing BERTweet training data...\")\n",
    "    \n",
    "    # Process sentiment data (SST-2)\n",
    "    sentiment_texts = sentiment_data['train']['sentence'][:max_samples]\n",
    "    sentiment_labels = sentiment_data['train']['label'][:max_samples]\n",
    "    \n",
    "    # Process emotion data (filter to first 6 classes)\n",
    "    emotion_texts = []\n",
    "    emotion_labels = []\n",
    "    count = 0\n",
    "    \n",
    "    for i, label in enumerate(emotion_data['train']['labels']):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):  # Only use first 6 emotions\n",
    "                emotion_texts.append(emotion_data['train']['text'][i])\n",
    "                emotion_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                emotion_texts.append(emotion_data['train']['text'][i])\n",
    "                emotion_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoders\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # For SST-2: 0 = Negative, 1 = Positive\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Positive'])\n",
    "    \n",
    "    # For GoEmotions: First 6 emotions\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    training_data = {\n",
    "        'sentiment_data': {\n",
    "            'texts': sentiment_texts,\n",
    "            'labels': sentiment_labels,\n",
    "            'encoder': sentiment_encoder\n",
    "        },\n",
    "        'emotion_data': {\n",
    "            'texts': emotion_texts,\n",
    "            'labels': emotion_labels,\n",
    "            'encoder': emotion_encoder\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Training data prepared:\")\n",
    "    print(f\"   Sentiment: {len(sentiment_texts)} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion: {len(emotion_texts)} samples\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "print(\"✅ Modified data loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afd1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet training functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_bertweet_single_task(\n",
    "    task_type: str,  \n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 5000\n",
    ") -> Tuple[any, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet {task_type} model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Get appropriate data\n",
    "    if task_type == 'sentiment':\n",
    "        texts = training_data['sentiment_data']['texts'][:max_samples]\n",
    "        labels = training_data['sentiment_data']['labels'][:max_samples]\n",
    "        encoder = training_data['sentiment_data']['encoder']\n",
    "        num_classes = 3\n",
    "    else:  # emotion\n",
    "        texts = training_data['emotion_data']['texts'][:max_samples]\n",
    "        labels = training_data['emotion_data']['labels'][:max_samples]\n",
    "        encoder = training_data['emotion_data']['encoder']\n",
    "        num_classes = 6\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetSingleTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        num_classes=num_classes,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetDataset(texts, labels, tokenizer, max_length=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs['logits'], labels_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_{task_type}_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"task_type\": task_type,\n",
    "        \"model_type\": \"BERTweetSingleTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoder\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(encoder, os.path.join(output_dir, f'{task_type}_encoder.pkl'))\n",
    "    \n",
    "    print(f\"✅ BERTweet {task_type} model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "def train_bertweet_multitask(\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 2000\n",
    ") -> Tuple[any, LabelEncoder, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet multitask model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    min_length = min(len(training_data['sentiment_data']['texts']), \n",
    "                     len(training_data['emotion_data']['texts']))\n",
    "    min_length = min(min_length, max_samples)\n",
    "    \n",
    "    combined_texts = training_data['sentiment_data']['texts'][:min_length]\n",
    "    combined_sentiment_labels = training_data['sentiment_data']['labels'][:min_length]\n",
    "    combined_emotion_labels = training_data['emotion_data']['labels'][:min_length]\n",
    "    \n",
    "    sentiment_encoder = training_data['sentiment_data']['encoder']\n",
    "    emotion_encoder = training_data['emotion_data']['encoder']\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetMultiTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        sentiment_num_classes=3,\n",
    "        emotion_num_classes=6,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetMultiTaskDataset(\n",
    "        combined_texts, combined_sentiment_labels, combined_emotion_labels, \n",
    "        tokenizer, max_length=128\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    emotion_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    alpha = best_params['alpha']\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = sentiment_criterion(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = emotion_criterion(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss_batch = alpha * sentiment_loss + (1 - alpha) * emotion_loss\n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"sentiment_num_classes\": 3,\n",
    "        \"emotion_num_classes\": 6,\n",
    "        \"model_type\": \"BERTweetMultiTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoders\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    print(f\"BERTweet multitask model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, sentiment_encoder, emotion_encoder\n",
    "\n",
    "print(\"BERTweet training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d9d59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bertweet_single_task(model, tokenizer, label_encoder, reddit_data: Dict, task_type: str) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_labels = reddit_data[f'{task_type}_labels']\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 16):  # Batch size 16\n",
    "            batch_texts = texts[i:i+16]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs['logits']\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                pred_id = preds[j].item()\n",
    "                confidence = probs[j][pred_id].item()\n",
    "                \n",
    "                # Handle out of range predictions\n",
    "                if pred_id >= len(label_encoder.classes_):\n",
    "                    pred_id = 0\n",
    "                \n",
    "                predictions.append(pred_id)\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'predictions': predictions,\n",
    "        'confidences': confidences,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "def evaluate_bertweet_multitask(model, tokenizer, sentiment_encoder, emotion_encoder, \n",
    "                               reddit_data: Dict, max_length: int = 128) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_sentiment_labels = reddit_data['sentiment_labels']\n",
    "    true_emotion_labels = reddit_data['emotion_labels']\n",
    "    \n",
    "    sentiment_predictions = []\n",
    "    emotion_predictions = []\n",
    "    sentiment_confidences = []\n",
    "    emotion_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 8):  # Smaller batch size for multitask\n",
    "            batch_texts = texts[i:i+8]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Process sentiment\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "            \n",
    "            # Process emotion\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                # Sentiment\n",
    "                sent_id = sentiment_preds[j].item()\n",
    "                sent_conf = sentiment_probs[j][sent_id].item()\n",
    "                if sent_id >= len(sentiment_encoder.classes_):\n",
    "                    sent_id = 0\n",
    "                sentiment_predictions.append(sent_id)\n",
    "                sentiment_confidences.append(sent_conf)\n",
    "                \n",
    "                # Emotion\n",
    "                emot_id = emotion_preds[j].item()\n",
    "                emot_conf = emotion_probs[j][emot_id].item()\n",
    "                if emot_id >= len(emotion_encoder.classes_):\n",
    "                    emot_id = 0\n",
    "                emotion_predictions.append(emot_id)\n",
    "                emotion_confidences.append(emot_conf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sentiment_accuracy = accuracy_score(true_sentiment_labels, sentiment_predictions)\n",
    "    sentiment_f1 = f1_score(true_sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    emotion_accuracy = accuracy_score(true_emotion_labels, emotion_predictions)\n",
    "    emotion_f1 = f1_score(true_emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'sentiment': {\n",
    "            'accuracy': sentiment_accuracy,\n",
    "            'macro_f1': sentiment_f1,\n",
    "            'predictions': sentiment_predictions,\n",
    "            'confidences': sentiment_confidences\n",
    "        },\n",
    "        'emotion': {\n",
    "            'accuracy': emotion_accuracy,\n",
    "            'macro_f1': emotion_f1,\n",
    "            'predictions': emotion_predictions,\n",
    "            'confidences': emotion_confidences\n",
    "        },\n",
    "        'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "        'combined_f1': (sentiment_f1 + emotion_f1) / 2\n",
    "    }\n",
    "\n",
    "print(\"BERTweet evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bertweet_seed_stability(all_results: Dict, seeds: List[int]) -> Dict:\n",
    "    \n",
    "    stability_stats = {}\n",
    "    \n",
    "    # Define model-task combinations\n",
    "    evaluations = [\n",
    "        ('bertweet_sentiment', 'sentiment'),\n",
    "        ('bertweet_emotion', 'emotion'),\n",
    "        ('bertweet_multitask', 'sentiment'),\n",
    "        ('bertweet_multitask', 'emotion')\n",
    "    ]\n",
    "    \n",
    "    for model_name, task in evaluations:\n",
    "        print(f\"\\n🔍 {model_name.upper()} - {task.upper()}\")\n",
    "        \n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            if model_name in all_results[seed]:\n",
    "                result = all_results[seed][model_name]\n",
    "                \n",
    "                if model_name.endswith('_multitask'):\n",
    "                    acc = result[task]['accuracy']\n",
    "                    f1 = result[task]['macro_f1']\n",
    "                else:\n",
    "                    acc = result['accuracy']\n",
    "                    f1 = result['macro_f1']\n",
    "                \n",
    "                accuracies.append(acc)\n",
    "                f1_scores.append(f1)\n",
    "        \n",
    "        if accuracies:\n",
    "            acc_mean = np.mean(accuracies)\n",
    "            acc_std = np.std(accuracies)\n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            \n",
    "            stability_stats[f\"{model_name}_{task}\"] = {\n",
    "                'accuracy_mean': acc_mean,\n",
    "                'accuracy_std': acc_std,\n",
    "                'f1_mean': f1_mean,\n",
    "                'f1_std': f1_std,\n",
    "                'accuracy_values': accuracies,\n",
    "                'f1_values': f1_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "            print(f\"   Macro F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    return stability_stats\n",
    "\n",
    "def save_bertweet_results(all_results: Dict, stability_analysis: Dict, seeds: List[int]):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_raw_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        serializable_results = {}\n",
    "        for seed, seed_results in all_results.items():\n",
    "            serializable_results[str(seed)] = {}\n",
    "            for model, results in seed_results.items():\n",
    "                if isinstance(results, dict):\n",
    "                    serializable_results[str(seed)][model] = {}\n",
    "                    for key, value in results.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            serializable_results[str(seed)][model][key] = {\n",
    "                                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                                   [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in v] if isinstance(v, list) else v\n",
    "                                for k, v in value.items()\n",
    "                            }\n",
    "                        else:\n",
    "                            serializable_results[str(seed)][model][key] = float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # Save stability analysis\n",
    "    stability_file = f\"./bertweet_seed_analysis_results/bertweet_stability_analysis_{timestamp}.json\"\n",
    "    with open(stability_file, 'w') as f:\n",
    "        serializable_stability = {}\n",
    "        for key, stats in stability_analysis.items():\n",
    "            serializable_stability[key] = {\n",
    "                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                   [float(x) for x in v] if isinstance(v, list) else v\n",
    "                for k, v in stats.items()\n",
    "            }\n",
    "        json.dump(serializable_stability, f, indent=2)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = f\"./bertweet_seed_analysis_results/bertweet_summary_report_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"BERTWEET RANDOM SEED ANALYSIS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Seeds tested: {seeds}\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"STABILITY ANALYSIS (Mean ± Std)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        for key, stats in stability_analysis.items():\n",
    "            model_task = key.replace('_', ' ').title()\n",
    "            f.write(f\"\\n{model_task}:\\n\")\n",
    "            f.write(f\"  Accuracy: {stats['accuracy_mean']:.4f} ± {stats['accuracy_std']:.4f}\\n\")\n",
    "            f.write(f\"  Macro F1: {stats['f1_mean']:.4f} ± {stats['f1_std']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nBest Performers (by mean F1 score):\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Find best performers\n",
    "        sentiment_best = max([k for k in stability_analysis.keys() if 'sentiment' in k], \n",
    "                           key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        emotion_best = max([k for k in stability_analysis.keys() if 'emotion' in k], \n",
    "                         key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        \n",
    "        f.write(f\"Sentiment: {sentiment_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[sentiment_best]['f1_mean']:.4f})\\n\")\n",
    "        f.write(f\"Emotion: {emotion_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[emotion_best]['f1_mean']:.4f})\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 BERTweet results saved:\")\n",
    "    print(f\"   Raw results: {results_file}\")\n",
    "    print(f\"   Stability analysis: {stability_file}\")\n",
    "    print(f\"   Summary report: {summary_file}\")\n",
    "\n",
    "print(\"BERTweet stability analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1097d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modified BERTweet random seed analysis function defined!\n"
     ]
    }
   ],
   "source": [
    "def run_bertweet_seed_analysis(\n",
    "    seeds: List[int] = [42, 123, 456, 789, 999],\n",
    "    max_training_samples: int = 3000,\n",
    "    max_eval_samples: int = 1000\n",
    "):\n",
    "    \n",
    "    print(\"🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Seeds to test: {seeds}\")\n",
    "    print(f\"Max training samples per dataset: {max_training_samples}\")\n",
    "    print(f\"Max evaluation samples per dataset: {max_eval_samples}\")\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(\"\\n🔄 Preparing BERTweet training data...\")\n",
    "    training_data = prepare_bertweet_training_data(sentiment_data, emotion_data, max_training_samples)\n",
    "    \n",
    "    # Prepare evaluation data\n",
    "    print(\"\\n📂 Preparing evaluation datasets...\")\n",
    "    sst2_eval_data = prepare_sst2_evaluation_data(sentiment_data, max_eval_samples)\n",
    "    goemotions_eval_data = prepare_goemotions_evaluation_data(emotion_data, max_eval_samples)\n",
    "    multitask_eval_data = prepare_multitask_evaluation_data(sst2_eval_data, goemotions_eval_data)\n",
    "    \n",
    "    # Define best parameters for each BERTweet model\n",
    "    best_params = {\n",
    "        'sentiment': {\n",
    "            'learning_rate': 3.65445235521325e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'emotion': {\n",
    "            'learning_rate': 2.503410215228042e-05, \n",
    "            'batch_size': 32,\n",
    "            'warmup_ratio': 0.1456069984217036,\n",
    "            'weight_decay': 0.08066583652537122,\n",
    "            'hidden_dropout_prob': 0.13993475643167194,\n",
    "            'classifier_dropout': 0.2028468876827223\n",
    "        },\n",
    "        'multitask': {\n",
    "            'learning_rate': 2.2207471217033647e-05,\n",
    "            'batch_size': 32,\n",
    "            'warmup_ratio': 0.1808397348116461,\n",
    "            'weight_decay': 0.037415239225603365,\n",
    "            'hidden_dropout_prob': 0.11953442280127678,\n",
    "            'classifier_dropout': 0.23684660530243137,\n",
    "            'alpha': 0.48803049874792026\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    # Store results for each seed\n",
    "    all_results = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n🌱 TRAINING AND EVALUATING BERTWEET WITH SEED {seed}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        seed_results = {}\n",
    "        \n",
    "        # 1. Train and evaluate BERTweet Sentiment on SST-2\n",
    "        print(f\"\\n1️⃣ BERTweet Sentiment on SST-2 (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'sentiment', best_params['sentiment'], seed, \n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_sentiment_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on SST-2 validation set\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, sst2_eval_data, 'sentiment')\n",
    "        seed_results['bertweet_sentiment'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 2. Train and evaluate BERTweet Emotion on GoEmotions\n",
    "        print(f\"\\n2️⃣ BERTweet Emotion on GoEmotions (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'emotion', best_params['emotion'], seed,\n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_emotion_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on GoEmotions test set\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, goemotions_eval_data, 'emotion')\n",
    "        seed_results['bertweet_emotion'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 3. Train and evaluate BERTweet Multitask on both datasets\n",
    "        print(f\"\\n3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed {seed})\")\n",
    "        model, sent_enc, emot_enc = train_bertweet_multitask(\n",
    "            best_params['multitask'], seed, training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate on combined test sets\n",
    "        results = evaluate_bertweet_multitask(\n",
    "            model, tokenizer, sent_enc, emot_enc, multitask_eval_data, 128\n",
    "        )\n",
    "        seed_results['bertweet_multitask'] = results\n",
    "        print(f\"   Sentiment - Accuracy: {results['sentiment']['accuracy']:.4f}, F1: {results['sentiment']['macro_f1']:.4f}\")\n",
    "        print(f\"   Emotion - Accuracy: {results['emotion']['accuracy']:.4f}, F1: {results['emotion']['macro_f1']:.4f}\")\n",
    "        print(f\"   Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        all_results[seed] = seed_results\n",
    "        \n",
    "        print(f\"\\n✅ Completed evaluation for seed {seed}\")\n",
    "    \n",
    "    # Analyze stability across seeds\n",
    "    print(f\"\\n📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    stability_analysis = analyze_bertweet_seed_stability(all_results, seeds)\n",
    "    \n",
    "    # Save results\n",
    "    save_bertweet_results(all_results, stability_analysis, seeds)\n",
    "    \n",
    "    return all_results, stability_analysis\n",
    "\n",
    "print(\"✅ Modified BERTweet random seed analysis function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\n",
      "======================================================================\n",
      "Seeds to test: [42, 123, 456, 789, 999]\n",
      "Max training samples per dataset: 3000\n",
      "Max evaluation samples per dataset: 1000\n",
      "\n",
      "📂 Loading external datasets...\n",
      "Loading external datasets...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 validation samples\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 validation, 5427 test samples\n",
      "\n",
      "🔄 Preparing BERTweet training data...\n",
      "Preparing BERTweet training data...\n",
      "✅ Training data prepared:\n",
      "   Sentiment: 3000 samples\n",
      "   Sentiment classes: ['Negative', 'Positive']\n",
      "   Emotion: 3000 samples\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
      "\n",
      "📂 Preparing evaluation datasets...\n",
      "Preparing SST-2 evaluation data...\n",
      "✅ SST-2 evaluation data prepared: 872 samples\n",
      "   Sentiment classes: ['Negative', 'Neutral', 'Positive']\n",
      "Preparing GoEmotions evaluation data...\n",
      "✅ GoEmotions evaluation data prepared: 1000 samples\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
      "Preparing multitask evaluation data...\n",
      "✅ Multitask evaluation data prepared: 872 samples\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 42\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 42)\n",
      "🚀 Training BERTweet sentiment model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.5872\n",
      "Epoch 2/3, Average Loss: 0.2380\n",
      "Epoch 3/3, Average Loss: 0.1185\n",
      "✅ BERTweet sentiment model trained and saved with seed 42\n",
      "   Accuracy: 0.5161, Macro F1: 0.3870\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 42)\n",
      "🚀 Training BERTweet emotion model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6296\n",
      "Epoch 2/3, Average Loss: 0.9459\n",
      "Epoch 3/3, Average Loss: 0.6962\n",
      "✅ BERTweet emotion model trained and saved with seed 42\n",
      "   Accuracy: 0.7200, Macro F1: 0.6846\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 42)\n",
      "🚀 Training BERTweet multitask model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5238\n",
      "Epoch 2/3, Average Loss: 1.1787\n",
      "Epoch 3/3, Average Loss: 1.0967\n",
      "BERTweet multitask model trained and saved with seed 42\n",
      "   Sentiment - Accuracy: 0.5149, F1: 0.3838\n",
      "   Emotion - Accuracy: 0.2959, F1: 0.1018\n",
      "   Combined - Accuracy: 0.4054, F1: 0.2428\n",
      "\n",
      "✅ Completed evaluation for seed 42\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 123\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 123)\n",
      "🚀 Training BERTweet sentiment model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6605\n",
      "Epoch 2/3, Average Loss: 0.3059\n",
      "Epoch 3/3, Average Loss: 0.1587\n",
      "✅ BERTweet sentiment model trained and saved with seed 123\n",
      "   Accuracy: 0.5034, Macro F1: 0.3844\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 123)\n",
      "🚀 Training BERTweet emotion model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6251\n",
      "Epoch 2/3, Average Loss: 0.9329\n",
      "Epoch 3/3, Average Loss: 0.6839\n",
      "✅ BERTweet emotion model trained and saved with seed 123\n",
      "   Accuracy: 0.7130, Macro F1: 0.6781\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 123)\n",
      "🚀 Training BERTweet multitask model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5768\n",
      "Epoch 2/3, Average Loss: 1.1922\n",
      "Epoch 3/3, Average Loss: 1.0833\n",
      "BERTweet multitask model trained and saved with seed 123\n",
      "   Sentiment - Accuracy: 0.5092, F1: 0.3838\n",
      "   Emotion - Accuracy: 0.2924, F1: 0.1074\n",
      "   Combined - Accuracy: 0.4008, F1: 0.2456\n",
      "\n",
      "✅ Completed evaluation for seed 123\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 456\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 456)\n",
      "🚀 Training BERTweet sentiment model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6452\n",
      "Epoch 2/3, Average Loss: 0.2622\n",
      "Epoch 3/3, Average Loss: 0.1454\n",
      "✅ BERTweet sentiment model trained and saved with seed 456\n",
      "   Accuracy: 0.5161, Macro F1: 0.3876\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 456)\n",
      "🚀 Training BERTweet emotion model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6056\n",
      "Epoch 2/3, Average Loss: 0.9359\n",
      "Epoch 3/3, Average Loss: 0.6983\n",
      "✅ BERTweet emotion model trained and saved with seed 456\n",
      "   Accuracy: 0.7280, Macro F1: 0.6903\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 456)\n",
      "🚀 Training BERTweet multitask model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5904\n",
      "Epoch 2/3, Average Loss: 1.2239\n",
      "Epoch 3/3, Average Loss: 1.1226\n",
      "BERTweet multitask model trained and saved with seed 456\n",
      "   Sentiment - Accuracy: 0.5126, F1: 0.3856\n",
      "   Emotion - Accuracy: 0.3108, F1: 0.0940\n",
      "   Combined - Accuracy: 0.4117, F1: 0.2398\n",
      "\n",
      "✅ Completed evaluation for seed 456\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 789\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 789)\n",
      "🚀 Training BERTweet sentiment model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.6038\n",
      "Epoch 2/3, Average Loss: 0.2392\n",
      "Epoch 3/3, Average Loss: 0.1423\n",
      "✅ BERTweet sentiment model trained and saved with seed 789\n",
      "   Accuracy: 0.5195, Macro F1: 0.3890\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 789)\n",
      "🚀 Training BERTweet emotion model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6244\n",
      "Epoch 2/3, Average Loss: 0.9621\n",
      "Epoch 3/3, Average Loss: 0.6710\n",
      "✅ BERTweet emotion model trained and saved with seed 789\n",
      "   Accuracy: 0.7360, Macro F1: 0.7011\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 789)\n",
      "🚀 Training BERTweet multitask model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.4732\n",
      "Epoch 2/3, Average Loss: 1.1708\n",
      "Epoch 3/3, Average Loss: 1.0700\n",
      "BERTweet multitask model trained and saved with seed 789\n",
      "   Sentiment - Accuracy: 0.5138, F1: 0.3888\n",
      "   Emotion - Accuracy: 0.2993, F1: 0.0999\n",
      "   Combined - Accuracy: 0.4065, F1: 0.2443\n",
      "\n",
      "✅ Completed evaluation for seed 789\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 999\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment on SST-2 (Seed 999)\n",
      "🚀 Training BERTweet sentiment model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.5978\n",
      "Epoch 2/3, Average Loss: 0.2331\n",
      "Epoch 3/3, Average Loss: 0.1184\n",
      "✅ BERTweet sentiment model trained and saved with seed 999\n",
      "   Accuracy: 0.5172, Macro F1: 0.3887\n",
      "\n",
      "2️⃣ BERTweet Emotion on GoEmotions (Seed 999)\n",
      "🚀 Training BERTweet emotion model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6253\n",
      "Epoch 2/3, Average Loss: 1.5770\n",
      "Epoch 3/3, Average Loss: 1.3145\n",
      "✅ BERTweet emotion model trained and saved with seed 999\n",
      "   Accuracy: 0.5480, Macro F1: 0.4431\n",
      "\n",
      "3️⃣ BERTweet Multitask on SST-2 + GoEmotions (Seed 999)\n",
      "🚀 Training BERTweet multitask model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5349\n",
      "Epoch 2/3, Average Loss: 1.2290\n",
      "Epoch 3/3, Average Loss: 1.1571\n",
      "BERTweet multitask model trained and saved with seed 999\n",
      "   Sentiment - Accuracy: 0.5034, F1: 0.3824\n",
      "   Emotion - Accuracy: 0.2638, F1: 0.1237\n",
      "   Combined - Accuracy: 0.3836, F1: 0.2530\n",
      "\n",
      "✅ Completed evaluation for seed 999\n",
      "\n",
      "📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\n",
      "======================================================================\n",
      "\n",
      "🔍 BERTWEET_SENTIMENT - SENTIMENT\n",
      "   Accuracy: 0.5144 ± 0.0056\n",
      "   Macro F1: 0.3874 ± 0.0016\n",
      "\n",
      "🔍 BERTWEET_EMOTION - EMOTION\n",
      "   Accuracy: 0.6890 ± 0.0709\n",
      "   Macro F1: 0.6395 ± 0.0985\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - SENTIMENT\n",
      "   Accuracy: 0.5108 ± 0.0041\n",
      "   Macro F1: 0.3849 ± 0.0022\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - EMOTION\n",
      "   Accuracy: 0.2924 ± 0.0156\n",
      "   Macro F1: 0.1054 ± 0.0101\n",
      "\n",
      "💾 BERTweet results saved:\n",
      "   Raw results: ./bertweet_seed_analysis_results/bertweet_raw_results_20250813_114832.json\n",
      "   Stability analysis: ./bertweet_seed_analysis_results/bertweet_stability_analysis_20250813_114832.json\n",
      "   Summary report: ./bertweet_seed_analysis_results/bertweet_summary_report_20250813_114832.txt\n",
      "\n",
      "BERTWEET RANDOM SEED ANALYSIS COMPLETED\n",
      "============================================================\n",
      "Check the './bertweet_seed_analysis_results/' directory for detailed results.\n",
      "\n",
      "📊 QUICK STABILITY SUMMARY:\n",
      "----------------------------------------\n",
      "\n",
      "Bertweet Sentiment Sentiment:\n",
      "  Accuracy: 0.514 ± 0.006\n",
      "  F1 Score: 0.387 ± 0.002\n",
      "\n",
      "Bertweet Emotion Emotion:\n",
      "  Accuracy: 0.689 ± 0.071\n",
      "  F1 Score: 0.639 ± 0.098\n",
      "\n",
      "Bertweet Multitask Sentiment:\n",
      "  Accuracy: 0.511 ± 0.004\n",
      "  F1 Score: 0.385 ± 0.002\n",
      "\n",
      "Bertweet Multitask Emotion:\n",
      "  Accuracy: 0.292 ± 0.016\n",
      "  F1 Score: 0.105 ± 0.010\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Run BERTweet random seed analysis with the modified function signature\n",
    "try:\n",
    "    all_results, stability_analysis = run_bertweet_seed_analysis(\n",
    "        seeds=[42, 123, 456, 789, 999],  # 5 different seeds\n",
    "        max_training_samples=3000,  # Reduced for faster training\n",
    "        max_eval_samples=1000  # Max evaluation samples per dataset\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBERTWEET RANDOM SEED ANALYSIS COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Check the './bertweet_seed_analysis_results/' directory for detailed results.\")\n",
    "    \n",
    "    # Display quick summary\n",
    "    print(\"\\n📊 QUICK STABILITY SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Updated to match the actual structure of stability_analysis\n",
    "    for key, stats in stability_analysis.items():\n",
    "        model_task = key.replace('_', ' ').title()\n",
    "        print(f\"\\n{model_task}:\")\n",
    "        print(f\"  Accuracy: {stats['accuracy_mean']:.3f} ± {stats['accuracy_std']:.3f}\")\n",
    "        print(f\"  F1 Score: {stats['f1_mean']:.3f} ± {stats['f1_std']:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during analysis: {str(e)}\")\n",
    "    print(\"🔧 Try restarting the kernel and running cells 1-9 again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet bootstrap analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_bertweet_model_for_bootstrap(model_path: str, model_type: str):\n",
    "    print(f\"📥 Loading BERTweet {model_type} model from {model_path}...\")\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_path, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        # Load multitask model\n",
    "        model = BERTweetMultiTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            sentiment_num_classes=config['sentiment_num_classes'],\n",
    "            emotion_num_classes=config['emotion_num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoders\n",
    "        sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "        emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, sentiment_encoder, emotion_encoder\n",
    "        \n",
    "    else:\n",
    "        # Load single-task model\n",
    "        model = BERTweetSingleTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            num_classes=config['num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoder\n",
    "        encoder = joblib.load(os.path.join(model_path, f'{config[\"task_type\"]}_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, encoder\n",
    "\n",
    "def evaluate_bertweet_on_bootstrap_sample(model, tokenizer, texts, sentiment_labels, emotion_labels, \n",
    "                                        model_sentiment_encoder, model_emotion_encoder, \n",
    "                                        data_sentiment_encoder, data_emotion_encoder, \n",
    "                                        model_type=\"multitask\", max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 8):\n",
    "                batch_texts = texts[i:i+8]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': inputs['input_ids'].to(device),\n",
    "                    'attention_mask': inputs['attention_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                outputs = model(**filtered_inputs)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                for j in range(len(batch_texts)):\n",
    "                    sent_id = sentiment_preds[j].item()\n",
    "                    emot_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    if sent_id >= len(model_sentiment_encoder.classes_):\n",
    "                        sent_id = 0\n",
    "                    if emot_id >= len(model_emotion_encoder.classes_):\n",
    "                        emot_id = 0\n",
    "                    \n",
    "                    sentiment_predictions.append(sent_id)\n",
    "                    emotion_predictions.append(emot_id)\n",
    "        \n",
    "        # Map predictions to data label space\n",
    "        mapped_sentiment_preds = []\n",
    "        mapped_emotion_preds = []\n",
    "        \n",
    "        for sent_pred, emot_pred in zip(sentiment_predictions, emotion_predictions):\n",
    "            sent_class = model_sentiment_encoder.classes_[sent_pred]\n",
    "            emot_class = model_emotion_encoder.classes_[emot_pred]\n",
    "            \n",
    "            try:\n",
    "                mapped_sent = data_sentiment_encoder.transform([sent_class])[0]\n",
    "                mapped_emot = data_emotion_encoder.transform([emot_class])[0]\n",
    "            except ValueError:\n",
    "                mapped_sent = 0\n",
    "                mapped_emot = 0\n",
    "            \n",
    "            mapped_sentiment_preds.append(mapped_sent)\n",
    "            mapped_emotion_preds.append(mapped_emot)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, mapped_sentiment_preds)\n",
    "        sentiment_f1 = f1_score(sentiment_labels, mapped_sentiment_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        emotion_accuracy = accuracy_score(emotion_labels, mapped_emotion_preds)\n",
    "        emotion_f1 = f1_score(emotion_labels, mapped_emotion_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1': emotion_f1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def bootstrap_evaluation_bertweet(model, tokenizer, data, model_sentiment_encoder, model_emotion_encoder,\n",
    "                                data_sentiment_encoder, data_emotion_encoder, \n",
    "                                n_iterations=1000, sample_size=95):\n",
    "    print(f\"🔄 Starting BERTweet bootstrap evaluation...\")\n",
    "    print(f\"   Iterations: {n_iterations}\")\n",
    "    print(f\"   Sample size: {sample_size}\")\n",
    "    \n",
    "    results = {\n",
    "        'sentiment_accuracy': [],\n",
    "        'sentiment_f1': [],\n",
    "        'emotion_accuracy': [],\n",
    "        'emotion_f1': []\n",
    "    }\n",
    "    \n",
    "    texts = data['texts']\n",
    "    sentiment_labels = data['sentiment_labels']\n",
    "    emotion_labels = data['emotion_labels']\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Bootstrap sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
    "        \n",
    "        sample_texts = [texts[idx] for idx in indices]\n",
    "        sample_sentiment_labels = [sentiment_labels[idx] for idx in indices]\n",
    "        sample_emotion_labels = [emotion_labels[idx] for idx in indices]\n",
    "        \n",
    "        # Evaluate on bootstrap sample\n",
    "        metrics = evaluate_bertweet_on_bootstrap_sample(\n",
    "            model, tokenizer, sample_texts, sample_sentiment_labels, sample_emotion_labels,\n",
    "            model_sentiment_encoder, model_emotion_encoder,\n",
    "            data_sentiment_encoder, data_emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results['sentiment_accuracy'].append(metrics['sentiment_accuracy'])\n",
    "        results['sentiment_f1'].append(metrics['sentiment_f1'])\n",
    "        results['emotion_accuracy'].append(metrics['emotion_accuracy'])\n",
    "        results['emotion_f1'].append(metrics['emotion_f1'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"BERTweet bootstrap analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05733328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bertweet_bootstrap_analysis():\n",
    "    print(\"🚀 Running BERTweet Bootstrap Analysis on General Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load external datasets\n",
    "    print(\"\\n📂 Loading general evaluation datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # 2. Prepare evaluation data\n",
    "    print(\"Preparing SST-2 evaluation data...\")\n",
    "    sst2_eval_data = prepare_sst2_evaluation_data(sentiment_data)\n",
    "    print(\"Preparing GoEmotions evaluation data...\")\n",
    "    goemotions_eval_data = prepare_goemotions_evaluation_data(emotion_data)\n",
    "    print(\"Preparing multitask evaluation data...\")\n",
    "    multitask_eval_data = prepare_multitask_evaluation_data(sst2_eval_data, goemotions_eval_data)\n",
    "    \n",
    "    # 3. Load models\n",
    "    # Load single task models\n",
    "    sentiment_model_path = \"./bertweet_trained_models_seeds/bertweet_sentiment_seed_42\"\n",
    "    sentiment_model, sentiment_tokenizer, sentiment_encoder = load_bertweet_model_for_bootstrap(\n",
    "        sentiment_model_path, \"sentiment\"\n",
    "    )\n",
    "    \n",
    "    emotion_model_path = \"./bertweet_trained_models_seeds/bertweet_emotion_seed_42\"\n",
    "    emotion_model, emotion_tokenizer, emotion_encoder = load_bertweet_model_for_bootstrap(\n",
    "        emotion_model_path, \"emotion\"\n",
    "    )\n",
    "    \n",
    "    # Load multitask model\n",
    "    multitask_model_path = \"./bertweet_trained_models_seeds/bertweet_multitask_seed_42\"\n",
    "    multitask_model, multitask_tokenizer, multitask_sent_encoder, multitask_emot_encoder = load_bertweet_model_for_bootstrap(\n",
    "        multitask_model_path, \"multitask\"\n",
    "    )\n",
    "    \n",
    "    # 4. Run bootstrap evaluation\n",
    "    print(\"\\n🔄 Starting bootstrap evaluation...\")\n",
    "    n_iterations = 1000\n",
    "    sample_size = 95\n",
    "    \n",
    "    # Initialize results dictionary for F1 scores\n",
    "    f1_results = {\n",
    "        'sentiment_single': [],\n",
    "        'emotion_single': [],\n",
    "        'multitask_sentiment': [],\n",
    "        'multitask_emotion': []\n",
    "    }\n",
    "    \n",
    "    # Run bootstrap iterations\n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Sample indices with replacement for each dataset\n",
    "        sst2_indices = np.random.choice(len(sst2_eval_data['texts']), size=sample_size, replace=True)\n",
    "        goemotions_indices = np.random.choice(len(goemotions_eval_data['texts']), size=sample_size, replace=True)\n",
    "        \n",
    "        # Prepare bootstrap samples\n",
    "        sst2_sample = {\n",
    "            'texts': [sst2_eval_data['texts'][i] for i in sst2_indices],\n",
    "            'sentiment_labels': [sst2_eval_data['sentiment_labels'][i] for i in sst2_indices]\n",
    "        }\n",
    "        \n",
    "        goemotions_sample = {\n",
    "            'texts': [goemotions_eval_data['texts'][i] for i in goemotions_indices],\n",
    "            'emotion_labels': [goemotions_eval_data['emotion_labels'][i] for i in goemotions_indices]\n",
    "        }\n",
    "        \n",
    "        multitask_sample = {\n",
    "            'texts': sst2_sample['texts'],  # Use SST-2 texts for multitask\n",
    "            'sentiment_labels': sst2_sample['sentiment_labels'],\n",
    "            'emotion_labels': [goemotions_eval_data['emotion_labels'][i] for i in sst2_indices]\n",
    "        }\n",
    "        \n",
    "        # Evaluate single task models\n",
    "        sentiment_results = evaluate_bertweet_single_task(\n",
    "            sentiment_model, sentiment_tokenizer, sentiment_encoder, \n",
    "            sst2_sample, 'sentiment'\n",
    "        )\n",
    "        f1_results['sentiment_single'].append(sentiment_results['macro_f1'])\n",
    "        \n",
    "        emotion_results = evaluate_bertweet_single_task(\n",
    "            emotion_model, emotion_tokenizer, emotion_encoder, \n",
    "            goemotions_sample, 'emotion'\n",
    "        )\n",
    "        f1_results['emotion_single'].append(emotion_results['macro_f1'])\n",
    "        \n",
    "        # Evaluate multitask model\n",
    "        multitask_results = evaluate_bertweet_multitask(\n",
    "            multitask_model, multitask_tokenizer, \n",
    "            multitask_sent_encoder, multitask_emot_encoder,\n",
    "            multitask_sample\n",
    "        )\n",
    "        f1_results['multitask_sentiment'].append(multitask_results['sentiment']['macro_f1'])\n",
    "        f1_results['multitask_emotion'].append(multitask_results['emotion']['macro_f1'])\n",
    "    \n",
    "    # 5. Calculate and display statistics\n",
    "    print(\"\\n📊 BERTweet Bootstrap Analysis Results (General Datasets)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, f1_scores in f1_results.items():\n",
    "        values = np.array(f1_scores)\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        ci_lower = np.percentile(values, 2.5)\n",
    "        ci_upper = np.percentile(values, 97.5)\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name.replace('_', ' ').upper()} - F1\")\n",
    "        print(f\"   Mean: {mean:.4f}\")\n",
    "        print(f\"   Std:  {std:.4f}\")\n",
    "        print(f\"   95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # 6. Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_bootstrap_general_datasets_{timestamp}.json\"\n",
    "    \n",
    "    results_to_save = {\n",
    "        model_name: {\n",
    "            'values': [float(x) for x in values],\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)),\n",
    "            'ci_lower': float(np.percentile(values, 2.5)),\n",
    "            'ci_upper': float(np.percentile(values, 97.5))\n",
    "        }\n",
    "        for model_name, values in f1_results.items()\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Bootstrap results saved to: {results_file}\")\n",
    "    \n",
    "    return results_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dec111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BERTweet Bootstrap Analysis on General Datasets\n",
      "============================================================\n",
      "\n",
      "📂 Loading general evaluation datasets...\n",
      "Loading external datasets...\n",
      "✅ SST-2 dataset loaded: 67349 train, 872 validation samples\n",
      "✅ GoEmotions dataset loaded: 43410 train, 5426 validation, 5427 test samples\n",
      "Preparing SST-2 evaluation data...\n",
      "Preparing SST-2 evaluation data...\n",
      "✅ SST-2 evaluation data prepared: 872 samples\n",
      "   Sentiment classes: ['Negative', 'Neutral', 'Positive']\n",
      "Preparing GoEmotions evaluation data...\n",
      "Preparing GoEmotions evaluation data...\n",
      "✅ GoEmotions evaluation data prepared: 1000 samples\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
      "Preparing multitask evaluation data...\n",
      "Preparing multitask evaluation data...\n",
      "✅ Multitask evaluation data prepared: 872 samples\n",
      "📥 Loading BERTweet sentiment model from ./bertweet_trained_models_seeds/bertweet_sentiment_seed_42...\n",
      "📥 Loading BERTweet emotion model from ./bertweet_trained_models_seeds/bertweet_emotion_seed_42...\n",
      "📥 Loading BERTweet multitask model from ./bertweet_trained_models_seeds/bertweet_multitask_seed_42...\n",
      "\n",
      "🔄 Starting bootstrap evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|██████████| 1000/1000 [17:11<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 BERTweet Bootstrap Analysis Results (General Datasets)\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT SINGLE - F1\n",
      "   Mean: 0.3804\n",
      "   Std:  0.0286\n",
      "   95% CI: [0.3238, 0.4370]\n",
      "\n",
      "🎯 EMOTION SINGLE - F1\n",
      "   Mean: 0.6758\n",
      "   Std:  0.0519\n",
      "   95% CI: [0.5717, 0.7720]\n",
      "\n",
      "🎯 MULTITASK SENTIMENT - F1\n",
      "   Mean: 0.3776\n",
      "   Std:  0.0289\n",
      "   95% CI: [0.3206, 0.4331]\n",
      "\n",
      "🎯 MULTITASK EMOTION - F1\n",
      "   Mean: 0.1003\n",
      "   Std:  0.0195\n",
      "   95% CI: [0.0661, 0.1402]\n",
      "\n",
      "💾 Bootstrap results saved to: ./bertweet_seed_analysis_results/bertweet_bootstrap_general_datasets_20250813_120612.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run bootstrap analysis\n",
    "bootstrap_stats = run_bertweet_bootstrap_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ead5e",
   "metadata": {},
   "source": [
    "# Reddit specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b40f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Libraries imported and setup complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import random\n",
    "from collections import Counter\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"./bertweet_seed_analysis_results\", exist_ok=True)\n",
    "os.makedirs(\"./bertweet_trained_models_seeds\", exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31672e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def set_random_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e948c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet model architectures defined\n"
     ]
    }
   ],
   "source": [
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.bertweet.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERTweet outputs\n",
    "        outputs = self.bertweet(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTweet model\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        self.bertweet = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.bertweet.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Shared attention for common features\n",
    "        self.shared_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        self.shared_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.sentiment_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.emotion_dropout = nn.Dropout(classifier_dropout)\n",
    "        self.shared_dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Classification heads\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Shared encoder\n",
    "        encoder_outputs = self.bertweet(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply shared attention\n",
    "        shared_attended, _ = self.shared_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        shared_attended = self.shared_norm(shared_attended + sequence_output)\n",
    "        shared_attended = self.shared_dropout(shared_attended)\n",
    "        shared_pooled = shared_attended[:, 0, :]\n",
    "        \n",
    "        outputs = {}\n",
    "        \n",
    "        # Sentiment branch\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        sentiment_attended = self.sentiment_dropout(sentiment_attended)\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        sentiment_features = torch.cat([shared_pooled, sentiment_pooled], dim=-1)\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_features)\n",
    "        outputs[\"sentiment_logits\"] = sentiment_logits\n",
    "        \n",
    "        # Emotion branch\n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        emotion_attended = self.emotion_dropout(emotion_attended)\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        emotion_features = torch.cat([shared_pooled, emotion_pooled], dim=-1)\n",
    "        emotion_logits = self.emotion_classifier(emotion_features)\n",
    "        outputs[\"emotion_logits\"] = emotion_logits\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"BERTweet model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "class BERTweetDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BERTweetMultiTaskDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], sentiment_labels: List[int], \n",
    "                 emotion_labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"BERTweet dataset classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_external_datasets() -> Tuple[Dict, Dict]:\n",
    "    print(\"Loading external datasets...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        sentiment_data = {\n",
    "            'train': sst2_dataset['train'],\n",
    "            'validation': sst2_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ SST-2 dataset loaded: {len(sentiment_data['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load SST-2: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Load GoEmotions for emotion\n",
    "    try:\n",
    "        emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        emotion_data = {\n",
    "            'train': emotions_dataset['train'],\n",
    "            'validation': emotions_dataset['validation']\n",
    "        }\n",
    "        print(f\"✅ GoEmotions dataset loaded: {len(emotion_data['train'])} train samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not load GoEmotions: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def prepare_reddit_evaluation_data(reddit_data_path: str) -> Dict:\n",
    "    print(f\"Loading Reddit evaluation data from {reddit_data_path}...\")\n",
    "    \n",
    "    df = pd.read_csv(reddit_data_path)\n",
    "    \n",
    "    # Create label encoders that match BERTweet models\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # Fit encoders\n",
    "    sentiment_encoder.fit(df['sentiment'].tolist())\n",
    "    emotion_encoder.fit(df['emotion'].tolist())\n",
    "    \n",
    "    reddit_data = {\n",
    "        'texts': df['text_content'].tolist(),\n",
    "        'sentiment_labels_text': df['sentiment'].tolist(),\n",
    "        'emotion_labels_text': df['emotion'].tolist(),\n",
    "        'sentiment_labels': sentiment_encoder.transform(df['sentiment'].tolist()),\n",
    "        'emotion_labels': emotion_encoder.transform(df['emotion'].tolist()),\n",
    "        'sentiment_encoder': sentiment_encoder,\n",
    "        'emotion_encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Reddit data prepared: {len(reddit_data['texts'])} samples\")\n",
    "    print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "    print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "    \n",
    "    return reddit_data\n",
    "\n",
    "def prepare_bertweet_training_data(sentiment_data: Dict, emotion_data: Dict, max_samples: int = 5000):\n",
    "\n",
    "    # Process sentiment data (SST-2 to 3 classes)\n",
    "    sentiment_texts = sentiment_data['train']['sentence'][:max_samples]\n",
    "    sentiment_labels_raw = sentiment_data['train']['label'][:max_samples]\n",
    "    \n",
    "    # Convert SST-2 binary to 3-class sentiment\n",
    "    sentiment_labels = []\n",
    "    for label in sentiment_labels_raw:\n",
    "        if label == 0:  # Negative\n",
    "            sentiment_labels.append(0)\n",
    "        elif label == 1:  # Positive\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                sentiment_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                sentiment_labels.append(2)  # Positive\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in sentiment_labels:\n",
    "        neutral_indices = np.random.choice(len(sentiment_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            sentiment_labels[idx] = 1\n",
    "    \n",
    "    # Process emotion data (filter to first 6 classes)\n",
    "    emotion_texts_all = emotion_data['train']['text']\n",
    "    emotion_labels_all = emotion_data['train']['labels']\n",
    "    \n",
    "    emotion_texts = []\n",
    "    emotion_labels = []\n",
    "    count = 0\n",
    "    for i, label in enumerate(emotion_labels_all):\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        if isinstance(label, list):\n",
    "            if label and label[0] in range(6):\n",
    "                emotion_texts.append(emotion_texts_all[i])\n",
    "                emotion_labels.append(label[0])\n",
    "                count += 1\n",
    "        else:\n",
    "            if label in range(6):\n",
    "                emotion_texts.append(emotion_texts_all[i])\n",
    "                emotion_labels.append(label)\n",
    "                count += 1\n",
    "    \n",
    "    # Create encoders\n",
    "    sentiment_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    sentiment_encoder.classes_ = np.array(['Negative', 'Neutral', 'Positive'])\n",
    "    emotion_encoder.classes_ = np.array(['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise'])\n",
    "    \n",
    "    return {\n",
    "        'sentiment_data': {\n",
    "            'texts': sentiment_texts,\n",
    "            'labels': sentiment_labels,\n",
    "            'encoder': sentiment_encoder\n",
    "        },\n",
    "        'emotion_data': {\n",
    "            'texts': emotion_texts,\n",
    "            'labels': emotion_labels,\n",
    "            'encoder': emotion_encoder\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet training functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_bertweet_single_task(\n",
    "    task_type: str,  # 'sentiment' or 'emotion'\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 5000\n",
    ") -> Tuple[any, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet {task_type} model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Get appropriate data\n",
    "    if task_type == 'sentiment':\n",
    "        texts = training_data['sentiment_data']['texts'][:max_samples]\n",
    "        labels = training_data['sentiment_data']['labels'][:max_samples]\n",
    "        encoder = training_data['sentiment_data']['encoder']\n",
    "        num_classes = 3\n",
    "    else:  # emotion\n",
    "        texts = training_data['emotion_data']['texts'][:max_samples]\n",
    "        labels = training_data['emotion_data']['labels'][:max_samples]\n",
    "        encoder = training_data['emotion_data']['encoder']\n",
    "        num_classes = 6\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetSingleTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        num_classes=num_classes,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetDataset(texts, labels, tokenizer, max_length=128)\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_batch = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs['logits'], labels_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_{task_type}_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"task_type\": task_type,\n",
    "        \"model_type\": \"BERTweetSingleTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoder\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(encoder, os.path.join(output_dir, f'{task_type}_encoder.pkl'))\n",
    "    \n",
    "    print(f\"✅ BERTweet {task_type} model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, encoder\n",
    "\n",
    "def train_bertweet_multitask(\n",
    "    best_params: Dict,\n",
    "    seed: int,\n",
    "    training_data: Dict,\n",
    "    max_samples: int = 2000\n",
    ") -> Tuple[any, LabelEncoder, LabelEncoder]:\n",
    "    \n",
    "    print(f\"🚀 Training BERTweet multitask model with seed {seed}\")\n",
    "    set_random_seed(seed)\n",
    "    clear_memory()\n",
    "    \n",
    "    # Prepare multitask data (combine sentiment and emotion data)\n",
    "    min_length = min(len(training_data['sentiment_data']['texts']), \n",
    "                     len(training_data['emotion_data']['texts']))\n",
    "    min_length = min(min_length, max_samples)\n",
    "    \n",
    "    combined_texts = training_data['sentiment_data']['texts'][:min_length]\n",
    "    combined_sentiment_labels = training_data['sentiment_data']['labels'][:min_length]\n",
    "    combined_emotion_labels = training_data['emotion_data']['labels'][:min_length]\n",
    "    \n",
    "    sentiment_encoder = training_data['sentiment_data']['encoder']\n",
    "    emotion_encoder = training_data['emotion_data']['encoder']\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERTweetMultiTaskTransformer(\n",
    "        model_name='vinai/bertweet-base',\n",
    "        sentiment_num_classes=3,\n",
    "        emotion_num_classes=6,\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        attention_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = BERTweetMultiTaskDataset(\n",
    "        combined_texts, combined_sentiment_labels, combined_emotion_labels, \n",
    "        tokenizer, max_length=128\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=best_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(dataloader) * 3  # 3 epochs\n",
    "    warmup_steps = int(total_steps * best_params['warmup_ratio'])\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Loss functions\n",
    "    sentiment_criterion = nn.CrossEntropyLoss()\n",
    "    emotion_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    alpha = best_params['alpha']\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    print(f\"Starting training for 3 epochs...\")\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(device)\n",
    "            emotion_labels = batch['emotion_labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = sentiment_criterion(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = emotion_criterion(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss_batch = alpha * sentiment_loss + (1 - alpha) * emotion_loss\n",
    "            total_loss += total_loss_batch.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/3, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    config = {\n",
    "        \"model_name\": \"vinai/bertweet-base\",\n",
    "        \"sentiment_num_classes\": 3,\n",
    "        \"emotion_num_classes\": 6,\n",
    "        \"model_type\": \"BERTweetMultiTaskTransformer\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer and encoders\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    joblib.dump(sentiment_encoder, os.path.join(output_dir, 'sentiment_encoder.pkl'))\n",
    "    joblib.dump(emotion_encoder, os.path.join(output_dir, 'emotion_encoder.pkl'))\n",
    "    \n",
    "    print(f\"BERTweet multitask model trained and saved with seed {seed}\")\n",
    "    clear_memory()\n",
    "    \n",
    "    return model, sentiment_encoder, emotion_encoder\n",
    "\n",
    "print(\"BERTweet training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d537cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluation Functions for BERTweet Models\n",
    "def evaluate_bertweet_single_task(model, tokenizer, label_encoder, reddit_data: Dict, task_type: str) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_labels = reddit_data[f'{task_type}_labels']\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 16):  # Batch size 16\n",
    "            batch_texts = texts[i:i+16]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs['logits']\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                pred_id = preds[j].item()\n",
    "                confidence = probs[j][pred_id].item()\n",
    "                \n",
    "                # Handle out of range predictions\n",
    "                if pred_id >= len(label_encoder.classes_):\n",
    "                    pred_id = 0\n",
    "                \n",
    "                predictions.append(pred_id)\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'predictions': predictions,\n",
    "        'confidences': confidences,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "def evaluate_bertweet_multitask(model, tokenizer, sentiment_encoder, emotion_encoder, \n",
    "                               reddit_data: Dict, max_length: int = 128) -> Dict:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    texts = reddit_data['texts']\n",
    "    true_sentiment_labels = reddit_data['sentiment_labels']\n",
    "    true_emotion_labels = reddit_data['emotion_labels']\n",
    "    \n",
    "    sentiment_predictions = []\n",
    "    emotion_predictions = []\n",
    "    sentiment_confidences = []\n",
    "    emotion_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), 8):  # Smaller batch size for multitask\n",
    "            batch_texts = texts[i:i+8]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Process sentiment\n",
    "            sentiment_logits = outputs['sentiment_logits']\n",
    "            sentiment_probs = F.softmax(sentiment_logits, dim=-1)\n",
    "            sentiment_preds = torch.argmax(sentiment_logits, dim=-1)\n",
    "            \n",
    "            # Process emotion\n",
    "            emotion_logits = outputs['emotion_logits']\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=-1)\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=-1)\n",
    "            \n",
    "            # Collect results\n",
    "            for j in range(len(batch_texts)):\n",
    "                # Sentiment\n",
    "                sent_id = sentiment_preds[j].item()\n",
    "                sent_conf = sentiment_probs[j][sent_id].item()\n",
    "                if sent_id >= len(sentiment_encoder.classes_):\n",
    "                    sent_id = 0\n",
    "                sentiment_predictions.append(sent_id)\n",
    "                sentiment_confidences.append(sent_conf)\n",
    "                \n",
    "                # Emotion\n",
    "                emot_id = emotion_preds[j].item()\n",
    "                emot_conf = emotion_probs[j][emot_id].item()\n",
    "                if emot_id >= len(emotion_encoder.classes_):\n",
    "                    emot_id = 0\n",
    "                emotion_predictions.append(emot_id)\n",
    "                emotion_confidences.append(emot_conf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sentiment_accuracy = accuracy_score(true_sentiment_labels, sentiment_predictions)\n",
    "    sentiment_f1 = f1_score(true_sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    emotion_accuracy = accuracy_score(true_emotion_labels, emotion_predictions)\n",
    "    emotion_f1 = f1_score(true_emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'sentiment': {\n",
    "            'accuracy': sentiment_accuracy,\n",
    "            'macro_f1': sentiment_f1,\n",
    "            'predictions': sentiment_predictions,\n",
    "            'confidences': sentiment_confidences\n",
    "        },\n",
    "        'emotion': {\n",
    "            'accuracy': emotion_accuracy,\n",
    "            'macro_f1': emotion_f1,\n",
    "            'predictions': emotion_predictions,\n",
    "            'confidences': emotion_confidences\n",
    "        },\n",
    "        'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "        'combined_f1': (sentiment_f1 + emotion_f1) / 2\n",
    "    }\n",
    "\n",
    "print(\"BERTweet evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7906c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet random seed analysis function defined!\n"
     ]
    }
   ],
   "source": [
    "def run_bertweet_seed_analysis(\n",
    "    reddit_data_path: str = \"annotated_reddit_posts.csv\",\n",
    "    seeds: List[int] = [42, 123, 456, 789, 999],\n",
    "    max_training_samples: int = 3000\n",
    "):\n",
    "    \n",
    "    print(\"🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Seeds to test: {seeds}\")\n",
    "    print(f\"Max training samples per dataset: {max_training_samples}\")\n",
    "    \n",
    "    # Load external datasets\n",
    "    print(\"\\n📂 Loading external datasets...\")\n",
    "    sentiment_data, emotion_data = load_external_datasets()\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(\"\\n🔄 Preparing BERTweet training data...\")\n",
    "    training_data = prepare_bertweet_training_data(sentiment_data, emotion_data, max_training_samples)\n",
    "    \n",
    "    # Load Reddit evaluation data\n",
    "    print(\"\\n📂 Loading Reddit evaluation data...\")\n",
    "    reddit_data = prepare_reddit_evaluation_data(reddit_data_path)\n",
    "    \n",
    "    # Define best parameters for each BERTweet model\n",
    "    best_params = {\n",
    "        'sentiment': {\n",
    "            'learning_rate': 3.65445235521325e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'emotion': {\n",
    "            'learning_rate': 3.65445235521325e-05, \n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15986584841970367,\n",
    "            'weight_decay': 0.02404167763981929,\n",
    "            'hidden_dropout_prob': 0.13119890406724052,\n",
    "            'classifier_dropout': 0.1116167224336399\n",
    "        },\n",
    "        'multitask': {\n",
    "            'learning_rate': 4.166863122305896e-05,\n",
    "            'batch_size': 16,\n",
    "            'warmup_ratio': 0.15142344384136117,\n",
    "            'weight_decay': 0.06331731119758383,\n",
    "            'hidden_dropout_prob': 0.10929008254399955,\n",
    "            'classifier_dropout': 0.22150897038028766,\n",
    "            'alpha': 0.4341048247374583\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    # Store results for each seed\n",
    "    all_results = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n🌱 TRAINING AND EVALUATING BERTWEET WITH SEED {seed}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        seed_results = {}\n",
    "        \n",
    "        # 1. Train and evaluate BERTweet Sentiment\n",
    "        print(f\"\\n1️⃣ BERTweet Sentiment (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'sentiment', best_params['sentiment'], seed, \n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_sentiment_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, reddit_data, 'sentiment')\n",
    "        seed_results['bertweet_sentiment'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 2. Train and evaluate BERTweet Emotion\n",
    "        print(f\"\\n2️⃣ BERTweet Emotion (Seed {seed})\")\n",
    "        model, encoder = train_bertweet_single_task(\n",
    "            'emotion', best_params['emotion'], seed,\n",
    "            training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_emotion_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_bertweet_single_task(model, tokenizer, encoder, reddit_data, 'emotion')\n",
    "        seed_results['bertweet_emotion'] = results\n",
    "        print(f\"   Accuracy: {results['accuracy']:.4f}, Macro F1: {results['macro_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        # 3. Train and evaluate BERTweet Multitask\n",
    "        print(f\"\\n3️⃣ BERTweet Multitask (Seed {seed})\")\n",
    "        model, sent_enc, emot_enc = train_bertweet_multitask(\n",
    "            best_params['multitask'], seed, training_data, max_training_samples\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"./bertweet_trained_models_seeds/bertweet_multitask_seed_{seed}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluate_bertweet_multitask(\n",
    "            model, tokenizer, sent_enc, emot_enc, reddit_data, 128\n",
    "        )\n",
    "        seed_results['bertweet_multitask'] = results\n",
    "        print(f\"   Sentiment - Accuracy: {results['sentiment']['accuracy']:.4f}, F1: {results['sentiment']['macro_f1']:.4f}\")\n",
    "        print(f\"   Emotion - Accuracy: {results['emotion']['accuracy']:.4f}, F1: {results['emotion']['macro_f1']:.4f}\")\n",
    "        print(f\"   Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1']:.4f}\")\n",
    "        \n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        \n",
    "        all_results[seed] = seed_results\n",
    "        \n",
    "        print(f\"\\n✅ Completed evaluation for seed {seed}\")\n",
    "    \n",
    "    # Analyze stability across seeds\n",
    "    print(f\"\\n📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    stability_analysis = analyze_bertweet_seed_stability(all_results, seeds)\n",
    "    \n",
    "    # Save results\n",
    "    save_bertweet_results(all_results, stability_analysis, seeds)\n",
    "    \n",
    "    return all_results, stability_analysis\n",
    "\n",
    "print(\"✅ BERTweet random seed analysis function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3da3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet stability analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_bertweet_seed_stability(all_results: Dict, seeds: List[int]) -> Dict:\n",
    "    \n",
    "    stability_stats = {}\n",
    "    \n",
    "    # Define model-task combinations\n",
    "    evaluations = [\n",
    "        ('bertweet_sentiment', 'sentiment'),\n",
    "        ('bertweet_emotion', 'emotion'),\n",
    "        ('bertweet_multitask', 'sentiment'),\n",
    "        ('bertweet_multitask', 'emotion')\n",
    "    ]\n",
    "    \n",
    "    for model_name, task in evaluations:\n",
    "        print(f\"\\n🔍 {model_name.upper()} - {task.upper()}\")\n",
    "        \n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            if model_name in all_results[seed]:\n",
    "                result = all_results[seed][model_name]\n",
    "                \n",
    "                if model_name.endswith('_multitask'):\n",
    "                    acc = result[task]['accuracy']\n",
    "                    f1 = result[task]['macro_f1']\n",
    "                else:\n",
    "                    acc = result['accuracy']\n",
    "                    f1 = result['macro_f1']\n",
    "                \n",
    "                accuracies.append(acc)\n",
    "                f1_scores.append(f1)\n",
    "        \n",
    "        if accuracies:\n",
    "            acc_mean = np.mean(accuracies)\n",
    "            acc_std = np.std(accuracies)\n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            \n",
    "            stability_stats[f\"{model_name}_{task}\"] = {\n",
    "                'accuracy_mean': acc_mean,\n",
    "                'accuracy_std': acc_std,\n",
    "                'f1_mean': f1_mean,\n",
    "                'f1_std': f1_std,\n",
    "                'accuracy_values': accuracies,\n",
    "                'f1_values': f1_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy: {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "            print(f\"   Macro F1: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "    \n",
    "    return stability_stats\n",
    "\n",
    "def save_bertweet_results(all_results: Dict, stability_analysis: Dict, seeds: List[int]):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save raw results\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_raw_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        serializable_results = {}\n",
    "        for seed, seed_results in all_results.items():\n",
    "            serializable_results[str(seed)] = {}\n",
    "            for model, results in seed_results.items():\n",
    "                if isinstance(results, dict):\n",
    "                    serializable_results[str(seed)][model] = {}\n",
    "                    for key, value in results.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            serializable_results[str(seed)][model][key] = {\n",
    "                                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                                   [float(x) if isinstance(x, (np.floating, np.integer)) else x for x in v] if isinstance(v, list) else v\n",
    "                                for k, v in value.items()\n",
    "                            }\n",
    "                        else:\n",
    "                            serializable_results[str(seed)][model][key] = float(value) if isinstance(value, (np.floating, np.integer)) else value\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    # Save stability analysis\n",
    "    stability_file = f\"./bertweet_seed_analysis_results/bertweet_stability_analysis_{timestamp}.json\"\n",
    "    with open(stability_file, 'w') as f:\n",
    "        serializable_stability = {}\n",
    "        for key, stats in stability_analysis.items():\n",
    "            serializable_stability[key] = {\n",
    "                k: float(v) if isinstance(v, (np.floating, np.integer)) else \n",
    "                   [float(x) for x in v] if isinstance(v, list) else v\n",
    "                for k, v in stats.items()\n",
    "            }\n",
    "        json.dump(serializable_stability, f, indent=2)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = f\"./bertweet_seed_analysis_results/bertweet_summary_report_{timestamp}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"BERTWEET RANDOM SEED ANALYSIS SUMMARY REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Seeds tested: {seeds}\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"STABILITY ANALYSIS (Mean ± Std)\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\")\n",
    "        \n",
    "        for key, stats in stability_analysis.items():\n",
    "            model_task = key.replace('_', ' ').title()\n",
    "            f.write(f\"\\n{model_task}:\\n\")\n",
    "            f.write(f\"  Accuracy: {stats['accuracy_mean']:.4f} ± {stats['accuracy_std']:.4f}\\n\")\n",
    "            f.write(f\"  Macro F1: {stats['f1_mean']:.4f} ± {stats['f1_std']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nBest Performers (by mean F1 score):\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Find best performers\n",
    "        sentiment_best = max([k for k in stability_analysis.keys() if 'sentiment' in k], \n",
    "                           key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        emotion_best = max([k for k in stability_analysis.keys() if 'emotion' in k], \n",
    "                         key=lambda x: stability_analysis[x]['f1_mean'])\n",
    "        \n",
    "        f.write(f\"Sentiment: {sentiment_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[sentiment_best]['f1_mean']:.4f})\\n\")\n",
    "        f.write(f\"Emotion: {emotion_best.replace('_', ' ').title()} \")\n",
    "        f.write(f\"(F1: {stability_analysis[emotion_best]['f1_mean']:.4f})\\n\")\n",
    "    \n",
    "    print(f\"\\n💾 BERTweet results saved:\")\n",
    "    print(f\"   Raw results: {results_file}\")\n",
    "    print(f\"   Stability analysis: {stability_file}\")\n",
    "    print(f\"   Summary report: {summary_file}\")\n",
    "\n",
    "print(\"BERTweet stability analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95302a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 STARTING BERTWEET RANDOM SEED ANALYSIS\n",
      "======================================================================\n",
      "Seeds to test: [42, 123, 456, 789, 999]\n",
      "Max training samples per dataset: 3000\n",
      "\n",
      "📂 Loading external datasets...\n",
      "Loading external datasets...\n",
      "✅ SST-2 dataset loaded: 67349 train samples\n",
      "✅ GoEmotions dataset loaded: 43410 train samples\n",
      "\n",
      "🔄 Preparing BERTweet training data...\n",
      "\n",
      "📂 Loading Reddit evaluation data...\n",
      "Loading Reddit evaluation data from annotated_reddit_posts.csv...\n",
      "✅ Reddit data prepared: 95 samples\n",
      "   Sentiment classes: ['Negative', 'Neutral', 'Positive']\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 42\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 42)\n",
      "🚀 Training BERTweet sentiment model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.9036\n",
      "Epoch 2/3, Average Loss: 0.5374\n",
      "Epoch 3/3, Average Loss: 0.4057\n",
      "✅ BERTweet sentiment model trained and saved with seed 42\n",
      "   Accuracy: 0.6105, Macro F1: 0.4038\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 42)\n",
      "🚀 Training BERTweet emotion model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3897\n",
      "Epoch 2/3, Average Loss: 0.6943\n",
      "Epoch 3/3, Average Loss: 0.4847\n",
      "✅ BERTweet emotion model trained and saved with seed 42\n",
      "   Accuracy: 0.1579, Macro F1: 0.0941\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 42)\n",
      "🚀 Training BERTweet multitask model with seed 42\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5970\n",
      "Epoch 2/3, Average Loss: 1.2914\n",
      "Epoch 3/3, Average Loss: 1.1998\n",
      "BERTweet multitask model trained and saved with seed 42\n",
      "   Sentiment - Accuracy: 0.6105, F1: 0.4152\n",
      "   Emotion - Accuracy: 0.2842, F1: 0.1249\n",
      "   Combined - Accuracy: 0.4474, F1: 0.2700\n",
      "\n",
      "✅ Completed evaluation for seed 42\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 123\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 123)\n",
      "🚀 Training BERTweet sentiment model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8028\n",
      "Epoch 2/3, Average Loss: 0.4705\n",
      "Epoch 3/3, Average Loss: 0.3504\n",
      "✅ BERTweet sentiment model trained and saved with seed 123\n",
      "   Accuracy: 0.6000, Macro F1: 0.4073\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 123)\n",
      "🚀 Training BERTweet emotion model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3615\n",
      "Epoch 2/3, Average Loss: 0.6956\n",
      "Epoch 3/3, Average Loss: 0.4921\n",
      "✅ BERTweet emotion model trained and saved with seed 123\n",
      "   Accuracy: 0.1684, Macro F1: 0.0996\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 123)\n",
      "🚀 Training BERTweet multitask model with seed 123\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5860\n",
      "Epoch 2/3, Average Loss: 1.3214\n",
      "Epoch 3/3, Average Loss: 1.1987\n",
      "BERTweet multitask model trained and saved with seed 123\n",
      "   Sentiment - Accuracy: 0.6000, F1: 0.3988\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0702\n",
      "   Combined - Accuracy: 0.4263, F1: 0.2345\n",
      "\n",
      "✅ Completed evaluation for seed 123\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 456\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 456)\n",
      "🚀 Training BERTweet sentiment model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8234\n",
      "Epoch 2/3, Average Loss: 0.4725\n",
      "Epoch 3/3, Average Loss: 0.3779\n",
      "✅ BERTweet sentiment model trained and saved with seed 456\n",
      "   Accuracy: 0.6105, Macro F1: 0.4123\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 456)\n",
      "🚀 Training BERTweet emotion model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3945\n",
      "Epoch 2/3, Average Loss: 0.7019\n",
      "Epoch 3/3, Average Loss: 0.4818\n",
      "✅ BERTweet emotion model trained and saved with seed 456\n",
      "   Accuracy: 0.1579, Macro F1: 0.0937\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 456)\n",
      "🚀 Training BERTweet multitask model with seed 456\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6678\n",
      "Epoch 2/3, Average Loss: 1.5130\n",
      "Epoch 3/3, Average Loss: 1.4590\n",
      "BERTweet multitask model trained and saved with seed 456\n",
      "   Sentiment - Accuracy: 0.5474, F1: 0.2358\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "   Combined - Accuracy: 0.4000, F1: 0.1515\n",
      "\n",
      "✅ Completed evaluation for seed 456\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 789\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 789)\n",
      "🚀 Training BERTweet sentiment model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.8127\n",
      "Epoch 2/3, Average Loss: 0.4849\n",
      "Epoch 3/3, Average Loss: 0.3785\n",
      "✅ BERTweet sentiment model trained and saved with seed 789\n",
      "   Accuracy: 0.5895, Macro F1: 0.3846\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 789)\n",
      "🚀 Training BERTweet emotion model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3752\n",
      "Epoch 2/3, Average Loss: 0.6841\n",
      "Epoch 3/3, Average Loss: 0.4946\n",
      "✅ BERTweet emotion model trained and saved with seed 789\n",
      "   Accuracy: 0.1579, Macro F1: 0.0944\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 789)\n",
      "🚀 Training BERTweet multitask model with seed 789\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.6214\n",
      "Epoch 2/3, Average Loss: 1.2824\n",
      "Epoch 3/3, Average Loss: 1.1796\n",
      "BERTweet multitask model trained and saved with seed 789\n",
      "   Sentiment - Accuracy: 0.5579, F1: 0.3885\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "   Combined - Accuracy: 0.4053, F1: 0.2279\n",
      "\n",
      "✅ Completed evaluation for seed 789\n",
      "\n",
      "🌱 TRAINING AND EVALUATING BERTWEET WITH SEED 999\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ BERTweet Sentiment (Seed 999)\n",
      "🚀 Training BERTweet sentiment model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 0.9292\n",
      "Epoch 2/3, Average Loss: 0.9331\n",
      "Epoch 3/3, Average Loss: 0.9326\n",
      "✅ BERTweet sentiment model trained and saved with seed 999\n",
      "   Accuracy: 0.2632, Macro F1: 0.1725\n",
      "\n",
      "2️⃣ BERTweet Emotion (Seed 999)\n",
      "🚀 Training BERTweet emotion model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.3259\n",
      "Epoch 2/3, Average Loss: 0.6866\n",
      "Epoch 3/3, Average Loss: 0.4806\n",
      "✅ BERTweet emotion model trained and saved with seed 999\n",
      "   Accuracy: 0.1789, Macro F1: 0.1019\n",
      "\n",
      "3️⃣ BERTweet Multitask (Seed 999)\n",
      "🚀 Training BERTweet multitask model with seed 999\n",
      "Starting training for 3 epochs...\n",
      "Epoch 1/3, Average Loss: 1.5872\n",
      "Epoch 2/3, Average Loss: 1.3075\n",
      "Epoch 3/3, Average Loss: 1.1884\n",
      "BERTweet multitask model trained and saved with seed 999\n",
      "   Sentiment - Accuracy: 0.5684, F1: 0.3300\n",
      "   Emotion - Accuracy: 0.2526, F1: 0.1043\n",
      "   Combined - Accuracy: 0.4105, F1: 0.2172\n",
      "\n",
      "✅ Completed evaluation for seed 999\n",
      "\n",
      "📊 ANALYZING BERTWEET STABILITY ACROSS SEEDS\n",
      "======================================================================\n",
      "\n",
      "🔍 BERTWEET_SENTIMENT - SENTIMENT\n",
      "   Accuracy: 0.5347 ± 0.1360\n",
      "   Macro F1: 0.3561 ± 0.0923\n",
      "\n",
      "🔍 BERTWEET_EMOTION - EMOTION\n",
      "   Accuracy: 0.1642 ± 0.0084\n",
      "   Macro F1: 0.0967 ± 0.0034\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - SENTIMENT\n",
      "   Accuracy: 0.5768 ± 0.0244\n",
      "   Macro F1: 0.3537 ± 0.0656\n",
      "\n",
      "🔍 BERTWEET_MULTITASK - EMOTION\n",
      "   Accuracy: 0.2589 ± 0.0126\n",
      "   Macro F1: 0.0868 ± 0.0237\n",
      "❌ Error during analysis: Object of type ndarray is not JSON serializable\n",
      "🔧 Try restarting the kernel and running cells 1-9 again.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Run BERTweet random seed analysis with the fixed saving function\n",
    "try:\n",
    "    all_results, stability_analysis = run_bertweet_seed_analysis(\n",
    "        reddit_data_path=\"annotated_reddit_posts.csv\",\n",
    "        seeds=[42, 123, 456, 789, 999],  # 5 different seeds\n",
    "        max_training_samples=3000  # Reduced for faster training\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🎉 BERTWEET RANDOM SEED ANALYSIS COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Check the './bertweet_seed_analysis_results/' directory for detailed results.\")\n",
    "    \n",
    "    # Display quick summary\n",
    "    print(\"\\n📊 QUICK STABILITY SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name in ['BERTWEET_SENTIMENT', 'BERTWEET_EMOTION', 'BERTWEET_MULTITASK']:\n",
    "        if model_name in stability_analysis:\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            for task in ['sentiment', 'emotion']:\n",
    "                if task in stability_analysis[model_name]:\n",
    "                    metrics = stability_analysis[model_name][task]\n",
    "                    print(f\"  {task.title()}:\")\n",
    "                    print(f\"    Accuracy: {metrics.get('accuracy_mean', 0):.3f} ± {metrics.get('accuracy_std', 0):.3f}\")\n",
    "                    print(f\"    F1 Score: {metrics.get('f1_mean', 0):.3f} ± {metrics.get('f1_std', 0):.3f}\")\n",
    "                    print(f\"    Stability: {metrics.get('stability_score', 0):.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during analysis: {str(e)}\")\n",
    "    print(\"🔧 Try restarting the kernel and running cells 1-9 again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4886efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet bootstrap analysis functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_bertweet_model_for_bootstrap(model_path: str, model_type: str):\n",
    "    print(f\"📥 Loading BERTweet {model_type} model from {model_path}...\")\n",
    "    \n",
    "    # Load config\n",
    "    with open(os.path.join(model_path, 'config.json'), 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        # Load multitask model\n",
    "        model = BERTweetMultiTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            sentiment_num_classes=config['sentiment_num_classes'],\n",
    "            emotion_num_classes=config['emotion_num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoders\n",
    "        sentiment_encoder = joblib.load(os.path.join(model_path, 'sentiment_encoder.pkl'))\n",
    "        emotion_encoder = joblib.load(os.path.join(model_path, 'emotion_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, sentiment_encoder, emotion_encoder\n",
    "        \n",
    "    else:\n",
    "        # Load single-task model\n",
    "        model = BERTweetSingleTaskTransformer(\n",
    "            model_name=\"vinai/bertweet-base\",\n",
    "            num_classes=config['num_classes']\n",
    "        )\n",
    "        \n",
    "        # Load weights\n",
    "        state_dict = torch.load(os.path.join(model_path, 'pytorch_model.bin'), map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load encoder\n",
    "        encoder = joblib.load(os.path.join(model_path, f'{config[\"task_type\"]}_encoder.pkl'))\n",
    "        \n",
    "        return model, tokenizer, encoder\n",
    "\n",
    "def evaluate_bertweet_on_bootstrap_sample(model, tokenizer, texts, sentiment_labels, emotion_labels, \n",
    "                                        model_sentiment_encoder, model_emotion_encoder, \n",
    "                                        data_sentiment_encoder, data_emotion_encoder, \n",
    "                                        model_type=\"multitask\", max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    if model_type == \"multitask\":\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 8):\n",
    "                batch_texts = texts[i:i+8]\n",
    "                \n",
    "                inputs = tokenizer(\n",
    "                    batch_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length\n",
    "                )\n",
    "                \n",
    "                filtered_inputs = {\n",
    "                    'input_ids': inputs['input_ids'].to(device),\n",
    "                    'attention_mask': inputs['attention_mask'].to(device)\n",
    "                }\n",
    "                \n",
    "                outputs = model(**filtered_inputs)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                for j in range(len(batch_texts)):\n",
    "                    sent_id = sentiment_preds[j].item()\n",
    "                    emot_id = emotion_preds[j].item()\n",
    "                    \n",
    "                    if sent_id >= len(model_sentiment_encoder.classes_):\n",
    "                        sent_id = 0\n",
    "                    if emot_id >= len(model_emotion_encoder.classes_):\n",
    "                        emot_id = 0\n",
    "                    \n",
    "                    sentiment_predictions.append(sent_id)\n",
    "                    emotion_predictions.append(emot_id)\n",
    "        \n",
    "        # Map predictions to data label space\n",
    "        mapped_sentiment_preds = []\n",
    "        mapped_emotion_preds = []\n",
    "        \n",
    "        for sent_pred, emot_pred in zip(sentiment_predictions, emotion_predictions):\n",
    "            sent_class = model_sentiment_encoder.classes_[sent_pred]\n",
    "            emot_class = model_emotion_encoder.classes_[emot_pred]\n",
    "            \n",
    "            try:\n",
    "                mapped_sent = data_sentiment_encoder.transform([sent_class])[0]\n",
    "                mapped_emot = data_emotion_encoder.transform([emot_class])[0]\n",
    "            except ValueError:\n",
    "                mapped_sent = 0\n",
    "                mapped_emot = 0\n",
    "            \n",
    "            mapped_sentiment_preds.append(mapped_sent)\n",
    "            mapped_emotion_preds.append(mapped_emot)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, mapped_sentiment_preds)\n",
    "        sentiment_f1 = f1_score(sentiment_labels, mapped_sentiment_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        emotion_accuracy = accuracy_score(emotion_labels, mapped_emotion_preds)\n",
    "        emotion_f1 = f1_score(emotion_labels, mapped_emotion_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'sentiment_f1': sentiment_f1,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'emotion_f1': emotion_f1\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # Single task evaluation logic here\n",
    "        pass\n",
    "\n",
    "def bootstrap_evaluation_bertweet(model, tokenizer, data, model_sentiment_encoder, model_emotion_encoder,\n",
    "                                data_sentiment_encoder, data_emotion_encoder, \n",
    "                                n_iterations=1000, sample_size=95):\n",
    "    print(f\"🔄 Starting BERTweet bootstrap evaluation...\")\n",
    "    print(f\"   Iterations: {n_iterations}\")\n",
    "    print(f\"   Sample size: {sample_size}\")\n",
    "    \n",
    "    results = {\n",
    "        'sentiment_accuracy': [],\n",
    "        'sentiment_f1': [],\n",
    "        'emotion_accuracy': [],\n",
    "        'emotion_f1': []\n",
    "    }\n",
    "    \n",
    "    texts = data['texts']\n",
    "    sentiment_labels = data['sentiment_labels']\n",
    "    emotion_labels = data['emotion_labels']\n",
    "    n_samples = len(texts)\n",
    "    \n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Bootstrap sample with replacement\n",
    "        indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
    "        \n",
    "        sample_texts = [texts[idx] for idx in indices]\n",
    "        sample_sentiment_labels = [sentiment_labels[idx] for idx in indices]\n",
    "        sample_emotion_labels = [emotion_labels[idx] for idx in indices]\n",
    "        \n",
    "        # Evaluate on bootstrap sample\n",
    "        metrics = evaluate_bertweet_on_bootstrap_sample(\n",
    "            model, tokenizer, sample_texts, sample_sentiment_labels, sample_emotion_labels,\n",
    "            model_sentiment_encoder, model_emotion_encoder,\n",
    "            data_sentiment_encoder, data_emotion_encoder\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results['sentiment_accuracy'].append(metrics['sentiment_accuracy'])\n",
    "        results['sentiment_f1'].append(metrics['sentiment_f1'])\n",
    "        results['emotion_accuracy'].append(metrics['emotion_accuracy'])\n",
    "        results['emotion_f1'].append(metrics['emotion_f1'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"BERTweet bootstrap analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75296fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bertweet_bootstrap_analysis():\n",
    "    print(\"🚀 Running BERTweet Bootstrap Analysis on General Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load all models (single task and multitask)\n",
    "    print(\"\\n📥 Loading models...\")\n",
    "    \n",
    "    # Load single task models\n",
    "    sentiment_model_path = \"./bertweet_trained_models_seeds/bertweet_sentiment_seed_42\"\n",
    "    sentiment_model, sentiment_tokenizer, sentiment_encoder = load_bertweet_model_for_bootstrap(\n",
    "        sentiment_model_path, \"sentiment\"\n",
    "    )\n",
    "    \n",
    "    emotion_model_path = \"./bertweet_trained_models_seeds/bertweet_emotion_seed_42\"\n",
    "    emotion_model, emotion_tokenizer, emotion_encoder = load_bertweet_model_for_bootstrap(\n",
    "        emotion_model_path, \"emotion\"\n",
    "    )\n",
    "    \n",
    "    # Load multitask model\n",
    "    multitask_model_path = \"./bertweet_trained_models_seeds/bertweet_multitask_seed_42\"\n",
    "    multitask_model, multitask_tokenizer, multitask_sent_encoder, multitask_emot_encoder = load_bertweet_model_for_bootstrap(\n",
    "        multitask_model_path, \"multitask\"\n",
    "    )\n",
    "    \n",
    "    # 2. Load evaluation data\n",
    "    print(\"\\n📂 Loading evaluation datasets...\")\n",
    "    reddit_data = prepare_reddit_evaluation_data(\"annotated_reddit_posts.csv\")\n",
    "    \n",
    "    # 3. Run bootstrap evaluation for each model\n",
    "    print(\"\\n🔄 Starting bootstrap evaluation...\")\n",
    "    n_iterations = 1000\n",
    "    sample_size = 95\n",
    "    \n",
    "    # Initialize results dictionary for F1 scores\n",
    "    f1_results = {\n",
    "        'sentiment_single': [],\n",
    "        'emotion_single': [],\n",
    "        'multitask_sentiment': [],\n",
    "        'multitask_emotion': []\n",
    "    }\n",
    "    \n",
    "    # Run bootstrap iterations\n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        # Sample indices with replacement\n",
    "        indices = np.random.choice(len(reddit_data['texts']), size=sample_size, replace=True)\n",
    "        \n",
    "        # Prepare bootstrap sample\n",
    "        sample_data = {\n",
    "            'texts': [reddit_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [reddit_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [reddit_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "        \n",
    "        # Evaluate single task models\n",
    "        sentiment_results = evaluate_bertweet_single_task(\n",
    "            sentiment_model, sentiment_tokenizer, sentiment_encoder, \n",
    "            sample_data, 'sentiment'\n",
    "        )\n",
    "        f1_results['sentiment_single'].append(sentiment_results['macro_f1'])\n",
    "        \n",
    "        emotion_results = evaluate_bertweet_single_task(\n",
    "            emotion_model, emotion_tokenizer, emotion_encoder, \n",
    "            sample_data, 'emotion'\n",
    "        )\n",
    "        f1_results['emotion_single'].append(emotion_results['macro_f1'])\n",
    "        \n",
    "        # Evaluate multitask model\n",
    "        multitask_results = evaluate_bertweet_multitask(\n",
    "            multitask_model, multitask_tokenizer, \n",
    "            multitask_sent_encoder, multitask_emot_encoder,\n",
    "            sample_data\n",
    "        )\n",
    "        f1_results['multitask_sentiment'].append(multitask_results['sentiment']['macro_f1'])\n",
    "        f1_results['multitask_emotion'].append(multitask_results['emotion']['macro_f1'])\n",
    "    \n",
    "    # 4. Calculate and display statistics\n",
    "    print(\"\\n📊 BERTweet Bootstrap Analysis Results\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, f1_scores in f1_results.items():\n",
    "        values = np.array(f1_scores)\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        ci_lower = np.percentile(values, 2.5)\n",
    "        ci_upper = np.percentile(values, 97.5)\n",
    "        \n",
    "        print(f\"\\n🎯 {model_name.replace('_', ' ').upper()} - F1\")\n",
    "        print(f\"   Mean: {mean:.4f}\")\n",
    "        print(f\"   Std:  {std:.4f}\")\n",
    "        print(f\"   95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    \n",
    "    # 5. Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"./bertweet_seed_analysis_results/bertweet_bootstrap_results_{timestamp}.json\"\n",
    "    \n",
    "    results_to_save = {\n",
    "        model_name: {\n",
    "            'values': [float(x) for x in values],\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)),\n",
    "            'ci_lower': float(np.percentile(values, 2.5)),\n",
    "            'ci_upper': float(np.percentile(values, 97.5))\n",
    "        }\n",
    "        for model_name, values in f1_results.items()\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Bootstrap results saved to: {results_file}\")\n",
    "    \n",
    "    return results_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a3198c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BERTweet Bootstrap Analysis on General Datasets\n",
      "============================================================\n",
      "\n",
      "📥 Loading models...\n",
      "📥 Loading BERTweet sentiment model from ./bertweet_trained_models_seeds/bertweet_sentiment_seed_42...\n",
      "📥 Loading BERTweet emotion model from ./bertweet_trained_models_seeds/bertweet_emotion_seed_42...\n",
      "📥 Loading BERTweet multitask model from ./bertweet_trained_models_seeds/bertweet_multitask_seed_42...\n",
      "\n",
      "📂 Loading evaluation datasets...\n",
      "Loading Reddit evaluation data from annotated_reddit_posts.csv...\n",
      "✅ Reddit data prepared: 95 samples\n",
      "   Sentiment classes: ['Negative', 'Neutral', 'Positive']\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
      "\n",
      "🔄 Starting bootstrap evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrap iterations: 100%|██████████| 1000/1000 [17:46<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 BERTweet Bootstrap Analysis Results\n",
      "============================================================\n",
      "\n",
      "🎯 SENTIMENT SINGLE - F1\n",
      "   Mean: 0.3979\n",
      "   Std:  0.0403\n",
      "   95% CI: [0.3197, 0.4701]\n",
      "\n",
      "🎯 EMOTION SINGLE - F1\n",
      "   Mean: 0.0919\n",
      "   Std:  0.0228\n",
      "   95% CI: [0.0478, 0.1391]\n",
      "\n",
      "🎯 MULTITASK SENTIMENT - F1\n",
      "   Mean: 0.4087\n",
      "   Std:  0.0406\n",
      "   95% CI: [0.3344, 0.4865]\n",
      "\n",
      "🎯 MULTITASK EMOTION - F1\n",
      "   Mean: 0.1236\n",
      "   Std:  0.0215\n",
      "   95% CI: [0.0799, 0.1637]\n",
      "\n",
      "💾 Bootstrap results saved to: ./bertweet_seed_analysis_results/bertweet_bootstrap_results_20250813_125444.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment_single': {'values': [0.36790505675954593,\n",
       "   0.43923240938166314,\n",
       "   0.3378169071492622,\n",
       "   0.409813596491228,\n",
       "   0.3525641025641026,\n",
       "   0.33855072463768116,\n",
       "   0.39849624060150374,\n",
       "   0.45283243887895047,\n",
       "   0.45899280575539575,\n",
       "   0.30839002267573695,\n",
       "   0.3669099756690997,\n",
       "   0.3681014347681015,\n",
       "   0.3910323253388947,\n",
       "   0.4496423117112773,\n",
       "   0.41988742964352727,\n",
       "   0.3982142857142857,\n",
       "   0.4229133439659756,\n",
       "   0.3686274509803922,\n",
       "   0.4229133439659756,\n",
       "   0.361564560527495,\n",
       "   0.3501699854298203,\n",
       "   0.38713195201744816,\n",
       "   0.2800799709196656,\n",
       "   0.40366530021702435,\n",
       "   0.49752496014766345,\n",
       "   0.4528250878615842,\n",
       "   0.42352092352092346,\n",
       "   0.40433749257278667,\n",
       "   0.4296296296296296,\n",
       "   0.32973057644110276,\n",
       "   0.45912614517265676,\n",
       "   0.4294883550339263,\n",
       "   0.3930183056619838,\n",
       "   0.3681014347681015,\n",
       "   0.4454422990570274,\n",
       "   0.44498030704927255,\n",
       "   0.40552526354053064,\n",
       "   0.4034159472755963,\n",
       "   0.3874643874643875,\n",
       "   0.4484848484848485,\n",
       "   0.4024767801857585,\n",
       "   0.4488667772249862,\n",
       "   0.46589119108203075,\n",
       "   0.3445903689806129,\n",
       "   0.32915758896151054,\n",
       "   0.35648842245543894,\n",
       "   0.414426523297491,\n",
       "   0.37142857142857144,\n",
       "   0.3974358974358974,\n",
       "   0.347041294167731,\n",
       "   0.38862243538502533,\n",
       "   0.35897435897435903,\n",
       "   0.38702817650186067,\n",
       "   0.35811789826889345,\n",
       "   0.3707652215114902,\n",
       "   0.3994708994708995,\n",
       "   0.3206012378426171,\n",
       "   0.3137254901960784,\n",
       "   0.4608096468561585,\n",
       "   0.42228446934329283,\n",
       "   0.4220430107526882,\n",
       "   0.37251908396946565,\n",
       "   0.3799533799533799,\n",
       "   0.3610480084809935,\n",
       "   0.3304442036836403,\n",
       "   0.353395061728395,\n",
       "   0.4381989832970225,\n",
       "   0.3783068783068783,\n",
       "   0.42955874534821903,\n",
       "   0.42522680714076677,\n",
       "   0.4257602862254026,\n",
       "   0.41904761904761906,\n",
       "   0.4526977087952697,\n",
       "   0.3747553816046967,\n",
       "   0.3669753086419753,\n",
       "   0.3788996029495179,\n",
       "   0.3919349593495935,\n",
       "   0.43240093240093236,\n",
       "   0.42714164546225614,\n",
       "   0.39344815940560623,\n",
       "   0.3861194527861194,\n",
       "   0.3830409356725146,\n",
       "   0.5103485838779956,\n",
       "   0.484,\n",
       "   0.38913876395135766,\n",
       "   0.37697791688402016,\n",
       "   0.4589598997493734,\n",
       "   0.4253018372703412,\n",
       "   0.34567430025445295,\n",
       "   0.45588235294117646,\n",
       "   0.3481165600568586,\n",
       "   0.34722222222222215,\n",
       "   0.35303776683087024,\n",
       "   0.3812363347247068,\n",
       "   0.4543302701197438,\n",
       "   0.3484480431848853,\n",
       "   0.4014698162729659,\n",
       "   0.4018518518518519,\n",
       "   0.37843273768082303,\n",
       "   0.4024864024864025,\n",
       "   0.3719905825168983,\n",
       "   0.3495186713212737,\n",
       "   0.34289688749782643,\n",
       "   0.4268199233716475,\n",
       "   0.38706407137064075,\n",
       "   0.45260879471405785,\n",
       "   0.41058491573247924,\n",
       "   0.32938076416337286,\n",
       "   0.40049751243781095,\n",
       "   0.4041374708041375,\n",
       "   0.35384615384615387,\n",
       "   0.40723019670388094,\n",
       "   0.4386083052749719,\n",
       "   0.42279202279202277,\n",
       "   0.35954684528239483,\n",
       "   0.45220458553791887,\n",
       "   0.4785095000922339,\n",
       "   0.40585241730279903,\n",
       "   0.4544506983531374,\n",
       "   0.4066509557475779,\n",
       "   0.40585241730279903,\n",
       "   0.4144406669904484,\n",
       "   0.4273621596888584,\n",
       "   0.3186274509803922,\n",
       "   0.3752165013383719,\n",
       "   0.41290139244103435,\n",
       "   0.43663003663003663,\n",
       "   0.43703703703703706,\n",
       "   0.37385469092786167,\n",
       "   0.40726817042606517,\n",
       "   0.4005561348627042,\n",
       "   0.3617817705408946,\n",
       "   0.41484684684684686,\n",
       "   0.3657407407407407,\n",
       "   0.3551386071670047,\n",
       "   0.42798913043478254,\n",
       "   0.42144349036899514,\n",
       "   0.3717618802539029,\n",
       "   0.4028709917971663,\n",
       "   0.3878713878713878,\n",
       "   0.45028011204481794,\n",
       "   0.4792412312097351,\n",
       "   0.43777727648695386,\n",
       "   0.3602060707323865,\n",
       "   0.40979134409791346,\n",
       "   0.3992945326278659,\n",
       "   0.36142815335089257,\n",
       "   0.4202516827626573,\n",
       "   0.4114865804477819,\n",
       "   0.34637964774951074,\n",
       "   0.39814814814814814,\n",
       "   0.390625,\n",
       "   0.4433194361723248,\n",
       "   0.4579831932773109,\n",
       "   0.3761904761904762,\n",
       "   0.38338945005611674,\n",
       "   0.3987562189054727,\n",
       "   0.3938466368416922,\n",
       "   0.3672218079376916,\n",
       "   0.3754899938106045,\n",
       "   0.38737373737373737,\n",
       "   0.3771106941838649,\n",
       "   0.4523809523809524,\n",
       "   0.4248826291079812,\n",
       "   0.2813295165394402,\n",
       "   0.43695159223109536,\n",
       "   0.4319744204636291,\n",
       "   0.3862215553888472,\n",
       "   0.3678073990042226,\n",
       "   0.36904761904761907,\n",
       "   0.4425925925925926,\n",
       "   0.4106933019976498,\n",
       "   0.38392434988179663,\n",
       "   0.4313725490196078,\n",
       "   0.31357664233576643,\n",
       "   0.42141984999127863,\n",
       "   0.45128896882494,\n",
       "   0.44432736213558127,\n",
       "   0.41729323308270677,\n",
       "   0.3797331165752218,\n",
       "   0.4598674001974891,\n",
       "   0.36923076923076925,\n",
       "   0.3979783158066357,\n",
       "   0.3700280112044818,\n",
       "   0.4354633939906898,\n",
       "   0.37664473684210525,\n",
       "   0.3421448527831507,\n",
       "   0.36582224130528124,\n",
       "   0.4348063284233497,\n",
       "   0.36631016042780756,\n",
       "   0.4269162210338681,\n",
       "   0.3678073990042226,\n",
       "   0.49367402629620444,\n",
       "   0.37464601032816924,\n",
       "   0.4334521687462864,\n",
       "   0.39338061465721036,\n",
       "   0.3815178815178815,\n",
       "   0.3145009416195857,\n",
       "   0.4859206840630989,\n",
       "   0.36970521541950113,\n",
       "   0.42291666666666666,\n",
       "   0.40792540792540793,\n",
       "   0.2883668903803132,\n",
       "   0.38737922705314015,\n",
       "   0.3348111658456486,\n",
       "   0.33140747873535553,\n",
       "   0.41732979664014147,\n",
       "   0.3547506822128504,\n",
       "   0.4602150537634409,\n",
       "   0.38781127968760004,\n",
       "   0.4185185185185185,\n",
       "   0.37359307359307364,\n",
       "   0.3285382156349898,\n",
       "   0.4082428292954609,\n",
       "   0.4356751824817518,\n",
       "   0.430325932923949,\n",
       "   0.4297385620915033,\n",
       "   0.38730158730158726,\n",
       "   0.4203040386246493,\n",
       "   0.4113756613756614,\n",
       "   0.3669753086419753,\n",
       "   0.36657169990503324,\n",
       "   0.35468409586056643,\n",
       "   0.4293361140076469,\n",
       "   0.43677058353317344,\n",
       "   0.3891857506361323,\n",
       "   0.3780413625304136,\n",
       "   0.42950467212436944,\n",
       "   0.43524251805985553,\n",
       "   0.43950617283950616,\n",
       "   0.4146874146874147,\n",
       "   0.4525583705911575,\n",
       "   0.41059408838162126,\n",
       "   0.45621045621045625,\n",
       "   0.4552516041877744,\n",
       "   0.3557625948930297,\n",
       "   0.40503627827571487,\n",
       "   0.4166666666666667,\n",
       "   0.3941743011510453,\n",
       "   0.3876422764227643,\n",
       "   0.40375180375180375,\n",
       "   0.3845410628019324,\n",
       "   0.39845339845339844,\n",
       "   0.4399841017488077,\n",
       "   0.3177622377622378,\n",
       "   0.3063719905825169,\n",
       "   0.4273621596888584,\n",
       "   0.32471875950136825,\n",
       "   0.345679012345679,\n",
       "   0.3281705948372615,\n",
       "   0.4232507402212872,\n",
       "   0.41290139244103435,\n",
       "   0.4370629370629371,\n",
       "   0.389010989010989,\n",
       "   0.31610219845513965,\n",
       "   0.33204384268214054,\n",
       "   0.3969043670536208,\n",
       "   0.42623965343853953,\n",
       "   0.33164471462343803,\n",
       "   0.43972572439725727,\n",
       "   0.3968253968253968,\n",
       "   0.40547263681592033,\n",
       "   0.4124099872131368,\n",
       "   0.3974527887571366,\n",
       "   0.43045843045843046,\n",
       "   0.4240669240669241,\n",
       "   0.36085053158223895,\n",
       "   0.33748443337484435,\n",
       "   0.36007130124777187,\n",
       "   0.3847517730496454,\n",
       "   0.38872002701789937,\n",
       "   0.32769726247987113,\n",
       "   0.34868686868686866,\n",
       "   0.4209563029055136,\n",
       "   0.4342022940563086,\n",
       "   0.3634428223844282,\n",
       "   0.41084656084656085,\n",
       "   0.4136887880506712,\n",
       "   0.41765480895915674,\n",
       "   0.4029514244722279,\n",
       "   0.43617021276595747,\n",
       "   0.4047056174715749,\n",
       "   0.45306836645419324,\n",
       "   0.4007633587786259,\n",
       "   0.43081914030819135,\n",
       "   0.4319744204636291,\n",
       "   0.4746031746031745,\n",
       "   0.40234479083399943,\n",
       "   0.3408182544873192,\n",
       "   0.39153439153439146,\n",
       "   0.37637385686718683,\n",
       "   0.41087470449172575,\n",
       "   0.34313725490196084,\n",
       "   0.3215501440167583,\n",
       "   0.4331974637681159,\n",
       "   0.4269449715370019,\n",
       "   0.40775462962962966,\n",
       "   0.40802845528455284,\n",
       "   0.3759398496240601,\n",
       "   0.3740573152337858,\n",
       "   0.40317460317460324,\n",
       "   0.3526570048309179,\n",
       "   0.4500782472613459,\n",
       "   0.42565947242206237,\n",
       "   0.4373294816241374,\n",
       "   0.42163815880962313,\n",
       "   0.4208916083916084,\n",
       "   0.3889804567930304,\n",
       "   0.41258741258741255,\n",
       "   0.43870772946859904,\n",
       "   0.328042328042328,\n",
       "   0.4265792686845318,\n",
       "   0.3695781342840167,\n",
       "   0.43939393939393945,\n",
       "   0.3855257556406982,\n",
       "   0.35527065527065527,\n",
       "   0.4185185185185185,\n",
       "   0.3837371205792259,\n",
       "   0.3658164377588838,\n",
       "   0.3263888888888889,\n",
       "   0.38737922705314015,\n",
       "   0.32290036515388626,\n",
       "   0.38095238095238093,\n",
       "   0.3818947368421053,\n",
       "   0.3894137596899225,\n",
       "   0.42025699168556313,\n",
       "   0.37782554882418085,\n",
       "   0.43229813664596267,\n",
       "   0.37210648148148145,\n",
       "   0.34366925064599485,\n",
       "   0.4396456256921373,\n",
       "   0.3423243423243423,\n",
       "   0.41087470449172575,\n",
       "   0.4698966408268734,\n",
       "   0.4303303303303303,\n",
       "   0.4599610836701093,\n",
       "   0.33302034428794997,\n",
       "   0.41732979664014147,\n",
       "   0.3894137596899225,\n",
       "   0.411313518696069,\n",
       "   0.3197399527186761,\n",
       "   0.3650914634146341,\n",
       "   0.4371669915529564,\n",
       "   0.40196078431372556,\n",
       "   0.4194838240639767,\n",
       "   0.40350877192982454,\n",
       "   0.3893436045578558,\n",
       "   0.37697791688402016,\n",
       "   0.4575757575757575,\n",
       "   0.41228070175438597,\n",
       "   0.3594244149272612,\n",
       "   0.4174603174603175,\n",
       "   0.3140830800405268,\n",
       "   0.3584149184149184,\n",
       "   0.3620370370370371,\n",
       "   0.386018457960904,\n",
       "   0.3943187289359653,\n",
       "   0.449182658137882,\n",
       "   0.3371969091393552,\n",
       "   0.43248484848484847,\n",
       "   0.3593869731800767,\n",
       "   0.36870026525198935,\n",
       "   0.40687643517656547,\n",
       "   0.5016680802940345,\n",
       "   0.4143226282761166,\n",
       "   0.42142857142857143,\n",
       "   0.3596549435965494,\n",
       "   0.4195538057742782,\n",
       "   0.3485191710713986,\n",
       "   0.40301413936161207,\n",
       "   0.4358192779245411,\n",
       "   0.41572184429327286,\n",
       "   0.4579831932773109,\n",
       "   0.42773029439696103,\n",
       "   0.4499832719973235,\n",
       "   0.38252314814814814,\n",
       "   0.3982142857142857,\n",
       "   0.4045584045584046,\n",
       "   0.3809280544912729,\n",
       "   0.4230769230769231,\n",
       "   0.4251599147121535,\n",
       "   0.33695652173913043,\n",
       "   0.3042735042735043,\n",
       "   0.41264451451208667,\n",
       "   0.3783671777869872,\n",
       "   0.36256157635467984,\n",
       "   0.4405615292712067,\n",
       "   0.46230627790911477,\n",
       "   0.44857142857142857,\n",
       "   0.3879699248120301,\n",
       "   0.46493212669683254,\n",
       "   0.30295895141668633,\n",
       "   0.3208097443815406,\n",
       "   0.4244549893404856,\n",
       "   0.4120300751879699,\n",
       "   0.33946078431372556,\n",
       "   0.4343059239610964,\n",
       "   0.4619588062211013,\n",
       "   0.30069318258294636,\n",
       "   0.3863799283154122,\n",
       "   0.46288976723759334,\n",
       "   0.3551459293394777,\n",
       "   0.4075564498099709,\n",
       "   0.4338365293077655,\n",
       "   0.39056112969156453,\n",
       "   0.4483695652173913,\n",
       "   0.3547506822128504,\n",
       "   0.43628899835796386,\n",
       "   0.410989010989011,\n",
       "   0.4459459459459459,\n",
       "   0.38271604938271603,\n",
       "   0.35118306351183065,\n",
       "   0.3724198506807202,\n",
       "   0.3773536405115352,\n",
       "   0.36960431654676257,\n",
       "   0.4163178199129935,\n",
       "   0.32887864823348695,\n",
       "   0.3786848072562358,\n",
       "   0.43737373737373736,\n",
       "   0.45954198473282437,\n",
       "   0.3707958707958708,\n",
       "   0.4067142008318479,\n",
       "   0.3238380809595202,\n",
       "   0.34301075268817205,\n",
       "   0.3681257014590347,\n",
       "   0.36594202898550726,\n",
       "   0.38958932389589324,\n",
       "   0.34161490683229817,\n",
       "   0.3191753601589667,\n",
       "   0.3994708994708995,\n",
       "   0.4215686274509804,\n",
       "   0.4049467932125395,\n",
       "   0.3747276688453159,\n",
       "   0.3911869225302061,\n",
       "   0.4035886818495514,\n",
       "   0.3877151799687011,\n",
       "   0.3606060606060606,\n",
       "   0.3840966921119593,\n",
       "   0.4443089430894309,\n",
       "   0.4090909090909091,\n",
       "   0.45899280575539575,\n",
       "   0.4028871391076116,\n",
       "   0.3351851851851852,\n",
       "   0.3482177830003917,\n",
       "   0.4588103254769922,\n",
       "   0.43298245614035086,\n",
       "   0.41282051282051285,\n",
       "   0.3514279485502507,\n",
       "   0.43659176749247813,\n",
       "   0.31746031746031744,\n",
       "   0.32317682317682317,\n",
       "   0.4220846233230134,\n",
       "   0.42558094872287305,\n",
       "   0.3449419568822554,\n",
       "   0.39411520928305016,\n",
       "   0.39570533109911254,\n",
       "   0.38941798941798944,\n",
       "   0.39637188208616775,\n",
       "   0.3931824095758522,\n",
       "   0.41606714628297364,\n",
       "   0.40740740740740744,\n",
       "   0.418,\n",
       "   0.3160493827160494,\n",
       "   0.34637964774951074,\n",
       "   0.34331983805668015,\n",
       "   0.4248826291079812,\n",
       "   0.4251428571428571,\n",
       "   0.32454212454212455,\n",
       "   0.41220159151193636,\n",
       "   0.33174825174825173,\n",
       "   0.3735294117647059,\n",
       "   0.43526418200868133,\n",
       "   0.3595518207282913,\n",
       "   0.4035914702581369,\n",
       "   0.3815085158150852,\n",
       "   0.41021324354657684,\n",
       "   0.41575963718820863,\n",
       "   0.3695781342840167,\n",
       "   0.4022556390977443,\n",
       "   0.4265792686845318,\n",
       "   0.4197530864197531,\n",
       "   0.40349831047505463,\n",
       "   0.46553446553446554,\n",
       "   0.363371245261009,\n",
       "   0.4011286509570655,\n",
       "   0.36286310383566045,\n",
       "   0.39275246469491076,\n",
       "   0.40550473062788334,\n",
       "   0.4210594315245479,\n",
       "   0.4097521982414069,\n",
       "   0.46079846079846076,\n",
       "   0.39503875968992247,\n",
       "   0.34110097939885176,\n",
       "   0.43777727648695386,\n",
       "   0.3818592185075469,\n",
       "   0.36931027628702046,\n",
       "   0.41044776119402987,\n",
       "   0.42291666666666666,\n",
       "   0.3802216538789429,\n",
       "   0.3978883861236802,\n",
       "   0.3927907840951319,\n",
       "   0.4090909090909091,\n",
       "   0.4381046886628628,\n",
       "   0.36521860206070733,\n",
       "   0.4090909090909091,\n",
       "   0.3617571059431524,\n",
       "   0.44495559389176415,\n",
       "   0.4035886818495514,\n",
       "   0.4111866969009826,\n",
       "   0.4249955618675661,\n",
       "   0.3828703703703704,\n",
       "   0.467000835421888,\n",
       "   0.3619269619269619,\n",
       "   0.372843874391862,\n",
       "   0.3837939186435151,\n",
       "   0.39192924267551127,\n",
       "   0.4635887141741943,\n",
       "   0.387264457439896,\n",
       "   0.39810874704491733,\n",
       "   0.3330316742081448,\n",
       "   0.3784511784511784,\n",
       "   0.3896753896753897,\n",
       "   0.3465509304925363,\n",
       "   0.41904761904761906,\n",
       "   0.3529508331281293,\n",
       "   0.3555739743058433,\n",
       "   0.44946899623158615,\n",
       "   0.45694799658994034,\n",
       "   0.4111515151515151,\n",
       "   0.33946078431372556,\n",
       "   0.49837486457204766,\n",
       "   0.4597529944765097,\n",
       "   0.3535163070046791,\n",
       "   0.4568965517241379,\n",
       "   0.4475839475839476,\n",
       "   0.44710128581096326,\n",
       "   0.48805361305361306,\n",
       "   0.34954507857733663,\n",
       "   0.4618797902379992,\n",
       "   0.38725490196078427,\n",
       "   0.3605463455322699,\n",
       "   0.44819078947368424,\n",
       "   0.3992248062015504,\n",
       "   0.4251428571428571,\n",
       "   0.46589119108203075,\n",
       "   0.35669515669515667,\n",
       "   0.39145715058123814,\n",
       "   0.38656056587091076,\n",
       "   0.46753081713051214,\n",
       "   0.38722807017543853,\n",
       "   0.41859114015976767,\n",
       "   0.4134680134680135,\n",
       "   0.4330283837673001,\n",
       "   0.4304954304954305,\n",
       "   0.39933665008291874,\n",
       "   0.3924731182795698,\n",
       "   0.4082013047530289,\n",
       "   0.3741302972802025,\n",
       "   0.4547192353643967,\n",
       "   0.44109225504574345,\n",
       "   0.40614478114478114,\n",
       "   0.497228144989339,\n",
       "   0.38621179815209666,\n",
       "   0.43240093240093236,\n",
       "   0.40256410256410263,\n",
       "   0.36051159072741806,\n",
       "   0.37121212121212127,\n",
       "   0.3938466368416922,\n",
       "   0.3514779045946737,\n",
       "   0.38555008210180625,\n",
       "   0.36708358149692283,\n",
       "   0.3290689410092395,\n",
       "   0.39080459770114945,\n",
       "   0.4500782472613459,\n",
       "   0.398989898989899,\n",
       "   0.40990410762845286,\n",
       "   0.4180746041211158,\n",
       "   0.42464577071704596,\n",
       "   0.32804726368159204,\n",
       "   0.4458933882406772,\n",
       "   0.32748538011695905,\n",
       "   0.40993265993266,\n",
       "   0.36324281778827233,\n",
       "   0.4139393939393939,\n",
       "   0.38829151732377537,\n",
       "   0.34848484848484845,\n",
       "   0.38685446009389673,\n",
       "   0.40317460317460324,\n",
       "   0.42303172737955347,\n",
       "   0.45324991310392776,\n",
       "   0.39762724837351704,\n",
       "   0.40547263681592033,\n",
       "   0.409884801189149,\n",
       "   0.3837371205792259,\n",
       "   0.350067842605156,\n",
       "   0.3686868686868687,\n",
       "   0.32314148681055155,\n",
       "   0.3449419568822554,\n",
       "   0.41044776119402987,\n",
       "   0.3142043142043142,\n",
       "   0.4030049678904641,\n",
       "   0.44948349710254476,\n",
       "   0.3696145124716553,\n",
       "   0.38706467661691546,\n",
       "   0.40495603517186246,\n",
       "   0.4090909090909091,\n",
       "   0.47110705596107055,\n",
       "   0.46005291005291005,\n",
       "   0.38647764449291166,\n",
       "   0.3428571428571428,\n",
       "   0.38675732556257086,\n",
       "   0.37878787878787884,\n",
       "   0.3090082865543972,\n",
       "   0.40686274509803927,\n",
       "   0.4673270768161279,\n",
       "   0.33455882352941174,\n",
       "   0.3668188736681887,\n",
       "   0.4026023727516265,\n",
       "   0.40687643517656547,\n",
       "   0.35335836462244474,\n",
       "   0.351421188630491,\n",
       "   0.40665154950869237,\n",
       "   0.40729635182408797,\n",
       "   0.3483726150392817,\n",
       "   0.3608233181378644,\n",
       "   0.39154929577464787,\n",
       "   0.4405594405594406,\n",
       "   0.40166927490871157,\n",
       "   0.3494252873563218,\n",
       "   0.40491763565891475,\n",
       "   0.376060872110038,\n",
       "   0.4433862433862434,\n",
       "   0.4055366682855755,\n",
       "   0.41097979412886004,\n",
       "   0.411965811965812,\n",
       "   0.4404572036150984,\n",
       "   0.39153439153439146,\n",
       "   0.35027165228507506,\n",
       "   0.44268774703557306,\n",
       "   0.36633912824389014,\n",
       "   0.4101335159192057,\n",
       "   0.41,\n",
       "   0.3669099756690997,\n",
       "   0.3237660360948032,\n",
       "   0.5490107913669066,\n",
       "   0.41250000000000003,\n",
       "   0.40789473684210525,\n",
       "   0.45155555555555554,\n",
       "   0.44659359552976574,\n",
       "   0.3244778613199666,\n",
       "   0.4124495129484438,\n",
       "   0.4138342277877161,\n",
       "   0.41780125990652306,\n",
       "   0.41384432560903156,\n",
       "   0.3437862950058072,\n",
       "   0.45467372134038797,\n",
       "   0.5111111111111111,\n",
       "   0.44806605823554974,\n",
       "   0.46558008390069466,\n",
       "   0.39714638804628694,\n",
       "   0.4259259259259259,\n",
       "   0.44946899623158615,\n",
       "   0.38302972195589646,\n",
       "   0.4358319795776282,\n",
       "   0.4325092355782893,\n",
       "   0.37872063435706255,\n",
       "   0.36008052919183203,\n",
       "   0.34146341463414637,\n",
       "   0.38116760828625235,\n",
       "   0.37530864197530867,\n",
       "   0.3562832905898599,\n",
       "   0.35802469135802467,\n",
       "   0.42306788818416724,\n",
       "   0.40059523809523806,\n",
       "   0.38023088023088025,\n",
       "   0.38296003513394816,\n",
       "   0.3740478066719201,\n",
       "   0.45254119285216987,\n",
       "   0.3981762917933131,\n",
       "   0.362390350877193,\n",
       "   0.2969639468690702,\n",
       "   0.35607728050868187,\n",
       "   0.44126416219439474,\n",
       "   0.4272904483430799,\n",
       "   0.37333333333333335,\n",
       "   0.3847517730496454,\n",
       "   0.4358765129878961,\n",
       "   0.42196608373078964,\n",
       "   0.38338945005611674,\n",
       "   0.44370616463639717,\n",
       "   0.37877593905962703,\n",
       "   0.45447636418908655,\n",
       "   0.38436848017974645,\n",
       "   0.4385964912280702,\n",
       "   0.405037927579791,\n",
       "   0.4047619047619048,\n",
       "   0.39615384615384613,\n",
       "   0.37338696162225576,\n",
       "   0.38725490196078427,\n",
       "   0.4002229654403567,\n",
       "   0.35543766578249336,\n",
       "   0.418240723760621,\n",
       "   0.44965277777777785,\n",
       "   0.34487612612612617,\n",
       "   0.34882478632478636,\n",
       "   0.4097521982414069,\n",
       "   0.32915758896151054,\n",
       "   0.4666666666666666,\n",
       "   0.3693398799781779,\n",
       "   0.35919295254333616,\n",
       "   0.44418331374853115,\n",
       "   0.38392434988179663,\n",
       "   0.35954226651901067,\n",
       "   0.47203845066440486,\n",
       "   0.40324663346246076,\n",
       "   0.32712712712712716,\n",
       "   0.36316316316316316,\n",
       "   0.4751573590464711,\n",
       "   0.4294871794871795,\n",
       "   0.35294117647058826,\n",
       "   0.37923021060275963,\n",
       "   0.4546147978642258,\n",
       "   0.42510121457489874,\n",
       "   0.43719535058117737,\n",
       "   0.3428571428571428,\n",
       "   0.4239453357100416,\n",
       "   0.4351851851851852,\n",
       "   0.4003277242074746,\n",
       "   0.357799671592775,\n",
       "   0.4103723805216342,\n",
       "   0.392921146953405,\n",
       "   0.3363728470111449,\n",
       "   0.4385964912280702,\n",
       "   0.3804195804195804,\n",
       "   0.4134484325306678,\n",
       "   0.3987577639751552,\n",
       "   0.41238095238095235,\n",
       "   0.38974358974358975,\n",
       "   0.3952380952380952,\n",
       "   0.35962236746550474,\n",
       "   0.3681024774774775,\n",
       "   0.4189522856189523,\n",
       "   0.43450088636224704,\n",
       "   0.4303482587064676,\n",
       "   0.3760064412238325,\n",
       "   0.42270531400966177,\n",
       "   0.3833298947802765,\n",
       "   0.40373134328358207,\n",
       "   0.4042285426629751,\n",
       "   0.4355089355089355,\n",
       "   0.42339181286549704,\n",
       "   0.4332171893147503,\n",
       "   0.411965811965812,\n",
       "   0.4022435897435897,\n",
       "   0.25219298245614036,\n",
       "   0.40113717128642506,\n",
       "   0.4177323103154305,\n",
       "   0.450109649122807,\n",
       "   0.3598541980894922,\n",
       "   0.4240472356414385,\n",
       "   0.40606991932385705,\n",
       "   0.3235294117647059,\n",
       "   0.42916993977481016,\n",
       "   0.4086662691313854,\n",
       "   0.4361168941321613,\n",
       "   0.4248826291079812,\n",
       "   0.4139609027602987,\n",
       "   0.3756650474207726,\n",
       "   0.3225304640912626,\n",
       "   0.43441358024691357,\n",
       "   0.35510948905109485,\n",
       "   0.4348595443485954,\n",
       "   0.33302238805970147,\n",
       "   0.34259259259259256,\n",
       "   0.3830290736984449,\n",
       "   0.40075973409306737,\n",
       "   0.43663003663003663,\n",
       "   0.37746478873239436,\n",
       "   0.41570338058887674,\n",
       "   0.3952991452991453,\n",
       "   0.3821428571428571,\n",
       "   0.4092698412698413,\n",
       "   0.3695781342840167,\n",
       "   0.38380969288123556,\n",
       "   0.38099747474747475,\n",
       "   0.3975507765830346,\n",
       "   0.40620782726045884,\n",
       "   0.4507936507936508,\n",
       "   0.4606392694063927,\n",
       "   0.4627039627039627,\n",
       "   0.37188208616780044,\n",
       "   0.3950617283950617,\n",
       "   0.4137715179968701,\n",
       "   0.3466666666666667,\n",
       "   0.4486834596885954,\n",
       "   0.37745098039215685,\n",
       "   0.4409321175278622,\n",
       "   0.4676056338028169,\n",
       "   0.38974358974358975,\n",
       "   0.43663003663003663,\n",
       "   0.3748951422252726,\n",
       "   0.39996114996114995,\n",
       "   0.3584149184149184,\n",
       "   0.38507090540751127,\n",
       "   0.4123292727943891,\n",
       "   0.42873563218390803,\n",
       "   0.38080495356037153,\n",
       "   0.3473350351976483,\n",
       "   0.34640522875817,\n",
       "   0.390452876376989,\n",
       "   0.40554840554840554,\n",
       "   0.38725490196078427,\n",
       "   0.4155692127597308,\n",
       "   0.4276623619689313,\n",
       "   0.4241545893719807,\n",
       "   0.41798941798941797,\n",
       "   0.4220551378446115,\n",
       "   0.4083542188805347,\n",
       "   0.35468409586056643,\n",
       "   0.35571542765787373,\n",
       "   0.3444168734491315,\n",
       "   0.3311415525114155,\n",
       "   0.3761904761904762,\n",
       "   0.4381046886628628,\n",
       "   0.4601503759398496,\n",
       "   0.44168560937226786,\n",
       "   0.39145715058123814,\n",
       "   0.4101259215763033,\n",
       "   0.3944055944055944,\n",
       "   0.4076804915514593,\n",
       "   0.47222222222222215,\n",
       "   0.46694383304801496,\n",
       "   0.3867521367521367,\n",
       "   0.4240669240669241,\n",
       "   0.42773029439696103,\n",
       "   0.37785640572018275,\n",
       "   0.43066723092114545,\n",
       "   0.3503787878787879,\n",
       "   0.45054945054945056,\n",
       "   0.368448098663926,\n",
       "   0.37916748865653976,\n",
       "   0.43209876543209874,\n",
       "   0.4525498142519419,\n",
       "   0.3289910600255428,\n",
       "   0.38825757575757575,\n",
       "   0.3772074405462679,\n",
       "   0.4139813660910332,\n",
       "   0.40539725614352484,\n",
       "   0.42156673735621103,\n",
       "   0.42916993977481016,\n",
       "   0.38252314814814814,\n",
       "   0.36743002544529263,\n",
       "   0.35536515707874333,\n",
       "   0.41814326821141906,\n",
       "   0.33293508562325763,\n",
       "   0.4700307983890073,\n",
       "   0.3549587257295663,\n",
       "   0.36889991728701405,\n",
       "   0.36491050320837554,\n",
       "   0.41385281385281386,\n",
       "   0.39012255794199735,\n",
       "   0.34935644864722876,\n",
       "   0.351201275496179,\n",
       "   0.36007130124777187,\n",
       "   0.4163715072805982,\n",
       "   0.3740942028985507,\n",
       "   0.42128603104212864,\n",
       "   0.4084757834757835,\n",
       "   0.4056189640035119,\n",
       "   0.3770739064856712,\n",
       "   0.41572184429327286,\n",
       "   0.41290139244103435,\n",
       "   0.37315849117694677,\n",
       "   0.44910714285714287,\n",
       "   0.37142857142857144,\n",
       "   0.4381046886628628,\n",
       "   0.4021695460024106,\n",
       "   0.349264705882353,\n",
       "   0.45976002341235,\n",
       "   0.3561927518916766,\n",
       "   0.4142857142857143,\n",
       "   0.41582410528005137,\n",
       "   0.45441795231416554,\n",
       "   0.4496423117112773,\n",
       "   0.411764705882353,\n",
       "   0.41087470449172575,\n",
       "   0.4104627766599598,\n",
       "   0.3634428223844282,\n",
       "   0.43935483870967745,\n",
       "   0.38553473667977484,\n",
       "   0.4057859703020994,\n",
       "   0.39873916469661147,\n",
       "   0.41156759906759904,\n",
       "   0.38255813953488377,\n",
       "   0.42358126721763084,\n",
       "   0.421875,\n",
       "   0.4348348348348348,\n",
       "   0.3592862935928629,\n",
       "   0.4189033189033189,\n",
       "   0.39344815940560623,\n",
       "   0.369813041495668,\n",
       "   0.4230769230769231,\n",
       "   0.3965085126609627,\n",
       "   0.35777777777777775,\n",
       "   0.3628287135749822,\n",
       "   0.47839095744680854,\n",
       "   0.3685789276340457,\n",
       "   0.4754901960784314,\n",
       "   0.3965591397849462,\n",
       "   0.3569023569023569,\n",
       "   0.37540906965871895,\n",
       "   0.3163596966413868,\n",
       "   0.43739805571866636,\n",
       "   0.4152096659559346,\n",
       "   0.4468671679197995,\n",
       "   0.40896057347670256,\n",
       "   0.3553656220322887,\n",
       "   0.36195286195286197,\n",
       "   0.38675595238095234,\n",
       "   0.41798941798941797,\n",
       "   0.4304213771839671,\n",
       "   0.38301282051282054,\n",
       "   0.3055555555555555,\n",
       "   0.43725768321513003,\n",
       "   0.4195538057742782,\n",
       "   0.4825650118203309,\n",
       "   0.443001443001443,\n",
       "   0.3733333333333333,\n",
       "   0.35287846481876334,\n",
       "   0.40594325668952536,\n",
       "   0.35543766578249336,\n",
       "   0.37948717948717947,\n",
       "   0.3313875168387966,\n",
       "   0.3882783882783883,\n",
       "   0.4001349527665317,\n",
       "   0.38600988829884636,\n",
       "   0.35709876543209873,\n",
       "   0.4087145969498911,\n",
       "   0.38184397163120565,\n",
       "   0.3845315904139433,\n",
       "   0.3582718651211802,\n",
       "   0.42140565762613,\n",
       "   0.3773809523809524,\n",
       "   0.38395989974937345,\n",
       "   0.44122383252818037,\n",
       "   0.4011286509570655,\n",
       "   0.4085385878489327,\n",
       "   0.3733108108108108,\n",
       "   0.526829268292683,\n",
       "   0.40432493953620713,\n",
       "   0.3535762483130904,\n",
       "   0.4277389277389278,\n",
       "   0.38907953938839324,\n",
       "   0.39057239057239057,\n",
       "   0.3562366964665815,\n",
       "   0.36459154189650644,\n",
       "   0.38847117794486213,\n",
       "   0.3992945326278659,\n",
       "   0.38392434988179663,\n",
       "   0.45705619412515963,\n",
       "   0.4222399857891464,\n",
       "   0.4153130481201861,\n",
       "   0.3819789939192924,\n",
       "   0.35689535689535684,\n",
       "   0.4289405684754522,\n",
       "   0.3808210784313726,\n",
       "   0.5074074074074074,\n",
       "   0.504116735958262,\n",
       "   0.40657759506680374,\n",
       "   0.3578082191780822,\n",
       "   0.45496632996632996,\n",
       "   0.3586372632174159,\n",
       "   0.41462434712736035,\n",
       "   0.3543123543123543,\n",
       "   0.4151161829157364,\n",
       "   0.2623891497130934,\n",
       "   0.3760147898078932,\n",
       "   0.39434129089301506,\n",
       "   0.44176553131777013,\n",
       "   0.42565947242206237,\n",
       "   0.4088500264970853,\n",
       "   0.44244469441319834,\n",
       "   0.38974358974358975,\n",
       "   0.43047619047619046,\n",
       "   0.45359566842065124,\n",
       "   0.3825533848250795,\n",
       "   0.38941798941798944,\n",
       "   0.4075774971297359,\n",
       "   0.38725490196078427,\n",
       "   0.41585555085343495,\n",
       "   0.3808095952023988,\n",
       "   0.39458955223880593,\n",
       "   0.36843559977888335,\n",
       "   0.40686274509803927,\n",
       "   0.3533853511200604,\n",
       "   0.41032001984619204,\n",
       "   0.39111592632719394,\n",
       "   0.4054395951929159,\n",
       "   0.4754521963824289,\n",
       "   0.41786283891547055],\n",
       "  'mean': 0.39788061669601726,\n",
       "  'std': 0.040328294507640944,\n",
       "  'ci_lower': 0.3197258379046834,\n",
       "  'ci_upper': 0.47005770482830883},\n",
       " 'emotion_single': {'values': [0.07279611031404432,\n",
       "   0.10710237181723055,\n",
       "   0.10891009575220101,\n",
       "   0.1291197992259313,\n",
       "   0.08769818862986563,\n",
       "   0.10570163549290684,\n",
       "   0.07937634126045635,\n",
       "   0.04949038185472293,\n",
       "   0.09231069476971117,\n",
       "   0.10224350645332937,\n",
       "   0.09619989238633307,\n",
       "   0.1057504873294347,\n",
       "   0.08399900460370785,\n",
       "   0.11460611715280646,\n",
       "   0.04495414979285947,\n",
       "   0.09298046914325984,\n",
       "   0.08709935897435898,\n",
       "   0.073334109972041,\n",
       "   0.08484848484848485,\n",
       "   0.07159884069901047,\n",
       "   0.12805857815043362,\n",
       "   0.08553459119496855,\n",
       "   0.0630863783037696,\n",
       "   0.11927437641723355,\n",
       "   0.0415819209039548,\n",
       "   0.07827721923466603,\n",
       "   0.06908382066276803,\n",
       "   0.06931731502669718,\n",
       "   0.0951035781544256,\n",
       "   0.09240958788898233,\n",
       "   0.06019862490450725,\n",
       "   0.09596603768866215,\n",
       "   0.06324692038977753,\n",
       "   0.07649640315896473,\n",
       "   0.06153016473844338,\n",
       "   0.12237934330957585,\n",
       "   0.05368589743589743,\n",
       "   0.10072017534704103,\n",
       "   0.10028527540048277,\n",
       "   0.06122477976697926,\n",
       "   0.09487463755756438,\n",
       "   0.10202477217302015,\n",
       "   0.05123413987324064,\n",
       "   0.08268921095008051,\n",
       "   0.051754385964912275,\n",
       "   0.12886095298334388,\n",
       "   0.05551437556154537,\n",
       "   0.07996632996632996,\n",
       "   0.09210526315789473,\n",
       "   0.12303143844037234,\n",
       "   0.13987157889596916,\n",
       "   0.07106988667976645,\n",
       "   0.13977961928781601,\n",
       "   0.0840343823979125,\n",
       "   0.10397946084724004,\n",
       "   0.11360187680942398,\n",
       "   0.10431891578788673,\n",
       "   0.06657931990000301,\n",
       "   0.07529090789560187,\n",
       "   0.10384933379972654,\n",
       "   0.07647243107769423,\n",
       "   0.13191712562286914,\n",
       "   0.05706163062239821,\n",
       "   0.12926811750341163,\n",
       "   0.10844678670765627,\n",
       "   0.09925444925444926,\n",
       "   0.0743145743145743,\n",
       "   0.091287223303771,\n",
       "   0.10600639670407112,\n",
       "   0.09170274170274169,\n",
       "   0.09268053671038745,\n",
       "   0.1292404979909097,\n",
       "   0.07323687323687324,\n",
       "   0.10615768670688579,\n",
       "   0.10607315389924084,\n",
       "   0.04109014675052411,\n",
       "   0.10454291365669073,\n",
       "   0.08025865539452497,\n",
       "   0.11525234025234025,\n",
       "   0.09212242545575879,\n",
       "   0.1123375478743009,\n",
       "   0.07990550179455726,\n",
       "   0.06681849551414769,\n",
       "   0.1180636777128005,\n",
       "   0.09624284990581304,\n",
       "   0.1048474945533769,\n",
       "   0.09014988109064764,\n",
       "   0.05158730158730159,\n",
       "   0.11462332217049198,\n",
       "   0.05398907103825137,\n",
       "   0.06188526188526189,\n",
       "   0.11289708111438045,\n",
       "   0.07404209547066691,\n",
       "   0.0935064935064935,\n",
       "   0.0985797827903091,\n",
       "   0.12839727322485942,\n",
       "   0.08753700910563655,\n",
       "   0.12623326286116984,\n",
       "   0.03193494452044384,\n",
       "   0.1151736237511629,\n",
       "   0.10580275826177465,\n",
       "   0.11968078694216072,\n",
       "   0.09463051568314725,\n",
       "   0.0911714762008304,\n",
       "   0.10860919660261766,\n",
       "   0.06525903714809261,\n",
       "   0.07851313783517173,\n",
       "   0.1472450277520814,\n",
       "   0.0960200293533627,\n",
       "   0.09307817133904091,\n",
       "   0.09757799671592775,\n",
       "   0.059964726631393295,\n",
       "   0.05018895122331308,\n",
       "   0.12862318840579712,\n",
       "   0.08796296296296297,\n",
       "   0.10029137529137529,\n",
       "   0.08302795395818652,\n",
       "   0.09326976229961305,\n",
       "   0.07678678678678678,\n",
       "   0.10555555555555556,\n",
       "   0.07320653987320654,\n",
       "   0.11665590008613265,\n",
       "   0.06871213932727561,\n",
       "   0.08932980599647267,\n",
       "   0.0469399881164587,\n",
       "   0.08081996882486478,\n",
       "   0.09580598228912489,\n",
       "   0.11686633779657037,\n",
       "   0.09366397721879449,\n",
       "   0.040145801910507796,\n",
       "   0.07229437229437229,\n",
       "   0.11927935687625608,\n",
       "   0.0907888407888408,\n",
       "   0.13060253699788585,\n",
       "   0.09949470504195983,\n",
       "   0.11819923371647507,\n",
       "   0.10631663418548665,\n",
       "   0.0966286799620133,\n",
       "   0.09452003023431595,\n",
       "   0.10846560846560845,\n",
       "   0.08636598270744611,\n",
       "   0.09355843786790079,\n",
       "   0.10297979797979799,\n",
       "   0.09809750137618989,\n",
       "   0.058251366120218584,\n",
       "   0.09818361453108375,\n",
       "   0.12044708165397822,\n",
       "   0.08456790123456791,\n",
       "   0.09426038614679588,\n",
       "   0.09122101123741815,\n",
       "   0.09567005321722304,\n",
       "   0.1141923436041083,\n",
       "   0.06496212121212121,\n",
       "   0.07896890343698854,\n",
       "   0.04983568075117371,\n",
       "   0.06762477718360071,\n",
       "   0.04426151075087245,\n",
       "   0.08593387663155105,\n",
       "   0.11649616368286446,\n",
       "   0.09386388677006526,\n",
       "   0.10813260813260812,\n",
       "   0.08577212575745223,\n",
       "   0.07511606886067627,\n",
       "   0.10436283741368486,\n",
       "   0.13386243386243388,\n",
       "   0.08788706739526413,\n",
       "   0.06481481481481481,\n",
       "   0.08263024195227585,\n",
       "   0.0916014140813427,\n",
       "   0.1102901291580537,\n",
       "   0.0715686274509804,\n",
       "   0.06021505376344086,\n",
       "   0.07934223282869714,\n",
       "   0.1179330619629127,\n",
       "   0.09621848739495797,\n",
       "   0.11544011544011545,\n",
       "   0.09563386524822694,\n",
       "   0.1311842918985776,\n",
       "   0.10593979663747105,\n",
       "   0.16320987654320987,\n",
       "   0.09531414716424204,\n",
       "   0.10173779602670979,\n",
       "   0.08858014304867821,\n",
       "   0.0539311241065627,\n",
       "   0.077931108258451,\n",
       "   0.10820810564663023,\n",
       "   0.09485251590514748,\n",
       "   0.08366437484084543,\n",
       "   0.07806186868686868,\n",
       "   0.07830009496676163,\n",
       "   0.09818401937046005,\n",
       "   0.11039889336274768,\n",
       "   0.1247274231145199,\n",
       "   0.08730158730158731,\n",
       "   0.06558485463150777,\n",
       "   0.11283121065729762,\n",
       "   0.1293831187812036,\n",
       "   0.0837882547559967,\n",
       "   0.10893323829888335,\n",
       "   0.08200878723266782,\n",
       "   0.09564258725024526,\n",
       "   0.0812738319715064,\n",
       "   0.08402851628658081,\n",
       "   0.09798893916540975,\n",
       "   0.07396135265700483,\n",
       "   0.08327033301699133,\n",
       "   0.08889902877847856,\n",
       "   0.08770762941087788,\n",
       "   0.07732224202812438,\n",
       "   0.05944584964715937,\n",
       "   0.10522959718761275,\n",
       "   0.13809199603444774,\n",
       "   0.06559851000368944,\n",
       "   0.09741940254362615,\n",
       "   0.09151804479673332,\n",
       "   0.10902777777777778,\n",
       "   0.06787717252484869,\n",
       "   0.09876543209876543,\n",
       "   0.10389610389610389,\n",
       "   0.09489791532344723,\n",
       "   0.06016628873771731,\n",
       "   0.07347032312275609,\n",
       "   0.15043720190779014,\n",
       "   0.09311660561660562,\n",
       "   0.08709784411276948,\n",
       "   0.10118389533812418,\n",
       "   0.1084180312441182,\n",
       "   0.10494224346600484,\n",
       "   0.104880791722897,\n",
       "   0.04697128240902221,\n",
       "   0.0873484340559419,\n",
       "   0.0687488268682206,\n",
       "   0.1174785623061485,\n",
       "   0.05963164251207729,\n",
       "   0.1411532597973276,\n",
       "   0.10610346958020356,\n",
       "   0.05960866526904263,\n",
       "   0.05333333333333334,\n",
       "   0.11095013477088948,\n",
       "   0.08882704596990311,\n",
       "   0.09000246055423984,\n",
       "   0.11664851125635438,\n",
       "   0.07503813504432123,\n",
       "   0.03153300600109111,\n",
       "   0.13243431031122635,\n",
       "   0.09307081807081807,\n",
       "   0.09299142998160363,\n",
       "   0.08137618301552728,\n",
       "   0.07906787127996738,\n",
       "   0.07591896407685882,\n",
       "   0.1079719387755102,\n",
       "   0.06142857142857142,\n",
       "   0.11743589743589744,\n",
       "   0.0662202380952381,\n",
       "   0.10184417027559804,\n",
       "   0.11666666666666665,\n",
       "   0.10467182274247493,\n",
       "   0.07605016815543131,\n",
       "   0.08032293377120964,\n",
       "   0.08561284162787923,\n",
       "   0.11547619047619047,\n",
       "   0.08088246528043024,\n",
       "   0.07647243107769423,\n",
       "   0.07572871572871574,\n",
       "   0.08203590512436089,\n",
       "   0.059418550797861146,\n",
       "   0.13905345069524175,\n",
       "   0.07929252782193959,\n",
       "   0.09876543209876543,\n",
       "   0.08274991077066689,\n",
       "   0.15385094182545303,\n",
       "   0.08576421273812969,\n",
       "   0.061801685755089465,\n",
       "   0.08138664270739743,\n",
       "   0.09143763213530655,\n",
       "   0.10613433946767281,\n",
       "   0.10901788229351828,\n",
       "   0.06495405179615706,\n",
       "   0.07407407407407407,\n",
       "   0.09448256146369353,\n",
       "   0.1440251572327044,\n",
       "   0.05057821832507307,\n",
       "   0.08534322820037106,\n",
       "   0.07646968722237539,\n",
       "   0.0623015873015873,\n",
       "   0.041941150636802815,\n",
       "   0.09325136612021857,\n",
       "   0.06948921842538863,\n",
       "   0.0902014652014652,\n",
       "   0.09806990088680229,\n",
       "   0.1144833427442123,\n",
       "   0.10573073073073073,\n",
       "   0.09336917562724016,\n",
       "   0.0814067728961346,\n",
       "   0.09694194694194695,\n",
       "   0.11115267711012393,\n",
       "   0.059697855750487326,\n",
       "   0.11595710627400768,\n",
       "   0.0905055222752451,\n",
       "   0.10475378787878788,\n",
       "   0.07081219945985112,\n",
       "   0.08065880479673583,\n",
       "   0.10319082648592189,\n",
       "   0.08833216518226006,\n",
       "   0.11915954415954415,\n",
       "   0.06522925554402552,\n",
       "   0.07496859392713791,\n",
       "   0.10843710927731932,\n",
       "   0.08172465060957479,\n",
       "   0.08889902877847856,\n",
       "   0.08152707036408224,\n",
       "   0.09760200878844948,\n",
       "   0.07055175476228108,\n",
       "   0.08105994152046785,\n",
       "   0.09793345977556504,\n",
       "   0.12825118631927657,\n",
       "   0.0762801878354204,\n",
       "   0.10777933613838242,\n",
       "   0.0936112906701142,\n",
       "   0.08181605975723623,\n",
       "   0.13127884723629404,\n",
       "   0.11425017768301349,\n",
       "   0.06092012562600798,\n",
       "   0.08440441567376551,\n",
       "   0.11699033680457831,\n",
       "   0.09563949352681746,\n",
       "   0.09076023391812865,\n",
       "   0.07813174050107848,\n",
       "   0.08119778466159767,\n",
       "   0.09451659451659451,\n",
       "   0.05473447443493526,\n",
       "   0.09353741496598639,\n",
       "   0.08037058410276997,\n",
       "   0.04606754982694832,\n",
       "   0.07178451178451178,\n",
       "   0.11482607227288077,\n",
       "   0.08391056696141443,\n",
       "   0.07349956111476848,\n",
       "   0.05717712914834066,\n",
       "   0.05764698951337339,\n",
       "   0.08842805939580134,\n",
       "   0.09894179894179894,\n",
       "   0.07459004090209764,\n",
       "   0.089126386001386,\n",
       "   0.09351427930662903,\n",
       "   0.121278952981497,\n",
       "   0.09678011344678011,\n",
       "   0.11326338444982513,\n",
       "   0.10552730672494243,\n",
       "   0.09198412698412699,\n",
       "   0.10170296328488419,\n",
       "   0.11562446188285651,\n",
       "   0.08385547201336674,\n",
       "   0.11036706349206349,\n",
       "   0.08799078202063276,\n",
       "   0.07335722819593787,\n",
       "   0.07412819014221704,\n",
       "   0.08038110574342457,\n",
       "   0.13413788119670472,\n",
       "   0.0717911877394636,\n",
       "   0.123471557682084,\n",
       "   0.1441298228183474,\n",
       "   0.08897973603855958,\n",
       "   0.11578975990428801,\n",
       "   0.08449829139484312,\n",
       "   0.07723214285714286,\n",
       "   0.09791128337639965,\n",
       "   0.06096866096866097,\n",
       "   0.1058211424590735,\n",
       "   0.1188465107425825,\n",
       "   0.12383519557120704,\n",
       "   0.1018397798058815,\n",
       "   0.06973995271867613,\n",
       "   0.07154185274247109,\n",
       "   0.1037037037037037,\n",
       "   0.1073037323037323,\n",
       "   0.05828664799253034,\n",
       "   0.07655820881627333,\n",
       "   0.10859456635318704,\n",
       "   0.12342992165153588,\n",
       "   0.11960237181723055,\n",
       "   0.08061974789915967,\n",
       "   0.0707070707070707,\n",
       "   0.09589528118939883,\n",
       "   0.07696212813483601,\n",
       "   0.12336601307189543,\n",
       "   0.12645477545857053,\n",
       "   0.09836601307189542,\n",
       "   0.05584896584896584,\n",
       "   0.10277777777777779,\n",
       "   0.07484367484367484,\n",
       "   0.10043290043290043,\n",
       "   0.11756885090218423,\n",
       "   0.08169934640522876,\n",
       "   0.08320714807224136,\n",
       "   0.1227502102607233,\n",
       "   0.09868823000898473,\n",
       "   0.08116382538214174,\n",
       "   0.08230706075533663,\n",
       "   0.08359810479375697,\n",
       "   0.025285947712418304,\n",
       "   0.08301075268817204,\n",
       "   0.10626780626780626,\n",
       "   0.05376378974691759,\n",
       "   0.08342530530207183,\n",
       "   0.0991499066970765,\n",
       "   0.13786358568967264,\n",
       "   0.08915045630146823,\n",
       "   0.07431967776795363,\n",
       "   0.07211175068317925,\n",
       "   0.07447823494335122,\n",
       "   0.05979163715503586,\n",
       "   0.07831958397996135,\n",
       "   0.08713860544217687,\n",
       "   0.05944683908045977,\n",
       "   0.09288236605309774,\n",
       "   0.08804478897502153,\n",
       "   0.07040816326530613,\n",
       "   0.07193089121953691,\n",
       "   0.10202446040006148,\n",
       "   0.0984823248010996,\n",
       "   0.11758932891008363,\n",
       "   0.08400167084377612,\n",
       "   0.09993099879247887,\n",
       "   0.04935493132542395,\n",
       "   0.15312719959778784,\n",
       "   0.11690771065925669,\n",
       "   0.06389709207970841,\n",
       "   0.07572474614284425,\n",
       "   0.1095450128669208,\n",
       "   0.09755249617577527,\n",
       "   0.07884861407249467,\n",
       "   0.09216011357735314,\n",
       "   0.06948552325910816,\n",
       "   0.06390111681747779,\n",
       "   0.13908688562776614,\n",
       "   0.09734943419153945,\n",
       "   0.1259259259259259,\n",
       "   0.056143039591315454,\n",
       "   0.06011814982403218,\n",
       "   0.09432257751585482,\n",
       "   0.08469916930691508,\n",
       "   0.10286561788230308,\n",
       "   0.07719298245614035,\n",
       "   0.09543037554004652,\n",
       "   0.07479245697949309,\n",
       "   0.06885455911798379,\n",
       "   0.11948517432388399,\n",
       "   0.10879309888597814,\n",
       "   0.09772470322215941,\n",
       "   0.12487304356884293,\n",
       "   0.04815546772068511,\n",
       "   0.09034378398827257,\n",
       "   0.06765979364849421,\n",
       "   0.05434210629658428,\n",
       "   0.11179487179487181,\n",
       "   0.09161324786324787,\n",
       "   0.1187363834422658,\n",
       "   0.08542512881953575,\n",
       "   0.09588819864783547,\n",
       "   0.14568402611880873,\n",
       "   0.07309165929855585,\n",
       "   0.10425580860363469,\n",
       "   0.10268398268398267,\n",
       "   0.06901876930387264,\n",
       "   0.08242835595776772,\n",
       "   0.07500265816055289,\n",
       "   0.11683290433290433,\n",
       "   0.10205335205335204,\n",
       "   0.09625004282728611,\n",
       "   0.0874293710257364,\n",
       "   0.08084428514536042,\n",
       "   0.08637978095270726,\n",
       "   0.0718529836176895,\n",
       "   0.10996334118879288,\n",
       "   0.07996323529411764,\n",
       "   0.10150098902377536,\n",
       "   0.08189954272903581,\n",
       "   0.09722222222222221,\n",
       "   0.1058296783625731,\n",
       "   0.08948617967988427,\n",
       "   0.08750338845215505,\n",
       "   0.02471655328798186,\n",
       "   0.07528581212791739,\n",
       "   0.11381932021466906,\n",
       "   0.09293611793611793,\n",
       "   0.09476986543985833,\n",
       "   0.12707140756546775,\n",
       "   0.10732034301828353,\n",
       "   0.1454072668493921,\n",
       "   0.11703703703703705,\n",
       "   0.09750566893424036,\n",
       "   0.1086117173073695,\n",
       "   0.07107921318447634,\n",
       "   0.0861111111111111,\n",
       "   0.10411495679352822,\n",
       "   0.15998475319230035,\n",
       "   0.05748213535098781,\n",
       "   0.09257256617592842,\n",
       "   0.09120920581022447,\n",
       "   0.11791261673337144,\n",
       "   0.09401709401709402,\n",
       "   0.07666035353535354,\n",
       "   0.10915750915750916,\n",
       "   0.1183533447684391,\n",
       "   0.09845559845559847,\n",
       "   0.11686535764375877,\n",
       "   0.12588652482269502,\n",
       "   0.11756087050204696,\n",
       "   0.0961534319743275,\n",
       "   0.11447665980355755,\n",
       "   0.062248093681917215,\n",
       "   0.11252268602540834,\n",
       "   0.050489589775304065,\n",
       "   0.09810819262038774,\n",
       "   0.1002454991816694,\n",
       "   0.13588834333515185,\n",
       "   0.07012226512226512,\n",
       "   0.09659268743775785,\n",
       "   0.10713362557829158,\n",
       "   0.10598290598290598,\n",
       "   0.08719405594405594,\n",
       "   0.08267020335985854,\n",
       "   0.13977574967405476,\n",
       "   0.07164263655135866,\n",
       "   0.08138651304163426,\n",
       "   0.06179721745759481,\n",
       "   0.07692743764172336,\n",
       "   0.0815238579540639,\n",
       "   0.04725218284540319,\n",
       "   0.07626763315922254,\n",
       "   0.09015345268542198,\n",
       "   0.0639588561514391,\n",
       "   0.07861212861212862,\n",
       "   0.08406862745098038,\n",
       "   0.07619346794853062,\n",
       "   0.03770795365551969,\n",
       "   0.0964607170099161,\n",
       "   0.0927974489216659,\n",
       "   0.06799910900512322,\n",
       "   0.11465201465201465,\n",
       "   0.08083935328517372,\n",
       "   0.07132183908045976,\n",
       "   0.08621081200841918,\n",
       "   0.10151982520403573,\n",
       "   0.07378705083623116,\n",
       "   0.10707885304659498,\n",
       "   0.08658451364387858,\n",
       "   0.08225523459217216,\n",
       "   0.051169590643274844,\n",
       "   0.09226150851766617,\n",
       "   0.06742424242424243,\n",
       "   0.08606727574750832,\n",
       "   0.10443307757885763,\n",
       "   0.07103174603174603,\n",
       "   0.08576238576238576,\n",
       "   0.11009661835748792,\n",
       "   0.11666666666666665,\n",
       "   0.0985434632493456,\n",
       "   0.05735790571648446,\n",
       "   0.10200070200070199,\n",
       "   0.08472765464290888,\n",
       "   0.08671828623048135,\n",
       "   0.10664132615352127,\n",
       "   0.0921838646796202,\n",
       "   0.07773528667435255,\n",
       "   0.0866975677830941,\n",
       "   0.050552517657780814,\n",
       "   0.0774098539596507,\n",
       "   0.07816780642867599,\n",
       "   0.11771368954467547,\n",
       "   0.12544263885520252,\n",
       "   0.10320666987333654,\n",
       "   0.10412592746688858,\n",
       "   0.08210918861325366,\n",
       "   0.1137133209020994,\n",
       "   0.10892599464028035,\n",
       "   0.09969495920079312,\n",
       "   0.12160493827160494,\n",
       "   0.07997618526888775,\n",
       "   0.1318295739348371,\n",
       "   0.09325609031491384,\n",
       "   0.1005342027521391,\n",
       "   0.11592240931681626,\n",
       "   0.05773325590398761,\n",
       "   0.06258835494533435,\n",
       "   0.06821489825244519,\n",
       "   0.10778688524590163,\n",
       "   0.11140819964349376,\n",
       "   0.0966166724799442,\n",
       "   0.0728996511005639,\n",
       "   0.12378983268772918,\n",
       "   0.11984049917447988,\n",
       "   0.11786786786786786,\n",
       "   0.11117452545627038,\n",
       "   0.08888888888888889,\n",
       "   0.06953042974548351,\n",
       "   0.09252297410192147,\n",
       "   0.09590643274853801,\n",
       "   0.09864942618920587,\n",
       "   0.08765391740220115,\n",
       "   0.14379084967320263,\n",
       "   0.10712314093849133,\n",
       "   0.1301667367241138,\n",
       "   0.10321958743011374,\n",
       "   0.07272990434084659,\n",
       "   0.0589125160553732,\n",
       "   0.07372757613683771,\n",
       "   0.08521756021756022,\n",
       "   0.08042328042328041,\n",
       "   0.07799564270152505,\n",
       "   0.07964280710182349,\n",
       "   0.10378530627286846,\n",
       "   0.044223484848484845,\n",
       "   0.08222222222222221,\n",
       "   0.11248196248196247,\n",
       "   0.10650347095359884,\n",
       "   0.060057517604687416,\n",
       "   0.07313557901793195,\n",
       "   0.10249812615386163,\n",
       "   0.11197848902766934,\n",
       "   0.13233737939620294,\n",
       "   0.10168058193284839,\n",
       "   0.13910500889905925,\n",
       "   0.08364197530864197,\n",
       "   0.07953805453805453,\n",
       "   0.0865603421217828,\n",
       "   0.09985569985569986,\n",
       "   0.08013897866839044,\n",
       "   0.08073268106162843,\n",
       "   0.10421573647380099,\n",
       "   0.05288952745849298,\n",
       "   0.08057020669992873,\n",
       "   0.08161530550289332,\n",
       "   0.0753419647428864,\n",
       "   0.056017556017556015,\n",
       "   0.0939462653748368,\n",
       "   0.11935504504998447,\n",
       "   0.11675485008818343,\n",
       "   0.07198013896277668,\n",
       "   0.0856191222570533,\n",
       "   0.0761904761904762,\n",
       "   0.08390188434048083,\n",
       "   0.09393939393939395,\n",
       "   0.06416689642496094,\n",
       "   0.1177564661435629,\n",
       "   0.1086489898989899,\n",
       "   0.08315125420388579,\n",
       "   0.09087301587301588,\n",
       "   0.07663437199965085,\n",
       "   0.08928571428571429,\n",
       "   0.13739316239316238,\n",
       "   0.11169830313488639,\n",
       "   0.09272735795437899,\n",
       "   0.08416285534929602,\n",
       "   0.056964199821342675,\n",
       "   0.11678743961352657,\n",
       "   0.03964862298195631,\n",
       "   0.08759909786768727,\n",
       "   0.09175054637239512,\n",
       "   0.10473588342440802,\n",
       "   0.10205022359821121,\n",
       "   0.12067297238530116,\n",
       "   0.10045776410843288,\n",
       "   0.08675224795443374,\n",
       "   0.0953809881041689,\n",
       "   0.10849222318035195,\n",
       "   0.08049155145929339,\n",
       "   0.08109014675052412,\n",
       "   0.09428815004262574,\n",
       "   0.09375179752660338,\n",
       "   0.072856221792392,\n",
       "   0.09726146869004011,\n",
       "   0.1274074074074074,\n",
       "   0.11114189482833552,\n",
       "   0.08617722199811752,\n",
       "   0.08440783537561487,\n",
       "   0.09311740890688258,\n",
       "   0.13327664399092973,\n",
       "   0.08437418683320323,\n",
       "   0.10060981636324101,\n",
       "   0.07173582995951416,\n",
       "   0.056432748538011696,\n",
       "   0.13303143266000064,\n",
       "   0.0980634155482466,\n",
       "   0.08823302469135802,\n",
       "   0.07036844869774651,\n",
       "   0.07907741518113859,\n",
       "   0.06677462887989204,\n",
       "   0.08020050125313284,\n",
       "   0.10378413370998117,\n",
       "   0.07954387428730357,\n",
       "   0.09437996901468555,\n",
       "   0.0828579858379229,\n",
       "   0.09218559218559219,\n",
       "   0.0725240206947524,\n",
       "   0.09012452107279695,\n",
       "   0.07436197682099321,\n",
       "   0.08164983164983165,\n",
       "   0.08391550001391633,\n",
       "   0.11833865059671511,\n",
       "   0.10711474164837505,\n",
       "   0.09182754182754183,\n",
       "   0.08330839972631017,\n",
       "   0.1345761541599975,\n",
       "   0.09214368674298988,\n",
       "   0.08601295792946646,\n",
       "   0.06740196078431372,\n",
       "   0.09887955182072829,\n",
       "   0.09040583136327818,\n",
       "   0.09320561941251597,\n",
       "   0.059090512350528056,\n",
       "   0.13254350540878054,\n",
       "   0.09596594813029148,\n",
       "   0.07118388857519292,\n",
       "   0.10091058366920436,\n",
       "   0.09822733918128655,\n",
       "   0.08973216326157503,\n",
       "   0.06537444037444037,\n",
       "   0.11679159318048206,\n",
       "   0.10277777777777779,\n",
       "   0.09331779331779332,\n",
       "   0.08223409481699828,\n",
       "   0.0998861498861499,\n",
       "   0.13351722802942315,\n",
       "   0.11011142808377831,\n",
       "   0.10568429391958804,\n",
       "   0.11250672536072391,\n",
       "   0.07403913347309574,\n",
       "   0.06241565452091768,\n",
       "   0.1103626635839521,\n",
       "   0.10110420979986197,\n",
       "   0.13560253699788583,\n",
       "   0.09519230769230769,\n",
       "   0.1317070526372852,\n",
       "   0.08479897752215831,\n",
       "   0.09757326007326007,\n",
       "   0.14260416510562623,\n",
       "   0.08366772750334395,\n",
       "   0.125,\n",
       "   0.0570480928689884,\n",
       "   0.07013018914271678,\n",
       "   0.10456053223975871,\n",
       "   0.0528604118993135,\n",
       "   0.10891249235072997,\n",
       "   0.06896828454031843,\n",
       "   0.07763686978038974,\n",
       "   0.11000144738746563,\n",
       "   0.09022581443613031,\n",
       "   0.10461298947439672,\n",
       "   0.10890126707937516,\n",
       "   0.09545683075094841,\n",
       "   0.07680097680097679,\n",
       "   0.09389292440139897,\n",
       "   0.07671957671957672,\n",
       "   0.1221064814814815,\n",
       "   0.11537675984421109,\n",
       "   0.08663174081893293,\n",
       "   0.09138655462184875,\n",
       "   0.11251270343558696,\n",
       "   0.07896723337452821,\n",
       "   0.07777777777777778,\n",
       "   0.09288879535117565,\n",
       "   0.09399313751202028,\n",
       "   0.08829916225749558,\n",
       "   0.06414141414141414,\n",
       "   0.08422547544992774,\n",
       "   0.08808651026392962,\n",
       "   0.09778839047131731,\n",
       "   0.09275539275539275,\n",
       "   0.08419731060230613,\n",
       "   0.051077641693331864,\n",
       "   0.09230965113318053,\n",
       "   0.039938556067588324,\n",
       "   0.08718639850715322,\n",
       "   0.12731269861999914,\n",
       "   0.08382235048901715,\n",
       "   0.10149425287356321,\n",
       "   0.11534962212928314,\n",
       "   0.09918320673037655,\n",
       "   0.09323648487735176,\n",
       "   0.11422902494331066,\n",
       "   0.1130952380952381,\n",
       "   0.05997373376945187,\n",
       "   0.08259342461845672,\n",
       "   0.12116501384794069,\n",
       "   0.0947606142728094,\n",
       "   0.08383720930232558,\n",
       "   0.06410565220656984,\n",
       "   0.06638937769956806,\n",
       "   0.1042557111274871,\n",
       "   0.11621315192743764,\n",
       "   0.07453703703703704,\n",
       "   0.08648989898989899,\n",
       "   0.10171568627450982,\n",
       "   0.12086512912565471,\n",
       "   0.07691569348904174,\n",
       "   0.1282652951891139,\n",
       "   0.12262516459255297,\n",
       "   0.145468509984639,\n",
       "   0.10728501927086624,\n",
       "   0.10666304501236955,\n",
       "   0.07175825559114368,\n",
       "   0.12918408797941314,\n",
       "   0.09553526128868595,\n",
       "   0.07309941520467836,\n",
       "   0.05796409472880062,\n",
       "   0.08626564148952208,\n",
       "   0.087861516432945,\n",
       "   0.06253652834599649,\n",
       "   0.045365501598102896,\n",
       "   0.05885791866571968,\n",
       "   0.11397108158883522,\n",
       "   0.07328234990041506,\n",
       "   0.03890918594887365,\n",
       "   0.11886036931408982,\n",
       "   0.05438833406611813,\n",
       "   0.10441016375975726,\n",
       "   0.07201542588866533,\n",
       "   0.06593263283404129,\n",
       "   0.06951154052603328,\n",
       "   0.06565934065934066,\n",
       "   0.08956228956228957,\n",
       "   0.0746031746031746,\n",
       "   0.06689919557566616,\n",
       "   0.09368642836384772,\n",
       "   0.1026284348864994,\n",
       "   0.10496860496860498,\n",
       "   0.09658260989389555,\n",
       "   0.06746031746031746,\n",
       "   0.10396825396825397,\n",
       "   0.07323323983941044,\n",
       "   0.07269334690366279,\n",
       "   0.04659180457052797,\n",
       "   0.11323710614328464,\n",
       "   0.060769656699889256,\n",
       "   0.10354775248392271,\n",
       "   0.04126602564102564,\n",
       "   0.07607843137254904,\n",
       "   0.1456081465468199,\n",
       "   0.11341328921974082,\n",
       "   0.12943284840818048,\n",
       "   0.1134494853659939,\n",
       "   0.09479298568742565,\n",
       "   0.05064499484004128,\n",
       "   0.14007936507936505,\n",
       "   0.07807334941481282,\n",
       "   0.10444680621659504,\n",
       "   0.07027001465928973,\n",
       "   0.07796535759897828,\n",
       "   0.1166415284062343,\n",
       "   0.10422117564974709,\n",
       "   0.06161468486029889,\n",
       "   0.09060090890691984,\n",
       "   0.08232410884267878,\n",
       "   0.11481435421243903,\n",
       "   0.0658166706555558,\n",
       "   0.11243873530448152,\n",
       "   0.0744316639185225,\n",
       "   0.08570111873887619,\n",
       "   0.09776730414816158,\n",
       "   0.08995890022675736,\n",
       "   0.08343915082152849,\n",
       "   0.07182142028195736,\n",
       "   0.08389790024536947,\n",
       "   0.10754579057707163,\n",
       "   0.07909428744274478,\n",
       "   0.06819735095597164,\n",
       "   0.08722943722943723,\n",
       "   0.09005291005291005,\n",
       "   0.12073435931749749,\n",
       "   0.07464678178963892,\n",
       "   0.11335497054934486,\n",
       "   0.1271089271089271,\n",
       "   0.08833027843239939,\n",
       "   0.14292853573213393,\n",
       "   0.10105444690810544,\n",
       "   0.09451779259864192,\n",
       "   0.0475297763033612,\n",
       "   0.07591093117408908,\n",
       "   0.11855579944177157,\n",
       "   0.07814236805833445,\n",
       "   0.14741382974202047,\n",
       "   0.1227994227994228,\n",
       "   0.06829161176987264,\n",
       "   0.10090483348751156,\n",
       "   0.0752723311546841,\n",
       "   0.10746880238405661,\n",
       "   0.10719770807490105,\n",
       "   0.10142315379790982,\n",
       "   0.09686379928315413,\n",
       "   0.10611012081600318,\n",
       "   0.07868821999256781,\n",
       "   0.10045672516398511,\n",
       "   0.09499438832772167,\n",
       "   0.0885587821071692,\n",
       "   0.10762979530542756,\n",
       "   0.14356295488994952,\n",
       "   0.08676136363636362,\n",
       "   0.07764482604636058,\n",
       "   0.056644880174291944,\n",
       "   0.12587379779498006,\n",
       "   0.11869122632711933,\n",
       "   0.051220344768731864,\n",
       "   0.08507665174331841,\n",
       "   0.07246912390072517,\n",
       "   0.0725635593220339,\n",
       "   0.0656225592962552,\n",
       "   0.12063389058846553,\n",
       "   0.10977443609022557,\n",
       "   0.1257082318239945,\n",
       "   0.08700096200096201,\n",
       "   0.07675997419832799,\n",
       "   0.15318443443443444,\n",
       "   0.1249478390461997,\n",
       "   0.10655400408165534,\n",
       "   0.06089901477832512,\n",
       "   0.11213486213486214,\n",
       "   0.13555806938159878,\n",
       "   0.14105339105339107,\n",
       "   0.06722566075610571,\n",
       "   0.09791319791319791,\n",
       "   0.15581502406361164,\n",
       "   0.06962121596267938,\n",
       "   0.11201318335752297,\n",
       "   0.11542791007388148,\n",
       "   0.06557922019382466,\n",
       "   0.07802695143656242,\n",
       "   0.12891138790202164,\n",
       "   0.0922083639031548,\n",
       "   0.11367197236762455,\n",
       "   0.09117977698781636,\n",
       "   0.10835718622603868,\n",
       "   0.05185338887913468,\n",
       "   0.09427924576603386,\n",
       "   0.10567940404293415,\n",
       "   0.06276701440635866,\n",
       "   0.08675213675213676,\n",
       "   0.08333333333333333,\n",
       "   0.07990922568870192,\n",
       "   0.08701412409852309,\n",
       "   0.06790123456790123,\n",
       "   0.09320082545888997,\n",
       "   0.12750290083603585,\n",
       "   0.08968253968253968,\n",
       "   0.09306480150933066,\n",
       "   0.06890868020777878,\n",
       "   0.1176989314244216,\n",
       "   0.13048394391677973,\n",
       "   0.04014272970561998,\n",
       "   0.13200710108604846,\n",
       "   0.08340935352429606,\n",
       "   0.0797486401322182,\n",
       "   0.07783389450056118,\n",
       "   0.09595959595959595,\n",
       "   0.11132345914954611,\n",
       "   0.09232370754921047,\n",
       "   0.1138095238095238,\n",
       "   0.08166150670794634,\n",
       "   0.06924926418770468,\n",
       "   0.11948545281878616,\n",
       "   0.07705072705072705,\n",
       "   0.10185814550047528,\n",
       "   0.08416967750568714,\n",
       "   0.10249845069963143,\n",
       "   0.07030139451192081,\n",
       "   0.07037037037037037,\n",
       "   0.1054924893934182,\n",
       "   0.08685064935064934,\n",
       "   0.06205280981845033,\n",
       "   0.13325353784621144,\n",
       "   0.10574677632246927,\n",
       "   0.11323196639914046,\n",
       "   0.09224598930481283,\n",
       "   0.06934411500449238,\n",
       "   0.10323541932737335,\n",
       "   0.06547431820035182,\n",
       "   0.07229280096793708,\n",
       "   0.11549595572584077,\n",
       "   0.10677824504764737,\n",
       "   0.04812199204854572,\n",
       "   0.09260181037184521,\n",
       "   0.10040218728986666,\n",
       "   0.10492830637114765,\n",
       "   0.11404465474232917,\n",
       "   0.08823660780752167,\n",
       "   0.12043749506436074,\n",
       "   0.047856560278920525,\n",
       "   0.10143937704430568,\n",
       "   0.11306216931216932,\n",
       "   0.09837164750957854,\n",
       "   0.10602185538246918,\n",
       "   0.07531526444569922,\n",
       "   0.0737647209421403,\n",
       "   0.11106386999244143,\n",
       "   0.10269764957264955,\n",
       "   0.0912750986280398,\n",
       "   0.09293826417114089,\n",
       "   0.043716737265124365,\n",
       "   0.09638520508085725],\n",
       "  'mean': 0.09188317101379244,\n",
       "  'std': 0.022839551186346133,\n",
       "  'ci_lower': 0.04784839067953154,\n",
       "  'ci_upper': 0.13912177741843412},\n",
       " 'multitask_sentiment': {'values': [0.36790505675954593,\n",
       "   0.42578668894458366,\n",
       "   0.3408816425120773,\n",
       "   0.40040962621607784,\n",
       "   0.36787391012743126,\n",
       "   0.3749559082892416,\n",
       "   0.40585241730279903,\n",
       "   0.46833631484794275,\n",
       "   0.4710924107838554,\n",
       "   0.34200072228241246,\n",
       "   0.3753086419753086,\n",
       "   0.3861194527861194,\n",
       "   0.3984504703929164,\n",
       "   0.45359566842065124,\n",
       "   0.43843951324266284,\n",
       "   0.39345238095238094,\n",
       "   0.4323089971944934,\n",
       "   0.36372549019607847,\n",
       "   0.43739805571866636,\n",
       "   0.40224828934506346,\n",
       "   0.3861751152073733,\n",
       "   0.4166964818127609,\n",
       "   0.27499091239549256,\n",
       "   0.41058491573247924,\n",
       "   0.5107025490718536,\n",
       "   0.4513457556935818,\n",
       "   0.4274672187715665,\n",
       "   0.4398400581606688,\n",
       "   0.43865137295794226,\n",
       "   0.3686868686868687,\n",
       "   0.45912614517265676,\n",
       "   0.45600331380555065,\n",
       "   0.3930183056619838,\n",
       "   0.3861194527861194,\n",
       "   0.4650501421942823,\n",
       "   0.47982917715244416,\n",
       "   0.4245728825881498,\n",
       "   0.4256410256410256,\n",
       "   0.39989639989639986,\n",
       "   0.4598674001974891,\n",
       "   0.45194805194805193,\n",
       "   0.4535198840360497,\n",
       "   0.4976372228280625,\n",
       "   0.35954226651901067,\n",
       "   0.3987654320987654,\n",
       "   0.3398215733982157,\n",
       "   0.4494238156209987,\n",
       "   0.40234479083399943,\n",
       "   0.4391025641025641,\n",
       "   0.3671641791044776,\n",
       "   0.3758483800742733,\n",
       "   0.40559440559440557,\n",
       "   0.38647764449291166,\n",
       "   0.35993740219092335,\n",
       "   0.38013638748032874,\n",
       "   0.4081951219512195,\n",
       "   0.3376623376623377,\n",
       "   0.34638447971781305,\n",
       "   0.46659044715447157,\n",
       "   0.4462659380692167,\n",
       "   0.4393518518518518,\n",
       "   0.38138858596873865,\n",
       "   0.4219858156028369,\n",
       "   0.34830418653948064,\n",
       "   0.3642178910544727,\n",
       "   0.3376623376623377,\n",
       "   0.45780682643427745,\n",
       "   0.37367724867724866,\n",
       "   0.43163841807909603,\n",
       "   0.4326599326599327,\n",
       "   0.41770833333333335,\n",
       "   0.40552385677664765,\n",
       "   0.4653511862814188,\n",
       "   0.40694789081885857,\n",
       "   0.38022011156339514,\n",
       "   0.4089781746031746,\n",
       "   0.38359788359788355,\n",
       "   0.4869976359338062,\n",
       "   0.4419564602770709,\n",
       "   0.4319744204636291,\n",
       "   0.38737373737373737,\n",
       "   0.35827326207639404,\n",
       "   0.4917517674783975,\n",
       "   0.5173333333333333,\n",
       "   0.3843078460769615,\n",
       "   0.36758824552251784,\n",
       "   0.47979323308270677,\n",
       "   0.44047619047619047,\n",
       "   0.35572232645403384,\n",
       "   0.4599277445992775,\n",
       "   0.3602060707323865,\n",
       "   0.35936724125700503,\n",
       "   0.34612310151878495,\n",
       "   0.43915343915343913,\n",
       "   0.4406224406224406,\n",
       "   0.38832341079316307,\n",
       "   0.4066666666666667,\n",
       "   0.4263915753277456,\n",
       "   0.3922101449275362,\n",
       "   0.4024864024864025,\n",
       "   0.38413547237076645,\n",
       "   0.32888888888888884,\n",
       "   0.344988344988345,\n",
       "   0.4268199233716475,\n",
       "   0.40019607843137256,\n",
       "   0.4573514769332891,\n",
       "   0.415812804302013,\n",
       "   0.32471875950136825,\n",
       "   0.42764507920873346,\n",
       "   0.4257575757575758,\n",
       "   0.3755112006348044,\n",
       "   0.4398400581606688,\n",
       "   0.4588103254769922,\n",
       "   0.43988603988603986,\n",
       "   0.39710144927536234,\n",
       "   0.49069643806485913,\n",
       "   0.47105508870214746,\n",
       "   0.39805959805959806,\n",
       "   0.45646215475437485,\n",
       "   0.41655420602789023,\n",
       "   0.42437093582131746,\n",
       "   0.3981566820276498,\n",
       "   0.4446428571428571,\n",
       "   0.3387654320987654,\n",
       "   0.3747332479726846,\n",
       "   0.4090757424090758,\n",
       "   0.41744382271960206,\n",
       "   0.43045843045843046,\n",
       "   0.3773968253968254,\n",
       "   0.4226044226044226,\n",
       "   0.4166666666666667,\n",
       "   0.37854030501089325,\n",
       "   0.44485294117647056,\n",
       "   0.3408963585434173,\n",
       "   0.35056390977443613,\n",
       "   0.44005137655665383,\n",
       "   0.4134038800705467,\n",
       "   0.361811631497684,\n",
       "   0.4028709917971663,\n",
       "   0.3991612657262677,\n",
       "   0.4519295414817803,\n",
       "   0.4547014413177763,\n",
       "   0.44166666666666665,\n",
       "   0.378724589250905,\n",
       "   0.4574350469872858,\n",
       "   0.4134038800705467,\n",
       "   0.35824915824915826,\n",
       "   0.41527655838454786,\n",
       "   0.414426523297491,\n",
       "   0.34181343770384864,\n",
       "   0.3952380952380952,\n",
       "   0.4322916666666667,\n",
       "   0.43333333333333335,\n",
       "   0.48395061728395056,\n",
       "   0.40346729708431833,\n",
       "   0.4035914702581369,\n",
       "   0.41322819243230025,\n",
       "   0.41527655838454786,\n",
       "   0.3848133848133848,\n",
       "   0.3810359964881475,\n",
       "   0.4289405684754522,\n",
       "   0.39337085678549094,\n",
       "   0.4819166993080037,\n",
       "   0.4392271949131817,\n",
       "   0.3438295165394402,\n",
       "   0.43695159223109536,\n",
       "   0.4232507402212872,\n",
       "   0.3986031746031746,\n",
       "   0.3994708994708995,\n",
       "   0.38967928623101034,\n",
       "   0.469359808284834,\n",
       "   0.4061887974931453,\n",
       "   0.40291858678955456,\n",
       "   0.4200779727095516,\n",
       "   0.3402433090024331,\n",
       "   0.4166278166278166,\n",
       "   0.4419753086419753,\n",
       "   0.39345238095238094,\n",
       "   0.4289473684210526,\n",
       "   0.3879428459581131,\n",
       "   0.4513457556935818,\n",
       "   0.36410256410256414,\n",
       "   0.4396825396825397,\n",
       "   0.40657759506680374,\n",
       "   0.4638346727898967,\n",
       "   0.37664473684210525,\n",
       "   0.35409386776293256,\n",
       "   0.37654320987654316,\n",
       "   0.4252204585537919,\n",
       "   0.3988380537400145,\n",
       "   0.4269162210338681,\n",
       "   0.39515976555114385,\n",
       "   0.47296130601993563,\n",
       "   0.3954470073873059,\n",
       "   0.4334521687462864,\n",
       "   0.39338061465721036,\n",
       "   0.3815178815178815,\n",
       "   0.3446630888491354,\n",
       "   0.46649703138252757,\n",
       "   0.39637188208616775,\n",
       "   0.4362170542635659,\n",
       "   0.39844866299244747,\n",
       "   0.3474903474903475,\n",
       "   0.38941798941798944,\n",
       "   0.3252873563218391,\n",
       "   0.35291285507944153,\n",
       "   0.386018457960904,\n",
       "   0.3748792270531401,\n",
       "   0.4869281045751634,\n",
       "   0.41798941798941797,\n",
       "   0.4386464263124605,\n",
       "   0.4035886818495514,\n",
       "   0.3502645502645503,\n",
       "   0.4239199829751011,\n",
       "   0.452020202020202,\n",
       "   0.414426523297491,\n",
       "   0.4460992907801418,\n",
       "   0.38730158730158726,\n",
       "   0.43739805571866636,\n",
       "   0.4148148148148148,\n",
       "   0.3828703703703704,\n",
       "   0.361633428300095,\n",
       "   0.37213930348258706,\n",
       "   0.4293361140076469,\n",
       "   0.42808165809568327,\n",
       "   0.40585241730279903,\n",
       "   0.4479302832244009,\n",
       "   0.4115942028985507,\n",
       "   0.4448243114909782,\n",
       "   0.48280423280423285,\n",
       "   0.4393162393162393,\n",
       "   0.47632850241545893,\n",
       "   0.4134680134680135,\n",
       "   0.46980734984620365,\n",
       "   0.4314772438342232,\n",
       "   0.3875746714456392,\n",
       "   0.4003414425949637,\n",
       "   0.411764705882353,\n",
       "   0.43045343137254904,\n",
       "   0.40927482103952695,\n",
       "   0.39567430025445294,\n",
       "   0.40552385677664765,\n",
       "   0.3931623931623931,\n",
       "   0.4722547108512021,\n",
       "   0.34200072228241246,\n",
       "   0.32804726368159204,\n",
       "   0.4351851851851852,\n",
       "   0.32471875950136825,\n",
       "   0.3864834500043211,\n",
       "   0.3333333333333333,\n",
       "   0.4269162210338681,\n",
       "   0.4080704745666382,\n",
       "   0.40482613908872905,\n",
       "   0.3944476576055524,\n",
       "   0.3314022578728461,\n",
       "   0.3273157102944337,\n",
       "   0.3869541182974019,\n",
       "   0.4348595443485954,\n",
       "   0.34612310151878495,\n",
       "   0.44353864734299514,\n",
       "   0.41735537190082644,\n",
       "   0.40049751243781095,\n",
       "   0.4047619047619048,\n",
       "   0.3952338129496403,\n",
       "   0.41475922451532216,\n",
       "   0.3963414634146341,\n",
       "   0.3416845621570031,\n",
       "   0.33748443337484435,\n",
       "   0.37966417910447764,\n",
       "   0.38002364066193856,\n",
       "   0.3920835281284089,\n",
       "   0.3693398799781779,\n",
       "   0.4230385487528345,\n",
       "   0.42544886807181886,\n",
       "   0.44351755300660406,\n",
       "   0.37429193899782137,\n",
       "   0.4191219512195122,\n",
       "   0.4416294088425236,\n",
       "   0.4693277310924369,\n",
       "   0.3861447017831523,\n",
       "   0.4187590187590187,\n",
       "   0.4351249572064361,\n",
       "   0.45617378048780494,\n",
       "   0.41607761607761606,\n",
       "   0.45304136253041366,\n",
       "   0.4662408759124088,\n",
       "   0.430672268907563,\n",
       "   0.41358024691358025,\n",
       "   0.3861194527861194,\n",
       "   0.3935727546251841,\n",
       "   0.37637385686718683,\n",
       "   0.41087470449172575,\n",
       "   0.35784313725490197,\n",
       "   0.3272727272727272,\n",
       "   0.43253968253968256,\n",
       "   0.41223908918406077,\n",
       "   0.4225694444444444,\n",
       "   0.41839904420549584,\n",
       "   0.3861693861693862,\n",
       "   0.36170212765957444,\n",
       "   0.38737922705314015,\n",
       "   0.3777391879599768,\n",
       "   0.4496423117112773,\n",
       "   0.4295119507657077,\n",
       "   0.43235435724602794,\n",
       "   0.40952380952380957,\n",
       "   0.41622960372960377,\n",
       "   0.3986531986531987,\n",
       "   0.41650763358778625,\n",
       "   0.44607843137254904,\n",
       "   0.3954022988505747,\n",
       "   0.4308080808080808,\n",
       "   0.4087938205585265,\n",
       "   0.43939393939393945,\n",
       "   0.42199180130214614,\n",
       "   0.36430921052631576,\n",
       "   0.41358024691358025,\n",
       "   0.40170940170940167,\n",
       "   0.3766423357664233,\n",
       "   0.36288759689922484,\n",
       "   0.37472058524690105,\n",
       "   0.3520370370370371,\n",
       "   0.39993811402490914,\n",
       "   0.3818947368421053,\n",
       "   0.3703703703703704,\n",
       "   0.42758620689655175,\n",
       "   0.3666210670314638,\n",
       "   0.43896103896103894,\n",
       "   0.3746031746031746,\n",
       "   0.37553832902670115,\n",
       "   0.4467878787878788,\n",
       "   0.36960431654676257,\n",
       "   0.41606714628297364,\n",
       "   0.47350427350427343,\n",
       "   0.4486870016823934,\n",
       "   0.4599483204134367,\n",
       "   0.3908146210304484,\n",
       "   0.4273621596888584,\n",
       "   0.37566137566137564,\n",
       "   0.411313518696069,\n",
       "   0.34476190476190477,\n",
       "   0.3650914634146341,\n",
       "   0.4430927277642606,\n",
       "   0.40547263681592033,\n",
       "   0.4194838240639767,\n",
       "   0.42105263157894735,\n",
       "   0.41565633887142645,\n",
       "   0.36758824552251784,\n",
       "   0.4945054945054945,\n",
       "   0.4152096659559346,\n",
       "   0.38743918620079604,\n",
       "   0.4115105567783484,\n",
       "   0.34413145539906104,\n",
       "   0.3537529137529138,\n",
       "   0.3678313430635412,\n",
       "   0.37880079568059105,\n",
       "   0.4419753086419753,\n",
       "   0.44704551880124405,\n",
       "   0.3598541980894922,\n",
       "   0.474910394265233,\n",
       "   0.3601823708206687,\n",
       "   0.3584438549955791,\n",
       "   0.4115416746995694,\n",
       "   0.49652777777777785,\n",
       "   0.42100393245431417,\n",
       "   0.4385964912280702,\n",
       "   0.34638447971781305,\n",
       "   0.4332171893147503,\n",
       "   0.35093167701863354,\n",
       "   0.3977647955295911,\n",
       "   0.4346153846153846,\n",
       "   0.42758620689655175,\n",
       "   0.464573268921095,\n",
       "   0.43988603988603986,\n",
       "   0.4499832719973235,\n",
       "   0.3958333333333333,\n",
       "   0.4011286509570655,\n",
       "   0.4084757834757835,\n",
       "   0.4137715179968701,\n",
       "   0.44169664268585135,\n",
       "   0.4327054327054327,\n",
       "   0.3613123993558776,\n",
       "   0.3264957264957265,\n",
       "   0.4457070707070707,\n",
       "   0.3809121621621621,\n",
       "   0.37540906965871895,\n",
       "   0.4412460019468781,\n",
       "   0.42898550724637685,\n",
       "   0.4438095238095238,\n",
       "   0.4257602862254026,\n",
       "   0.4567901234567901,\n",
       "   0.33946078431372556,\n",
       "   0.31583462000343115,\n",
       "   0.41936593081631246,\n",
       "   0.40620782726045884,\n",
       "   0.3811274509803922,\n",
       "   0.4552469135802469,\n",
       "   0.4819591836734694,\n",
       "   0.32136752136752134,\n",
       "   0.4027149321266968,\n",
       "   0.4539859739516245,\n",
       "   0.3791962174940899,\n",
       "   0.39264990328820115,\n",
       "   0.45306757138818216,\n",
       "   0.40196078431372556,\n",
       "   0.43972572439725727,\n",
       "   0.3429951690821256,\n",
       "   0.41841982234689107,\n",
       "   0.4153191489361702,\n",
       "   0.42496194824961947,\n",
       "   0.37545787545787546,\n",
       "   0.381486093814861,\n",
       "   0.40990410762845286,\n",
       "   0.38552721405076645,\n",
       "   0.42464577071704596,\n",
       "   0.453091684434968,\n",
       "   0.3735294117647059,\n",
       "   0.46184419713831476,\n",
       "   0.42255602068456044,\n",
       "   0.47226259689922484,\n",
       "   0.37188940092165895,\n",
       "   0.41844589687726946,\n",
       "   0.3468265867066467,\n",
       "   0.3763440860215053,\n",
       "   0.41414141414141414,\n",
       "   0.3613123993558776,\n",
       "   0.38958932389589324,\n",
       "   0.336783988957902,\n",
       "   0.32886239443616494,\n",
       "   0.3866016260162602,\n",
       "   0.4241545893719807,\n",
       "   0.4258404746209625,\n",
       "   0.3924242424242424,\n",
       "   0.3995358995358996,\n",
       "   0.4048083170890188,\n",
       "   0.36630036630036633,\n",
       "   0.38011695906432746,\n",
       "   0.3923884514435696,\n",
       "   0.4443089430894309,\n",
       "   0.43107769423558895,\n",
       "   0.4495870344397694,\n",
       "   0.4209563029055136,\n",
       "   0.36944507532742826,\n",
       "   0.3482177830003917,\n",
       "   0.4538720538720538,\n",
       "   0.453405017921147,\n",
       "   0.4385026737967914,\n",
       "   0.34299516908212563,\n",
       "   0.42857142857142855,\n",
       "   0.3446283162595219,\n",
       "   0.3487654320987654,\n",
       "   0.4170720919946926,\n",
       "   0.4395833333333334,\n",
       "   0.36509152830991914,\n",
       "   0.40374440374440373,\n",
       "   0.37209302325581395,\n",
       "   0.4354066985645933,\n",
       "   0.39637188208616775,\n",
       "   0.39848484848484844,\n",
       "   0.4079929732103645,\n",
       "   0.391452161243322,\n",
       "   0.4884991739738213,\n",
       "   0.33580246913580253,\n",
       "   0.34637964774951074,\n",
       "   0.3538064003180283,\n",
       "   0.4115105567783484,\n",
       "   0.4369156041287188,\n",
       "   0.3344594594594595,\n",
       "   0.41526374859708187,\n",
       "   0.3490909090909091,\n",
       "   0.41401831939466344,\n",
       "   0.45054945054945056,\n",
       "   0.36077097505668937,\n",
       "   0.4202516827626573,\n",
       "   0.3843078460769615,\n",
       "   0.4595959595959596,\n",
       "   0.42377538829151734,\n",
       "   0.3572519083969466,\n",
       "   0.4022556390977443,\n",
       "   0.4282256563023646,\n",
       "   0.43939393939393945,\n",
       "   0.434375,\n",
       "   0.48458208458208457,\n",
       "   0.4084642955610698,\n",
       "   0.40323955669224204,\n",
       "   0.4230889724310776,\n",
       "   0.40821256038647347,\n",
       "   0.393607305936073,\n",
       "   0.39805586317214225,\n",
       "   0.4662408759124088,\n",
       "   0.46079846079846076,\n",
       "   0.4010416666666667,\n",
       "   0.357799671592775,\n",
       "   0.47152374811949277,\n",
       "   0.3987577639751552,\n",
       "   0.37731259202355805,\n",
       "   0.41478129713423834,\n",
       "   0.4395833333333334,\n",
       "   0.45925925925925926,\n",
       "   0.39313329077108605,\n",
       "   0.3800813008130081,\n",
       "   0.4258344559197725,\n",
       "   0.4465897496271878,\n",
       "   0.3919914289540645,\n",
       "   0.4257602862254026,\n",
       "   0.3564661006521472,\n",
       "   0.4295051353874883,\n",
       "   0.38434022257551675,\n",
       "   0.4315068493150685,\n",
       "   0.41794871794871796,\n",
       "   0.3752449871852857,\n",
       "   0.4951844021611464,\n",
       "   0.3821289821289821,\n",
       "   0.4088023088023088,\n",
       "   0.4215686274509804,\n",
       "   0.3869541182974019,\n",
       "   0.47166558931264807,\n",
       "   0.42633653609263367,\n",
       "   0.4238303383519508,\n",
       "   0.33495257922738836,\n",
       "   0.3956687152472929,\n",
       "   0.3843843843843844,\n",
       "   0.34168475044387453,\n",
       "   0.40323955669224204,\n",
       "   0.371474358974359,\n",
       "   0.34839234839234834,\n",
       "   0.4859237048142595,\n",
       "   0.47655583972719523,\n",
       "   0.400941230486685,\n",
       "   0.3986531986531987,\n",
       "   0.5240158902130734,\n",
       "   0.4709240985539626,\n",
       "   0.3923076923076923,\n",
       "   0.47988505747126436,\n",
       "   0.4592846270928463,\n",
       "   0.4686066621550493,\n",
       "   0.48805361305361306,\n",
       "   0.3720858422109001,\n",
       "   0.46049382716049386,\n",
       "   0.4154228855721393,\n",
       "   0.3605463455322699,\n",
       "   0.46944619037642293,\n",
       "   0.41215277777777776,\n",
       "   0.4198095238095238,\n",
       "   0.4766751484308736,\n",
       "   0.37213930348258706,\n",
       "   0.3830532212885154,\n",
       "   0.43253757736516363,\n",
       "   0.4620663362561952,\n",
       "   0.3896753896753897,\n",
       "   0.4454075425790755,\n",
       "   0.4538720538720538,\n",
       "   0.44330737856878005,\n",
       "   0.4459325396825397,\n",
       "   0.39436152570480926,\n",
       "   0.40762463343108496,\n",
       "   0.41994402710266604,\n",
       "   0.3620370370370371,\n",
       "   0.4429705215419501,\n",
       "   0.45911027306376145,\n",
       "   0.4116037219485495,\n",
       "   0.5299359509885826,\n",
       "   0.4239316239316239,\n",
       "   0.4367283950617284,\n",
       "   0.4004362050163577,\n",
       "   0.35303776683087024,\n",
       "   0.35468409586056643,\n",
       "   0.3889804567930304,\n",
       "   0.3553921568627451,\n",
       "   0.3891423357664234,\n",
       "   0.3593869731800767,\n",
       "   0.3621890547263682,\n",
       "   0.38852205103122195,\n",
       "   0.472300469483568,\n",
       "   0.42684766214177977,\n",
       "   0.41091838106763484,\n",
       "   0.4103059581320451,\n",
       "   0.43209876543209874,\n",
       "   0.3923076923076923,\n",
       "   0.45812423329987734,\n",
       "   0.34502923976608185,\n",
       "   0.42550505050505044,\n",
       "   0.37571638011462083,\n",
       "   0.4139393939393939,\n",
       "   0.39080459770114945,\n",
       "   0.3686868686868687,\n",
       "   0.4005291005291005,\n",
       "   0.42144349036899514,\n",
       "   0.42584213172448465,\n",
       "   0.45324991310392776,\n",
       "   0.39762724837351704,\n",
       "   0.40552526354053064,\n",
       "   0.43030303030303035,\n",
       "   0.38270187523918864,\n",
       "   0.3720667232117614,\n",
       "   0.3728937728937729,\n",
       "   0.33648203807070276,\n",
       "   0.34638447971781305,\n",
       "   0.39775113459323985,\n",
       "   0.326007326007326,\n",
       "   0.4373842592592592,\n",
       "   0.44041320231796427,\n",
       "   0.3623744292237443,\n",
       "   0.3848446147296722,\n",
       "   0.4479302832244009,\n",
       "   0.41493868450390187,\n",
       "   0.4774146695715323,\n",
       "   0.46005291005291005,\n",
       "   0.38647764449291166,\n",
       "   0.33990018387181503,\n",
       "   0.3981566820276498,\n",
       "   0.398989898989899,\n",
       "   0.32682883364519294,\n",
       "   0.4423280423280423,\n",
       "   0.47407407407407404,\n",
       "   0.3465027802165643,\n",
       "   0.3604004449388209,\n",
       "   0.4373285629099583,\n",
       "   0.41562881562881565,\n",
       "   0.37903010924593655,\n",
       "   0.3328732459167242,\n",
       "   0.4241379310344828,\n",
       "   0.4294753086419753,\n",
       "   0.35889724310776944,\n",
       "   0.38000000000000006,\n",
       "   0.3796232746129795,\n",
       "   0.4219858156028369,\n",
       "   0.3969744392279604,\n",
       "   0.3954022988505747,\n",
       "   0.40491763565891475,\n",
       "   0.36892230576441104,\n",
       "   0.4380952380952381,\n",
       "   0.4647754137115839,\n",
       "   0.4346666666666666,\n",
       "   0.4040587611691655,\n",
       "   0.4679487179487179,\n",
       "   0.4190379403794038,\n",
       "   0.33638962831509417,\n",
       "   0.42142857142857143,\n",
       "   0.35726883345930965,\n",
       "   0.4197289972899729,\n",
       "   0.45725786163522014,\n",
       "   0.42430703624733473,\n",
       "   0.32693219535324797,\n",
       "   0.5442146282973621,\n",
       "   0.4045835733947891,\n",
       "   0.40075973409306737,\n",
       "   0.45155555555555554,\n",
       "   0.5018741369106333,\n",
       "   0.36390977443609024,\n",
       "   0.4124495129484438,\n",
       "   0.40740740740740744,\n",
       "   0.4154228855721393,\n",
       "   0.44870956015994184,\n",
       "   0.3466666666666667,\n",
       "   0.45467372134038797,\n",
       "   0.5003481086098863,\n",
       "   0.48652759669708817,\n",
       "   0.47935222672064776,\n",
       "   0.4028871391076116,\n",
       "   0.43601482450403317,\n",
       "   0.46191832858499526,\n",
       "   0.39637188208616775,\n",
       "   0.4275362318840579,\n",
       "   0.4500500500500501,\n",
       "   0.4006890611541774,\n",
       "   0.3368176538908246,\n",
       "   0.3739837398373984,\n",
       "   0.3834988540870894,\n",
       "   0.3812999652415711,\n",
       "   0.3588900308324769,\n",
       "   0.37530864197530867,\n",
       "   0.4276180075233588,\n",
       "   0.4038607691138471,\n",
       "   0.3876769996172982,\n",
       "   0.3686274509803922,\n",
       "   0.3619047619047619,\n",
       "   0.45714285714285713,\n",
       "   0.4001598721023181,\n",
       "   0.3952892985151049,\n",
       "   0.30940766550522647,\n",
       "   0.39705882352941174,\n",
       "   0.4511784511784512,\n",
       "   0.40726817042606517,\n",
       "   0.4000000000000001,\n",
       "   0.3984504703929164,\n",
       "   0.47096423228614165,\n",
       "   0.4673164392462638,\n",
       "   0.4035914702581369,\n",
       "   0.4513840697046804,\n",
       "   0.4151161829157364,\n",
       "   0.4493084055327558,\n",
       "   0.36117347702439756,\n",
       "   0.45260295260295264,\n",
       "   0.45765345765345766,\n",
       "   0.4242546604751329,\n",
       "   0.41005291005291006,\n",
       "   0.3978779840848807,\n",
       "   0.40979134409791346,\n",
       "   0.42588335461898685,\n",
       "   0.3376623376623377,\n",
       "   0.41322819243230025,\n",
       "   0.46201058201058204,\n",
       "   0.33803011465672056,\n",
       "   0.3384081196581197,\n",
       "   0.42717825739408477,\n",
       "   0.3511463844797178,\n",
       "   0.4647939246479393,\n",
       "   0.36155202821869487,\n",
       "   0.36102027468933945,\n",
       "   0.44418331374853115,\n",
       "   0.40291858678955456,\n",
       "   0.42307246376811597,\n",
       "   0.49568420473912805,\n",
       "   0.44351755300660406,\n",
       "   0.33909400366588116,\n",
       "   0.36316316316316316,\n",
       "   0.4751573590464711,\n",
       "   0.4380160080670574,\n",
       "   0.3634604754007739,\n",
       "   0.3815178815178815,\n",
       "   0.47259013980868286,\n",
       "   0.41507615191825714,\n",
       "   0.4399136178861789,\n",
       "   0.3371969091393552,\n",
       "   0.4224196855775803,\n",
       "   0.4500500500500501,\n",
       "   0.3807605444668254,\n",
       "   0.37314148681055154,\n",
       "   0.4663128409258441,\n",
       "   0.40690559440559443,\n",
       "   0.35303776683087024,\n",
       "   0.4739918873777141,\n",
       "   0.3804195804195804,\n",
       "   0.39855699855699855,\n",
       "   0.3987577639751552,\n",
       "   0.42273933455807305,\n",
       "   0.39697662601626016,\n",
       "   0.3979783158066357,\n",
       "   0.3481165600568586,\n",
       "   0.40116038126813097,\n",
       "   0.4468864468864469,\n",
       "   0.4507610489638731,\n",
       "   0.44549977385798273,\n",
       "   0.37872744539411207,\n",
       "   0.4348595443485954,\n",
       "   0.3833298947802765,\n",
       "   0.4249955618675661,\n",
       "   0.42287581699346405,\n",
       "   0.4493638676844784,\n",
       "   0.4276891405043746,\n",
       "   0.4300320275930032,\n",
       "   0.4302589731940027,\n",
       "   0.4121818940342094,\n",
       "   0.25219298245614036,\n",
       "   0.40458640458640466,\n",
       "   0.4308080808080808,\n",
       "   0.4449013157894737,\n",
       "   0.38345864661654133,\n",
       "   0.4099756690997567,\n",
       "   0.40606991932385705,\n",
       "   0.3380471380471381,\n",
       "   0.4308080808080808,\n",
       "   0.4395833333333334,\n",
       "   0.4420819490586932,\n",
       "   0.4418654631420589,\n",
       "   0.4272170484101015,\n",
       "   0.38825757575757575,\n",
       "   0.31414356787491116,\n",
       "   0.45077642092567466,\n",
       "   0.3640211640211641,\n",
       "   0.4423280423280423,\n",
       "   0.3974358974358974,\n",
       "   0.37341329335332335,\n",
       "   0.3861661452902329,\n",
       "   0.41044776119402987,\n",
       "   0.4621212121212121,\n",
       "   0.3681188787571766,\n",
       "   0.4632478632478632,\n",
       "   0.38215962441314555,\n",
       "   0.4058567833447724,\n",
       "   0.4221007893139041,\n",
       "   0.36697805118857746,\n",
       "   0.39931356885022784,\n",
       "   0.3878338737069631,\n",
       "   0.4459741343345849,\n",
       "   0.39833035181872384,\n",
       "   0.4661246612466125,\n",
       "   0.4745021855269549,\n",
       "   0.47979797979797983,\n",
       "   0.37188208616780044,\n",
       "   0.40350877192982454,\n",
       "   0.4359937402190923,\n",
       "   0.35601888276947286,\n",
       "   0.44395532730088855,\n",
       "   0.3500797448165869,\n",
       "   0.44876531207466464,\n",
       "   0.45359566842065124,\n",
       "   0.3702997967479675,\n",
       "   0.40821256038647347,\n",
       "   0.3678571428571429,\n",
       "   0.40418679549114334,\n",
       "   0.3584149184149184,\n",
       "   0.3779376498800959,\n",
       "   0.42042151162790703,\n",
       "   0.43885003885003887,\n",
       "   0.40257309941520464,\n",
       "   0.38472314384723144,\n",
       "   0.36606929510155317,\n",
       "   0.41094941094941095,\n",
       "   0.43380452956376353,\n",
       "   0.3992945326278659,\n",
       "   0.4250843732831018,\n",
       "   0.42279618192026946,\n",
       "   0.4583333333333333,\n",
       "   0.4275315315315315,\n",
       "   0.4220551378446115,\n",
       "   0.40745920745920744,\n",
       "   0.3887496519075466,\n",
       "   0.3779376498800959,\n",
       "   0.34707379134860056,\n",
       "   0.40277777777777773,\n",
       "   0.3812999652415711,\n",
       "   0.4680951259712738,\n",
       "   0.4886871604428857,\n",
       "   0.46694939214023184,\n",
       "   0.4028948488718604,\n",
       "   0.41658788680910064,\n",
       "   0.38974358974358975,\n",
       "   0.4464928057553957,\n",
       "   0.47570429192664027,\n",
       "   0.4740465691962559,\n",
       "   0.38365987962607245,\n",
       "   0.44610705596107064,\n",
       "   0.4294883550339263,\n",
       "   0.43070855433156535,\n",
       "   0.43018018018018017,\n",
       "   0.35239018087855295,\n",
       "   0.4548720251459362,\n",
       "   0.39073371283997466,\n",
       "   0.39231932662589597,\n",
       "   0.433096926713948,\n",
       "   0.4614432061240572,\n",
       "   0.32405278842060453,\n",
       "   0.4209450830140485,\n",
       "   0.4153520131857782,\n",
       "   0.44650169129428524,\n",
       "   0.4170720919946926,\n",
       "   0.4386607544502281,\n",
       "   0.42419481539670073,\n",
       "   0.37977113325950534,\n",
       "   0.37222222222222223,\n",
       "   0.40531517094017094,\n",
       "   0.4166278166278166,\n",
       "   0.3513071895424837,\n",
       "   0.4696969696969697,\n",
       "   0.36367864201836997,\n",
       "   0.35814722911497104,\n",
       "   0.3920835281284089,\n",
       "   0.4170720919946926,\n",
       "   0.4239316239316239,\n",
       "   0.37703141928494044,\n",
       "   0.4010416666666667,\n",
       "   0.41570338058887674,\n",
       "   0.4267348812803358,\n",
       "   0.3541403951240017,\n",
       "   0.4646293152235428,\n",
       "   0.42924976258309594,\n",
       "   0.4326599326599327,\n",
       "   0.43647509578544064,\n",
       "   0.4031311154598826,\n",
       "   0.41101664123246856,\n",
       "   0.3724198506807202,\n",
       "   0.4438988095238095,\n",
       "   0.35083588415352013,\n",
       "   0.451207729468599,\n",
       "   0.4262273901808786,\n",
       "   0.3810359964881475,\n",
       "   0.4889705882352941,\n",
       "   0.3561927518916766,\n",
       "   0.4142857142857143,\n",
       "   0.4197530864197531,\n",
       "   0.4342572850035536,\n",
       "   0.44498030704927255,\n",
       "   0.42328042328042326,\n",
       "   0.3952338129496403,\n",
       "   0.4131054131054131,\n",
       "   0.3837371205792259,\n",
       "   0.4473118279569892,\n",
       "   0.36943521594684386,\n",
       "   0.42751322751322746,\n",
       "   0.3940110323089046,\n",
       "   0.4380952380952381,\n",
       "   0.41648484848484846,\n",
       "   0.4238562091503268,\n",
       "   0.4382406744611469,\n",
       "   0.4525525525525526,\n",
       "   0.3598440545808967,\n",
       "   0.41385281385281386,\n",
       "   0.39691444600280507,\n",
       "   0.39567430025445294,\n",
       "   0.41448757511775214,\n",
       "   0.3864834500043211,\n",
       "   0.3864919569617556,\n",
       "   0.3735200069138363,\n",
       "   0.4846894138232721,\n",
       "   0.39805586317214225,\n",
       "   0.4853615520282187,\n",
       "   0.41153741496598634,\n",
       "   0.34331983805668015,\n",
       "   0.3891423357664234,\n",
       "   0.3333333333333333,\n",
       "   0.46461208798134496,\n",
       "   0.3995358995358996,\n",
       "   0.4759570494864613,\n",
       "   0.42377538829151734,\n",
       "   0.39223057644110276,\n",
       "   0.35185185185185186,\n",
       "   0.36862058909303,\n",
       "   0.41300097751710657,\n",
       "   0.4304213771839671,\n",
       "   0.4143044619422572,\n",
       "   0.3089785532533624,\n",
       "   0.4271062271062271,\n",
       "   0.43622047244094486,\n",
       "   0.5038792495415433,\n",
       "   0.45600331380555065,\n",
       "   0.39985870717061106,\n",
       "   0.3717128642501777,\n",
       "   0.3912198912198912,\n",
       "   0.35083996463306805,\n",
       "   0.3713515940727436,\n",
       "   0.3878713878713878,\n",
       "   0.3831501831501831,\n",
       "   0.3789246316439096,\n",
       "   0.3728305368160065,\n",
       "   0.3678313430635412,\n",
       "   0.4087145969498911,\n",
       "   0.384368620589093,\n",
       "   0.3796296296296296,\n",
       "   0.3763303533418476,\n",
       "   0.42863657090743273,\n",
       "   0.3706126008284281,\n",
       "   0.41793754538852584,\n",
       "   0.4478844169246646,\n",
       "   0.40323955669224204,\n",
       "   0.4447561165653733,\n",
       "   0.358974358974359,\n",
       "   0.5175341454411222,\n",
       "   0.45714285714285713,\n",
       "   0.33671238322401115,\n",
       "   0.4523809523809524,\n",
       "   0.41058491573247924,\n",
       "   0.4090909090909091,\n",
       "   0.37367724867724866,\n",
       "   0.349954519143306,\n",
       "   0.39578996067545685,\n",
       "   0.40492516404925166,\n",
       "   0.41606714628297364,\n",
       "   0.4855500821018062,\n",
       "   0.41797385620915034,\n",
       "   0.41058491573247924,\n",
       "   0.39775113459323985,\n",
       "   0.3628188117352204,\n",
       "   0.43406240886555847,\n",
       "   0.40418018018018015,\n",
       "   0.5209089686701627,\n",
       "   0.48888888888888893,\n",
       "   0.41520352469257576,\n",
       "   0.37345679012345673,\n",
       "   0.4390342052313883,\n",
       "   0.3466389466389466,\n",
       "   0.42630303030303035,\n",
       "   0.3696386355960824,\n",
       "   0.4151161829157364,\n",
       "   0.28411556469110427,\n",
       "   0.41846635367762125,\n",
       "   0.3796232746129795,\n",
       "   0.46895787139689576,\n",
       "   0.4356853203568532,\n",
       "   0.4296466386018625,\n",
       "   0.4547192353643967,\n",
       "   0.40115248226950356,\n",
       "   0.4300320275930032,\n",
       "   0.4446428571428571,\n",
       "   0.36406926406926404,\n",
       "   0.3819789939192924,\n",
       "   0.4075774971297359,\n",
       "   0.42579421526789946,\n",
       "   0.40841959972394754,\n",
       "   0.3991228070175439,\n",
       "   0.4362562189054726,\n",
       "   0.37472058524690105,\n",
       "   0.3930183056619838,\n",
       "   0.3611310234966149,\n",
       "   0.41032001984619204,\n",
       "   0.4238095238095238,\n",
       "   0.4393518518518518,\n",
       "   0.487766472868217,\n",
       "   0.4101259215763033],\n",
       "  'mean': 0.4087029892154536,\n",
       "  'std': 0.04060703474659068,\n",
       "  'ci_lower': 0.33443130630630635,\n",
       "  'ci_upper': 0.48653760939404006},\n",
       " 'multitask_emotion': {'values': [0.11786085150571131,\n",
       "   0.12579710144927536,\n",
       "   0.14931145066968496,\n",
       "   0.09730511846626877,\n",
       "   0.12044817927170869,\n",
       "   0.09939898289412852,\n",
       "   0.11555555555555556,\n",
       "   0.13680347013680347,\n",
       "   0.1391025641025641,\n",
       "   0.13656633221850614,\n",
       "   0.10422282120395328,\n",
       "   0.08278867102396514,\n",
       "   0.12103960396039604,\n",
       "   0.14423076923076925,\n",
       "   0.11219314262527896,\n",
       "   0.14640111809923131,\n",
       "   0.10761904761904761,\n",
       "   0.1027419097720389,\n",
       "   0.11004273504273504,\n",
       "   0.10707502374169041,\n",
       "   0.10363447559709242,\n",
       "   0.14941902687000727,\n",
       "   0.12133234151582777,\n",
       "   0.14619741100323624,\n",
       "   0.09556285257219836,\n",
       "   0.16340395101457048,\n",
       "   0.12398417835311039,\n",
       "   0.11631340293156174,\n",
       "   0.0866916588566073,\n",
       "   0.09672619047619047,\n",
       "   0.09105221726580952,\n",
       "   0.11053837342497136,\n",
       "   0.10015360983102918,\n",
       "   0.13255131964809383,\n",
       "   0.10755148741418763,\n",
       "   0.16552214065829976,\n",
       "   0.058997050147492625,\n",
       "   0.1122053872053872,\n",
       "   0.1477124183006536,\n",
       "   0.15151515151515152,\n",
       "   0.15675675675675677,\n",
       "   0.1336904761904762,\n",
       "   0.1138072128651632,\n",
       "   0.14847622060529367,\n",
       "   0.11339339339339338,\n",
       "   0.11794738663897542,\n",
       "   0.12063492063492064,\n",
       "   0.15334689528237913,\n",
       "   0.10764790764790766,\n",
       "   0.1315988886082344,\n",
       "   0.16216688510266491,\n",
       "   0.11595441595441595,\n",
       "   0.1529602132114982,\n",
       "   0.1449204406364749,\n",
       "   0.15921375921375922,\n",
       "   0.11238599588114151,\n",
       "   0.13236201048411964,\n",
       "   0.13554934244589417,\n",
       "   0.13170731707317074,\n",
       "   0.08940809968847352,\n",
       "   0.09571577847439916,\n",
       "   0.10210210210210209,\n",
       "   0.1441621467313731,\n",
       "   0.13383838383838384,\n",
       "   0.1563467492260062,\n",
       "   0.14374858180167915,\n",
       "   0.10013860013860014,\n",
       "   0.12877846790890268,\n",
       "   0.14106583072100312,\n",
       "   0.09325396825396826,\n",
       "   0.1378205128205128,\n",
       "   0.16845075097502282,\n",
       "   0.14108108108108108,\n",
       "   0.13648867313915858,\n",
       "   0.1131752305665349,\n",
       "   0.11356189562854631,\n",
       "   0.14869015356820237,\n",
       "   0.1383177570093458,\n",
       "   0.11904761904761903,\n",
       "   0.13223905723905724,\n",
       "   0.07980707980707981,\n",
       "   0.10273786934128488,\n",
       "   0.12586679567811643,\n",
       "   0.13509803921568628,\n",
       "   0.14222821203953281,\n",
       "   0.14508005036877136,\n",
       "   0.12207851145904243,\n",
       "   0.10833333333333334,\n",
       "   0.14463441757019738,\n",
       "   0.12583551293228712,\n",
       "   0.10476190476190476,\n",
       "   0.1590373783922171,\n",
       "   0.11983099604458829,\n",
       "   0.1226815050344462,\n",
       "   0.10902255639097742,\n",
       "   0.13221802482460876,\n",
       "   0.15097259062776305,\n",
       "   0.12178571428571427,\n",
       "   0.13765432098765432,\n",
       "   0.14933993399339934,\n",
       "   0.1310472510665701,\n",
       "   0.1483887733887734,\n",
       "   0.16013796501601382,\n",
       "   0.10952380952380952,\n",
       "   0.17052526761264628,\n",
       "   0.16711779448621553,\n",
       "   0.09178743961352658,\n",
       "   0.12923728813559324,\n",
       "   0.11723856209150325,\n",
       "   0.10208262205530898,\n",
       "   0.10368217054263566,\n",
       "   0.09731097753875356,\n",
       "   0.10651074589127686,\n",
       "   0.11517039554422732,\n",
       "   0.12193362193362194,\n",
       "   0.07348980377068103,\n",
       "   0.14176470588235293,\n",
       "   0.10386326860841423,\n",
       "   0.09051383399209485,\n",
       "   0.13462783171521034,\n",
       "   0.13416213416213416,\n",
       "   0.1202020202020202,\n",
       "   0.12715840840840842,\n",
       "   0.10666666666666667,\n",
       "   0.14326948307530832,\n",
       "   0.12698412698412698,\n",
       "   0.1409142121358442,\n",
       "   0.12042124542124542,\n",
       "   0.15185185185185185,\n",
       "   0.07531824611032532,\n",
       "   0.12612842786578096,\n",
       "   0.1175862068965517,\n",
       "   0.1130952380952381,\n",
       "   0.1299264421215641,\n",
       "   0.1198220064724919,\n",
       "   0.09852455833818675,\n",
       "   0.10152098222466556,\n",
       "   0.11473684210526315,\n",
       "   0.11474558670820352,\n",
       "   0.0929881337648328,\n",
       "   0.17036625971143174,\n",
       "   0.14309444708115804,\n",
       "   0.15295190335080922,\n",
       "   0.12351254480286739,\n",
       "   0.12654639175257731,\n",
       "   0.13680347013680347,\n",
       "   0.12415540540540541,\n",
       "   0.12125220458553791,\n",
       "   0.15358477743798846,\n",
       "   0.1315039437926692,\n",
       "   0.12859097127222982,\n",
       "   0.11183923110528615,\n",
       "   0.11331399097356544,\n",
       "   0.12409983229752393,\n",
       "   0.11675706537253783,\n",
       "   0.13392857142857142,\n",
       "   0.10052910052910052,\n",
       "   0.1379853581688444,\n",
       "   0.11428571428571428,\n",
       "   0.13085255066387144,\n",
       "   0.09615384615384615,\n",
       "   0.13348765432098766,\n",
       "   0.09314264759809314,\n",
       "   0.12532299741602068,\n",
       "   0.1431372549019608,\n",
       "   0.1214625077463334,\n",
       "   0.07947530864197531,\n",
       "   0.10784313725490197,\n",
       "   0.1365079365079365,\n",
       "   0.13616986391211675,\n",
       "   0.08274143302180685,\n",
       "   0.15342809364548496,\n",
       "   0.1286159323542501,\n",
       "   0.13826244171662386,\n",
       "   0.09805801965955407,\n",
       "   0.14811416921508666,\n",
       "   0.10952380952380952,\n",
       "   0.13793103448275865,\n",
       "   0.13102453102453102,\n",
       "   0.15880503144654087,\n",
       "   0.14333814333814332,\n",
       "   0.11434511434511435,\n",
       "   0.1298893563970703,\n",
       "   0.14690853346959631,\n",
       "   0.13553113553113552,\n",
       "   0.09055752497582985,\n",
       "   0.17426400759734093,\n",
       "   0.13692532043902064,\n",
       "   0.14317258434905494,\n",
       "   0.1111111111111111,\n",
       "   0.14176417641764175,\n",
       "   0.13489409141583056,\n",
       "   0.14482406935237124,\n",
       "   0.15072463768115943,\n",
       "   0.1402140214021402,\n",
       "   0.10548941798941798,\n",
       "   0.09126984126984126,\n",
       "   0.11447811447811447,\n",
       "   0.11325529895602658,\n",
       "   0.11165577342047932,\n",
       "   0.14191334191334193,\n",
       "   0.11861861861861862,\n",
       "   0.15179487179487178,\n",
       "   0.0820829655781112,\n",
       "   0.12542087542087543,\n",
       "   0.09749793217535152,\n",
       "   0.14954954954954952,\n",
       "   0.15523809523809523,\n",
       "   0.13047461975444383,\n",
       "   0.09117161716171618,\n",
       "   0.1222860791826309,\n",
       "   0.12193362193362194,\n",
       "   0.12952488687782804,\n",
       "   0.1161236424394319,\n",
       "   0.11324786324786325,\n",
       "   0.17056277056277055,\n",
       "   0.08333333333333333,\n",
       "   0.1123800066203244,\n",
       "   0.13415453527435608,\n",
       "   0.15862068965517243,\n",
       "   0.11082251082251082,\n",
       "   0.10893986594921175,\n",
       "   0.12491873316615586,\n",
       "   0.12250712250712252,\n",
       "   0.12211902304781562,\n",
       "   0.11624649859943977,\n",
       "   0.12252252252252253,\n",
       "   0.14646464646464646,\n",
       "   0.1126984126984127,\n",
       "   0.10521885521885521,\n",
       "   0.10213907964322362,\n",
       "   0.10525282277859598,\n",
       "   0.10854123354123353,\n",
       "   0.12882040919424098,\n",
       "   0.10833333333333334,\n",
       "   0.1390835579514825,\n",
       "   0.13975922825480347,\n",
       "   0.1579685746352413,\n",
       "   0.13352007469654528,\n",
       "   0.09423118629364563,\n",
       "   0.1209761163032191,\n",
       "   0.12713675213675213,\n",
       "   0.13485252046956694,\n",
       "   0.14112554112554113,\n",
       "   0.1281305114638448,\n",
       "   0.1372434017595308,\n",
       "   0.12558012558012557,\n",
       "   0.13226694186446508,\n",
       "   0.12345679012345678,\n",
       "   0.11764705882352942,\n",
       "   0.1421881651766709,\n",
       "   0.09500000000000001,\n",
       "   0.11174968071519796,\n",
       "   0.12277777777777778,\n",
       "   0.1211524024024024,\n",
       "   0.12172183034906854,\n",
       "   0.13974842767295595,\n",
       "   0.08741830065359478,\n",
       "   0.13440594059405941,\n",
       "   0.13279395900755123,\n",
       "   0.1475095785440613,\n",
       "   0.10875747768951653,\n",
       "   0.11542045189843932,\n",
       "   0.13333333333333333,\n",
       "   0.13207396779850153,\n",
       "   0.10888752052545157,\n",
       "   0.09908551904331224,\n",
       "   0.16774599748156147,\n",
       "   0.13750134278655066,\n",
       "   0.10058651026392962,\n",
       "   0.13037075537075538,\n",
       "   0.0777172679827547,\n",
       "   0.10974632843791722,\n",
       "   0.12035448827901658,\n",
       "   0.11120050058103155,\n",
       "   0.14446910839807622,\n",
       "   0.10929824561403507,\n",
       "   0.1443001443001443,\n",
       "   0.16527777777777777,\n",
       "   0.14518468920879365,\n",
       "   0.14506172839506173,\n",
       "   0.11826086956521738,\n",
       "   0.15948519948519949,\n",
       "   0.11331070889894419,\n",
       "   0.12133933033483257,\n",
       "   0.12389954591789454,\n",
       "   0.1250841750841751,\n",
       "   0.11779448621553884,\n",
       "   0.138014763014763,\n",
       "   0.12115384615384617,\n",
       "   0.14168877099911584,\n",
       "   0.1393939393939394,\n",
       "   0.13129251700680272,\n",
       "   0.08388138917733934,\n",
       "   0.11048133153396311,\n",
       "   0.1328335832083958,\n",
       "   0.12323405880106912,\n",
       "   0.12380952380952381,\n",
       "   0.1187138406764575,\n",
       "   0.1358024691358025,\n",
       "   0.141025641025641,\n",
       "   0.1590909090909091,\n",
       "   0.1281305114638448,\n",
       "   0.1568627450980392,\n",
       "   0.10998877665544332,\n",
       "   0.15961199294532627,\n",
       "   0.12409983229752393,\n",
       "   0.16149320348955162,\n",
       "   0.1297398589065256,\n",
       "   0.12866479925303456,\n",
       "   0.11893472419788209,\n",
       "   0.09775694676933377,\n",
       "   0.07439271255060728,\n",
       "   0.13257667230137649,\n",
       "   0.10268317853457172,\n",
       "   0.10178035178035177,\n",
       "   0.12735935495693604,\n",
       "   0.11408090117767537,\n",
       "   0.11230648123852009,\n",
       "   0.14202898550724638,\n",
       "   0.1349206349206349,\n",
       "   0.11992753623188405,\n",
       "   0.135973597359736,\n",
       "   0.07423048048048048,\n",
       "   0.1324173636001593,\n",
       "   0.11176088369070825,\n",
       "   0.08782679738562092,\n",
       "   0.1264367816091954,\n",
       "   0.11428571428571428,\n",
       "   0.14603960396039603,\n",
       "   0.07605466428995841,\n",
       "   0.1334094585593301,\n",
       "   0.12099146807884671,\n",
       "   0.13465608465608467,\n",
       "   0.12504170837504172,\n",
       "   0.10332821300563237,\n",
       "   0.1343915343915344,\n",
       "   0.1168091168091168,\n",
       "   0.12381989832970225,\n",
       "   0.11091954022988505,\n",
       "   0.13516351635163515,\n",
       "   0.1101110111011101,\n",
       "   0.12063492063492064,\n",
       "   0.13438607798904287,\n",
       "   0.10933706816059757,\n",
       "   0.13847117794486216,\n",
       "   0.13015873015873017,\n",
       "   0.1492325658992326,\n",
       "   0.10878112712975098,\n",
       "   0.12307098765432099,\n",
       "   0.12979830839297332,\n",
       "   0.17172435593488225,\n",
       "   0.14270152505446623,\n",
       "   0.10846560846560845,\n",
       "   0.13955342902711323,\n",
       "   0.11961683973275376,\n",
       "   0.16468253968253968,\n",
       "   0.09258241758241759,\n",
       "   0.11914098972922503,\n",
       "   0.09514101257220524,\n",
       "   0.150483202945237,\n",
       "   0.14424160468785216,\n",
       "   0.08663892741562644,\n",
       "   0.08374500117619384,\n",
       "   0.1101110111011101,\n",
       "   0.1863777089783282,\n",
       "   0.1244577911244578,\n",
       "   0.09219679825664236,\n",
       "   0.09119769119769121,\n",
       "   0.13756613756613756,\n",
       "   0.12728937728937728,\n",
       "   0.14999623692330852,\n",
       "   0.14527148507731033,\n",
       "   0.09367278977333764,\n",
       "   0.13336913712853563,\n",
       "   0.15554298642533937,\n",
       "   0.14614614614614616,\n",
       "   0.11772339636633296,\n",
       "   0.1173076923076923,\n",
       "   0.16243386243386246,\n",
       "   0.12398169847115277,\n",
       "   0.10858585858585858,\n",
       "   0.12540296582849775,\n",
       "   0.0964788067591806,\n",
       "   0.13541666666666666,\n",
       "   0.1287112010796221,\n",
       "   0.21771637545713124,\n",
       "   0.11728395061728396,\n",
       "   0.12037037037037036,\n",
       "   0.1367448065561273,\n",
       "   0.10072150072150073,\n",
       "   0.14333333333333334,\n",
       "   0.1206858890345129,\n",
       "   0.08213532681617788,\n",
       "   0.16870644229134793,\n",
       "   0.1407031124012256,\n",
       "   0.13663663663663664,\n",
       "   0.10773273273273272,\n",
       "   0.14931145066968496,\n",
       "   0.13409090909090907,\n",
       "   0.11840628507295174,\n",
       "   0.08731599112724338,\n",
       "   0.1274412446209864,\n",
       "   0.13103582554517135,\n",
       "   0.11923076923076921,\n",
       "   0.0964026227184122,\n",
       "   0.13643410852713178,\n",
       "   0.10775151181968114,\n",
       "   0.1278825995807128,\n",
       "   0.15886939571150097,\n",
       "   0.13333333333333333,\n",
       "   0.1111111111111111,\n",
       "   0.14616818075937574,\n",
       "   0.12035112035112035,\n",
       "   0.1293231961836613,\n",
       "   0.12389954591789454,\n",
       "   0.12165898617511521,\n",
       "   0.1425471046660748,\n",
       "   0.148840429214261,\n",
       "   0.12092352092352092,\n",
       "   0.13880678708264915,\n",
       "   0.12602040816326532,\n",
       "   0.1276276276276276,\n",
       "   0.1203643461288224,\n",
       "   0.11111111111111112,\n",
       "   0.11234396671289874,\n",
       "   0.14371381306865177,\n",
       "   0.1324173636001593,\n",
       "   0.10823444804027327,\n",
       "   0.13667150459603292,\n",
       "   0.1492877492877493,\n",
       "   0.08374500117619384,\n",
       "   0.1188811188811189,\n",
       "   0.10185185185185186,\n",
       "   0.0951103160405486,\n",
       "   0.16117890858853326,\n",
       "   0.1320872274143302,\n",
       "   0.09841269841269841,\n",
       "   0.11414141414141414,\n",
       "   0.10687830687830686,\n",
       "   0.0956386292834891,\n",
       "   0.07379295804343243,\n",
       "   0.11292478045055365,\n",
       "   0.13141025641025642,\n",
       "   0.12279783708355137,\n",
       "   0.08055555555555556,\n",
       "   0.15756056808688387,\n",
       "   0.1015873015873016,\n",
       "   0.1126984126984127,\n",
       "   0.12877846790890268,\n",
       "   0.15515940388605923,\n",
       "   0.12654320987654322,\n",
       "   0.11844863731656186,\n",
       "   0.13102422811160674,\n",
       "   0.07645687645687646,\n",
       "   0.11150793650793651,\n",
       "   0.11578947368421051,\n",
       "   0.09226190476190477,\n",
       "   0.143662484088016,\n",
       "   0.10422371587420132,\n",
       "   0.13325242718446603,\n",
       "   0.11737208907020229,\n",
       "   0.14934390635325215,\n",
       "   0.14911786786786788,\n",
       "   0.08471690080885481,\n",
       "   0.16257816257816257,\n",
       "   0.12118279569892472,\n",
       "   0.11844660194174757,\n",
       "   0.15254413291796468,\n",
       "   0.13382594417077176,\n",
       "   0.14479081214109926,\n",
       "   0.05839646464646464,\n",
       "   0.14894419306184012,\n",
       "   0.11412025208652699,\n",
       "   0.14926197805667377,\n",
       "   0.09840571742715777,\n",
       "   0.11197530864197532,\n",
       "   0.12467654669489532,\n",
       "   0.11812106918238995,\n",
       "   0.1425026968716289,\n",
       "   0.1029351395730706,\n",
       "   0.09908496732026144,\n",
       "   0.1091893780573026,\n",
       "   0.10552123173482397,\n",
       "   0.10775131567210776,\n",
       "   0.13252923976608186,\n",
       "   0.13452432199741712,\n",
       "   0.12205387205387204,\n",
       "   0.0782967032967033,\n",
       "   0.12267317668599327,\n",
       "   0.1298629531388152,\n",
       "   0.14444444444444446,\n",
       "   0.15179573512906847,\n",
       "   0.11171171171171172,\n",
       "   0.1322179322179322,\n",
       "   0.11918328584995252,\n",
       "   0.11914098972922503,\n",
       "   0.10459071283813552,\n",
       "   0.15522875816993464,\n",
       "   0.0942008486562942,\n",
       "   0.15443560271146475,\n",
       "   0.10437091503267974,\n",
       "   0.08680555555555554,\n",
       "   0.11245791245791244,\n",
       "   0.12678132678132678,\n",
       "   0.1281661600810537,\n",
       "   0.15132090132090134,\n",
       "   0.10700757575757576,\n",
       "   0.1437025260554672,\n",
       "   0.1699719887955182,\n",
       "   0.12072649572649573,\n",
       "   0.11635935000420983,\n",
       "   0.11351186853317102,\n",
       "   0.11466087675765095,\n",
       "   0.09441707717569786,\n",
       "   0.12558987558987558,\n",
       "   0.11795543905635648,\n",
       "   0.14103887168902648,\n",
       "   0.13121757705218187,\n",
       "   0.09991909385113269,\n",
       "   0.1383177570093458,\n",
       "   0.12172460590875873,\n",
       "   0.13179441649403176,\n",
       "   0.175,\n",
       "   0.10649087221095337,\n",
       "   0.13550378335324573,\n",
       "   0.16422066681718972,\n",
       "   0.13621997471554995,\n",
       "   0.08403137880600985,\n",
       "   0.15067340067340065,\n",
       "   0.15595238095238095,\n",
       "   0.1188608776844071,\n",
       "   0.10931034482758621,\n",
       "   0.11205513074671954,\n",
       "   0.12984548658172948,\n",
       "   0.14410156611991476,\n",
       "   0.13186813186813187,\n",
       "   0.10185185185185186,\n",
       "   0.15571581196581197,\n",
       "   0.10022577003709081,\n",
       "   0.11188811188811189,\n",
       "   0.1211524024024024,\n",
       "   0.10783349721402818,\n",
       "   0.11460761460761461,\n",
       "   0.10198135198135198,\n",
       "   0.11659475008989573,\n",
       "   0.14813815434430652,\n",
       "   0.12161844271936016,\n",
       "   0.08662486938349008,\n",
       "   0.09264367816091955,\n",
       "   0.1666204345815996,\n",
       "   0.10719373219373218,\n",
       "   0.13239116309291746,\n",
       "   0.15,\n",
       "   0.12800819252432155,\n",
       "   0.10606060606060606,\n",
       "   0.12612612612612614,\n",
       "   0.10493827160493825,\n",
       "   0.1313131313131313,\n",
       "   0.14832535885167464,\n",
       "   0.11334714412548091,\n",
       "   0.12222222222222223,\n",
       "   0.11614555732202791,\n",
       "   0.1272066458982347,\n",
       "   0.11379585326953749,\n",
       "   0.09414001157120423,\n",
       "   0.15498575498575498,\n",
       "   0.11759259259259258,\n",
       "   0.11186741639538327,\n",
       "   0.15213675213675212,\n",
       "   0.14496578690127077,\n",
       "   0.13529411764705881,\n",
       "   0.13440594059405941,\n",
       "   0.15337567631145613,\n",
       "   0.07894032646507894,\n",
       "   0.13935782853359815,\n",
       "   0.08758443978569765,\n",
       "   0.09607843137254903,\n",
       "   0.08942364824717768,\n",
       "   0.09663865546218486,\n",
       "   0.1528650295069484,\n",
       "   0.1278825995807128,\n",
       "   0.10925925925925926,\n",
       "   0.12404040404040405,\n",
       "   0.12277227722772277,\n",
       "   0.13532763532763534,\n",
       "   0.11907642669893122,\n",
       "   0.11357723577235773,\n",
       "   0.14382716049382716,\n",
       "   0.11536536536536536,\n",
       "   0.09810479375696768,\n",
       "   0.10562119584675976,\n",
       "   0.16825396825396824,\n",
       "   0.1060515873015873,\n",
       "   0.0876984126984127,\n",
       "   0.10733549083063647,\n",
       "   0.10230529595015576,\n",
       "   0.13945578231292519,\n",
       "   0.14926197805667377,\n",
       "   0.12057373375168086,\n",
       "   0.1257219471947195,\n",
       "   0.13447971781305115,\n",
       "   0.13239116309291746,\n",
       "   0.14603174603174604,\n",
       "   0.10555555555555556,\n",
       "   0.1240981240981241,\n",
       "   0.07852564102564102,\n",
       "   0.11252204585537919,\n",
       "   0.10185185185185186,\n",
       "   0.13638843426077468,\n",
       "   0.11183923110528615,\n",
       "   0.08802970729576236,\n",
       "   0.08785377358490565,\n",
       "   0.10398860398860399,\n",
       "   0.15619645916619074,\n",
       "   0.10052910052910052,\n",
       "   0.12656641604010024,\n",
       "   0.10691056910569106,\n",
       "   0.16626984126984126,\n",
       "   0.15751633986928104,\n",
       "   0.151994301994302,\n",
       "   0.13305195457742938,\n",
       "   0.13247863247863248,\n",
       "   0.12996727442730246,\n",
       "   0.07227319062181448,\n",
       "   0.13479800418477386,\n",
       "   0.15014477942428886,\n",
       "   0.12034229499359882,\n",
       "   0.12415540540540541,\n",
       "   0.10997732426303854,\n",
       "   0.12791979949874688,\n",
       "   0.09506057781919852,\n",
       "   0.12106843685791054,\n",
       "   0.12946428571428573,\n",
       "   0.10548632744894426,\n",
       "   0.1162155745489079,\n",
       "   0.135202492211838,\n",
       "   0.15961199294532627,\n",
       "   0.10909090909090909,\n",
       "   0.136908962597036,\n",
       "   0.1252941176470588,\n",
       "   0.12815533980582525,\n",
       "   0.10649397373628351,\n",
       "   0.15229885057471262,\n",
       "   0.1114024864024864,\n",
       "   0.07988429016466399,\n",
       "   0.12573099415204678,\n",
       "   0.11549019607843138,\n",
       "   0.15860465116279068,\n",
       "   0.10207377188509265,\n",
       "   0.08704557091653865,\n",
       "   0.10176017601760175,\n",
       "   0.14103887168902648,\n",
       "   0.145992145992146,\n",
       "   0.12839506172839507,\n",
       "   0.07568134171907757,\n",
       "   0.15195303680289005,\n",
       "   0.15098905723905723,\n",
       "   0.11431976530986432,\n",
       "   0.11156462585034015,\n",
       "   0.14028651292802238,\n",
       "   0.12875816993464054,\n",
       "   0.09114027057267826,\n",
       "   0.1328476344666307,\n",
       "   0.12309074573225516,\n",
       "   0.15303030303030304,\n",
       "   0.12969058591178406,\n",
       "   0.10142857142857142,\n",
       "   0.0754985754985755,\n",
       "   0.1393939393939394,\n",
       "   0.16319948395420095,\n",
       "   0.1519607843137255,\n",
       "   0.14081790123456792,\n",
       "   0.1053735255570118,\n",
       "   0.10447330447330448,\n",
       "   0.13323412698412698,\n",
       "   0.11064425770308124,\n",
       "   0.09812409812409813,\n",
       "   0.10733598612386491,\n",
       "   0.11887125220458554,\n",
       "   0.06182345720343712,\n",
       "   0.1362380614097403,\n",
       "   0.11866931479642502,\n",
       "   0.12534883720930232,\n",
       "   0.08554639804639803,\n",
       "   0.11741122565864832,\n",
       "   0.1263034879539734,\n",
       "   0.14738788719371243,\n",
       "   0.08782051282051283,\n",
       "   0.10209790209790211,\n",
       "   0.11483212184146763,\n",
       "   0.12606837606837606,\n",
       "   0.11794871794871796,\n",
       "   0.11033576051779936,\n",
       "   0.12756410256410255,\n",
       "   0.08312447786131996,\n",
       "   0.1602364406102724,\n",
       "   0.09828795379537954,\n",
       "   0.1334946561806816,\n",
       "   0.11251167133520075,\n",
       "   0.11981787682722263,\n",
       "   0.12535885167464114,\n",
       "   0.12587991718426503,\n",
       "   0.09728526924788607,\n",
       "   0.13443895553987298,\n",
       "   0.15784832451499117,\n",
       "   0.09615384615384615,\n",
       "   0.15488215488215487,\n",
       "   0.11774891774891776,\n",
       "   0.12200040330711837,\n",
       "   0.13590573844216586,\n",
       "   0.14214106695309703,\n",
       "   0.1726851851851852,\n",
       "   0.13876088069636458,\n",
       "   0.11929824561403508,\n",
       "   0.1396151053013798,\n",
       "   0.09703062894552256,\n",
       "   0.12313341493268054,\n",
       "   0.12091503267973856,\n",
       "   0.10562338142804291,\n",
       "   0.1331009874699195,\n",
       "   0.1479960899315738,\n",
       "   0.09896411394746578,\n",
       "   0.08588235294117648,\n",
       "   0.13382978723404257,\n",
       "   0.12269412269412268,\n",
       "   0.1281305114638448,\n",
       "   0.14684581665713742,\n",
       "   0.08840970350404313,\n",
       "   0.10051921079958463,\n",
       "   0.14295977011494254,\n",
       "   0.135547201336675,\n",
       "   0.14143302180685358,\n",
       "   0.15372589890568003,\n",
       "   0.12987012987012989,\n",
       "   0.12115878254492114,\n",
       "   0.11861861861861862,\n",
       "   0.1264957264957265,\n",
       "   0.14634920634920637,\n",
       "   0.13833333333333334,\n",
       "   0.11791383219954647,\n",
       "   0.1287363125184966,\n",
       "   0.13713713713713713,\n",
       "   0.14235816104974983,\n",
       "   0.12983739837398375,\n",
       "   0.12471655328798185,\n",
       "   0.1498168498168498,\n",
       "   0.11301044634377967,\n",
       "   0.13679699116592323,\n",
       "   0.16121495327102806,\n",
       "   0.11972332417861865,\n",
       "   0.11333333333333334,\n",
       "   0.14057239057239057,\n",
       "   0.12093023255813955,\n",
       "   0.11356811356811358,\n",
       "   0.1150197628458498,\n",
       "   0.15195569401176878,\n",
       "   0.10853404698186142,\n",
       "   0.10015246807699636,\n",
       "   0.15130081300813006,\n",
       "   0.1176904176904177,\n",
       "   0.14185094185094185,\n",
       "   0.12845891868880374,\n",
       "   0.11696466581524052,\n",
       "   0.07910906298003072,\n",
       "   0.08968253968253968,\n",
       "   0.10845238095238095,\n",
       "   0.10810810810810811,\n",
       "   0.10015929908403026,\n",
       "   0.14325147534742053,\n",
       "   0.13698955365622031,\n",
       "   0.12900641025641027,\n",
       "   0.07823008849557522,\n",
       "   0.148984593837535,\n",
       "   0.11857195708472068,\n",
       "   0.14228070175438598,\n",
       "   0.08623188405797101,\n",
       "   0.10006006006006006,\n",
       "   0.12641138929798723,\n",
       "   0.1176904176904177,\n",
       "   0.11815661815661815,\n",
       "   0.09205851619644723,\n",
       "   0.12237762237762238,\n",
       "   0.12696078431372548,\n",
       "   0.1122334455667789,\n",
       "   0.11551073946548614,\n",
       "   0.15132090132090134,\n",
       "   0.15010482180293502,\n",
       "   0.11373320222877743,\n",
       "   0.11692084241103849,\n",
       "   0.12455173620222164,\n",
       "   0.11800356506238859,\n",
       "   0.1175862068965517,\n",
       "   0.0976608187134503,\n",
       "   0.10289115646258502,\n",
       "   0.11784511784511785,\n",
       "   0.09355529595015577,\n",
       "   0.13040123456790123,\n",
       "   0.1091091091091091,\n",
       "   0.12481060606060605,\n",
       "   0.14837235865219872,\n",
       "   0.11381172839506172,\n",
       "   0.12396049896049895,\n",
       "   0.14921652421652423,\n",
       "   0.12968099861303745,\n",
       "   0.11171171171171172,\n",
       "   0.13636363636363635,\n",
       "   0.10784313725490197,\n",
       "   0.09311180339217721,\n",
       "   0.10434472934472934,\n",
       "   0.16286411249820634,\n",
       "   0.13142857142857142,\n",
       "   0.13173990020923868,\n",
       "   0.13774774774774776,\n",
       "   0.12281746031746033,\n",
       "   0.08061749571183534,\n",
       "   0.10763888888888888,\n",
       "   0.1261018168735384,\n",
       "   0.0767690253671562,\n",
       "   0.09346818373367045,\n",
       "   0.06771799628942486,\n",
       "   0.13549382716049382,\n",
       "   0.10555555555555556,\n",
       "   0.10687351863822452,\n",
       "   0.05585264408793821,\n",
       "   0.12453703703703704,\n",
       "   0.09206349206349207,\n",
       "   0.15555555555555556,\n",
       "   0.15493697478991597,\n",
       "   0.14869281045751634,\n",
       "   0.10971055088702147,\n",
       "   0.09594298245614034,\n",
       "   0.11875000000000001,\n",
       "   0.1246246246246246,\n",
       "   0.09514101257220524,\n",
       "   0.14415584415584415,\n",
       "   0.1313915857605178,\n",
       "   0.09252336448598131,\n",
       "   0.15490196078431373,\n",
       "   0.12607888124112313,\n",
       "   0.12176814011676397,\n",
       "   0.12286324786324787,\n",
       "   0.12114060963618485,\n",
       "   0.11635935000420983,\n",
       "   0.11587301587301586,\n",
       "   0.13888888888888887,\n",
       "   0.14743589743589744,\n",
       "   0.1152755626439837,\n",
       "   0.11807017543859649,\n",
       "   0.12418921056794811,\n",
       "   0.11215932914046121,\n",
       "   0.12131268436578173,\n",
       "   0.11408090117767537,\n",
       "   0.12919020715630883,\n",
       "   0.18160627251536343,\n",
       "   0.11163032191069573,\n",
       "   0.1105121293800539,\n",
       "   0.08730393607576208,\n",
       "   0.14064171122994654,\n",
       "   0.14006116207951072,\n",
       "   0.12398867313915858,\n",
       "   0.12133933033483257,\n",
       "   0.1387989931679252,\n",
       "   0.12418300653594772,\n",
       "   0.13658861602893582,\n",
       "   0.12845651080945197,\n",
       "   0.12756410256410255,\n",
       "   0.15477701616315476,\n",
       "   0.09449541284403669,\n",
       "   0.10412979351032448,\n",
       "   0.1636904761904762,\n",
       "   0.12804597701149426,\n",
       "   0.14141939961825437,\n",
       "   0.1164957264957265,\n",
       "   0.10085669781931465,\n",
       "   0.11414141414141414,\n",
       "   0.10626102292768959,\n",
       "   0.12852233676975947,\n",
       "   0.10476190476190476,\n",
       "   0.133698518969365,\n",
       "   0.10562338142804291,\n",
       "   0.112275797810389,\n",
       "   0.14095449500554938,\n",
       "   0.14788677173998274,\n",
       "   0.10680907877169558,\n",
       "   0.13387000596302923,\n",
       "   0.1540835707502374,\n",
       "   0.13874386964087962,\n",
       "   0.11263434035711263,\n",
       "   0.1461675579322638,\n",
       "   0.1057022368903557,\n",
       "   0.17303510406958686,\n",
       "   0.10285204991087345,\n",
       "   0.12289182511547965,\n",
       "   0.11728395061728396,\n",
       "   0.135202492211838,\n",
       "   0.18007254107104761,\n",
       "   0.16071428571428573,\n",
       "   0.12549019607843137,\n",
       "   0.10802469135802469,\n",
       "   0.11246312684365782,\n",
       "   0.1309327469305479,\n",
       "   0.11064425770308123,\n",
       "   0.14044401544401544,\n",
       "   0.15418894830659535,\n",
       "   0.14684581665713742,\n",
       "   0.12169312169312169,\n",
       "   0.16595074455899197,\n",
       "   0.11544878211544878,\n",
       "   0.1263520157325467,\n",
       "   0.1523432067862606,\n",
       "   0.14001670843776107,\n",
       "   0.13391812865497077,\n",
       "   0.12205387205387207,\n",
       "   0.12800591934887162,\n",
       "   0.13606654783125371,\n",
       "   0.12380952380952381,\n",
       "   0.11904087605022184,\n",
       "   0.13325718636969353,\n",
       "   0.13232600732600733,\n",
       "   0.13237639553429026,\n",
       "   0.1295306001188354,\n",
       "   0.1270955165692008,\n",
       "   0.11483212184146763,\n",
       "   0.12690631808278865,\n",
       "   0.1281305114638448,\n",
       "   0.10211207630727304,\n",
       "   0.08946886446886447,\n",
       "   0.152991452991453,\n",
       "   0.1310272536687631,\n",
       "   0.1466973886328725,\n",
       "   0.14335285187594882,\n",
       "   0.11393939393939394,\n",
       "   0.10840840840840842,\n",
       "   0.15107458912768648,\n",
       "   0.12987012987012989,\n",
       "   0.1066137566137566,\n",
       "   0.14741532976827096,\n",
       "   0.09011191877235492,\n",
       "   0.1352015732546706,\n",
       "   0.11301044634377967,\n",
       "   0.09733262674439146,\n",
       "   0.14365079365079367,\n",
       "   0.10282874617737003,\n",
       "   0.13416742955955652,\n",
       "   0.12467654669489532,\n",
       "   0.15327587804652024,\n",
       "   0.12372448979591837,\n",
       "   0.12349501203990369,\n",
       "   0.13088301210219633,\n",
       "   0.095125786163522,\n",
       "   0.1209761163032191,\n",
       "   0.14662803798487042,\n",
       "   0.12838915470494416,\n",
       "   0.11717171717171719,\n",
       "   0.12231559290382821,\n",
       "   0.15615615615615616,\n",
       "   0.08442220368825874,\n",
       "   0.0811965811965812,\n",
       "   0.1324404761904762,\n",
       "   0.10675273088381332,\n",
       "   0.13833333333333334,\n",
       "   0.11174636174636175,\n",
       "   0.13892089093701998,\n",
       "   0.09577922077922078,\n",
       "   0.11303960836671116,\n",
       "   0.12576933369012577,\n",
       "   0.14001986097318767,\n",
       "   0.14835164835164835,\n",
       "   0.11026916094403662,\n",
       "   0.13333333333333333,\n",
       "   0.1054574824045229,\n",
       "   0.11053361053361055,\n",
       "   0.14322169059011164,\n",
       "   0.082010582010582,\n",
       "   0.08928571428571429,\n",
       "   0.072008547008547,\n",
       "   0.12884615384615383,\n",
       "   0.11568627450980391,\n",
       "   0.14247766240038637,\n",
       "   0.10213243546576879,\n",
       "   0.10135462022254475,\n",
       "   0.15029761904761904,\n",
       "   0.1510737628384687,\n",
       "   0.13981615868408323,\n",
       "   0.09114015976761075,\n",
       "   0.1324173636001593,\n",
       "   0.13764367816091955,\n",
       "   0.12317073170731707,\n",
       "   0.13971742543171115,\n",
       "   0.16310160427807488,\n",
       "   0.13658861602893582,\n",
       "   0.1315988886082344,\n",
       "   0.10238095238095239,\n",
       "   0.11602339181286549,\n",
       "   0.13182049974502805,\n",
       "   0.14771929824561403,\n",
       "   0.13273001508295626,\n",
       "   0.08511396011396011,\n",
       "   0.1343915343915344],\n",
       "  'mean': 0.12363651505278284,\n",
       "  'std': 0.021526954776560532,\n",
       "  'ci_lower': 0.07988235990572438,\n",
       "  'ci_upper': 0.16370373095614402}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_bertweet_bootstrap_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
