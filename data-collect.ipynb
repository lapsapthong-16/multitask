{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "import re\n",
    "import os  # Added for file existence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your own Reddit API credentials\n",
    "client_id = 'zwNVQTjvLRlJBm4IytY5nA'\n",
    "client_secret = 'OtY1GFZNIpqep-2UoiU8qiMQmSwhhg'\n",
    "user_agent = \"linux:note7_sentiment_scraper:1.0 (by /u/MinimumBeginning2278)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search parameters\n",
    "subreddit_name = 'samsung'\n",
    "search_queries = [\n",
    "    \"Galaxy Note 7 recall\",\n",
    "    \"Note 7 recall\",\n",
    "    \"Note 7 battery fire\",\n",
    "    \"Note 7 explosion\",\n",
    "    \"Note 7 refund\",\n",
    "    \"Note 7 replacement\",\n",
    "    \"My Note 7 exploded\",\n",
    "    \"Samsung Note 7 angry\",\n",
    "    \"Note 7 customer service\",\n",
    "    \"Samsung Note 7 disappointed\",\n",
    "    \"Note 7 overheating\"\n",
    "]\n",
    "max_posts_per_query = 100\n",
    "max_comments_per_post = 10\n",
    "\n",
    "# Define time range (August 1, 2016 to December 31, 2016)\n",
    "start_date = datetime(2016, 8, 1).timestamp()\n",
    "end_date = datetime(2016, 12, 31, 23, 59, 59).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Initialize lists to store post data\n",
    "posts_data = []\n",
    "processed_post_ids = set()  # To avoid duplicates across queries\n",
    "\n",
    "# File to track collected IDs\n",
    "collected_ids_file = \"collected_ids.json\"  # file to store previously collected IDs\n",
    "\n",
    "# Load previously collected IDs from file\n",
    "def load_collected_ids():\n",
    "    \"\"\"Load previously collected post and comment IDs from file\"\"\"\n",
    "    if os.path.exists(collected_ids_file):\n",
    "        try:\n",
    "            with open(collected_ids_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return set(data.get('post_ids', [])), set(data.get('comment_ids', []))\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            print(f\"Warning: Could not load {collected_ids_file}, starting fresh\")\n",
    "            return set(), set()\n",
    "    else:\n",
    "        print(f\"{collected_ids_file} not found, starting fresh\")\n",
    "        return set(), set()\n",
    "\n",
    "# Save collected IDs to file\n",
    "def save_collected_ids(post_ids, comment_ids):\n",
    "    \"\"\"Save collected post and comment IDs to file\"\"\"\n",
    "    data = {\n",
    "        'post_ids': list(post_ids),\n",
    "        'comment_ids': list(comment_ids),\n",
    "        'last_updated': datetime.now().isoformat()\n",
    "    }\n",
    "    with open(collected_ids_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Load existing collected IDs at startup\n",
    "existing_post_ids, existing_comment_ids = load_collected_ids()\n",
    "print(f\"Loaded {len(existing_post_ids)} previously collected post IDs and {len(existing_comment_ids)} comment IDs\")\n",
    "\n",
    "# News domains for classification\n",
    "news_domains = [\n",
    "    'cnn', 'bbc', 'reuters', 'nytimes', 'washingtonpost', 'theguardian', \n",
    "    'forbes', 'bloomberg', 'techcrunch', 'verge', 'engadget', 'androidpolice',\n",
    "    'androidcentral', 'gsmarena', 'phonearena', '9to5google', 'arstechnica'\n",
    "]\n",
    "\n",
    "print(f\"Collecting posts from r/{subreddit_name} for multiple queries...\")\n",
    "print(f\"Time range: August 1, 2016 - December 31, 2016\")\n",
    "print(f\"Queries: {search_queries}\")\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"Check if text is in English using langdetect\"\"\"\n",
    "    try:\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return False\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    \"\"\"Get subjectivity score using TextBlob, handle errors gracefully\"\"\"\n",
    "    try:\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return 0.0\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment.subjectivity\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def classify_post_type(post):\n",
    "    \"\"\"Classify post as 'news' or 'opinion' based on URL and content\"\"\"\n",
    "    try:\n",
    "        # Check if URL contains news domain\n",
    "        for domain in news_domains:\n",
    "            if domain.lower() in post.url.lower():\n",
    "                return \"news\"\n",
    "        \n",
    "        # Check if it's a self post (selftext exists and URL is Reddit permalink)\n",
    "        if post.selftext and post.selftext.strip() and 'reddit.com' in post.url:\n",
    "            return \"opinion\"\n",
    "        \n",
    "        # Default classification\n",
    "        return \"other\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def extract_comments(post, max_comments=10):\n",
    "    \"\"\"Extract top-level comments from a post\"\"\"\n",
    "    comments_data = []\n",
    "    try:\n",
    "        # Load all comments\n",
    "        post.comments.replace_more(limit=0)\n",
    "        \n",
    "        # Get top-level comments only\n",
    "        top_comments = post.comments[:max_comments]\n",
    "        \n",
    "        for comment in top_comments:\n",
    "            try:\n",
    "                if hasattr(comment, 'body') and comment.body != '[deleted]':\n",
    "                    # Check if comment ID already exists in collected IDs\n",
    "                    if comment.id in existing_comment_ids:\n",
    "                        print(f\"    Skipping already collected comment: {comment.id}\")\n",
    "                        continue  # Skip already collected comment\n",
    "                    \n",
    "                    comment_text = comment.body if comment.body else ''\n",
    "                    comment_subjectivity = get_subjectivity(comment_text)\n",
    "                    \n",
    "                    # Apply subjectivity filter to comments too (optional)\n",
    "                    if comment_subjectivity >= 0.4:\n",
    "                        comment_data = {\n",
    "                            'comment_id': comment.id,\n",
    "                            'comment_text': comment_text,\n",
    "                            'comment_score': comment.score if hasattr(comment, 'score') else 0,\n",
    "                            'comment_author': str(comment.author) if comment.author else '[deleted]',\n",
    "                            'comment_subjectivity': comment_subjectivity\n",
    "                        }\n",
    "                        comments_data.append(comment_data)\n",
    "                        # Add comment ID to existing set for this session\n",
    "                        existing_comment_ids.add(comment.id)\n",
    "            except Exception as comment_error:\n",
    "                continue  # Skip problematic comments\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"    Error extracting comments: {e}\")\n",
    "    \n",
    "    return comments_data\n",
    "\n",
    "def is_valid_post(post):\n",
    "    \"\"\"Check if post meets all filtering criteria\"\"\"\n",
    "    # Time filter\n",
    "    if not (start_date <= post.created_utc <= end_date):\n",
    "        return False, \"time_range\"\n",
    "    \n",
    "    # Score filter\n",
    "    if post.score < 2:\n",
    "        return False, \"low_score\"\n",
    "    \n",
    "    # Content filter - check if both title and selftext are empty/missing\n",
    "    title = post.title.strip() if post.title else \"\"\n",
    "    selftext = post.selftext.strip() if post.selftext else \"\"\n",
    "    \n",
    "    if not title and not selftext:\n",
    "        return False, \"empty_content\"\n",
    "    \n",
    "    # Language filter\n",
    "    combined_text = f\"{title} {selftext}\".strip()\n",
    "    if not is_english(combined_text):\n",
    "        return False, \"non_english\"\n",
    "    \n",
    "    # Subjectivity filter\n",
    "    subjectivity_score = get_subjectivity(combined_text)\n",
    "    if subjectivity_score < 0.4:\n",
    "        return False, \"low_subjectivity\"\n",
    "    \n",
    "    return True, \"valid\"\n",
    "\n",
    "try:\n",
    "    # Get the subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Search for each query\n",
    "    for query_idx, search_query in enumerate(search_queries):\n",
    "        print(f\"\\n--- Processing query {query_idx + 1}/{len(search_queries)}: '{search_query}' ---\")\n",
    "        \n",
    "        try:\n",
    "            # Search for posts in the subreddit\n",
    "            search_results = subreddit.search(search_query, limit=max_posts_per_query)\n",
    "            \n",
    "            query_posts_collected = 0\n",
    "            for post in search_results:\n",
    "                # Skip if we've already processed this post in this session\n",
    "                if post.id in processed_post_ids:\n",
    "                    continue\n",
    "                \n",
    "                # Check if post ID already exists in collected IDs from previous runs\n",
    "                if post.id in existing_post_ids:\n",
    "                    print(f\"  Skipping already collected post: {post.id}\")\n",
    "                    processed_post_ids.add(post.id)  # Add to session set to avoid reprocessing\n",
    "                    continue\n",
    "                \n",
    "                # Check if post meets filtering criteria\n",
    "                is_valid, reason = is_valid_post(post)\n",
    "                \n",
    "                if is_valid:\n",
    "                    # Get post content for analysis\n",
    "                    title = post.title if post.title else ''\n",
    "                    selftext = post.selftext if post.selftext else ''\n",
    "                    combined_text = f\"{title} {selftext}\".strip()\n",
    "                    \n",
    "                    # Get subjectivity and post type\n",
    "                    subjectivity_score = get_subjectivity(combined_text)\n",
    "                    post_type = classify_post_type(post)\n",
    "                    \n",
    "                    print(f\"  Extracting comments for post: {title[:30]}...\")\n",
    "                    \n",
    "                    # Extract comments (already handles comment ID checking internally)\n",
    "                    comments = extract_comments(post, max_comments_per_post)\n",
    "                    \n",
    "                    # Store post data\n",
    "                    post_data = {\n",
    "                        'id': post.id,\n",
    "                        'title': title,\n",
    "                        'selftext': selftext,\n",
    "                        'author': str(post.author) if post.author else '[deleted]',\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'created_date': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'score': post.score,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'url': post.url,\n",
    "                        'subreddit': str(post.subreddit),\n",
    "                        'search_query': search_query,\n",
    "                        'subjectivity': round(subjectivity_score, 3),\n",
    "                        'post_type': post_type,\n",
    "                        'extracted_comments': comments,\n",
    "                        'num_extracted_comments': len(comments)\n",
    "                    }\n",
    "                    posts_data.append(post_data)\n",
    "                    processed_post_ids.add(post.id)\n",
    "                    # Add post ID to existing set for this session\n",
    "                    existing_post_ids.add(post.id)\n",
    "                    query_posts_collected += 1\n",
    "                    \n",
    "                    # Print progress (show title, truncated if too long)\n",
    "                    title_preview = title[:50] + \"...\" if len(title) > 50 else title\n",
    "                    print(f\"  Collected post {len(posts_data)}: {title_preview} (subj: {subjectivity_score:.2f}, type: {post_type}, comments: {len(comments)})\")\n",
    "                else:\n",
    "                    if reason == \"low_subjectivity\":\n",
    "                        print(f\"  Skipped post (low subjectivity): {post.title[:30]}...\")\n",
    "                \n",
    "                # Add small delay to be respectful to Reddit's API\n",
    "                time.sleep(0.2)  # Increased delay due to comment extraction\n",
    "            \n",
    "            print(f\"  Query '{search_query}' yielded {query_posts_collected} valid posts\")\n",
    "            \n",
    "        except Exception as query_error:\n",
    "            print(f\"  Error processing query '{search_query}': {query_error}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nTotal collected: {len(posts_data)} posts\")\n",
    "\n",
    "    if posts_data:\n",
    "        # Create base filename\n",
    "        base_filename = f\"reddit_posts_note7_enhanced_{timestamp}\"\n",
    "        \n",
    "        # Save as JSON file\n",
    "        json_filename = f\"{base_filename}.json\"\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Posts saved to {json_filename}\")\n",
    "        \n",
    "        # Create DataFrame for summary statistics (only if posts_data is not empty)\n",
    "        df = pd.DataFrame(posts_data)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"- Total posts collected: {len(posts_data)}\")\n",
    "        print(f\"- Date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
    "        print(f\"- Average score: {df['score'].mean():.1f}\")\n",
    "        print(f\"- Average subjectivity: {df['subjectivity'].mean():.3f}\")\n",
    "        print(f\"- Total comments extracted: {df['num_extracted_comments'].sum()}\")\n",
    "        print(f\"- Post types distribution:\")\n",
    "        post_type_counts = df['post_type'].value_counts()\n",
    "        for post_type, count in post_type_counts.items():\n",
    "            print(f\"  {post_type}: {count} posts\")\n",
    "        print(f\"- Posts by query:\")\n",
    "        for query in search_queries:\n",
    "            count = len(df[df['search_query'] == query])\n",
    "            print(f\"  '{query}': {count} posts\")\n",
    "    else:\n",
    "        print(\"No new posts met the filtering criteria.\")\n",
    "\n",
    "    # Update collected_ids.json with all IDs (both existing and new)\n",
    "    save_collected_ids(existing_post_ids, existing_comment_ids)\n",
    "    print(f\"Updated {collected_ids_file} with {len(existing_post_ids)} post IDs and {len(existing_comment_ids)} comment IDs\")\n",
    "\n",
    "    print(\"\\nDownload complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    if posts_data:\n",
    "        # Save partial results if any were collected\n",
    "        json_filename = f\"reddit_posts_partial_{timestamp}.json\"\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(posts_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Partial results saved to {json_filename}\")\n",
    "    \n",
    "    # Still update collected IDs even if there was an error\n",
    "    save_collected_ids(existing_post_ids, existing_comment_ids)\n",
    "    print(f\"Updated {collected_ids_file} with current IDs\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def flatten_reddit_data(json_file_path, output_json_path):\n",
    "    \"\"\"\n",
    "    Flatten Reddit JSON data into a new JSON with posts and comments as separate entries.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the input JSON file\n",
    "        output_json_path: Path for the output JSON file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    flattened_rows = []\n",
    "    \n",
    "    # Process each post in the JSON\n",
    "    for post in data:\n",
    "        # Create a row for the post itself\n",
    "        post_title = post.get('title', '')\n",
    "        post_selftext = post.get('selftext', '')\n",
    "        \n",
    "        # Combine title and selftext with a space separator, handle empty selftext\n",
    "        if post_selftext:\n",
    "            text_content = f\"{post_title} {post_selftext}\".strip()\n",
    "        else:\n",
    "            text_content = post_title.strip()\n",
    "        \n",
    "        post_row = {\n",
    "            'id': post.get('id', ''),\n",
    "            'text_content': text_content,\n",
    "            'type': 'post',\n",
    "            'score': post.get('score', 0),\n",
    "            'subjectivity': post.get('subjectivity', 0.0),\n",
    "            'include_for_labeling': ''\n",
    "        }\n",
    "        flattened_rows.append(post_row)\n",
    "        \n",
    "        # Process all comments for this post\n",
    "        comments = post.get('extracted_comments', [])\n",
    "        for comment in comments:\n",
    "            comment_row = {\n",
    "                'id': comment.get('comment_id', ''),\n",
    "                'text_content': comment.get('comment_text', ''),\n",
    "                'type': 'comment',\n",
    "                'score': comment.get('comment_score', 0),\n",
    "                'subjectivity': comment.get('comment_subjectivity', 0.0),\n",
    "                'include_for_labeling': ''\n",
    "            }\n",
    "            flattened_rows.append(comment_row)\n",
    "    \n",
    "    # Export to JSON\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(flattened_rows, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    total_rows = len(flattened_rows)\n",
    "    posts = [row for row in flattened_rows if row['type'] == 'post']\n",
    "    comments = [row for row in flattened_rows if row['type'] == 'comment']\n",
    "    \n",
    "    print(f\"âœ… Data successfully flattened and exported to: {output_json_path}\")\n",
    "    print(f\"\\nðŸ“Š Summary:\")\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    print(f\"   Posts: {len(posts):,}\")\n",
    "    print(f\"   Comments: {len(comments):,}\")\n",
    "    \n",
    "    if posts:\n",
    "        post_scores = [p['score'] for p in posts]\n",
    "        post_subjectivities = [p['subjectivity'] for p in posts]\n",
    "        print(f\"\\nðŸ“ˆ Post statistics:\")\n",
    "        print(f\"   Scores - Min: {min(post_scores)}, Max: {max(post_scores)}, Avg: {sum(post_scores)/len(post_scores):.1f}\")\n",
    "        print(f\"   Subjectivity - Min: {min(post_subjectivities):.3f}, Max: {max(post_subjectivities):.3f}, Avg: {sum(post_subjectivities)/len(post_subjectivities):.3f}\")\n",
    "    \n",
    "    if comments:\n",
    "        comment_scores = [c['score'] for c in comments]\n",
    "        comment_subjectivities = [c['subjectivity'] for c in comments]\n",
    "        print(f\"\\nðŸ’¬ Comment statistics:\")\n",
    "        print(f\"   Scores - Min: {min(comment_scores)}, Max: {max(comment_scores)}, Avg: {sum(comment_scores)/len(comment_scores):.1f}\")\n",
    "        print(f\"   Subjectivity - Min: {min(comment_subjectivities):.3f}, Max: {max(comment_subjectivities):.3f}, Avg: {sum(comment_subjectivities)/len(comment_subjectivities):.3f}\")\n",
    "    \n",
    "    return flattened_rows\n",
    "\n",
    "def preview_data(json_file_path, num_rows=5):\n",
    "    \"\"\"\n",
    "    Preview the flattened JSON data.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file\n",
    "        num_rows: Number of rows to preview\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nðŸ‘€ Preview of first {num_rows} entries:\")\n",
    "    for i, entry in enumerate(data[:num_rows]):\n",
    "        print(f\"\\n{i+1}. ID: {entry['id']}\")\n",
    "        print(f\"   Type: {entry['type']}\")\n",
    "        print(f\"   Score: {entry['score']}\")\n",
    "        print(f\"   Subjectivity: {entry['subjectivity']}\")\n",
    "        print(f\"   Text: {entry['text_content'][:100]}{'...' if len(entry['text_content']) > 100 else ''}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths - update these to match your files\n",
    "    json_file_path = \"reddit-posts-5.json\"\n",
    "    output_json_path = \"flat5.json\"\n",
    "    \n",
    "    # Flatten the data\n",
    "    flattened_data = flatten_reddit_data(json_file_path, output_json_path)\n",
    "    \n",
    "    # Preview the results\n",
    "    preview_data(output_json_path, num_rows=5)\n",
    "    \n",
    "    print(f\"\\nâœ¨ Next steps:\")\n",
    "    print(f\"   1. Open {output_json_path} in your preferred JSON editor\")\n",
    "    print(f\"   2. Fill in the 'include_for_labeling' field with your labels\")\n",
    "    print(f\"   3. Use this data for training your model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
