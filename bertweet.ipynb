{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# COMPREHENSIVE MULTI-MODEL TRAINING SYSTEM - BERTWEET VERSION\n",
    "# ================================================================================================\n",
    "# This notebook trains:\n",
    "# 1. Single-task Sentiment Model (on SST-2) using BERTweet\n",
    "# 2. Single-task Emotion Model (on GoEmotion) using BERTweet\n",
    "# 3. Multi-task Model (on both datasets) using BERTweet\n",
    "# \n",
    "# Each model uses Bayesian optimization (TPE) for hyperparameter tuning\n",
    "# Only macro F1 and accuracy metrics are used for evaluation\n",
    "# ================================================================================================\n",
    "\n",
    "# Cell 1: Setup and Imports - BERTweet Version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Memory management\n",
    "def aggressive_memory_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"🧹 Memory cleaned!\")\n",
    "\n",
    "print(\"✅ BERTweet Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration Classes - BERTweet Version\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training parameters - BERTweet optimized\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",  # Changed to BERTweet\n",
    "        max_length: int = 128,\n",
    "        batch_size: int = 8,\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 3,\n",
    "        warmup_ratio: float = 0.1,\n",
    "        weight_decay: float = 0.01,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        output_dir: str = \"./bertweet_model_output\",\n",
    "        save_total_limit: int = 1,\n",
    "        # Multi-task specific\n",
    "        alpha: float = 0.5,  # Only used for multi-task\n",
    "        task_type: str = \"multitask\"  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_dropout_prob = attention_dropout_prob\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.alpha = alpha\n",
    "        self.task_type = task_type\n",
    "\n",
    "class BERTweetModelConfig:\n",
    "    \"\"\"Configuration for BERTweet model architecture\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "bertweet_model_config = BERTweetModelConfig()\n",
    "print(\"✅ BERTweet Configuration classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset Classes - BERTweet Version\n",
    "class BERTweetSingleTaskDataset(Dataset):\n",
    "    \"\"\"Dataset for single-task training with BERTweet (sentiment OR emotion)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # BERTweet specific preprocessing (handles tweets better)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "class BERTweetMultiTaskDataset(Dataset):\n",
    "    \"\"\"Dataset for multi-task training with BERTweet (sentiment AND emotion)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # BERTweet specific preprocessing\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "print(\"✅ BERTweet Dataset classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf89772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: BERTweet Model Architectures\n",
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \"\"\"Single-task BERTweet transformer for sentiment OR emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetSingleTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # BERTweet encoder\n",
    "        self.encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Classification head optimized for BERTweet\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),  # BERTweet uses GELU activation\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERTweet encoder output\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        pooled_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "    \n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        \"\"\"Save BERTweet model in HuggingFace format\"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"model_type\": \"BERTweetSingleTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"BERTweet single-task model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load BERTweet model from HuggingFace format\"\"\"\n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            num_classes=config[\"num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \"\"\"Multi-task BERTweet transformer for sentiment AND emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetMultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTweet configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared BERTweet encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers for BERTweet\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # BERTweet-base has 12 attention heads\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # BERTweet-base has 12 attention heads\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Classification heads optimized for BERTweet\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Shared BERTweet encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Task-specific attention\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        \n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        \n",
    "        # Classification\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_pooled)\n",
    "        emotion_logits = self.emotion_classifier(emotion_pooled)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "    \n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        \"\"\"Save BERTweet multi-task model in HuggingFace format\"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"sentiment_num_classes\": self.sentiment_num_classes,\n",
    "            \"emotion_num_classes\": self.emotion_num_classes,\n",
    "            \"model_type\": \"BERTweetMultiTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"BERTweet multi-task model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load BERTweet multi-task model from HuggingFace format\"\"\"\n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            sentiment_num_classes=config[\"sentiment_num_classes\"],\n",
    "            emotion_num_classes=config[\"emotion_num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "print(\"✅ BERTweet Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Loading and Processing - BERTweet Version\n",
    "def load_and_process_datasets_bertweet():\n",
    "    \"\"\"Load and process SST-2 and GoEmotion datasets for BERTweet\"\"\"\n",
    "    \n",
    "    print(\"📥 Loading datasets for BERTweet...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        print(f\"✅ SST-2 loaded: {len(sst2_dataset['train'])} train, {len(sst2_dataset['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load SST-2: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load GoEmotion for emotion\n",
    "    try:\n",
    "        emotion_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        print(f\"✅ GoEmotion loaded: {len(emotion_dataset['train'])} train, {len(emotion_dataset['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load GoEmotion: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Try to load existing encoders first\n",
    "    sentiment_encoder, emotion_encoder = load_existing_encoders_bertweet()\n",
    "    \n",
    "    # Process sentiment data (SST-2) for BERTweet\n",
    "    sentiment_data = process_sentiment_data_bertweet(sst2_dataset, sentiment_encoder)\n",
    "    \n",
    "    # Process emotion data (GoEmotion) for BERTweet\n",
    "    emotion_data = process_emotion_data_bertweet(emotion_dataset, emotion_encoder)\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def load_existing_encoders_bertweet():\n",
    "    \"\"\"Load existing encoders from enc/ directory or create new ones for BERTweet\"\"\"\n",
    "    \n",
    "    import joblib\n",
    "    \n",
    "    # Try to load existing encoders\n",
    "    try:\n",
    "        sentiment_encoder = joblib.load('enc/sentiment_label_encoder.pkl')\n",
    "        emotion_encoder = joblib.load('enc/emotion_label_encoder.pkl')\n",
    "        print(\"✅ Loaded existing encoders from enc/ directory for BERTweet\")\n",
    "        return sentiment_encoder, emotion_encoder\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load existing encoders: {e}\")\n",
    "        print(\"Creating new encoders for BERTweet...\")\n",
    "        \n",
    "        # Create new encoders\n",
    "        sentiment_encoder = LabelEncoder()\n",
    "        emotion_encoder = LabelEncoder()\n",
    "        sentiment_encoder.classes_ = np.array(bertweet_model_config.sentiment_classes)\n",
    "        emotion_encoder.classes_ = np.array(bertweet_model_config.emotion_classes)\n",
    "        \n",
    "        # Save new encoders\n",
    "        os.makedirs('enc', exist_ok=True)\n",
    "        joblib.dump(sentiment_encoder, 'enc/bertweet_sentiment_label_encoder.pkl')\n",
    "        joblib.dump(emotion_encoder, 'enc/bertweet_emotion_label_encoder.pkl')\n",
    "        print(\"✅ Created and saved new BERTweet encoders\")\n",
    "        \n",
    "        return sentiment_encoder, emotion_encoder\n",
    "\n",
    "def process_sentiment_data_bertweet(sst2_dataset, sentiment_encoder, max_samples=None):\n",
    "    \"\"\"Process SST-2 dataset for sentiment classification with BERTweet\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing sentiment data for BERTweet...\")\n",
    "    \n",
    "    # Use full dataset if max_samples is None\n",
    "    if max_samples is None:\n",
    "        max_samples = len(sst2_dataset['train'])\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = sst2_dataset['train']['sentence'][:max_samples]\n",
    "    train_labels = sst2_dataset['train']['label'][:max_samples]\n",
    "    \n",
    "    val_texts = sst2_dataset['validation']['sentence']\n",
    "    val_labels = sst2_dataset['validation']['label']\n",
    "    \n",
    "    # Map SST-2 labels to 3 classes: 0->Negative, 1->Positive\n",
    "    # Add some neutral examples by random assignment\n",
    "    expanded_labels = []\n",
    "    expanded_texts = []\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        if label == 0:  # Negative\n",
    "            expanded_labels.append(0)\n",
    "            expanded_texts.append(text)\n",
    "        elif label == 1:  # Positive\n",
    "            # Sometimes assign as positive, sometimes as neutral\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                expanded_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                expanded_labels.append(2)  # Positive\n",
    "            expanded_texts.append(text)\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in expanded_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(expanded_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            expanded_labels[idx] = 1\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        expanded_texts, expanded_labels, test_size=0.3, random_state=42, stratify=expanded_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    sentiment_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels},\n",
    "        'encoder': sentiment_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ BERTweet Sentiment data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return sentiment_data\n",
    "\n",
    "def process_emotion_data_bertweet(emotion_dataset, emotion_encoder, max_samples=None):\n",
    "    \"\"\"Process GoEmotion dataset for emotion classification with BERTweet\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing emotion data for BERTweet...\")\n",
    "    \n",
    "    # Filter to first 6 emotions only\n",
    "    def filter_emotions(example):\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    filtered_train = emotion_dataset['train'].filter(filter_emotions)\n",
    "    filtered_val = emotion_dataset['validation'].filter(filter_emotions)\n",
    "    \n",
    "    # Use full dataset if max_samples is None\n",
    "    if max_samples is None:\n",
    "        max_samples = len(filtered_train)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = filtered_train['text'][:max_samples]\n",
    "    train_labels_raw = filtered_train['labels'][:max_samples]\n",
    "    \n",
    "    val_texts = filtered_val['text']\n",
    "    val_labels_raw = filtered_val['labels']\n",
    "    \n",
    "    # Handle multi-label to single-label conversion\n",
    "    train_labels = []\n",
    "    for label in train_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            train_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "    \n",
    "    val_labels = []\n",
    "    for label in val_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            val_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            val_labels.append(label)\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.3, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    emotion_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels},\n",
    "        'encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ BERTweet Emotion data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return emotion_data\n",
    "\n",
    "def create_multitask_data_bertweet(sentiment_data, emotion_data):\n",
    "    \"\"\"Create combined dataset for multi-task learning with BERTweet\"\"\"\n",
    "    \n",
    "    print(\"🔄 Creating multi-task dataset for BERTweet...\")\n",
    "    \n",
    "    # Take minimum length to balance datasets\n",
    "    min_train_len = min(len(sentiment_data['train']['texts']), len(emotion_data['train']['texts']))\n",
    "    min_val_len = min(len(sentiment_data['val']['texts']), len(emotion_data['val']['texts']))\n",
    "    min_test_len = min(len(sentiment_data['test']['texts']), len(emotion_data['test']['texts']))\n",
    "    \n",
    "    multitask_data = {\n",
    "        'train': {\n",
    "            'texts': sentiment_data['train']['texts'][:min_train_len],\n",
    "            'sentiment_labels': sentiment_data['train']['labels'][:min_train_len],\n",
    "            'emotion_labels': emotion_data['train']['labels'][:min_train_len]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': sentiment_data['val']['texts'][:min_val_len],\n",
    "            'sentiment_labels': sentiment_data['val']['labels'][:min_val_len],\n",
    "            'emotion_labels': emotion_data['val']['labels'][:min_val_len]\n",
    "        },\n",
    "        'test': {\n",
    "            'texts': sentiment_data['test']['texts'][:min_test_len],\n",
    "            'sentiment_labels': sentiment_data['test']['labels'][:min_test_len],\n",
    "            'emotion_labels': emotion_data['test']['labels'][:min_test_len]\n",
    "        },\n",
    "        'sentiment_encoder': sentiment_data['encoder'],\n",
    "        'emotion_encoder': emotion_data['encoder']\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ BERTweet Multi-task data created:\")\n",
    "    print(f\"  Train: {len(multitask_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(multitask_data['val']['texts'])} samples\")\n",
    "    print(f\"  Test: {len(multitask_data['test']['texts'])} samples\")\n",
    "    \n",
    "    return multitask_data\n",
    "\n",
    "print(\"✅ BERTweet Data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da217a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: BERTweet Training Classes\n",
    "class BERTweetSingleTaskTrainer:\n",
    "    \"\"\"Trainer for single-task BERTweet models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize BERTweet tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize BERTweet model\n",
    "        self.model = BERTweetSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \"\"\"Create data loaders for BERTweet training\"\"\"\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for BERTweet\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            eps=1e-6  # BERTweet specific epsilon\n",
    "        )\n",
    "        \n",
    "        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train BERTweet for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate BERTweet on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        \"\"\"Main BERTweet training loop\"\"\"\n",
    "        print(f\"🚀 Starting BERTweet single-task training ({self.config.task_type})...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\n✅ BERTweet training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        \"\"\"Save BERTweet model and tokenizer\"\"\"\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"💾 Best BERTweet model saved to {model_dir}\")\n",
    "\n",
    "class BERTweetMultiTaskTrainer:\n",
    "    \"\"\"Trainer for multi-task BERTweet models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize BERTweet tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize BERTweet multi-task model\n",
    "        self.model = BERTweetMultiTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            sentiment_num_classes=bertweet_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=bertweet_model_config.emotion_num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_accuracy': [],\n",
    "            'train_emotion_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'val_sentiment_f1_macro': [],\n",
    "            'val_emotion_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \"\"\"Create data loaders for BERTweet multi-task training\"\"\"\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for BERTweet\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            eps=1e-6  # BERTweet specific epsilon\n",
    "        )\n",
    "        \n",
    "        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train BERTweet multi-task for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_correct = 0\n",
    "        emotion_correct = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss with alpha weighting\n",
    "            loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "            emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "            \n",
    "            sentiment_correct += (sentiment_preds == sentiment_labels).sum().item()\n",
    "            emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
    "            total_predictions += sentiment_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        sentiment_accuracy = sentiment_correct / total_predictions\n",
    "        emotion_accuracy = emotion_correct / total_predictions\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate BERTweet multi-task on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_labels = []\n",
    "        emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_true = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_true = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_true)\n",
    "                emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_true)\n",
    "                loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_labels.extend(sentiment_true.cpu().numpy())\n",
    "                emotion_labels.extend(emotion_true.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_labels, emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy, sentiment_f1_macro, emotion_f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        \"\"\"Main BERTweet multi-task training loop\"\"\"\n",
    "        print(f\"🚀 Starting BERTweet multi-task training...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_combined_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_sent_acc, train_emo_acc = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_sent_acc, val_emo_acc, val_sent_f1, val_emo_f1 = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_sentiment_accuracy'].append(train_sent_acc)\n",
    "            self.training_history['train_emotion_accuracy'].append(train_emo_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_sent_acc)\n",
    "            self.training_history['val_emotion_accuracy'].append(val_emo_acc)\n",
    "            self.training_history['val_sentiment_f1_macro'].append(val_sent_f1)\n",
    "            self.training_history['val_emotion_f1_macro'].append(val_emo_f1)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Train Sentiment Acc: {train_sent_acc:.4f}, Train Emotion Acc: {train_emo_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Sentiment Acc: {val_sent_acc:.4f}, F1: {val_sent_f1:.4f}\")\n",
    "            print(f\"  Val Emotion Acc: {val_emo_acc:.4f}, F1: {val_emo_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            combined_f1 = (val_sent_f1 + val_emo_f1) / 2\n",
    "            if combined_f1 > best_combined_f1:\n",
    "                best_combined_f1 = combined_f1\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\n✅ BERTweet training completed! Best Combined F1: {best_combined_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        \"\"\"Save BERTweet multi-task model and tokenizer\"\"\"\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"💾 Best BERTweet model saved to {model_dir}\")\n",
    "\n",
    "print(\"✅ BERTweet Training classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c966158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: BERTweet Hyperparameter Tuning\n",
    "class BERTweetHyperparameterTuner:\n",
    "    \"\"\"Hyperparameter tuning for BERTweet using TPE\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 20,\n",
    "        model_name: str = \"vinai/bertweet-base\"\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.data_splits = data_splits\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(f\"🔍 BERTweet hyperparameter tuner initialized for {model_type}\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function for BERTweet\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters optimized for BERTweet\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 3, 8)\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.05, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.001, 0.1)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.4)\n",
    "        max_length = trial.suggest_categorical('max_length', [128, 256])\n",
    "        \n",
    "        # Multi-task specific parameter\n",
    "        alpha = trial.suggest_float('alpha', 0.3, 0.7) if self.model_type == \"multitask\" else 0.5\n",
    "        \n",
    "        # Create config\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./bertweet_temp_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Train BERTweet model\n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = BERTweetMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return combined F1 score\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro'])\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro'])\n",
    "                combined_f1 = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "                print(f\"BERTweet Trial {trial.number}: Combined F1 = {combined_f1:.4f}\")\n",
    "                return combined_f1\n",
    "                \n",
    "            else:\n",
    "                # Single task training\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = bertweet_model_config.sentiment_num_classes\n",
    "                else:  # emotion\n",
    "                    num_classes = bertweet_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = BERTweetSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.data_splits)\n",
    "                \n",
    "                # Return best F1 score\n",
    "                best_f1 = max(history['val_f1_macro'])\n",
    "                print(f\"BERTweet Trial {trial.number}: F1 = {best_f1:.4f}\")\n",
    "                return best_f1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"BERTweet Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Clean up\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \"\"\"Run BERTweet hyperparameter optimization\"\"\"\n",
    "        \n",
    "        # Create study with TPE sampler\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42),\n",
    "            pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    "        )\n",
    "        \n",
    "        print(f\"🔍 Starting BERTweet hyperparameter optimization for {self.model_type}...\")\n",
    "        print(f\"Running {self.n_trials} trials with TPE sampler\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n🏆 BERTweet optimization completed for {self.model_type}!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best score: {study.best_value:.4f}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "def train_bertweet_with_best_params(model_type: str, data_splits: Dict, best_params: Dict, model_name: str = \"vinai/bertweet-base\"):\n",
    "    \"\"\"Train final BERTweet model with best hyperparameters\"\"\"\n",
    "    \n",
    "    print(f\"\\n🚀 Training final BERTweet {model_type} model with best parameters...\")\n",
    "    \n",
    "    # Create config with best parameters\n",
    "    config = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        num_epochs=best_params['num_epochs'],\n",
    "        warmup_ratio=best_params['warmup_ratio'],\n",
    "        weight_decay=best_params['weight_decay'],\n",
    "        hidden_dropout_prob=best_params['hidden_dropout_prob'],\n",
    "        classifier_dropout=best_params['classifier_dropout'],\n",
    "        max_length=best_params['max_length'],\n",
    "        alpha=best_params.get('alpha', 0.5),\n",
    "        task_type=model_type,\n",
    "        output_dir=f\"./final_bertweet_{model_type}_model\"\n",
    "    )\n",
    "    \n",
    "    # Train BERTweet model\n",
    "    if model_type == \"multitask\":\n",
    "        trainer = BERTweetMultiTaskTrainer(config)\n",
    "        history = trainer.train(data_splits)\n",
    "    else:\n",
    "        # Single-task training\n",
    "        if model_type == \"sentiment\":\n",
    "            num_classes = bertweet_model_config.sentiment_num_classes\n",
    "        else:  # emotion\n",
    "            num_classes = bertweet_model_config.emotion_num_classes\n",
    "        \n",
    "        trainer = BERTweetSingleTaskTrainer(config, num_classes)\n",
    "        history = trainer.train(data_splits)\n",
    "    \n",
    "    print(f\"✅ Final BERTweet {model_type} model training completed!\")\n",
    "    return trainer, history\n",
    "\n",
    "print(\"✅ BERTweet Hyperparameter tuning classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: BERTweet Evaluation Functions\n",
    "def evaluate_bertweet_model(model_path: str, model_type: str, test_data: Dict, model_name: str = \"vinai/bertweet-base\"):\n",
    "    \"\"\"Evaluation function for BERTweet models\"\"\"\n",
    "    \n",
    "    print(f\"📊 Evaluating BERTweet {model_type} model...\")\n",
    "    \n",
    "    # Load BERTweet tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Load BERTweet model\n",
    "    if model_type == \"multitask\":\n",
    "        model = BERTweetMultiTaskTransformer.from_pretrained(model_path)\n",
    "    else:\n",
    "        model = BERTweetSingleTaskTransformer.from_pretrained(model_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare test data\n",
    "    if model_type == \"multitask\":\n",
    "        test_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            sentiment_labels=test_data['sentiment_labels'],\n",
    "            emotion_labels=test_data['emotion_labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    else:\n",
    "        test_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            labels=test_data['labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    if model_type == \"multitask\":\n",
    "        all_sentiment_predictions = []\n",
    "        all_emotion_predictions = []\n",
    "        all_sentiment_labels = []\n",
    "        all_emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                all_sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                all_emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                all_sentiment_labels.extend(batch['sentiment_labels'].numpy())\n",
    "                all_emotion_labels.extend(batch['emotion_labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(all_sentiment_labels, all_sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(all_emotion_labels, all_emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(all_sentiment_labels, all_sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(all_emotion_labels, all_emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        results = {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'sentiment_f1_macro': sentiment_f1_macro,\n",
    "            'emotion_f1_macro': emotion_f1_macro,\n",
    "            'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "            'combined_f1_macro': (sentiment_f1_macro + emotion_f1_macro) / 2\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 BERTweet Multi-task Results:\")\n",
    "        print(f\"  Sentiment - Accuracy: {sentiment_accuracy:.4f}, F1: {sentiment_f1_macro:.4f}\")\n",
    "        print(f\"  Emotion - Accuracy: {emotion_accuracy:.4f}, F1: {emotion_f1_macro:.4f}\")\n",
    "        print(f\"  Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1_macro']:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # Single-task evaluation\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 BERTweet {model_type.capitalize()} Results:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1 Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_bertweet_results_summary(sentiment_results: Dict, emotion_results: Dict, multitask_results: Dict):\n",
    "    \"\"\"Create a summary of all BERTweet model results\"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📊 BERTWEET FINAL RESULTS SUMMARY\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🎯 BERTWEET SINGLE-TASK SENTIMENT MODEL:\")\n",
    "    print(f\"  Accuracy: {sentiment_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {sentiment_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 BERTWEET SINGLE-TASK EMOTION MODEL:\")\n",
    "    print(f\"  Accuracy: {emotion_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {emotion_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 BERTWEET MULTI-TASK MODEL:\")\n",
    "    print(f\"  Sentiment - Accuracy: {multitask_results['sentiment_accuracy']:.4f}, F1: {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"  Emotion - Accuracy: {multitask_results['emotion_accuracy']:.4f}, F1: {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"  Combined - Accuracy: {multitask_results['combined_accuracy']:.4f}, F1: {multitask_results['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 BERTWEET COMPARISON:\")\n",
    "    print(f\"  Single-task Sentiment vs Multi-task Sentiment:\")\n",
    "    print(f\"    Accuracy: {sentiment_results['accuracy']:.4f} vs {multitask_results['sentiment_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {sentiment_results['f1_macro']:.4f} vs {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"  Single-task Emotion vs Multi-task Emotion:\")\n",
    "    print(f\"    Accuracy: {emotion_results['accuracy']:.4f} vs {multitask_results['emotion_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {emotion_results['f1_macro']:.4f} vs {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"=\"*80)\n",
    "\n",
    "print(\"✅ BERTweet Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b37029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main BERTweet Training Pipeline\n",
    "def main_bertweet_training_pipeline():\n",
    "    \"\"\"Main pipeline for BERTweet: Initial training → Hyperparameter tuning → Final training\"\"\"\n",
    "    \n",
    "    print(\"🚀 STARTING COMPREHENSIVE BERTWEET TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and process datasets for BERTweet\n",
    "    print(\"\\n1️⃣ Loading and processing datasets for BERTweet...\")\n",
    "    sentiment_data, emotion_data = load_and_process_datasets_bertweet()\n",
    "    multitask_data = create_multitask_data_bertweet(sentiment_data, emotion_data)\n",
    "    \n",
    "    # Model configurations\n",
    "    model_name = \"vinai/bertweet-base\"\n",
    "    n_trials = 15  # Number of hyperparameter tuning trials\n",
    "    \n",
    "    # Store results\n",
    "    all_results = {}\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 1: INITIAL BERTWEET TRAINING WITH DEFAULT PARAMETERS\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 1: INITIAL BERTWEET TRAINING WITH DEFAULT PARAMETERS\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Default configuration for BERTweet\n",
    "    default_config_sentiment = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        task_type=\"sentiment\",\n",
    "        output_dir=\"./initial_bertweet_sentiment_model\"\n",
    "    )\n",
    "    \n",
    "    default_config_emotion = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        task_type=\"emotion\",\n",
    "        output_dir=\"./initial_bertweet_emotion_model\"\n",
    "    )\n",
    "    \n",
    "    default_config_multitask = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        alpha=0.5,\n",
    "        task_type=\"multitask\",\n",
    "        output_dir=\"./initial_bertweet_multitask_model\"\n",
    "    )\n",
    "    \n",
    "    # 1.1 Train Initial BERTweet Sentiment Model\n",
    "    print(f\"\\n2️⃣ Training Initial BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_sentiment_trainer = BERTweetSingleTaskTrainer(\n",
    "        config=default_config_sentiment,\n",
    "        num_classes=bertweet_model_config.sentiment_num_classes\n",
    "    )\n",
    "    initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet sentiment model\n",
    "    initial_sentiment_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_sentiment_model/model_best\",\n",
    "        model_type=\"sentiment\",\n",
    "        test_data=sentiment_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['initial_sentiment'] = initial_sentiment_results\n",
    "    \n",
    "    # 1.2 Train Initial BERTweet Emotion Model\n",
    "    print(f\"\\n3️⃣ Training Initial BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_emotion_trainer = BERTweetSingleTaskTrainer(\n",
    "        config=default_config_emotion,\n",
    "        num_classes=bertweet_model_config.emotion_num_classes\n",
    "    )\n",
    "    initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet emotion model\n",
    "    initial_emotion_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_emotion_model/model_best\",\n",
    "        model_type=\"emotion\",\n",
    "        test_data=emotion_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['initial_emotion'] = initial_emotion_results\n",
    "    \n",
    "    # 1.3 Train Initial BERTweet Multi-task Model\n",
    "    print(f\"\\n4️⃣ Training Initial BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_multitask_trainer = BERTweetMultiTaskTrainer(config=default_config_multitask)\n",
    "    initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet multi-task model\n",
    "    initial_multitask_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_multitask_model/model_best\",\n",
    "        model_type=\"multitask\",\n",
    "        test_data=multitask_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['initial_multitask'] = initial_multitask_results\n",
    "    \n",
    "    # Display initial BERTweet results summary\n",
    "    print(f\"\\n5️⃣ Initial BERTweet Results Summary...\")\n",
    "    print(\"=\"*60)\n",
    "    create_bertweet_initial_results_summary(\n",
    "        sentiment_results=all_results['initial_sentiment'],\n",
    "        emotion_results=all_results['initial_emotion'],\n",
    "        multitask_results=all_results['initial_multitask']\n",
    "    )\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 2: BERTWEET HYPERPARAMETER TUNING\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 2: BERTWEET HYPERPARAMETER TUNING\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # 2.1 Hyperparameter tuning for BERTweet sentiment\n",
    "    print(f\"\\n6️⃣ Hyperparameter Tuning for BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sentiment_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"sentiment\",\n",
    "        data_splits=sentiment_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    sentiment_study = sentiment_tuner.tune()\n",
    "    \n",
    "    # 2.2 Hyperparameter tuning for BERTweet emotion\n",
    "    print(f\"\\n7️⃣ Hyperparameter Tuning for BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    emotion_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"emotion\",\n",
    "        data_splits=emotion_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    emotion_study = emotion_tuner.tune()\n",
    "    \n",
    "    # 2.3 Hyperparameter tuning for BERTweet multi-task\n",
    "    print(f\"\\n8️⃣ Hyperparameter Tuning for BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    multitask_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"multitask\",\n",
    "        data_splits=multitask_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    multitask_study = multitask_tuner.tune()\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 3: FINAL BERTWEET TRAINING WITH OPTIMIZED PARAMETERS\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 3: FINAL BERTWEET TRAINING WITH OPTIMIZED PARAMETERS\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # 3.1 Train optimized BERTweet sentiment model\n",
    "    print(f\"\\n9️⃣ Training Optimized BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_sentiment_trainer, optimized_sentiment_history = train_bertweet_with_best_params(\n",
    "        model_type=\"sentiment\",\n",
    "        data_splits=sentiment_data,\n",
    "        best_params=sentiment_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet sentiment model\n",
    "    optimized_sentiment_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_sentiment_model/model_best\",\n",
    "        model_type=\"sentiment\",\n",
    "        test_data=sentiment_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['optimized_sentiment'] = optimized_sentiment_results\n",
    "    \n",
    "    # 3.2 Train optimized BERTweet emotion model\n",
    "    print(f\"\\n🔟 Training Optimized BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_emotion_trainer, optimized_emotion_history = train_bertweet_with_best_params(\n",
    "        model_type=\"emotion\",\n",
    "        data_splits=emotion_data,\n",
    "        best_params=emotion_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet emotion model\n",
    "    optimized_emotion_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_emotion_model/model_best\",\n",
    "        model_type=\"emotion\",\n",
    "        test_data=emotion_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['optimized_emotion'] = optimized_emotion_results\n",
    "    \n",
    "    # 3.3 Train optimized BERTweet multi-task model\n",
    "    print(f\"\\n1️⃣1️⃣ Training Optimized BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_multitask_trainer, optimized_multitask_history = train_bertweet_with_best_params(\n",
    "        model_type=\"multitask\",\n",
    "        data_splits=multitask_data,\n",
    "        best_params=multitask_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet multi-task model\n",
    "    optimized_multitask_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_multitask_model/model_best\",\n",
    "        model_type=\"multitask\",\n",
    "        test_data=multitask_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['optimized_multitask'] = optimized_multitask_results\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 4: COMPREHENSIVE BERTWEET RESULTS COMPARISON\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 4: COMPREHENSIVE BERTWEET RESULTS COMPARISON\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Create comprehensive BERTweet comparison\n",
    "    create_comprehensive_bertweet_results_comparison(all_results)\n",
    "    \n",
    "    # Save all BERTweet results\n",
    "    results_summary = {\n",
    "        'model_type': 'BERTweet',\n",
    "        'model_name': model_name,\n",
    "        'initial_models': {\n",
    "            'sentiment': all_results['initial_sentiment'],\n",
    "            'emotion': all_results['initial_emotion'],\n",
    "            'multitask': all_results['initial_multitask']\n",
    "        },\n",
    "        'optimized_models': {\n",
    "            'sentiment': all_results['optimized_sentiment'],\n",
    "            'emotion': all_results['optimized_emotion'],\n",
    "            'multitask': all_results['optimized_multitask']\n",
    "        },\n",
    "        'hyperparameter_studies': {\n",
    "            'sentiment': sentiment_study.best_params,\n",
    "            'emotion': emotion_study.best_params,\n",
    "            'multitask': multitask_study.best_params\n",
    "        },\n",
    "        'improvements': {\n",
    "            'sentiment': {\n",
    "                'accuracy_improvement': all_results['optimized_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy'],\n",
    "                'f1_improvement': all_results['optimized_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro']\n",
    "            },\n",
    "            'emotion': {\n",
    "                'accuracy_improvement': all_results['optimized_emotion']['accuracy'] - all_results['initial_emotion']['accuracy'],\n",
    "                'f1_improvement': all_results['optimized_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro']\n",
    "            },\n",
    "            'multitask': {\n",
    "                'sentiment_accuracy_improvement': all_results['optimized_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy'],\n",
    "                'emotion_accuracy_improvement': all_results['optimized_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy'],\n",
    "                'combined_f1_improvement': all_results['optimized_multitask']['combined_f1_macro'] - all_results['initial_multitask']['combined_f1_macro']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('comprehensive_bertweet_results_summary.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ COMPLETE BERTWEET PIPELINE FINISHED!\")\n",
    "    print(f\"📁 Results saved to: comprehensive_bertweet_results_summary.json\")\n",
    "    print(f\"📁 Initial models saved to: ./initial_bertweet_*_model/\")\n",
    "    print(f\"📁 Optimized models saved to: ./final_bertweet_*_model/\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def create_bertweet_initial_results_summary(sentiment_results: Dict, emotion_results: Dict, multitask_results: Dict):\n",
    "    \"\"\"Create a summary of initial BERTweet model results\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 INITIAL BERTWEET MODELS RESULTS SUMMARY\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n🎯 INITIAL BERTWEET SENTIMENT MODEL:\")\n",
    "    print(f\"  Accuracy: {sentiment_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {sentiment_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 INITIAL BERTWEET EMOTION MODEL:\")\n",
    "    print(f\"  Accuracy: {emotion_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {emotion_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 INITIAL BERTWEET MULTI-TASK MODEL:\")\n",
    "    print(f\"  Sentiment - Accuracy: {multitask_results['sentiment_accuracy']:.4f}, F1: {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"  Emotion - Accuracy: {multitask_results['emotion_accuracy']:.4f}, F1: {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"  Combined - Accuracy: {multitask_results['combined_accuracy']:.4f}, F1: {multitask_results['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n💡 These are BERTweet baseline results. Hyperparameter tuning will aim to improve them!\")\n",
    "\n",
    "def create_comprehensive_bertweet_results_comparison(all_results: Dict):\n",
    "    \"\"\"Create comprehensive comparison between initial and optimized BERTweet models\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 COMPREHENSIVE BERTWEET RESULTS COMPARISON\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🎯 BERTWEET SENTIMENT MODEL COMPARISON:\")\n",
    "    print(f\"  Initial    - Accuracy: {all_results['initial_sentiment']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['f1_macro']:.4f}\")\n",
    "    print(f\"  Optimized  - Accuracy: {all_results['optimized_sentiment']['accuracy']:.4f}, F1: {all_results['optimized_sentiment']['f1_macro']:.4f}\")\n",
    "    \n",
    "    sent_acc_improve = all_results['optimized_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy']\n",
    "    sent_f1_improve = all_results['optimized_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro']\n",
    "    print(f\"  Improvement - Accuracy: {sent_acc_improve:+.4f}, F1: {sent_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 BERTWEET EMOTION MODEL COMPARISON:\")\n",
    "    print(f\"  Initial    - Accuracy: {all_results['initial_emotion']['accuracy']:.4f}, F1: {all_results['initial_emotion']['f1_macro']:.4f}\")\n",
    "    print(f\"  Optimized  - Accuracy: {all_results['optimized_emotion']['accuracy']:.4f}, F1: {all_results['optimized_emotion']['f1_macro']:.4f}\")\n",
    "    \n",
    "    emo_acc_improve = all_results['optimized_emotion']['accuracy'] - all_results['initial_emotion']['accuracy']\n",
    "    emo_f1_improve = all_results['optimized_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro']\n",
    "    print(f\"  Improvement - Accuracy: {emo_acc_improve:+.4f}, F1: {emo_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 BERTWEET MULTI-TASK MODEL COMPARISON:\")\n",
    "    print(f\"  SENTIMENT TASK:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['optimized_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_sent_acc_improve = all_results['optimized_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy']\n",
    "    mt_sent_f1_improve = all_results['optimized_multitask']['sentiment_f1_macro'] - all_results['initial_multitask']['sentiment_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_sent_acc_improve:+.4f}, F1: {mt_sent_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"  EMOTION TASK:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['emotion_accuracy']:.4f}, F1: {all_results['optimized_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_emo_acc_improve = all_results['optimized_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy']\n",
    "    mt_emo_f1_improve = all_results['optimized_multitask']['emotion_f1_macro'] - all_results['initial_multitask']['emotion_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_emo_acc_improve:+.4f}, F1: {mt_emo_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"  COMBINED:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['combined_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['combined_accuracy']:.4f}, F1: {all_results['optimized_multitask']['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_combined_acc_improve = all_results['optimized_multitask']['combined_accuracy'] - all_results['initial_multitask']['combined_accuracy']\n",
    "    mt_combined_f1_improve = all_results['optimized_multitask']['combined_f1_macro'] - all_results['initial_multitask']['combined_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_combined_acc_improve:+.4f}, F1: {mt_combined_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 BERTWEET SINGLE-TASK vs MULTI-TASK COMPARISON (OPTIMIZED):\")\n",
    "    print(f\"  SENTIMENT:\")\n",
    "    print(f\"    Single-task: Accuracy: {all_results['optimized_sentiment']['accuracy']:.4f}, F1: {all_results['optimized_sentiment']['f1_macro']:.4f}\")\n",
    "    print(f\"    Multi-task:  Accuracy: {all_results['optimized_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['optimized_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"  EMOTION:\")\n",
    "    print(f\"    Single-task: Accuracy: {all_results['optimized_emotion']['accuracy']:.4f}, F1: {all_results['optimized_emotion']['f1_macro']:.4f}\")\n",
    "    print(f\"    Multi-task:  Accuracy: {all_results['optimized_multitask']['emotion_accuracy']:.4f}, F1: {all_results['optimized_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"=\"*80)\n",
    "\n",
    "print(\"✅ BERTweet Main training pipeline defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b307c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Execute BERTweet Training Pipeline\n",
    "\n",
    "# Clear memory before starting\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "print(\"🚀 STARTING BERTWEET COMPREHENSIVE TRAINING PIPELINE\")\n",
    "print(\"🐦 Using BERTweet model for Twitter-style text processing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the complete BERTweet pipeline\n",
    "bertweet_results = main_bertweet_training_pipeline()\n",
    "\n",
    "print(f\"\\n🎉 ALL BERTWEET TRAINING COMPLETED!\")\n",
    "print(f\"📊 Check comprehensive_bertweet_results_summary.json for detailed comparison\")\n",
    "print(f\"📁 Initial BERTweet models in: ./initial_bertweet_*_model/\")\n",
    "print(f\"📁 Optimized BERTweet models in: ./final_bertweet_*_model/\")\n",
    "print(f\"🐦 BERTweet models are optimized for social media text processing!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
