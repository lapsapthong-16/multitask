{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b3039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "CUDA Version: 12.1\n",
      "✅ BERTweet Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================================\n",
    "# COMPREHENSIVE MULTI-MODEL TRAINING SYSTEM - BERTWEET VERSION\n",
    "# ================================================================================================\n",
    "# This notebook trains:\n",
    "# 1. Single-task Sentiment Model (on SST-2) using BERTweet\n",
    "# 2. Single-task Emotion Model (on GoEmotion) using BERTweet\n",
    "# 3. Multi-task Model (on both datasets) using BERTweet\n",
    "# \n",
    "# Each model uses Bayesian optimization (TPE) for hyperparameter tuning\n",
    "# Only macro F1 and accuracy metrics are used for evaluation\n",
    "# ================================================================================================\n",
    "\n",
    "# Cell 1: Setup and Imports - BERTweet Version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Memory management\n",
    "def aggressive_memory_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"🧹 Memory cleaned!\")\n",
    "\n",
    "print(\"✅ BERTweet Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e3f994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Configuration classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Classes - BERTweet Version\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training parameters - BERTweet optimized\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",  # Changed to BERTweet\n",
    "        max_length: int = 128,\n",
    "        batch_size: int = 8,\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 3,\n",
    "        warmup_ratio: float = 0.1,\n",
    "        weight_decay: float = 0.01,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        output_dir: str = \"./bertweet_model_output\",\n",
    "        save_total_limit: int = 1,\n",
    "        # Multi-task specific\n",
    "        alpha: float = 0.5,  # Only used for multi-task\n",
    "        task_type: str = \"multitask\"  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_dropout_prob = attention_dropout_prob\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.alpha = alpha\n",
    "        self.task_type = task_type\n",
    "\n",
    "class BERTweetModelConfig:\n",
    "    \"\"\"Configuration for BERTweet model architecture\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "bertweet_model_config = BERTweetModelConfig()\n",
    "print(\"✅ BERTweet Configuration classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075f5314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Dataset classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Classes - BERTweet Version\n",
    "class BERTweetSingleTaskDataset(Dataset):\n",
    "    \"\"\"Dataset for single-task training with BERTweet (sentiment OR emotion)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # BERTweet specific preprocessing (handles tweets better)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "class BERTweetMultiTaskDataset(Dataset):\n",
    "    \"\"\"Dataset for multi-task training with BERTweet (sentiment AND emotion)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # BERTweet specific preprocessing\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "print(\"✅ BERTweet Dataset classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf89772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Model architectures defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: BERTweet Model Architectures\n",
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \"\"\"Single-task BERTweet transformer for sentiment OR emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetSingleTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # BERTweet encoder\n",
    "        self.encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Classification head optimized for BERTweet\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),  # BERTweet uses GELU activation\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERTweet encoder output\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        pooled_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "    \n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        \"\"\"Save BERTweet model in HuggingFace format\"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"model_type\": \"BERTweetSingleTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"BERTweet single-task model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load BERTweet model from HuggingFace format\"\"\"\n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            num_classes=config[\"num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \"\"\"Multi-task BERTweet transformer for sentiment AND emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetMultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTweet configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared BERTweet encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers for BERTweet\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # BERTweet-base has 12 attention heads\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # BERTweet-base has 12 attention heads\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Classification heads optimized for BERTweet\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classification head weights\"\"\"\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Shared BERTweet encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Task-specific attention\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        \n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        \n",
    "        # Classification\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_pooled)\n",
    "        emotion_logits = self.emotion_classifier(emotion_pooled)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "    \n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        \"\"\"Save BERTweet multi-task model in HuggingFace format\"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"sentiment_num_classes\": self.sentiment_num_classes,\n",
    "            \"emotion_num_classes\": self.emotion_num_classes,\n",
    "            \"model_type\": \"BERTweetMultiTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"BERTweet multi-task model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        \"\"\"Load BERTweet multi-task model from HuggingFace format\"\"\"\n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            sentiment_num_classes=config[\"sentiment_num_classes\"],\n",
    "            emotion_num_classes=config[\"emotion_num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "print(\"✅ BERTweet Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef5f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Data processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data Loading and Processing - BERTweet Version\n",
    "def load_and_process_datasets_bertweet():\n",
    "    \"\"\"Load and process SST-2 and GoEmotion datasets for BERTweet\"\"\"\n",
    "    \n",
    "    print(\"📥 Loading datasets for BERTweet...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        print(f\"✅ SST-2 loaded: {len(sst2_dataset['train'])} train, {len(sst2_dataset['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load SST-2: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load GoEmotion for emotion\n",
    "    try:\n",
    "        emotion_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        print(f\"✅ GoEmotion loaded: {len(emotion_dataset['train'])} train, {len(emotion_dataset['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load GoEmotion: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Try to load existing encoders first\n",
    "    sentiment_encoder, emotion_encoder = load_existing_encoders_bertweet()\n",
    "    \n",
    "    # Process sentiment data (SST-2) for BERTweet\n",
    "    sentiment_data = process_sentiment_data_bertweet(sst2_dataset, sentiment_encoder)\n",
    "    \n",
    "    # Process emotion data (GoEmotion) for BERTweet\n",
    "    emotion_data = process_emotion_data_bertweet(emotion_dataset, emotion_encoder)\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def load_existing_encoders_bertweet():\n",
    "    \"\"\"Load existing encoders from enc/ directory or create new ones for BERTweet\"\"\"\n",
    "    \n",
    "    import joblib\n",
    "    \n",
    "    # Try to load existing encoders\n",
    "    try:\n",
    "        sentiment_encoder = joblib.load('enc/sentiment_label_encoder.pkl')\n",
    "        emotion_encoder = joblib.load('enc/emotion_label_encoder.pkl')\n",
    "        print(\"✅ Loaded existing encoders from enc/ directory for BERTweet\")\n",
    "        return sentiment_encoder, emotion_encoder\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load existing encoders: {e}\")\n",
    "        print(\"Creating new encoders for BERTweet...\")\n",
    "        \n",
    "        # Create new encoders\n",
    "        sentiment_encoder = LabelEncoder()\n",
    "        emotion_encoder = LabelEncoder()\n",
    "        sentiment_encoder.classes_ = np.array(bertweet_model_config.sentiment_classes)\n",
    "        emotion_encoder.classes_ = np.array(bertweet_model_config.emotion_classes)\n",
    "        \n",
    "        # Save new encoders\n",
    "        os.makedirs('enc', exist_ok=True)\n",
    "        joblib.dump(sentiment_encoder, 'enc/bertweet_sentiment_label_encoder.pkl')\n",
    "        joblib.dump(emotion_encoder, 'enc/bertweet_emotion_label_encoder.pkl')\n",
    "        print(\"✅ Created and saved new BERTweet encoders\")\n",
    "        \n",
    "        return sentiment_encoder, emotion_encoder\n",
    "\n",
    "def process_sentiment_data_bertweet(sst2_dataset, sentiment_encoder, max_samples=None):\n",
    "    \"\"\"Process SST-2 dataset for sentiment classification with BERTweet\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing sentiment data for BERTweet...\")\n",
    "    \n",
    "    # Use full dataset if max_samples is None\n",
    "    if max_samples is None:\n",
    "        max_samples = len(sst2_dataset['train'])\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = sst2_dataset['train']['sentence'][:max_samples]\n",
    "    train_labels = sst2_dataset['train']['label'][:max_samples]\n",
    "    \n",
    "    val_texts = sst2_dataset['validation']['sentence']\n",
    "    val_labels = sst2_dataset['validation']['label']\n",
    "    \n",
    "    # Map SST-2 labels to 3 classes: 0->Negative, 1->Positive\n",
    "    # Add some neutral examples by random assignment\n",
    "    expanded_labels = []\n",
    "    expanded_texts = []\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        if label == 0:  # Negative\n",
    "            expanded_labels.append(0)\n",
    "            expanded_texts.append(text)\n",
    "        elif label == 1:  # Positive\n",
    "            # Sometimes assign as positive, sometimes as neutral\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                expanded_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                expanded_labels.append(2)  # Positive\n",
    "            expanded_texts.append(text)\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in expanded_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(expanded_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            expanded_labels[idx] = 1\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        expanded_texts, expanded_labels, test_size=0.3, random_state=42, stratify=expanded_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    sentiment_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels},\n",
    "        'encoder': sentiment_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ BERTweet Sentiment data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return sentiment_data\n",
    "\n",
    "def process_emotion_data_bertweet(emotion_dataset, emotion_encoder, max_samples=None):\n",
    "    \"\"\"Process GoEmotion dataset for emotion classification with BERTweet\"\"\"\n",
    "    \n",
    "    print(\"🔄 Processing emotion data for BERTweet...\")\n",
    "    \n",
    "    # Filter to first 6 emotions only\n",
    "    def filter_emotions(example):\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    filtered_train = emotion_dataset['train'].filter(filter_emotions)\n",
    "    filtered_val = emotion_dataset['validation'].filter(filter_emotions)\n",
    "    \n",
    "    # Use full dataset if max_samples is None\n",
    "    if max_samples is None:\n",
    "        max_samples = len(filtered_train)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = filtered_train['text'][:max_samples]\n",
    "    train_labels_raw = filtered_train['labels'][:max_samples]\n",
    "    \n",
    "    val_texts = filtered_val['text']\n",
    "    val_labels_raw = filtered_val['labels']\n",
    "    \n",
    "    # Handle multi-label to single-label conversion\n",
    "    train_labels = []\n",
    "    for label in train_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            train_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "    \n",
    "    val_labels = []\n",
    "    for label in val_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            val_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            val_labels.append(label)\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.3, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    emotion_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels},\n",
    "        'encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ BERTweet Emotion data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return emotion_data\n",
    "\n",
    "def create_multitask_data_bertweet(sentiment_data, emotion_data):\n",
    "    \"\"\"Create combined dataset for multi-task learning with BERTweet\"\"\"\n",
    "    \n",
    "    print(\"🔄 Creating multi-task dataset for BERTweet...\")\n",
    "    \n",
    "    # Take minimum length to balance datasets\n",
    "    min_train_len = min(len(sentiment_data['train']['texts']), len(emotion_data['train']['texts']))\n",
    "    min_val_len = min(len(sentiment_data['val']['texts']), len(emotion_data['val']['texts']))\n",
    "    min_test_len = min(len(sentiment_data['test']['texts']), len(emotion_data['test']['texts']))\n",
    "    \n",
    "    multitask_data = {\n",
    "        'train': {\n",
    "            'texts': sentiment_data['train']['texts'][:min_train_len],\n",
    "            'sentiment_labels': sentiment_data['train']['labels'][:min_train_len],\n",
    "            'emotion_labels': emotion_data['train']['labels'][:min_train_len]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': sentiment_data['val']['texts'][:min_val_len],\n",
    "            'sentiment_labels': sentiment_data['val']['labels'][:min_val_len],\n",
    "            'emotion_labels': emotion_data['val']['labels'][:min_val_len]\n",
    "        },\n",
    "        'test': {\n",
    "            'texts': sentiment_data['test']['texts'][:min_test_len],\n",
    "            'sentiment_labels': sentiment_data['test']['labels'][:min_test_len],\n",
    "            'emotion_labels': emotion_data['test']['labels'][:min_test_len]\n",
    "        },\n",
    "        'sentiment_encoder': sentiment_data['encoder'],\n",
    "        'emotion_encoder': emotion_data['encoder']\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ BERTweet Multi-task data created:\")\n",
    "    print(f\"  Train: {len(multitask_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(multitask_data['val']['texts'])} samples\")\n",
    "    print(f\"  Test: {len(multitask_data['test']['texts'])} samples\")\n",
    "    \n",
    "    return multitask_data\n",
    "\n",
    "print(\"✅ BERTweet Data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da217a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Training classes defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: BERTweet Training Classes\n",
    "class BERTweetSingleTaskTrainer:\n",
    "    \"\"\"Trainer for single-task BERTweet models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize BERTweet tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize BERTweet model\n",
    "        self.model = BERTweetSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \"\"\"Create data loaders for BERTweet training\"\"\"\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for BERTweet\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            eps=1e-6  # BERTweet specific epsilon\n",
    "        )\n",
    "        \n",
    "        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train BERTweet for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate BERTweet on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        \"\"\"Main BERTweet training loop\"\"\"\n",
    "        print(f\"🚀 Starting BERTweet single-task training ({self.config.task_type})...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\n✅ BERTweet training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        \"\"\"Save BERTweet model and tokenizer\"\"\"\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"💾 Best BERTweet model saved to {model_dir}\")\n",
    "\n",
    "class BERTweetMultiTaskTrainer:\n",
    "    \"\"\"Trainer for multi-task BERTweet models\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize BERTweet tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize BERTweet multi-task model\n",
    "        self.model = BERTweetMultiTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            sentiment_num_classes=bertweet_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=bertweet_model_config.emotion_num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_accuracy': [],\n",
    "            'train_emotion_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'val_sentiment_f1_macro': [],\n",
    "            'val_emotion_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \"\"\"Create data loaders for BERTweet multi-task training\"\"\"\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for BERTweet\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            eps=1e-6  # BERTweet specific epsilon\n",
    "        )\n",
    "        \n",
    "        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train BERTweet multi-task for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_correct = 0\n",
    "        emotion_correct = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss with alpha weighting\n",
    "            loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "            emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "            \n",
    "            sentiment_correct += (sentiment_preds == sentiment_labels).sum().item()\n",
    "            emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
    "            total_predictions += sentiment_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        sentiment_accuracy = sentiment_correct / total_predictions\n",
    "        emotion_accuracy = emotion_correct / total_predictions\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate BERTweet multi-task on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_labels = []\n",
    "        emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_true = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_true = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_true)\n",
    "                emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_true)\n",
    "                loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_labels.extend(sentiment_true.cpu().numpy())\n",
    "                emotion_labels.extend(emotion_true.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_labels, emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy, sentiment_f1_macro, emotion_f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        \"\"\"Main BERTweet multi-task training loop\"\"\"\n",
    "        print(f\"🚀 Starting BERTweet multi-task training...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_combined_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_sent_acc, train_emo_acc = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_sent_acc, val_emo_acc, val_sent_f1, val_emo_f1 = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_sentiment_accuracy'].append(train_sent_acc)\n",
    "            self.training_history['train_emotion_accuracy'].append(train_emo_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_sent_acc)\n",
    "            self.training_history['val_emotion_accuracy'].append(val_emo_acc)\n",
    "            self.training_history['val_sentiment_f1_macro'].append(val_sent_f1)\n",
    "            self.training_history['val_emotion_f1_macro'].append(val_emo_f1)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Train Sentiment Acc: {train_sent_acc:.4f}, Train Emotion Acc: {train_emo_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Sentiment Acc: {val_sent_acc:.4f}, F1: {val_sent_f1:.4f}\")\n",
    "            print(f\"  Val Emotion Acc: {val_emo_acc:.4f}, F1: {val_emo_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            combined_f1 = (val_sent_f1 + val_emo_f1) / 2\n",
    "            if combined_f1 > best_combined_f1:\n",
    "                best_combined_f1 = combined_f1\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\n✅ BERTweet training completed! Best Combined F1: {best_combined_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        \"\"\"Save BERTweet multi-task model and tokenizer\"\"\"\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"💾 Best BERTweet model saved to {model_dir}\")\n",
    "\n",
    "print(\"✅ BERTweet Training classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636cba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified training method for ASHA support\n",
    "def train_with_pruning(self, data_splits: Dict, trial=None):\n",
    "    \"\"\"Main BERTweet training loop with ASHA pruning support\"\"\"\n",
    "    print(f\"🚀 Starting BERTweet single-task training ({self.config.task_type})...\")\n",
    "    \n",
    "    # Setup data loaders\n",
    "    self.create_data_loaders(data_splits)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(self.config.num_epochs):\n",
    "        print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_accuracy = self.train_epoch()\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "        \n",
    "        # Track metrics\n",
    "        self.training_history['train_loss'].append(train_loss)\n",
    "        self.training_history['train_accuracy'].append(train_accuracy)\n",
    "        self.training_history['val_loss'].append(val_loss)\n",
    "        self.training_history['val_accuracy'].append(val_accuracy)\n",
    "        self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "        \n",
    "        # Report to ASHA for pruning decision\n",
    "        if trial is not None:\n",
    "            trial.report(val_f1_macro, epoch)\n",
    "            \n",
    "            # Check if trial should be pruned\n",
    "            if trial.should_prune():\n",
    "                print(f\"  🚫 Trial pruned at epoch {epoch + 1}\")\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1_macro > best_f1:\n",
    "            best_f1 = val_f1_macro\n",
    "            self.save_model(is_best=True)\n",
    "    \n",
    "    print(f\"\\n✅ BERTweet training completed! Best F1: {best_f1:.4f}\")\n",
    "    return self.training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705702e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Modified BERTweet Hyperparameter Tuner (Quick Fix)\n",
    "class BERTweetHyperparameterTuner:\n",
    "    \"\"\"Hyperparameter tuning for BERTweet using Random Search (without ASHA for now)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 20,\n",
    "        model_name: str = \"vinai/bertweet-base\"\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.data_splits = data_splits\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        print(f\"🔍 BERTweet hyperparameter tuner initialized for {model_type}\")\n",
    "        print(f\"🚀 Using Random Search for fast optimization\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function for BERTweet\"\"\"\n",
    "        \n",
    "        # Sample hyperparameters optimized for BERTweet\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16])\n",
    "        num_epochs = trial.suggest_int('num_epochs', 3, 6)  # Reduced for speed\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.05, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.001, 0.1)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.4)\n",
    "        max_length = trial.suggest_categorical('max_length', [128, 256])\n",
    "        \n",
    "        # Multi-task specific parameter\n",
    "        alpha = trial.suggest_float('alpha', 0.3, 0.7) if self.model_type == \"multitask\" else 0.5\n",
    "        \n",
    "        # Create config\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./bertweet_temp_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            # Train BERTweet model using existing train method\n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = BERTweetMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.data_splits)  # Use existing train method\n",
    "                \n",
    "                # Return combined F1 score\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro'])\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro'])\n",
    "                combined_f1 = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "                print(f\"BERTweet Trial {trial.number}: Combined F1 = {combined_f1:.4f}\")\n",
    "                return combined_f1\n",
    "                \n",
    "            else:\n",
    "                # Single task training\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = bertweet_model_config.sentiment_num_classes\n",
    "                else:  # emotion\n",
    "                    num_classes = bertweet_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = BERTweetSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.data_splits)  # Use existing train method\n",
    "                \n",
    "                # Return best F1 score\n",
    "                best_f1 = max(history['val_f1_macro'])\n",
    "                print(f\"BERTweet Trial {trial.number}: F1 = {best_f1:.4f}\")\n",
    "                return best_f1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"BERTweet Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Clean up\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \"\"\"Run BERTweet hyperparameter optimization with Random Search\"\"\"\n",
    "        \n",
    "        # Create study with Random Search (no pruning for now)\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.RandomSampler(seed=42)  # Random Search only\n",
    "        )\n",
    "        \n",
    "        print(f\"🔍 Starting BERTweet hyperparameter optimization for {self.model_type}...\")\n",
    "        print(f\"🎯 Random Search: {self.n_trials} trials\")\n",
    "        print(\"==\" * 30)\n",
    "        \n",
    "        # Run optimization\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n🏆 BERTweet optimization completed for {self.model_type}!\")\n",
    "        print(f\"Best trial: {study.best_trial.number}\")\n",
    "        print(f\"Best score: {study.best_value:.4f}\")\n",
    "        print(f\"Completed trials: {len(study.trials)}\")\n",
    "        print(f\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47bdc526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed FastBERTweetHyperparameterTuner class defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Fixed Fast BERTweet Hyperparameter Tuner (Run this to replace the previous one!)\n",
    "import time\n",
    "import numpy as np\n",
    "import optuna\n",
    "from typing import Dict\n",
    "\n",
    "def create_tuning_subset(data_splits, subset_ratio=0.03):\n",
    "    \"\"\"Create small subset for fast hyperparameter tuning\"\"\"\n",
    "    print(f\"🔪 Creating {subset_ratio*100:.0f}% subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 50:  # Ensure minimum samples\n",
    "            n_samples = min(50, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'labels': [split_data['labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    # Handle different possible key names for validation set\n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "def create_multitask_tuning_subset(data_splits, subset_ratio=0.03):\n",
    "    \"\"\"Create small subset for multitask hyperparameter tuning\"\"\"\n",
    "    print(f\"🔪 Creating {subset_ratio*100:.0f}% multitask subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_multitask_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 50:  # Ensure minimum samples\n",
    "            n_samples = min(50, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [split_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [split_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_multitask_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_multitask_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_multitask_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_multitask_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Multitask tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "class FastBERTweetHyperparameterTuner:\n",
    "    \"\"\"Ultra-fast hyperparameter tuning for BERTweet\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 8,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        subset_ratio: float = 0.03,\n",
    "        max_epochs_per_trial: int = 2\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        self.max_epochs_per_trial = max_epochs_per_trial\n",
    "        \n",
    "        print(f\"🚀 Creating ultra-fast tuning setup for {model_type}...\")\n",
    "        \n",
    "        if model_type == \"multitask\":\n",
    "            self.tuning_data = create_multitask_tuning_subset(data_splits, subset_ratio)\n",
    "        else:\n",
    "            self.tuning_data = create_tuning_subset(data_splits, subset_ratio)\n",
    "        \n",
    "        print(f\"⚡ Speed optimizations:\")\n",
    "        print(f\"  - Using {subset_ratio*100:.0f}% of data ({len(self.tuning_data['train']['texts'])} samples)\")\n",
    "        print(f\"  - Max {max_epochs_per_trial} epochs per trial\")\n",
    "        print(f\"  - {n_trials} total trials\")\n",
    "        print(f\"  - Estimated time: {n_trials * max_epochs_per_trial * 1:.0f}-{n_trials * max_epochs_per_trial * 3:.0f} minutes\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Ultra-fast objective function\"\"\"\n",
    "        \n",
    "        # Fast hyperparameter suggestions\n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "        num_epochs = self.max_epochs_per_trial\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.3)\n",
    "        max_length = 128\n",
    "        \n",
    "        alpha = trial.suggest_float('alpha', 0.4, 0.6) if self.model_type == \"multitask\" else 0.5\n",
    "        \n",
    "        # Create speed-optimized config (removed unsupported parameters)\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./fast_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = BERTweetMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.tuning_data)\n",
    "                \n",
    "                # Get scores\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro']) if history['val_sentiment_f1_macro'] else 0.0\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro']) if history['val_emotion_f1_macro'] else 0.0\n",
    "                score = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "            else:\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = bertweet_model_config.sentiment_num_classes\n",
    "                else:\n",
    "                    num_classes = bertweet_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = BERTweetSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.tuning_data)\n",
    "                \n",
    "                score = max(history['val_f1_macro']) if history['val_f1_macro'] else 0.0\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"⚡ Trial {trial.number}: Score={score:.4f}, Time={elapsed/60:.1f}min\")\n",
    "            \n",
    "            return score\n",
    "            \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"❌ Trial {trial.number} failed after {elapsed/60:.1f}min: {str(e)[:100]}...\")\n",
    "            return 0.0\n",
    "            \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if 'trainer' in locals():\n",
    "                del trainer\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \"\"\"Run ultra-fast hyperparameter optimization\"\"\"\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.RandomSampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🚀 Starting FAST hyperparameter tuning for {self.model_type}...\")\n",
    "        print(f\"⚡ Target: Find good hyperparameters in ~{self.n_trials * 2:.0f} minutes\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n🏆 Fast tuning completed in {total_time/60:.1f} minutes!\")\n",
    "        print(f\"🎯 Best score: {study.best_value:.4f}\")\n",
    "        print(f\"📋 Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "print(\"✅ Fixed FastBERTweetHyperparameterTuner class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6166d46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: BERTweet Evaluation Functions\n",
    "def evaluate_bertweet_model(model_path: str, model_type: str, test_data: Dict, model_name: str = \"vinai/bertweet-base\"):\n",
    "    \"\"\"Evaluation function for BERTweet models\"\"\"\n",
    "    \n",
    "    print(f\"📊 Evaluating BERTweet {model_type} model...\")\n",
    "    \n",
    "    # Load BERTweet tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Load BERTweet model\n",
    "    if model_type == \"multitask\":\n",
    "        model = BERTweetMultiTaskTransformer.from_pretrained(model_path)\n",
    "    else:\n",
    "        model = BERTweetSingleTaskTransformer.from_pretrained(model_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare test data\n",
    "    if model_type == \"multitask\":\n",
    "        test_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            sentiment_labels=test_data['sentiment_labels'],\n",
    "            emotion_labels=test_data['emotion_labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    else:\n",
    "        test_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=test_data['texts'],\n",
    "            labels=test_data['labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    if model_type == \"multitask\":\n",
    "        all_sentiment_predictions = []\n",
    "        all_emotion_predictions = []\n",
    "        all_sentiment_labels = []\n",
    "        all_emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                all_sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                all_emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                all_sentiment_labels.extend(batch['sentiment_labels'].numpy())\n",
    "                all_emotion_labels.extend(batch['emotion_labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(all_sentiment_labels, all_sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(all_emotion_labels, all_emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(all_sentiment_labels, all_sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(all_emotion_labels, all_emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        results = {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'sentiment_f1_macro': sentiment_f1_macro,\n",
    "            'emotion_f1_macro': emotion_f1_macro,\n",
    "            'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "            'combined_f1_macro': (sentiment_f1_macro + emotion_f1_macro) / 2\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 BERTweet Multi-task Results:\")\n",
    "        print(f\"  Sentiment - Accuracy: {sentiment_accuracy:.4f}, F1: {sentiment_f1_macro:.4f}\")\n",
    "        print(f\"  Emotion - Accuracy: {emotion_accuracy:.4f}, F1: {emotion_f1_macro:.4f}\")\n",
    "        print(f\"  Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1_macro']:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # Single-task evaluation\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 BERTweet {model_type.capitalize()} Results:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1 Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_bertweet_results_summary(sentiment_results: Dict, emotion_results: Dict, multitask_results: Dict):\n",
    "    \"\"\"Create a summary of all BERTweet model results\"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📊 BERTWEET FINAL RESULTS SUMMARY\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🎯 BERTWEET SINGLE-TASK SENTIMENT MODEL:\")\n",
    "    print(f\"  Accuracy: {sentiment_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {sentiment_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 BERTWEET SINGLE-TASK EMOTION MODEL:\")\n",
    "    print(f\"  Accuracy: {emotion_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {emotion_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 BERTWEET MULTI-TASK MODEL:\")\n",
    "    print(f\"  Sentiment - Accuracy: {multitask_results['sentiment_accuracy']:.4f}, F1: {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"  Emotion - Accuracy: {multitask_results['emotion_accuracy']:.4f}, F1: {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"  Combined - Accuracy: {multitask_results['combined_accuracy']:.4f}, F1: {multitask_results['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 BERTWEET COMPARISON:\")\n",
    "    print(f\"  Single-task Sentiment vs Multi-task Sentiment:\")\n",
    "    print(f\"    Accuracy: {sentiment_results['accuracy']:.4f} vs {multitask_results['sentiment_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {sentiment_results['f1_macro']:.4f} vs {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"  Single-task Emotion vs Multi-task Emotion:\")\n",
    "    print(f\"    Accuracy: {emotion_results['accuracy']:.4f} vs {multitask_results['emotion_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {emotion_results['f1_macro']:.4f} vs {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"=\"*80)\n",
    "\n",
    "print(\"✅ BERTweet Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b37029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Main training pipeline defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Main BERTweet Training Pipeline\n",
    "def main_bertweet_training_pipeline():\n",
    "    \"\"\"Main pipeline for BERTweet: Initial training → Hyperparameter tuning → Final training\"\"\"\n",
    "    \n",
    "    print(\"🚀 STARTING COMPREHENSIVE BERTWEET TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and process datasets for BERTweet\n",
    "    print(\"\\n1️⃣ Loading and processing datasets for BERTweet...\")\n",
    "    sentiment_data, emotion_data = load_and_process_datasets_bertweet()\n",
    "    multitask_data = create_multitask_data_bertweet(sentiment_data, emotion_data)\n",
    "    \n",
    "    # Model configurations\n",
    "    model_name = \"vinai/bertweet-base\"\n",
    "    n_trials = 15  # Number of hyperparameter tuning trials\n",
    "    \n",
    "    # Store results\n",
    "    all_results = {}\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 1: INITIAL BERTWEET TRAINING WITH DEFAULT PARAMETERS\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 1: INITIAL BERTWEET TRAINING WITH DEFAULT PARAMETERS\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Default configuration for BERTweet\n",
    "    default_config_sentiment = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        task_type=\"sentiment\",\n",
    "        output_dir=\"./initial_bertweet_sentiment_model\"\n",
    "    )\n",
    "    \n",
    "    default_config_emotion = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        task_type=\"emotion\",\n",
    "        output_dir=\"./initial_bertweet_emotion_model\"\n",
    "    )\n",
    "    \n",
    "    default_config_multitask = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        alpha=0.5,\n",
    "        task_type=\"multitask\",\n",
    "        output_dir=\"./initial_bertweet_multitask_model\"\n",
    "    )\n",
    "    \n",
    "    # 1.1 Train Initial BERTweet Sentiment Model\n",
    "    print(f\"\\n2️⃣ Training Initial BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_sentiment_trainer = BERTweetSingleTaskTrainer(\n",
    "        config=default_config_sentiment,\n",
    "        num_classes=bertweet_model_config.sentiment_num_classes\n",
    "    )\n",
    "    initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet sentiment model\n",
    "    initial_sentiment_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_sentiment_model/model_best\",\n",
    "        model_type=\"sentiment\",\n",
    "        test_data=sentiment_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['initial_sentiment'] = initial_sentiment_results\n",
    "    \n",
    "    # 1.2 Train Initial BERTweet Emotion Model\n",
    "    print(f\"\\n3️⃣ Training Initial BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_emotion_trainer = BERTweetSingleTaskTrainer(\n",
    "        config=default_config_emotion,\n",
    "        num_classes=bertweet_model_config.emotion_num_classes\n",
    "    )\n",
    "    initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet emotion model\n",
    "    initial_emotion_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_emotion_model/model_best\",\n",
    "        model_type=\"emotion\",\n",
    "        test_data=emotion_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['initial_emotion'] = initial_emotion_results\n",
    "    \n",
    "    # 1.3 Train Initial BERTweet Multi-task Model\n",
    "    print(f\"\\n4️⃣ Training Initial BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_multitask_trainer = BERTweetMultiTaskTrainer(config=default_config_multitask)\n",
    "    initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet multi-task model\n",
    "    initial_multitask_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_multitask_model/model_best\",\n",
    "        model_type=\"multitask\",\n",
    "        test_data=multitask_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['initial_multitask'] = initial_multitask_results\n",
    "    \n",
    "    # Display initial BERTweet results summary\n",
    "    print(f\"\\n5️⃣ Initial BERTweet Results Summary...\")\n",
    "    print(\"=\"*60)\n",
    "    create_bertweet_initial_results_summary(\n",
    "        sentiment_results=all_results['initial_sentiment'],\n",
    "        emotion_results=all_results['initial_emotion'],\n",
    "        multitask_results=all_results['initial_multitask']\n",
    "    )\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 2: BERTWEET HYPERPARAMETER TUNING\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 2: BERTWEET HYPERPARAMETER TUNING\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # 2.1 Hyperparameter tuning for BERTweet sentiment\n",
    "    print(f\"\\n6️⃣ Hyperparameter Tuning for BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sentiment_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"sentiment\",\n",
    "        data_splits=sentiment_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    sentiment_study = sentiment_tuner.tune()\n",
    "    \n",
    "    # 2.2 Hyperparameter tuning for BERTweet emotion\n",
    "    print(f\"\\n7️⃣ Hyperparameter Tuning for BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    emotion_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"emotion\",\n",
    "        data_splits=emotion_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    emotion_study = emotion_tuner.tune()\n",
    "    \n",
    "    # 2.3 Hyperparameter tuning for BERTweet multi-task\n",
    "    print(f\"\\n8️⃣ Hyperparameter Tuning for BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    multitask_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"multitask\",\n",
    "        data_splits=multitask_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    multitask_study = multitask_tuner.tune()\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 3: FINAL BERTWEET TRAINING WITH OPTIMIZED PARAMETERS\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 3: FINAL BERTWEET TRAINING WITH OPTIMIZED PARAMETERS\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # 3.1 Train optimized BERTweet sentiment model\n",
    "    print(f\"\\n9️⃣ Training Optimized BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_sentiment_trainer, optimized_sentiment_history = train_bertweet_with_best_params(\n",
    "        model_type=\"sentiment\",\n",
    "        data_splits=sentiment_data,\n",
    "        best_params=sentiment_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet sentiment model\n",
    "    optimized_sentiment_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_sentiment_model/model_best\",\n",
    "        model_type=\"sentiment\",\n",
    "        test_data=sentiment_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['optimized_sentiment'] = optimized_sentiment_results\n",
    "    \n",
    "    # 3.2 Train optimized BERTweet emotion model\n",
    "    print(f\"\\n🔟 Training Optimized BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_emotion_trainer, optimized_emotion_history = train_bertweet_with_best_params(\n",
    "        model_type=\"emotion\",\n",
    "        data_splits=emotion_data,\n",
    "        best_params=emotion_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet emotion model\n",
    "    optimized_emotion_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_emotion_model/model_best\",\n",
    "        model_type=\"emotion\",\n",
    "        test_data=emotion_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['optimized_emotion'] = optimized_emotion_results\n",
    "    \n",
    "    # 3.3 Train optimized BERTweet multi-task model\n",
    "    print(f\"\\n1️⃣1️⃣ Training Optimized BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_multitask_trainer, optimized_multitask_history = train_bertweet_with_best_params(\n",
    "        model_type=\"multitask\",\n",
    "        data_splits=multitask_data,\n",
    "        best_params=multitask_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet multi-task model\n",
    "    optimized_multitask_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_multitask_model/model_best\",\n",
    "        model_type=\"multitask\",\n",
    "        test_data=multitask_data['test'],\n",
    "        model_name=model_name\n",
    "    )\n",
    "    all_results['optimized_multitask'] = optimized_multitask_results\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 4: COMPREHENSIVE BERTWEET RESULTS COMPARISON\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 4: COMPREHENSIVE BERTWEET RESULTS COMPARISON\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Create comprehensive BERTweet comparison\n",
    "    create_comprehensive_bertweet_results_comparison(all_results)\n",
    "    \n",
    "    # Save all BERTweet results\n",
    "    results_summary = {\n",
    "        'model_type': 'BERTweet',\n",
    "        'model_name': model_name,\n",
    "        'initial_models': {\n",
    "            'sentiment': all_results['initial_sentiment'],\n",
    "            'emotion': all_results['initial_emotion'],\n",
    "            'multitask': all_results['initial_multitask']\n",
    "        },\n",
    "        'optimized_models': {\n",
    "            'sentiment': all_results['optimized_sentiment'],\n",
    "            'emotion': all_results['optimized_emotion'],\n",
    "            'multitask': all_results['optimized_multitask']\n",
    "        },\n",
    "        'hyperparameter_studies': {\n",
    "            'sentiment': sentiment_study.best_params,\n",
    "            'emotion': emotion_study.best_params,\n",
    "            'multitask': multitask_study.best_params\n",
    "        },\n",
    "        'improvements': {\n",
    "            'sentiment': {\n",
    "                'accuracy_improvement': all_results['optimized_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy'],\n",
    "                'f1_improvement': all_results['optimized_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro']\n",
    "            },\n",
    "            'emotion': {\n",
    "                'accuracy_improvement': all_results['optimized_emotion']['accuracy'] - all_results['initial_emotion']['accuracy'],\n",
    "                'f1_improvement': all_results['optimized_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro']\n",
    "            },\n",
    "            'multitask': {\n",
    "                'sentiment_accuracy_improvement': all_results['optimized_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy'],\n",
    "                'emotion_accuracy_improvement': all_results['optimized_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy'],\n",
    "                'combined_f1_improvement': all_results['optimized_multitask']['combined_f1_macro'] - all_results['initial_multitask']['combined_f1_macro']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('comprehensive_bertweet_results_summary.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ COMPLETE BERTWEET PIPELINE FINISHED!\")\n",
    "    print(f\"📁 Results saved to: comprehensive_bertweet_results_summary.json\")\n",
    "    print(f\"📁 Initial models saved to: ./initial_bertweet_*_model/\")\n",
    "    print(f\"📁 Optimized models saved to: ./final_bertweet_*_model/\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def create_bertweet_initial_results_summary(sentiment_results: Dict, emotion_results: Dict, multitask_results: Dict):\n",
    "    \"\"\"Create a summary of initial BERTweet model results\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 INITIAL BERTWEET MODELS RESULTS SUMMARY\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n🎯 INITIAL BERTWEET SENTIMENT MODEL:\")\n",
    "    print(f\"  Accuracy: {sentiment_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {sentiment_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 INITIAL BERTWEET EMOTION MODEL:\")\n",
    "    print(f\"  Accuracy: {emotion_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {emotion_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 INITIAL BERTWEET MULTI-TASK MODEL:\")\n",
    "    print(f\"  Sentiment - Accuracy: {multitask_results['sentiment_accuracy']:.4f}, F1: {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"  Emotion - Accuracy: {multitask_results['emotion_accuracy']:.4f}, F1: {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"  Combined - Accuracy: {multitask_results['combined_accuracy']:.4f}, F1: {multitask_results['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n💡 These are BERTweet baseline results. Hyperparameter tuning will aim to improve them!\")\n",
    "\n",
    "def create_comprehensive_bertweet_results_comparison(all_results: Dict):\n",
    "    \"\"\"Create comprehensive comparison between initial and optimized BERTweet models\"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 COMPREHENSIVE BERTWEET RESULTS COMPARISON\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🎯 BERTWEET SENTIMENT MODEL COMPARISON:\")\n",
    "    print(f\"  Initial    - Accuracy: {all_results['initial_sentiment']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['f1_macro']:.4f}\")\n",
    "    print(f\"  Optimized  - Accuracy: {all_results['optimized_sentiment']['accuracy']:.4f}, F1: {all_results['optimized_sentiment']['f1_macro']:.4f}\")\n",
    "    \n",
    "    sent_acc_improve = all_results['optimized_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy']\n",
    "    sent_f1_improve = all_results['optimized_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro']\n",
    "    print(f\"  Improvement - Accuracy: {sent_acc_improve:+.4f}, F1: {sent_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 BERTWEET EMOTION MODEL COMPARISON:\")\n",
    "    print(f\"  Initial    - Accuracy: {all_results['initial_emotion']['accuracy']:.4f}, F1: {all_results['initial_emotion']['f1_macro']:.4f}\")\n",
    "    print(f\"  Optimized  - Accuracy: {all_results['optimized_emotion']['accuracy']:.4f}, F1: {all_results['optimized_emotion']['f1_macro']:.4f}\")\n",
    "    \n",
    "    emo_acc_improve = all_results['optimized_emotion']['accuracy'] - all_results['initial_emotion']['accuracy']\n",
    "    emo_f1_improve = all_results['optimized_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro']\n",
    "    print(f\"  Improvement - Accuracy: {emo_acc_improve:+.4f}, F1: {emo_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 BERTWEET MULTI-TASK MODEL COMPARISON:\")\n",
    "    print(f\"  SENTIMENT TASK:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['optimized_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_sent_acc_improve = all_results['optimized_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy']\n",
    "    mt_sent_f1_improve = all_results['optimized_multitask']['sentiment_f1_macro'] - all_results['initial_multitask']['sentiment_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_sent_acc_improve:+.4f}, F1: {mt_sent_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"  EMOTION TASK:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['emotion_accuracy']:.4f}, F1: {all_results['optimized_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_emo_acc_improve = all_results['optimized_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy']\n",
    "    mt_emo_f1_improve = all_results['optimized_multitask']['emotion_f1_macro'] - all_results['initial_multitask']['emotion_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_emo_acc_improve:+.4f}, F1: {mt_emo_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"  COMBINED:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['combined_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['combined_accuracy']:.4f}, F1: {all_results['optimized_multitask']['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_combined_acc_improve = all_results['optimized_multitask']['combined_accuracy'] - all_results['initial_multitask']['combined_accuracy']\n",
    "    mt_combined_f1_improve = all_results['optimized_multitask']['combined_f1_macro'] - all_results['initial_multitask']['combined_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_combined_acc_improve:+.4f}, F1: {mt_combined_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 BERTWEET SINGLE-TASK vs MULTI-TASK COMPARISON (OPTIMIZED):\")\n",
    "    print(f\"  SENTIMENT:\")\n",
    "    print(f\"    Single-task: Accuracy: {all_results['optimized_sentiment']['accuracy']:.4f}, F1: {all_results['optimized_sentiment']['f1_macro']:.4f}\")\n",
    "    print(f\"    Multi-task:  Accuracy: {all_results['optimized_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['optimized_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"  EMOTION:\")\n",
    "    print(f\"    Single-task: Accuracy: {all_results['optimized_emotion']['accuracy']:.4f}, F1: {all_results['optimized_emotion']['f1_macro']:.4f}\")\n",
    "    print(f\"    Multi-task:  Accuracy: {all_results['optimized_multitask']['emotion_accuracy']:.4f}, F1: {all_results['optimized_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"=\"*80)\n",
    "\n",
    "print(\"✅ BERTweet Main training pipeline defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b307c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING BERTWEET TRAINING PIPELINE\n",
      "================================================================================\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "1️⃣ Loading and processing datasets for BERTweet...\n",
      "📥 Loading datasets for BERTweet...\n",
      "✅ SST-2 loaded: 67349 train, 872 val\n",
      "✅ GoEmotion loaded: 43410 train, 5426 val\n",
      "✅ Loaded existing encoders from enc/ directory for BERTweet\n",
      "🔄 Processing sentiment data for BERTweet...\n",
      "✅ BERTweet Sentiment data processed:\n",
      "  Train: 47144 samples\n",
      "  Val: 10102 samples\n",
      "  Test: 10103 samples\n",
      "🔄 Processing emotion data for BERTweet...\n",
      "✅ BERTweet Emotion data processed:\n",
      "  Train: 9534 samples\n",
      "  Val: 2043 samples\n",
      "  Test: 2044 samples\n",
      "🔄 Creating multi-task dataset for BERTweet...\n",
      "✅ BERTweet Multi-task data created:\n",
      "  Train: 9534 samples\n",
      "  Val: 2043 samples\n",
      "  Test: 2044 samples\n",
      "✅ Data loading completed!\n",
      "📊 Sentiment data: 47144 train samples\n",
      "📊 Emotion data: 9534 train samples\n",
      "📊 Multitask data: 9534 train samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Data Loading and Processing\n",
    "print(\"🚀 STARTING BERTWEET TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear memory before starting\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Load and process datasets for BERTweet\n",
    "print(\"\\n1️⃣ Loading and processing datasets for BERTweet...\")\n",
    "sentiment_data, emotion_data = load_and_process_datasets_bertweet()\n",
    "multitask_data = create_multitask_data_bertweet(sentiment_data, emotion_data)\n",
    "\n",
    "# Model configurations\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "n_trials = 10 # Number of hyperparameter tuning trials\n",
    "\n",
    "# Store results globally\n",
    "all_results = {}\n",
    "\n",
    "print(\"✅ Data loading completed!\")\n",
    "print(f\"📊 Sentiment data: {len(sentiment_data['train']['texts'])} train samples\")\n",
    "print(f\"📊 Emotion data: {len(emotion_data['train']['texts'])} train samples\")\n",
    "print(f\"📊 Multitask data: {len(multitask_data['train']['texts'])} train samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2567c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL BERTWEET TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "2️⃣ Training Initial BERTweet Sentiment Model...\n",
      "============================================================\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 0.5466, Train Acc: 0.8116\n",
      "  Val Loss: 0.4787, Val Acc: 0.8633, Val F1: 0.6018\n",
      "BERTweet single-task model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.4029, Train Acc: 0.8780\n",
      "  Val Loss: 0.4581, Val Acc: 0.8703, Val F1: 0.6070\n",
      "BERTweet single-task model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.3369, Train Acc: 0.8957\n",
      "  Val Loss: 0.4867, Val Acc: 0.8726, Val F1: 0.6086\n",
      "BERTweet single-task model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.6086\n",
      "📊 Evaluating BERTweet sentiment model...\n",
      "📊 BERTweet Sentiment Results:\n",
      "  Accuracy: 0.8742\n",
      "  F1 Macro: 0.6098\n",
      "\n",
      "✅ Initial Sentiment Model Results:\n",
      "  Accuracy: 0.8742\n",
      "  F1 Macro: 0.6098\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initial Sentiment Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL BERTWEET TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for BERTweet sentiment\n",
    "default_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_bertweet_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Training Initial BERTweet Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model\n",
    "initial_sentiment_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=default_config_sentiment,\n",
    "    num_classes=bertweet_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate initial sentiment model\n",
    "initial_sentiment_results = evaluate_bertweet_model(\n",
    "    model_path=\"./initial_bertweet_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_sentiment'] = initial_sentiment_results\n",
    "\n",
    "print(f\"\\n✅ Initial Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {initial_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "985b1f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL BERTWEET TRAINING - EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "3️⃣ Training Initial BERTweet Emotion Model...\n",
      "============================================================\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 1.0579, Train Acc: 0.5967\n",
      "  Val Loss: 0.6974, Val Acc: 0.7533, Val F1: 0.7249\n",
      "BERTweet single-task model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.5841, Train Acc: 0.7993\n",
      "  Val Loss: 0.7449, Val Acc: 0.7572, Val F1: 0.7279\n",
      "BERTweet single-task model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.4233, Train Acc: 0.8592\n",
      "  Val Loss: 0.8308, Val Acc: 0.7611, Val F1: 0.7324\n",
      "BERTweet single-task model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.7324\n",
      "📊 Evaluating BERTweet emotion model...\n",
      "📊 BERTweet Emotion Results:\n",
      "  Accuracy: 0.7701\n",
      "  F1 Macro: 0.7391\n",
      "\n",
      "✅ Initial Emotion Model Results:\n",
      "  Accuracy: 0.7701\n",
      "  F1 Macro: 0.7391\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initial Emotion Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL BERTWEET TRAINING - EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for BERTweet emotion\n",
    "default_config_emotion = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./initial_bertweet_emotion_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n3️⃣ Training Initial BERTweet Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial emotion model\n",
    "initial_emotion_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=default_config_emotion,\n",
    "    num_classes=bertweet_model_config.emotion_num_classes\n",
    ")\n",
    "initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate initial emotion model\n",
    "initial_emotion_results = evaluate_bertweet_model(\n",
    "    model_path=\"./initial_bertweet_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_emotion'] = initial_emotion_results\n",
    "\n",
    "print(f\"\\n✅ Initial Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {initial_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {initial_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84b46b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL BERTWEET TRAINING - MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "4️⃣ Training Initial BERTweet Multi-task Model...\n",
      "============================================================\n",
      "🚀 Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 1.2223\n",
      "  Train Sentiment Acc: 0.7248, Train Emotion Acc: 0.2578\n",
      "  Val Loss: 1.0885\n",
      "  Val Sentiment Acc: 0.8365, F1: 0.5850\n",
      "  Val Emotion Acc: 0.3005, F1: 0.0872\n",
      "BERTweet multi-task model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 1.0712\n",
      "  Train Sentiment Acc: 0.8617, Train Emotion Acc: 0.2940\n",
      "  Val Loss: 1.0735\n",
      "  Val Sentiment Acc: 0.8502, F1: 0.5947\n",
      "  Val Emotion Acc: 0.3030, F1: 0.0775\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 1.0173\n",
      "  Train Sentiment Acc: 0.8900, Train Emotion Acc: 0.3021\n",
      "  Val Loss: 1.0881\n",
      "  Val Sentiment Acc: 0.8561, F1: 0.5988\n",
      "  Val Emotion Acc: 0.3045, F1: 0.0826\n",
      "BERTweet multi-task model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "💾 Best BERTweet model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best Combined F1: 0.3407\n",
      "📊 Evaluating BERTweet multitask model...\n",
      "📊 BERTweet Multi-task Results:\n",
      "  Sentiment - Accuracy: 0.8547, F1: 0.5955\n",
      "  Emotion - Accuracy: 0.3043, F1: 0.0817\n",
      "  Combined - Accuracy: 0.5795, F1: 0.3386\n",
      "\n",
      "✅ Initial Multitask Model Results:\n",
      "  Sentiment - Accuracy: 0.8547, F1: 0.5955\n",
      "  Emotion - Accuracy: 0.3043, F1: 0.0817\n",
      "  Combined - Accuracy: 0.5795, F1: 0.3386\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initial Multitask Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL BERTWEET TRAINING - MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for BERTweet multitask\n",
    "default_config_multitask = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    alpha=0.5,\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./initial_bertweet_multitask_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n4️⃣ Training Initial BERTweet Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial multitask model\n",
    "initial_multitask_trainer = BERTweetMultiTaskTrainer(config=default_config_multitask)\n",
    "initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate initial multitask model\n",
    "initial_multitask_results = evaluate_bertweet_model(\n",
    "    model_path=\"./initial_bertweet_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "all_results['initial_multitask'] = initial_multitask_results\n",
    "\n",
    "print(f\"\\n✅ Initial Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {initial_multitask_results['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {initial_multitask_results['emotion_accuracy']:.4f}, F1: {initial_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {initial_multitask_results['combined_accuracy']:.4f}, F1: {initial_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3357db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 INITIAL BERTWEET RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 INITIAL BERTWEET MODELS RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "🎯 INITIAL BERTWEET SENTIMENT MODEL:\n",
      "  Accuracy: 0.8742\n",
      "  F1 Macro: 0.6098\n",
      "\n",
      "😊 INITIAL BERTWEET EMOTION MODEL:\n",
      "  Accuracy: 0.7701\n",
      "  F1 Macro: 0.7391\n",
      "\n",
      "🔗 INITIAL BERTWEET MULTI-TASK MODEL:\n",
      "  Sentiment - Accuracy: 0.8547, F1: 0.5955\n",
      "  Emotion - Accuracy: 0.3043, F1: 0.0817\n",
      "  Combined - Accuracy: 0.5795, F1: 0.3386\n",
      "\n",
      "💡 These are BERTweet baseline results. Hyperparameter tuning will aim to improve them!\n",
      "\n",
      "💡 These are BERTweet baseline results. Now proceeding to hyperparameter tuning!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Initial Results Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 INITIAL BERTWEET RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "create_bertweet_initial_results_summary(\n",
    "    sentiment_results=all_results['initial_sentiment'],\n",
    "    emotion_results=all_results['initial_emotion'],\n",
    "    multitask_results=all_results['initial_multitask']\n",
    ")\n",
    "\n",
    "print(f\"\\n💡 These are BERTweet baseline results. Now proceeding to hyperparameter tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ba09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:11:08,639] A new study created in memory with name: no-name-3d4e70ce-6a95-49fd-bf22-9b8a9f9d25a6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - SENTIMENT\n",
      "================================================================================\n",
      "\n",
      "6️⃣ Fast Hyperparameter Tuning for BERTweet Sentiment Model...\n",
      "============================================================\n",
      "🚀 Creating ultra-fast tuning setup for sentiment...\n",
      "🔪 Creating 2% subset for hyperparameter tuning...\n",
      "📊 Tuning subset created:\n",
      "  Train: 942 samples\n",
      "  Val: 202 samples\n",
      "⚡ Speed optimizations:\n",
      "  - Using 2% of data (942 samples)\n",
      "  - Max 2 epochs per trial\n",
      "  - 6 total trials\n",
      "  - Estimated time: 12-36 minutes\n",
      "\n",
      "🚀 Starting FAST hyperparameter tuning for sentiment...\n",
      "⚡ Target: Find good hyperparameters in ~12 minutes\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9104, Train Acc: 0.5403\n",
      "  Val Loss: 0.6369, Val Acc: 0.8168, Val F1: 0.5619\n",
      "BERTweet single-task model saved to ./fast_trial_0\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:11:39,190] Trial 0 finished with value: 0.5618997638415114 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'warmup_ratio': 0.15986584841970367, 'weight_decay': 0.02404167763981929, 'hidden_dropout_prob': 0.13119890406724052, 'classifier_dropout': 0.1116167224336399}. Best is trial 0 with value: 0.5618997638415114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.6036, Train Acc: 0.7951\n",
      "  Val Loss: 0.5606, Val Acc: 0.8020, Val F1: 0.5527\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.5619\n",
      "⚡ Trial 0: Score=0.5619, Time=0.5min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9609, Train Acc: 0.4703\n",
      "  Val Loss: 0.8789, Val Acc: 0.5149, Val F1: 0.3516\n",
      "BERTweet single-task model saved to ./fast_trial_1\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:12:05,484] Trial 1 finished with value: 0.35162147793726745 and parameters: {'learning_rate': 8.062340576073854e-05, 'batch_size': 32, 'warmup_ratio': 0.10205844942958026, 'weight_decay': 0.0972918866945795, 'hidden_dropout_prob': 0.26648852816008434, 'classifier_dropout': 0.14246782213565523}. Best is trial 0 with value: 0.5618997638415114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.9249, Train Acc: 0.4724\n",
      "  Val Loss: 0.8825, Val Acc: 0.4851, Val F1: 0.2303\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.3516\n",
      "⚡ Trial 1: Score=0.3516, Time=0.4min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9477, Train Acc: 0.4597\n",
      "  Val Loss: 0.8653, Val Acc: 0.4802, Val F1: 0.2163\n",
      "BERTweet single-task model saved to ./fast_trial_2\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.9157, Train Acc: 0.5053\n",
      "  Val Loss: 0.8613, Val Acc: 0.6337, Val F1: 0.4346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:12:32,346] Trial 2 finished with value: 0.43457063906929266 and parameters: {'learning_rate': 2.679909904436601e-05, 'batch_size': 32, 'warmup_ratio': 0.1524756431632238, 'weight_decay': 0.048875051677790424, 'hidden_dropout_prob': 0.15824582803960838, 'classifier_dropout': 0.22237057894447587}. Best is trial 0 with value: 0.5618997638415114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_2\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.4346\n",
      "⚡ Trial 2: Score=0.4346, Time=0.4min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9616, Train Acc: 0.4756\n",
      "  Val Loss: 0.8597, Val Acc: 0.5743, Val F1: 0.3681\n",
      "BERTweet single-task model saved to ./fast_trial_3\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.8990, Train Acc: 0.5138\n",
      "  Val Loss: 0.8555, Val Acc: 0.5941, Val F1: 0.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:12:59,013] Trial 3 finished with value: 0.40039115553865373 and parameters: {'learning_rate': 2.503410215228042e-05, 'batch_size': 32, 'warmup_ratio': 0.1456069984217036, 'weight_decay': 0.08066583652537122, 'hidden_dropout_prob': 0.13993475643167194, 'classifier_dropout': 0.2028468876827223}. Best is trial 0 with value: 0.5618997638415114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_3\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.4004\n",
      "⚡ Trial 3: Score=0.4004, Time=0.4min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9469, Train Acc: 0.4713\n",
      "  Val Loss: 0.8741, Val Acc: 0.4802, Val F1: 0.2163\n",
      "BERTweet single-task model saved to ./fast_trial_4\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:13:25,334] Trial 4 finished with value: 0.21627647714604237 and parameters: {'learning_rate': 5.1893147077559305e-05, 'batch_size': 32, 'warmup_ratio': 0.11705241236872915, 'weight_decay': 0.015854643368675158, 'hidden_dropout_prob': 0.28977710745066665, 'classifier_dropout': 0.29312640661491185}. Best is trial 0 with value: 0.5618997638415114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.9041, Train Acc: 0.4958\n",
      "  Val Loss: 0.8711, Val Acc: 0.4802, Val F1: 0.2163\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.2163\n",
      "⚡ Trial 4: Score=0.2163, Time=0.4min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.8787, Train Acc: 0.5488\n",
      "  Val Loss: 0.6488, Val Acc: 0.7525, Val F1: 0.5155\n",
      "BERTweet single-task model saved to ./fast_trial_5\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_5\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.5517, Train Acc: 0.8100\n",
      "  Val Loss: 0.5912, Val Acc: 0.7970, Val F1: 0.5491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:13:54,481] Trial 5 finished with value: 0.5490837579333154 and parameters: {'learning_rate': 7.3464156009241e-05, 'batch_size': 16, 'warmup_ratio': 0.1684233026512157, 'weight_decay': 0.04961372443656412, 'hidden_dropout_prob': 0.12440764696895577, 'classifier_dropout': 0.19903538202225401}. Best is trial 0 with value: 0.5618997638415114.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_5\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_5\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.5491\n",
      "⚡ Trial 5: Score=0.5491, Time=0.5min\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "🏆 Fast tuning completed in 2.8 minutes!\n",
      "🎯 Best score: 0.5619\n",
      "📋 Best parameters:\n",
      "  learning_rate: 3.65445235521325e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.15986584841970367\n",
      "  weight_decay: 0.02404167763981929\n",
      "  hidden_dropout_prob: 0.13119890406724052\n",
      "  classifier_dropout: 0.1116167224336399\n",
      "\n",
      "✅ Sentiment Hyperparameter Tuning Completed!\n",
      "🏆 Best F1 Score: 0.5619\n",
      "📋 Best Parameters:\n",
      "  learning_rate: 3.65445235521325e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.15986584841970367\n",
      "  weight_decay: 0.02404167763981929\n",
      "  hidden_dropout_prob: 0.13119890406724052\n",
      "  classifier_dropout: 0.1116167224336399\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Ultra-Fast Hyperparameter Tuning - Sentiment (Fixed)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n6️⃣ Fast Hyperparameter Tuning for BERTweet Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create FAST tuner for sentiment\n",
    "sentiment_tuner = FastBERTweetHyperparameterTuner(\n",
    "    model_type=\"sentiment\",\n",
    "    data_splits=sentiment_data,\n",
    "    n_trials=6,  # Even fewer trials for speed\n",
    "    model_name=model_name,\n",
    "    subset_ratio=0.02,  # Only 2% of data!\n",
    "    max_epochs_per_trial=2  # Only 2 epochs per trial!\n",
    ")\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "sentiment_study = sentiment_tuner.tune()\n",
    "\n",
    "print(f\"\\n✅ Sentiment Hyperparameter Tuning Completed!\")\n",
    "print(f\"🏆 Best F1 Score: {sentiment_study.best_value:.4f}\")\n",
    "print(f\"📋 Best Parameters:\")\n",
    "for key, value in sentiment_study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e93af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:13:54,610] A new study created in memory with name: no-name-b93e2e92-fb29-477c-8b4f-76ad6c5c2a07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 2: HYPERPARAMETER TUNING - EMOTION\n",
      "================================================================================\n",
      "\n",
      "7️⃣ Hyperparameter Tuning for BERTweet Emotion Model...\n",
      "============================================================\n",
      "🚀 Creating ultra-fast tuning setup for emotion...\n",
      "🔪 Creating 2% subset for hyperparameter tuning...\n",
      "📊 Tuning subset created:\n",
      "  Train: 190 samples\n",
      "  Val: 50 samples\n",
      "⚡ Speed optimizations:\n",
      "  - Using 2% of data (190 samples)\n",
      "  - Max 2 epochs per trial\n",
      "  - 6 total trials\n",
      "  - Estimated time: 12-36 minutes\n",
      "\n",
      "🚀 Starting FAST hyperparameter tuning for emotion...\n",
      "⚡ Target: Find good hyperparameters in ~12 minutes\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7365, Train Acc: 0.2579\n",
      "  Val Loss: 1.9856, Val Acc: 0.3200, Val F1: 0.0808\n",
      "BERTweet single-task model saved to ./fast_trial_0\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:14:02,670] Trial 0 finished with value: 0.08080808080808081 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'warmup_ratio': 0.15986584841970367, 'weight_decay': 0.02404167763981929, 'hidden_dropout_prob': 0.13119890406724052, 'classifier_dropout': 0.1116167224336399}. Best is trial 0 with value: 0.08080808080808081.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.6284, Train Acc: 0.3316\n",
      "  Val Loss: 2.0164, Val Acc: 0.3200, Val F1: 0.0808\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0808\n",
      "⚡ Trial 0: Score=0.0808, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7727, Train Acc: 0.2000\n",
      "  Val Loss: 1.8008, Val Acc: 0.3200, Val F1: 0.0808\n",
      "BERTweet single-task model saved to ./fast_trial_1\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:14:10,167] Trial 1 finished with value: 0.08080808080808081 and parameters: {'learning_rate': 8.062340576073854e-05, 'batch_size': 32, 'warmup_ratio': 0.10205844942958026, 'weight_decay': 0.0972918866945795, 'hidden_dropout_prob': 0.26648852816008434, 'classifier_dropout': 0.14246782213565523}. Best is trial 0 with value: 0.08080808080808081.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.6645, Train Acc: 0.3211\n",
      "  Val Loss: 1.8373, Val Acc: 0.3200, Val F1: 0.0808\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0808\n",
      "⚡ Trial 1: Score=0.0808, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7829, Train Acc: 0.2368\n",
      "  Val Loss: 1.7388, Val Acc: 0.3200, Val F1: 0.0808\n",
      "BERTweet single-task model saved to ./fast_trial_2\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:14:17,727] Trial 2 finished with value: 0.08080808080808081 and parameters: {'learning_rate': 2.679909904436601e-05, 'batch_size': 32, 'warmup_ratio': 0.1524756431632238, 'weight_decay': 0.048875051677790424, 'hidden_dropout_prob': 0.15824582803960838, 'classifier_dropout': 0.22237057894447587}. Best is trial 0 with value: 0.08080808080808081.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.7014, Train Acc: 0.3105\n",
      "  Val Loss: 1.7333, Val Acc: 0.3200, Val F1: 0.0808\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0808\n",
      "⚡ Trial 2: Score=0.0808, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7878, Train Acc: 0.1789\n",
      "  Val Loss: 1.7672, Val Acc: 0.3200, Val F1: 0.0808\n",
      "BERTweet single-task model saved to ./fast_trial_3\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:14:25,298] Trial 3 finished with value: 0.08080808080808081 and parameters: {'learning_rate': 2.503410215228042e-05, 'batch_size': 32, 'warmup_ratio': 0.1456069984217036, 'weight_decay': 0.08066583652537122, 'hidden_dropout_prob': 0.13993475643167194, 'classifier_dropout': 0.2028468876827223}. Best is trial 0 with value: 0.08080808080808081.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.7130, Train Acc: 0.2842\n",
      "  Val Loss: 1.7613, Val Acc: 0.3200, Val F1: 0.0808\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0808\n",
      "⚡ Trial 3: Score=0.0808, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7690, Train Acc: 0.2211\n",
      "  Val Loss: 1.7645, Val Acc: 0.3200, Val F1: 0.0808\n",
      "BERTweet single-task model saved to ./fast_trial_4\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:14:32,859] Trial 4 finished with value: 0.08080808080808081 and parameters: {'learning_rate': 5.1893147077559305e-05, 'batch_size': 32, 'warmup_ratio': 0.11705241236872915, 'weight_decay': 0.015854643368675158, 'hidden_dropout_prob': 0.28977710745066665, 'classifier_dropout': 0.29312640661491185}. Best is trial 0 with value: 0.08080808080808081.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.7128, Train Acc: 0.2947\n",
      "  Val Loss: 1.7828, Val Acc: 0.3200, Val F1: 0.0808\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0808\n",
      "⚡ Trial 4: Score=0.0808, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7250, Train Acc: 0.2421\n",
      "  Val Loss: 2.1122, Val Acc: 0.3200, Val F1: 0.0808\n",
      "BERTweet single-task model saved to ./fast_trial_5\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_5\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.6320, Train Acc: 0.3579\n",
      "  Val Loss: 2.0820, Val Acc: 0.3400, Val F1: 0.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:14:41,517] Trial 5 finished with value: 0.1263227513227513 and parameters: {'learning_rate': 7.3464156009241e-05, 'batch_size': 16, 'warmup_ratio': 0.1684233026512157, 'weight_decay': 0.04961372443656412, 'hidden_dropout_prob': 0.12440764696895577, 'classifier_dropout': 0.19903538202225401}. Best is trial 5 with value: 0.1263227513227513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_5\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_5\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.1263\n",
      "⚡ Trial 5: Score=0.1263, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "🏆 Fast tuning completed in 0.8 minutes!\n",
      "🎯 Best score: 0.1263\n",
      "📋 Best parameters:\n",
      "  learning_rate: 7.3464156009241e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.1684233026512157\n",
      "  weight_decay: 0.04961372443656412\n",
      "  hidden_dropout_prob: 0.12440764696895577\n",
      "  classifier_dropout: 0.19903538202225401\n",
      "\n",
      "✅ Emotion Hyperparameter Tuning Completed!\n",
      "🏆 Best F1 Score: 0.1263\n",
      "📋 Best Parameters:\n",
      "  learning_rate: 7.3464156009241e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.1684233026512157\n",
      "  weight_decay: 0.04961372443656412\n",
      "  hidden_dropout_prob: 0.12440764696895577\n",
      "  classifier_dropout: 0.19903538202225401\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Hyperparameter Tuning - Emotion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - EMOTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n7️⃣ Hyperparameter Tuning for BERTweet Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for emotion\n",
    "emotion_tuner = FastBERTweetHyperparameterTuner(\n",
    "    model_type=\"emotion\",\n",
    "    data_splits=emotion_data,\n",
    "    n_trials=6,  # Even fewer trials for speed\n",
    "    model_name=model_name,\n",
    "    subset_ratio=0.02,  # Only 2% of data!\n",
    "    max_epochs_per_trial=2  # Only 2 epochs per trial!\n",
    ")\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "emotion_study = emotion_tuner.tune()\n",
    "\n",
    "print(f\"\\n✅ Emotion Hyperparameter Tuning Completed!\")\n",
    "print(f\"🏆 Best F1 Score: {emotion_study.best_value:.4f}\")\n",
    "print(f\"📋 Best Parameters:\")\n",
    "for key, value in emotion_study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af538a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:17:58,915] A new study created in memory with name: no-name-191e82d1-e22d-4f8c-b89e-8bfb5f1277ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - MULTITASK\n",
      "================================================================================\n",
      "\n",
      "8️⃣ Fast Hyperparameter Tuning for BERTweet Multi-task Model...\n",
      "============================================================\n",
      "🚀 Creating ultra-fast tuning setup for multitask...\n",
      "🔪 Creating 2% multitask subset for hyperparameter tuning...\n",
      "📊 Multitask tuning subset created:\n",
      "  Train: 190 samples\n",
      "  Val: 50 samples\n",
      "⚡ Speed optimizations:\n",
      "  - Using 2% of data (190 samples)\n",
      "  - Max 2 epochs per trial\n",
      "  - 6 total trials\n",
      "  - Estimated time: 12-36 minutes\n",
      "\n",
      "🚀 Starting FAST hyperparameter tuning for multitask...\n",
      "⚡ Target: Find good hyperparameters in ~12 minutes\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.4624\n",
      "  Train Sentiment Acc: 0.3895, Train Emotion Acc: 0.1947\n",
      "  Val Loss: 1.1684\n",
      "  Val Sentiment Acc: 0.5000, F1: 0.2840\n",
      "  Val Emotion Acc: 0.2800, F1: 0.1209\n",
      "BERTweet multi-task model saved to ./fast_trial_0\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.3352\n",
      "  Train Sentiment Acc: 0.5368, Train Emotion Acc: 0.2684\n",
      "  Val Loss: 1.1799\n",
      "  Val Sentiment Acc: 0.5400, F1: 0.3300\n",
      "  Val Emotion Acc: 0.3800, F1: 0.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:18:08,103] Trial 0 finished with value: 0.2254345718124386 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'warmup_ratio': 0.15986584841970367, 'weight_decay': 0.02404167763981929, 'hidden_dropout_prob': 0.13119890406724052, 'classifier_dropout': 0.1116167224336399, 'alpha': 0.573235229154987}. Best is trial 0 with value: 0.2254345718124386.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_0\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best Combined F1: 0.2234\n",
      "⚡ Trial 0: Score=0.2254, Time=0.2min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.5681\n",
      "  Train Sentiment Acc: 0.4737, Train Emotion Acc: 0.2053\n",
      "  Val Loss: 1.3258\n",
      "  Val Sentiment Acc: 0.4800, F1: 0.2162\n",
      "  Val Emotion Acc: 0.4000, F1: 0.1348\n",
      "BERTweet multi-task model saved to ./fast_trial_1\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.4310\n",
      "  Train Sentiment Acc: 0.5158, Train Emotion Acc: 0.2737\n",
      "  Val Loss: 1.3500\n",
      "  Val Sentiment Acc: 0.6000, F1: 0.4114\n",
      "  Val Emotion Acc: 0.1200, F1: 0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:18:17,376] Trial 1 finished with value: 0.27311542618770157 and parameters: {'learning_rate': 5.262490902114904e-05, 'batch_size': 16, 'warmup_ratio': 0.19699098521619945, 'weight_decay': 0.08491983767203796, 'hidden_dropout_prob': 0.14246782213565523, 'classifier_dropout': 0.1363649934414201, 'alpha': 0.43668090197068676}. Best is trial 1 with value: 0.27311542618770157.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_1\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best Combined F1: 0.2294\n",
      "⚡ Trial 1: Score=0.2731, Time=0.2min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.5920\n",
      "  Train Sentiment Acc: 0.4368, Train Emotion Acc: 0.2105\n",
      "  Val Loss: 1.2972\n",
      "  Val Sentiment Acc: 0.5000, F1: 0.2840\n",
      "  Val Emotion Acc: 0.4200, F1: 0.0986\n",
      "BERTweet multi-task model saved to ./fast_trial_2\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.4172\n",
      "  Train Sentiment Acc: 0.4316, Train Emotion Acc: 0.3053\n",
      "  Val Loss: 1.3025\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.3125\n",
      "  Val Emotion Acc: 0.3600, F1: 0.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:18:26,501] Trial 2 finished with value: 0.2055279072951143 and parameters: {'learning_rate': 3.2635193912846855e-05, 'batch_size': 16, 'warmup_ratio': 0.1291229140198042, 'weight_decay': 0.06506676052501416, 'hidden_dropout_prob': 0.12789877213040837, 'classifier_dropout': 0.15842892970704364, 'alpha': 0.47327236865873834}. Best is trial 1 with value: 0.27311542618770157.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_2\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best Combined F1: 0.2024\n",
      "⚡ Trial 2: Score=0.2055, Time=0.2min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7382\n",
      "  Train Sentiment Acc: 0.3789, Train Emotion Acc: 0.1789\n",
      "  Val Loss: 1.3078\n",
      "  Val Sentiment Acc: 0.4400, F1: 0.2037\n",
      "  Val Emotion Acc: 0.4200, F1: 0.0986\n",
      "BERTweet multi-task model saved to ./fast_trial_3\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.5113\n",
      "  Train Sentiment Acc: 0.4316, Train Emotion Acc: 0.2737\n",
      "  Val Loss: 1.3222\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.2100\n",
      "  Val Emotion Acc: 0.4200, F1: 0.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:18:35,464] Trial 3 finished with value: 0.15431860569811562 and parameters: {'learning_rate': 4.166863122305896e-05, 'batch_size': 16, 'warmup_ratio': 0.15142344384136117, 'weight_decay': 0.06331731119758383, 'hidden_dropout_prob': 0.10929008254399955, 'classifier_dropout': 0.22150897038028766, 'alpha': 0.4341048247374583}. Best is trial 1 with value: 0.27311542618770157.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_3\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best Combined F1: 0.1543\n",
      "⚡ Trial 3: Score=0.1543, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.5073\n",
      "  Train Sentiment Acc: 0.4211, Train Emotion Acc: 0.2368\n",
      "  Val Loss: 1.2912\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.3126\n",
      "  Val Emotion Acc: 0.4200, F1: 0.0986\n",
      "BERTweet multi-task model saved to ./fast_trial_4\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:18:43,810] Trial 4 finished with value: 0.20557283092494358 and parameters: {'learning_rate': 2.2207471217033647e-05, 'batch_size': 32, 'warmup_ratio': 0.1808397348116461, 'weight_decay': 0.037415239225603365, 'hidden_dropout_prob': 0.11953442280127678, 'classifier_dropout': 0.23684660530243137, 'alpha': 0.48803049874792026}. Best is trial 1 with value: 0.27311542618770157.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.4658\n",
      "  Train Sentiment Acc: 0.4684, Train Emotion Acc: 0.2316\n",
      "  Val Loss: 1.2830\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.2885\n",
      "  Val Emotion Acc: 0.4200, F1: 0.0986\n",
      "\n",
      "✅ BERTweet training completed! Best Combined F1: 0.2056\n",
      "⚡ Trial 4: Score=0.2056, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "🚀 Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.5163\n",
      "  Train Sentiment Acc: 0.4211, Train Emotion Acc: 0.1895\n",
      "  Val Loss: 1.3497\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.2100\n",
      "  Val Emotion Acc: 0.2600, F1: 0.1061\n",
      "BERTweet multi-task model saved to ./fast_trial_5\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_5\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.4312\n",
      "  Train Sentiment Acc: 0.4421, Train Emotion Acc: 0.2158\n",
      "  Val Loss: 1.2725\n",
      "  Val Sentiment Acc: 0.5400, F1: 0.3482\n",
      "  Val Emotion Acc: 0.2800, F1: 0.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-18 10:18:52,754] Trial 5 finished with value: 0.2295402560559429 and parameters: {'learning_rate': 2.4340587767477566e-05, 'batch_size': 16, 'warmup_ratio': 0.1909320402078782, 'weight_decay': 0.03329019834400153, 'hidden_dropout_prob': 0.23250445687079638, 'classifier_dropout': 0.1623422152178822, 'alpha': 0.5040136042355622}. Best is trial 1 with value: 0.27311542618770157.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_5\\model_best\n",
      "💾 Best BERTweet model saved to ./fast_trial_5\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best Combined F1: 0.2295\n",
      "⚡ Trial 5: Score=0.2295, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "🏆 Fast tuning completed in 0.9 minutes!\n",
      "🎯 Best score: 0.2731\n",
      "📋 Best parameters:\n",
      "  learning_rate: 5.262490902114904e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.19699098521619945\n",
      "  weight_decay: 0.08491983767203796\n",
      "  hidden_dropout_prob: 0.14246782213565523\n",
      "  classifier_dropout: 0.1363649934414201\n",
      "  alpha: 0.43668090197068676\n",
      "\n",
      "✅ Multitask Hyperparameter Tuning Completed!\n",
      "🏆 Best Combined F1 Score: 0.2731\n",
      "📋 Best Parameters:\n",
      "  learning_rate: 5.262490902114904e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.19699098521619945\n",
      "  weight_decay: 0.08491983767203796\n",
      "  hidden_dropout_prob: 0.14246782213565523\n",
      "  classifier_dropout: 0.1363649934414201\n",
      "  alpha: 0.43668090197068676\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Ultra-Fast Hyperparameter Tuning - Multitask (Updated)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - MULTITASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n8️⃣ Fast Hyperparameter Tuning for BERTweet Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create FAST tuner for multitask (using the new FastBERTweetHyperparameterTuner)\n",
    "multitask_tuner = FastBERTweetHyperparameterTuner(\n",
    "    model_type=\"multitask\",\n",
    "    data_splits=multitask_data,\n",
    "    n_trials=6,  # Reduced trials for speed\n",
    "    model_name=model_name,\n",
    "    subset_ratio=0.02,  # Only 2% of data!\n",
    "    max_epochs_per_trial=2  # Only 2 epochs per trial!\n",
    ")\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "multitask_study = multitask_tuner.tune()\n",
    "\n",
    "print(f\"\\n✅ Multitask Hyperparameter Tuning Completed!\")\n",
    "print(f\"🏆 Best Combined F1 Score: {multitask_study.best_value:.4f}\")\n",
    "print(f\"📋 Best Parameters:\")\n",
    "for key, value in multitask_study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6388d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "9️⃣ Training Final BERTweet Sentiment Model with Best Parameters...\n",
      "============================================================\n",
      "🎯 Using best hyperparameters:\n",
      "  learning_rate: 3.65445235521325e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.15986584841970367\n",
      "  weight_decay: 0.02404167763981929\n",
      "  hidden_dropout_prob: 0.13119890406724052\n",
      "  classifier_dropout: 0.1116167224336399\n",
      "\n",
      "🚀 Training final sentiment model:\n",
      "  Dataset: Full sentiment data (47144 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 16\n",
      "  Learning rate: 3.65e-05\n",
      "  Max length: 128\n",
      "🚀 Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "  Train Loss: 0.5627, Train Acc: 0.7852\n",
      "  Val Loss: 0.4454, Val Acc: 0.8551, Val F1: 0.5961\n",
      "BERTweet single-task model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "💾 Best BERTweet model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 2/5\n",
      "  Train Loss: 0.4115, Train Acc: 0.8667\n",
      "  Val Loss: 0.4050, Val Acc: 0.8718, Val F1: 0.6076\n",
      "BERTweet single-task model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "💾 Best BERTweet model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 3/5\n",
      "  Train Loss: 0.3509, Train Acc: 0.8893\n",
      "  Val Loss: 0.4888, Val Acc: 0.8688, Val F1: 0.6051\n",
      "\n",
      "📍 Epoch 4/5\n",
      "  Train Loss: 0.3135, Train Acc: 0.8991\n",
      "  Val Loss: 0.4583, Val Acc: 0.8741, Val F1: 0.6091\n",
      "BERTweet single-task model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "💾 Best BERTweet model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 5/5\n",
      "  Train Loss: 0.2828, Train Acc: 0.9065\n",
      "  Val Loss: 0.4912, Val Acc: 0.8752, Val F1: 0.6100\n",
      "BERTweet single-task model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "💾 Best BERTweet model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.6100\n",
      "📊 Evaluating BERTweet sentiment model...\n",
      "📊 BERTweet Sentiment Results:\n",
      "  Accuracy: 0.8708\n",
      "  F1 Macro: 0.6070\n",
      "\n",
      "✅ Final Sentiment Model Results:\n",
      "  Accuracy: 0.8708\n",
      "  F1 Macro: 0.6070\n",
      "\n",
      "📊 Comparison:\n",
      "  Tuning F1 (on subset): 0.5619\n",
      "  Final F1 (on full test): 0.6070\n",
      "🧹 Memory cleaned!\n",
      "💾 Final sentiment model saved to: ./final_bertweet_sentiment_model/\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Final Sentiment Model Training with Best Parameters (Fixed)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n9️⃣ Training Final BERTweet Sentiment Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from sentiment tuning\n",
    "best_sentiment_params = sentiment_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_sentiment_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training (full dataset, more epochs)\n",
    "final_sentiment_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_sentiment_params['learning_rate'],\n",
    "    batch_size=best_sentiment_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_sentiment_params['warmup_ratio'],\n",
    "    weight_decay=best_sentiment_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_sentiment_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_sentiment_params['classifier_dropout'],\n",
    "    max_length=best_sentiment_params.get('max_length', 128),  # Fixed: use .get() with default\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./final_bertweet_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final sentiment model:\")\n",
    "print(f\"  Dataset: Full sentiment data ({len(sentiment_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_sentiment_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_sentiment_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_sentiment_config.learning_rate:.2e}\")\n",
    "print(f\"  Max length: {final_sentiment_config.max_length}\")\n",
    "\n",
    "# Train final sentiment model\n",
    "final_sentiment_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=final_sentiment_config,\n",
    "    num_classes=bertweet_model_config.sentiment_num_classes\n",
    ")\n",
    "final_sentiment_history = final_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate final sentiment model\n",
    "final_sentiment_results = evaluate_bertweet_model(\n",
    "    model_path=\"./final_bertweet_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final Sentiment Model Results:\")\n",
    "print(f\"  Accuracy: {final_sentiment_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Compare with tuning results\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"  Tuning F1 (on subset): {sentiment_study.best_value:.4f}\")\n",
    "print(f\"  Final F1 (on full test): {final_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n",
    "print(f\"💾 Final sentiment model saved to: ./final_bertweet_sentiment_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272960ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "🔟 Training Final BERTweet Emotion Model with Best Parameters...\n",
      "============================================================\n",
      "🎯 Using best hyperparameters:\n",
      "  learning_rate: 7.3464156009241e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.1684233026512157\n",
      "  weight_decay: 0.04961372443656412\n",
      "  hidden_dropout_prob: 0.12440764696895577\n",
      "  classifier_dropout: 0.19903538202225401\n",
      "\n",
      "🚀 Training final emotion model:\n",
      "  Dataset: Full emotion data (9534 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 16\n",
      "  Learning rate: 7.35e-05\n",
      "  Max length: 128\n",
      "🚀 Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "  Train Loss: 1.1094, Train Acc: 0.5766\n",
      "  Val Loss: 0.7948, Val Acc: 0.7195, Val F1: 0.6911\n",
      "BERTweet single-task model saved to ./final_bertweet_emotion_model\\model_best\n",
      "💾 Best BERTweet model saved to ./final_bertweet_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 2/5\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Final Emotion Model Training with Best Parameters (Fixed)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🔟 Training Final BERTweet Emotion Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from emotion tuning\n",
    "best_emotion_params = emotion_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_emotion_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training (full dataset, more epochs)\n",
    "final_emotion_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_emotion_params['learning_rate'],\n",
    "    batch_size=best_emotion_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_emotion_params['warmup_ratio'],\n",
    "    weight_decay=best_emotion_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_emotion_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_emotion_params['classifier_dropout'],\n",
    "    max_length=best_emotion_params.get('max_length', 128),  # Fixed: use .get() with default\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./final_bertweet_emotion_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final emotion model:\")\n",
    "print(f\"  Dataset: Full emotion data ({len(emotion_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_emotion_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_emotion_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_emotion_config.learning_rate:.2e}\")\n",
    "print(f\"  Max length: {final_emotion_config.max_length}\")\n",
    "\n",
    "# Train final emotion model\n",
    "final_emotion_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=final_emotion_config,\n",
    "    num_classes=bertweet_model_config.emotion_num_classes\n",
    ")\n",
    "final_emotion_history = final_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate final emotion model\n",
    "final_emotion_results = evaluate_bertweet_model(\n",
    "    model_path=\"./final_bertweet_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final Emotion Model Results:\")\n",
    "print(f\"  Accuracy: {final_emotion_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {final_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Compare with tuning results\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"  Tuning F1 (on subset): {emotion_study.best_value:.4f}\")\n",
    "print(f\"  Final F1 (on full test): {final_emotion_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n",
    "print(f\"💾 Final emotion model saved to: ./final_bertweet_emotion_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Final Multitask Model Training with Best Parameters (Fixed)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1️⃣1️⃣ Training Final BERTweet Multi-task Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from multitask tuning\n",
    "best_multitask_params = multitask_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_multitask_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training (full dataset, more epochs)\n",
    "final_multitask_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_multitask_params['learning_rate'],\n",
    "    batch_size=best_multitask_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_multitask_params['warmup_ratio'],\n",
    "    weight_decay=best_multitask_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_multitask_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_multitask_params['classifier_dropout'],\n",
    "    max_length=best_multitask_params.get('max_length', 128),  # Fixed: use .get() with default\n",
    "    alpha=best_multitask_params['alpha'],  # Multitask-specific parameter\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./final_bertweet_multitask_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final multitask model:\")\n",
    "print(f\"  Dataset: Full multitask data ({len(multitask_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_multitask_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_multitask_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_multitask_config.learning_rate:.2e}\")\n",
    "print(f\"  Max length: {final_multitask_config.max_length}\")\n",
    "print(f\"  Alpha (loss weighting): {final_multitask_config.alpha:.3f}\")\n",
    "\n",
    "# Train final multitask model\n",
    "final_multitask_trainer = BERTweetMultiTaskTrainer(config=final_multitask_config)\n",
    "final_multitask_history = final_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate final multitask model\n",
    "final_multitask_results = evaluate_bertweet_model(\n",
    "    model_path=\"./final_bertweet_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final Multitask Model Results:\")\n",
    "print(f\"  Sentiment - Accuracy: {final_multitask_results['sentiment_accuracy']:.4f}, F1: {final_multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"  Emotion - Accuracy: {final_multitask_results['emotion_accuracy']:.4f}, F1: {final_multitask_results['emotion_f1_macro']:.4f}\")\n",
    "print(f\"  Combined - Accuracy: {final_multitask_results['combined_accuracy']:.4f}, F1: {final_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Compare with tuning results\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"  Tuning Combined F1 (on subset): {multitask_study.best_value:.4f}\")\n",
    "print(f\"  Final Combined F1 (on full test): {final_multitask_results['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n",
    "print(f\"💾 Final multitask model saved to: ./final_bertweet_multitask_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Final Results Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 4: COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison\n",
    "create_comprehensive_bertweet_results_comparison(all_results)\n",
    "\n",
    "# Save all results\n",
    "results_summary = {\n",
    "    'model_type': 'BERTweet',\n",
    "    'model_name': model_name,\n",
    "    'initial_models': {\n",
    "        'sentiment': all_results['initial_sentiment'],\n",
    "        'emotion': all_results['initial_emotion'],\n",
    "        'multitask': all_results['initial_multitask']\n",
    "    },\n",
    "    'optimized_models': {\n",
    "        'sentiment': all_results['optimized_sentiment'],\n",
    "        'emotion': all_results['optimized_emotion'],\n",
    "        'multitask': all_results['optimized_multitask']\n",
    "    },\n",
    "    'hyperparameter_studies': {\n",
    "        'sentiment': sentiment_study.best_params,\n",
    "        'emotion': emotion_study.best_params,\n",
    "        'multitask': multitask_study.best_params\n",
    "    },\n",
    "    'improvements': {\n",
    "        'sentiment': {\n",
    "            'accuracy_improvement': all_results['optimized_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy'],\n",
    "            'f1_improvement': all_results['optimized_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro']\n",
    "        },\n",
    "        'emotion': {\n",
    "            'accuracy_improvement': all_results['optimized_emotion']['accuracy'] - all_results['initial_emotion']['accuracy'],\n",
    "            'f1_improvement': all_results['optimized_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro']\n",
    "        },\n",
    "        'multitask': {\n",
    "            'sentiment_accuracy_improvement': all_results['optimized_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy'],\n",
    "            'emotion_accuracy_improvement': all_results['optimized_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy'],\n",
    "            'combined_f1_improvement': all_results['optimized_multitask']['combined_f1_macro'] - all_results['initial_multitask']['combined_f1_macro']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open('comprehensive_bertweet_results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n🎉 COMPLETE BERTWEET PIPELINE FINISHED!\")\n",
    "print(f\"📁 Results saved to: comprehensive_bertweet_results_summary.json\")\n",
    "print(f\"📁 Initial models saved to: ./initial_bertweet_*_model/\")\n",
    "print(f\"📁 Optimized models saved to: ./final_bertweet_*_model/\")\n",
    "print(f\"🐦 BERTweet models are optimized for social media text processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84e0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
