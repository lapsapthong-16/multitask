{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to env (Python 3.12.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b749f905-82cd-4417-94dd-a94bd46e1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "CUDA Version: 12.1\n",
      "✅ BERTweet Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Memory management\n",
    "def aggressive_memory_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"🧹 Memory cleaned!\")\n",
    "\n",
    "print(\"✅ BERTweet Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380f59cf-2042-4b98-b16e-84c427d80baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet Configuration classes defined!\n"
     ]
    }
   ],
   "source": [
    "class TrainingConfig:    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",  \n",
    "        max_length: int = 128,\n",
    "        batch_size: int = 8,\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 3,\n",
    "        warmup_ratio: float = 0.1,\n",
    "        weight_decay: float = 0.01,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1,\n",
    "        output_dir: str = \"./bertweet_model_output\",\n",
    "        save_total_limit: int = 1,\n",
    "        # Multi-task specific\n",
    "        alpha: float = 0.5,  # Only used for multi-task\n",
    "        task_type: str = \"multitask\"  # \"sentiment\", \"emotion\", \"multitask\"\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_dropout_prob = attention_dropout_prob\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.output_dir = output_dir\n",
    "        self.save_total_limit = save_total_limit\n",
    "        self.alpha = alpha\n",
    "        self.task_type = task_type\n",
    "\n",
    "class BERTweetModelConfig:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
    "        self.emotion_classes = ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
    "        self.sentiment_num_classes = len(self.sentiment_classes)\n",
    "        self.emotion_num_classes = len(self.emotion_classes)\n",
    "\n",
    "bertweet_model_config = BERTweetModelConfig()\n",
    "print(\"BERTweet Configuration classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e66115-b043-4dee-9ccd-1008c6eaad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet Dataset classes defined!\n"
     ]
    }
   ],
   "source": [
    "class BERTweetSingleTaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # BERTweet specific preprocessing (handles tweets better)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "class BERTweetMultiTaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        sentiment_labels: List[int],\n",
    "        emotion_labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.emotion_labels = emotion_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(texts) == len(sentiment_labels) == len(emotion_labels), \\\n",
    "            \"All inputs must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        sentiment_label = self.sentiment_labels[idx]\n",
    "        emotion_label = self.emotion_labels[idx]\n",
    "        \n",
    "        # BERTweet specific preprocessing\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_labels': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'emotion_labels': torch.tensor(emotion_label, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "print(\"BERTweet Dataset classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21a49ce-ee6d-4031-ac54-a279a9830a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet Model architectures defined!\n"
     ]
    }
   ],
   "source": [
    "class BERTweetSingleTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        num_classes: int = 3,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetSingleTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Load BERTweet configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # BERTweet encoder\n",
    "        self.encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Classification head optimized for BERTweet\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),  # BERTweet uses GELU activation\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERTweet encoder output\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        pooled_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "    \n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"model_type\": \"BERTweetSingleTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"BERTweet single-task model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            num_classes=config[\"num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "class BERTweetMultiTaskTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        sentiment_num_classes: int = 3,\n",
    "        emotion_num_classes: int = 6,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "        attention_dropout_prob: float = 0.1,\n",
    "        classifier_dropout: float = 0.1\n",
    "    ):\n",
    "        super(BERTweetMultiTaskTransformer, self).__init__()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.sentiment_num_classes = sentiment_num_classes\n",
    "        self.emotion_num_classes = emotion_num_classes\n",
    "        \n",
    "        # Load BERTweet configuration\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.hidden_dropout_prob = hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = attention_dropout_prob\n",
    "        \n",
    "        # Shared BERTweet encoder\n",
    "        self.shared_encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "        \n",
    "        hidden_size = self.shared_encoder.config.hidden_size\n",
    "        \n",
    "        # Task-specific attention layers for BERTweet\n",
    "        self.sentiment_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # BERTweet-base has 12 attention heads\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.emotion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=12,  # BERTweet-base has 12 attention heads\n",
    "            dropout=attention_dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.sentiment_norm = nn.LayerNorm(hidden_size)\n",
    "        self.emotion_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Classification heads optimized for BERTweet\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, sentiment_num_classes)\n",
    "        )\n",
    "        \n",
    "        self.emotion_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(hidden_size // 2, emotion_num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.sentiment_classifier, self.emotion_classifier]:\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Shared BERTweet encoder\n",
    "        encoder_outputs = self.shared_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Task-specific attention\n",
    "        sentiment_attended, _ = self.sentiment_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        sentiment_attended = self.sentiment_norm(sentiment_attended + sequence_output)\n",
    "        \n",
    "        emotion_attended, _ = self.emotion_attention(\n",
    "            sequence_output, sequence_output, sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool()\n",
    "        )\n",
    "        emotion_attended = self.emotion_norm(emotion_attended + sequence_output)\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        sentiment_pooled = sentiment_attended[:, 0, :]\n",
    "        emotion_pooled = emotion_attended[:, 0, :]\n",
    "        \n",
    "        # Classification\n",
    "        sentiment_logits = self.sentiment_classifier(sentiment_pooled)\n",
    "        emotion_logits = self.emotion_classifier(emotion_pooled)\n",
    "        \n",
    "        return {\n",
    "            'sentiment_logits': sentiment_logits,\n",
    "            'emotion_logits': emotion_logits\n",
    "        }\n",
    "    \n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        # Save model state dict\n",
    "        model_path = os.path.join(save_directory, \"pytorch_model.bin\")\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"sentiment_num_classes\": self.sentiment_num_classes,\n",
    "            \"emotion_num_classes\": self.emotion_num_classes,\n",
    "            \"model_type\": \"BERTweetMultiTaskTransformer\"\n",
    "        }\n",
    "        config_path = os.path.join(save_directory, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"BERTweet multi-task model saved to {save_directory}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: str, **kwargs):\n",
    "        # Load config\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = cls(\n",
    "            model_name=config[\"model_name\"],\n",
    "            sentiment_num_classes=config[\"sentiment_num_classes\"],\n",
    "            emotion_num_classes=config[\"emotion_num_classes\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Load state dict\n",
    "        model_file = os.path.join(model_path, \"pytorch_model.bin\")\n",
    "        state_dict = torch.load(model_file, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "        \n",
    "        return model\n",
    "\n",
    "print(\"BERTweet Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a04cfb8-b34d-49b5-9b1d-a294b60d9d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet Data processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "def load_and_process_datasets_bertweet():\n",
    "    \n",
    "    print(\"Loading datasets for BERTweet...\")\n",
    "    \n",
    "    # Load SST-2 for sentiment\n",
    "    try:\n",
    "        sst2_dataset = load_dataset(\"sst2\")\n",
    "        print(f\"✅ SST-2 loaded: {len(sst2_dataset['train'])} train, {len(sst2_dataset['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load SST-2: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load GoEmotion for emotion\n",
    "    try:\n",
    "        emotion_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        print(f\"✅ GoEmotion loaded: {len(emotion_dataset['train'])} train, {len(emotion_dataset['validation'])} val\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load GoEmotion: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Try to load existing encoders first\n",
    "    sentiment_encoder, emotion_encoder = load_existing_encoders_bertweet()\n",
    "    \n",
    "    # Process sentiment data (SST-2) for BERTweet\n",
    "    sentiment_data = process_sentiment_data_bertweet(sst2_dataset, sentiment_encoder)\n",
    "    \n",
    "    # Process emotion data (GoEmotion) for BERTweet\n",
    "    emotion_data = process_emotion_data_bertweet(emotion_dataset, emotion_encoder)\n",
    "    \n",
    "    return sentiment_data, emotion_data\n",
    "\n",
    "def load_existing_encoders_bertweet():        \n",
    "    try:\n",
    "        sentiment_encoder = joblib.load('enc/sentiment_label_encoder.pkl')\n",
    "        emotion_encoder = joblib.load('enc/emotion_label_encoder.pkl')\n",
    "        print(\"✅ Loaded existing encoders from enc/ directory for BERTweet\")\n",
    "        return sentiment_encoder, emotion_encoder\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load existing encoders: {e}\")\n",
    "        print(\"Creating new encoders for BERTweet...\")\n",
    "        \n",
    "        # Create new encoders\n",
    "        sentiment_encoder = LabelEncoder()\n",
    "        emotion_encoder = LabelEncoder()\n",
    "        sentiment_encoder.classes_ = np.array(bertweet_model_config.sentiment_classes)\n",
    "        emotion_encoder.classes_ = np.array(bertweet_model_config.emotion_classes)\n",
    "        \n",
    "        # Save new encoders\n",
    "        os.makedirs('enc', exist_ok=True)\n",
    "        joblib.dump(sentiment_encoder, 'enc/bertweet_sentiment_label_encoder.pkl')\n",
    "        joblib.dump(emotion_encoder, 'enc/bertweet_emotion_label_encoder.pkl')\n",
    "        print(\"✅ Created and saved new BERTweet encoders\")\n",
    "        \n",
    "        return sentiment_encoder, emotion_encoder\n",
    "\n",
    "def process_sentiment_data_bertweet(sst2_dataset, sentiment_encoder, max_samples=None):    \n",
    "    print(\"Processing sentiment data for BERTweet...\")\n",
    "    \n",
    "    # Use full dataset if max_samples is None\n",
    "    if max_samples is None:\n",
    "        max_samples = len(sst2_dataset['train'])\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = sst2_dataset['train']['sentence'][:max_samples]\n",
    "    train_labels = sst2_dataset['train']['label'][:max_samples]\n",
    "    \n",
    "    val_texts = sst2_dataset['validation']['sentence']\n",
    "    val_labels = sst2_dataset['validation']['label']\n",
    "    \n",
    "    # Map SST-2 labels to 3 classes: 0->Negative, 1->Positive\n",
    "    # Add some neutral examples by random assignment\n",
    "    expanded_labels = []\n",
    "    expanded_texts = []\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        if label == 0:  # Negative\n",
    "            expanded_labels.append(0)\n",
    "            expanded_texts.append(text)\n",
    "        elif label == 1:  # Positive\n",
    "            # Sometimes assign as positive, sometimes as neutral\n",
    "            if np.random.random() < 0.15:  # 15% chance to be neutral\n",
    "                expanded_labels.append(1)  # Neutral\n",
    "            else:\n",
    "                expanded_labels.append(2)  # Positive\n",
    "            expanded_texts.append(text)\n",
    "    \n",
    "    # Ensure we have all 3 classes\n",
    "    if 1 not in expanded_labels:\n",
    "        # Force some examples to be neutral\n",
    "        neutral_indices = np.random.choice(len(expanded_labels), size=100, replace=False)\n",
    "        for idx in neutral_indices:\n",
    "            expanded_labels[idx] = 1\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        expanded_texts, expanded_labels, test_size=0.3, random_state=42, stratify=expanded_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    sentiment_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels},\n",
    "        'encoder': sentiment_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"BERTweet Sentiment data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return sentiment_data\n",
    "\n",
    "def process_emotion_data_bertweet(emotion_dataset, emotion_encoder, max_samples=None):\n",
    "    \n",
    "    print(\"Processing emotion data for BERTweet...\")\n",
    "    \n",
    "    # Filter to first 6 emotions only\n",
    "    def filter_emotions(example):\n",
    "        if isinstance(example['labels'], list):\n",
    "            return example['labels'] and example['labels'][0] in range(6)\n",
    "        else:\n",
    "            return example['labels'] in range(6)\n",
    "    \n",
    "    filtered_train = emotion_dataset['train'].filter(filter_emotions)\n",
    "    filtered_val = emotion_dataset['validation'].filter(filter_emotions)\n",
    "    \n",
    "    # Use full dataset if max_samples is None\n",
    "    if max_samples is None:\n",
    "        max_samples = len(filtered_train)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    train_texts = filtered_train['text'][:max_samples]\n",
    "    train_labels_raw = filtered_train['labels'][:max_samples]\n",
    "    \n",
    "    val_texts = filtered_val['text']\n",
    "    val_labels_raw = filtered_val['labels']\n",
    "    \n",
    "    # Handle multi-label to single-label conversion\n",
    "    train_labels = []\n",
    "    for label in train_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            train_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            train_labels.append(label)\n",
    "    \n",
    "    val_labels = []\n",
    "    for label in val_labels_raw:\n",
    "        if isinstance(label, list):\n",
    "            val_labels.append(label[0] if label else 0)\n",
    "        else:\n",
    "            val_labels.append(label)\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.3, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "        temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    emotion_data = {\n",
    "        'train': {'texts': train_texts, 'labels': train_labels},\n",
    "        'val': {'texts': val_texts, 'labels': val_labels},\n",
    "        'test': {'texts': test_texts, 'labels': test_labels},\n",
    "        'encoder': emotion_encoder\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ BERTweet Emotion data processed:\")\n",
    "    print(f\"  Train: {len(train_texts)} samples\")\n",
    "    print(f\"  Val: {len(val_texts)} samples\")\n",
    "    print(f\"  Test: {len(test_texts)} samples\")\n",
    "    \n",
    "    return emotion_data\n",
    "\n",
    "def create_multitask_data_bertweet(sentiment_data, emotion_data):\n",
    "    \n",
    "    print(\"Creating multi-task dataset for BERTweet...\")\n",
    "    \n",
    "    # Take minimum length to balance datasets\n",
    "    min_train_len = min(len(sentiment_data['train']['texts']), len(emotion_data['train']['texts']))\n",
    "    min_val_len = min(len(sentiment_data['val']['texts']), len(emotion_data['val']['texts']))\n",
    "    min_test_len = min(len(sentiment_data['test']['texts']), len(emotion_data['test']['texts']))\n",
    "    \n",
    "    multitask_data = {\n",
    "        'train': {\n",
    "            'texts': sentiment_data['train']['texts'][:min_train_len],\n",
    "            'sentiment_labels': sentiment_data['train']['labels'][:min_train_len],\n",
    "            'emotion_labels': emotion_data['train']['labels'][:min_train_len]\n",
    "        },\n",
    "        'val': {\n",
    "            'texts': sentiment_data['val']['texts'][:min_val_len],\n",
    "            'sentiment_labels': sentiment_data['val']['labels'][:min_val_len],\n",
    "            'emotion_labels': emotion_data['val']['labels'][:min_val_len]\n",
    "        },\n",
    "        'test': {\n",
    "            'texts': sentiment_data['test']['texts'][:min_test_len],\n",
    "            'sentiment_labels': sentiment_data['test']['labels'][:min_test_len],\n",
    "            'emotion_labels': emotion_data['test']['labels'][:min_test_len]\n",
    "        },\n",
    "        'sentiment_encoder': sentiment_data['encoder'],\n",
    "        'emotion_encoder': emotion_data['encoder']\n",
    "    }\n",
    "    \n",
    "    print(f\"BERTweet Multi-task data created:\")\n",
    "    print(f\"  Train: {len(multitask_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(multitask_data['val']['texts'])} samples\")\n",
    "    print(f\"  Test: {len(multitask_data['test']['texts'])} samples\")\n",
    "    \n",
    "    return multitask_data\n",
    "\n",
    "def load_reddit_evaluation_data():\n",
    "    \"\"\"Load Reddit data for evaluation\"\"\"\n",
    "    print(\"Loading Reddit evaluation data...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the annotated Reddit posts\n",
    "        df = pd.read_csv('annotated_reddit_posts.csv')\n",
    "        print(f\"✅ Reddit data loaded: {len(df)} samples\")\n",
    "        \n",
    "        # Create label encoders that match the model classes\n",
    "        sentiment_encoder = LabelEncoder()\n",
    "        emotion_encoder = LabelEncoder()\n",
    "        \n",
    "        # Fit encoders on Reddit data\n",
    "        sentiment_encoder.fit(df['sentiment'].tolist())\n",
    "        emotion_encoder.fit(df['emotion'].tolist())\n",
    "        \n",
    "        # Transform labels\n",
    "        sentiment_labels = sentiment_encoder.transform(df['sentiment'].tolist())\n",
    "        emotion_labels = emotion_encoder.transform(df['emotion'].tolist())\n",
    "        \n",
    "        # Create Reddit data in the format expected by evaluation functions\n",
    "        reddit_data = {\n",
    "            # For single-task sentiment evaluation\n",
    "            'sentiment': {\n",
    "                'texts': df['text_content'].tolist(),\n",
    "                'labels': sentiment_labels,\n",
    "                'labels_text': df['sentiment'].tolist()\n",
    "            },\n",
    "            # For single-task emotion evaluation\n",
    "            'emotion': {\n",
    "                'texts': df['text_content'].tolist(),\n",
    "                'labels': emotion_labels,\n",
    "                'labels_text': df['emotion'].tolist()\n",
    "            },\n",
    "            # For multitask evaluation\n",
    "            'multitask': {\n",
    "                'texts': df['text_content'].tolist(),\n",
    "                'sentiment_labels': sentiment_labels,\n",
    "                'emotion_labels': emotion_labels,\n",
    "                'sentiment_labels_text': df['sentiment'].tolist(),\n",
    "                'emotion_labels_text': df['emotion'].tolist()\n",
    "            },\n",
    "            # Keep encoders for reference\n",
    "            'sentiment_encoder': sentiment_encoder,\n",
    "            'emotion_encoder': emotion_encoder\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Reddit data prepared: {len(reddit_data['sentiment']['texts'])} samples\")\n",
    "        print(f\"   Sentiment classes: {list(sentiment_encoder.classes_)}\")\n",
    "        print(f\"   Emotion classes: {list(emotion_encoder.classes_)}\")\n",
    "        \n",
    "        return reddit_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Reddit data: {e}\")\n",
    "        print(\"Falling back to empty Reddit data\")\n",
    "        return None\n",
    "\n",
    "print(\"BERTweet Data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42e58cb-5f38-40a6-999c-a1fbdbe1f590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet Training classes defined!\n"
     ]
    }
   ],
   "source": [
    "class BERTweetSingleTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, num_classes: int):\n",
    "        self.config = config\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize BERTweet tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize BERTweet model\n",
    "        self.model = BERTweetSingleTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            labels=data_splits['train']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            labels=data_splits['val']['labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for BERTweet\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            eps=1e-6  # BERTweet specific epsilon\n",
    "        )\n",
    "        \n",
    "        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.loss_fn(outputs['logits'], labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.loss_fn(outputs['logits'], labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting BERTweet single-task training ({self.config.task_type})...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_accuracy'].append(train_accuracy)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_accuracy'].append(val_accuracy)\n",
    "            self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_f1_macro > best_f1:\n",
    "                best_f1 = val_f1_macro\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\n✅ BERTweet training completed! Best F1: {best_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best BERTweet model saved to {model_dir}\")\n",
    "\n",
    "class BERTweetMultiTaskTrainer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize BERTweet tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Initialize BERTweet multi-task model\n",
    "        self.model = BERTweetMultiTaskTransformer(\n",
    "            model_name=config.model_name,\n",
    "            sentiment_num_classes=bertweet_model_config.sentiment_num_classes,\n",
    "            emotion_num_classes=bertweet_model_config.emotion_num_classes,\n",
    "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "            attention_dropout_prob=config.attention_dropout_prob,\n",
    "            classifier_dropout=config.classifier_dropout\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Initialize tracking\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'train_sentiment_accuracy': [],\n",
    "            'train_emotion_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_sentiment_accuracy': [],\n",
    "            'val_emotion_accuracy': [],\n",
    "            'val_sentiment_f1_macro': [],\n",
    "            'val_emotion_f1_macro': []\n",
    "        }\n",
    "    \n",
    "    def create_data_loaders(self, data_splits: Dict):\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=data_splits['train']['texts'],\n",
    "            sentiment_labels=data_splits['train']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['train']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=data_splits['val']['texts'],\n",
    "            sentiment_labels=data_splits['val']['sentiment_labels'],\n",
    "            emotion_labels=data_splits['val']['emotion_labels'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Setup optimizer and scheduler for BERTweet\n",
    "        num_training_steps = len(self.train_loader) * self.config.num_epochs\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            eps=1e-6  # BERTweet specific epsilon\n",
    "        )\n",
    "        \n",
    "        num_warmup_steps = int(num_training_steps * self.config.warmup_ratio)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_correct = 0\n",
    "        emotion_correct = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            sentiment_labels = batch['sentiment_labels'].to(self.device)\n",
    "            emotion_labels = batch['emotion_labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Calculate losses\n",
    "            sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_labels)\n",
    "            emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_labels)\n",
    "            \n",
    "            # Combined loss with alpha weighting\n",
    "            loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "            emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "            \n",
    "            sentiment_correct += (sentiment_preds == sentiment_labels).sum().item()\n",
    "            emotion_correct += (emotion_preds == emotion_labels).sum().item()\n",
    "            total_predictions += sentiment_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        sentiment_accuracy = sentiment_correct / total_predictions\n",
    "        emotion_accuracy = emotion_correct / total_predictions\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        sentiment_predictions = []\n",
    "        emotion_predictions = []\n",
    "        sentiment_labels = []\n",
    "        emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                sentiment_true = batch['sentiment_labels'].to(self.device)\n",
    "                emotion_true = batch['emotion_labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                sentiment_loss = self.loss_fn(outputs['sentiment_logits'], sentiment_true)\n",
    "                emotion_loss = self.loss_fn(outputs['emotion_logits'], emotion_true)\n",
    "                loss = self.config.alpha * sentiment_loss + (1 - self.config.alpha) * emotion_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                sentiment_labels.extend(sentiment_true.cpu().numpy())\n",
    "                emotion_labels.extend(emotion_true.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(sentiment_labels, sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(emotion_labels, emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(sentiment_labels, sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(emotion_labels, emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        return avg_loss, sentiment_accuracy, emotion_accuracy, sentiment_f1_macro, emotion_f1_macro\n",
    "    \n",
    "    def train(self, data_splits: Dict):\n",
    "        print(f\"Starting BERTweet multi-task training...\")\n",
    "        \n",
    "        # Setup data loaders\n",
    "        self.create_data_loaders(data_splits)\n",
    "        \n",
    "        best_combined_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_sent_acc, train_emo_acc = self.train_epoch()\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss, val_sent_acc, val_emo_acc, val_sent_f1, val_emo_f1 = self.evaluate()\n",
    "            \n",
    "            # Track metrics\n",
    "            self.training_history['train_loss'].append(train_loss)\n",
    "            self.training_history['train_sentiment_accuracy'].append(train_sent_acc)\n",
    "            self.training_history['train_emotion_accuracy'].append(train_emo_acc)\n",
    "            self.training_history['val_loss'].append(val_loss)\n",
    "            self.training_history['val_sentiment_accuracy'].append(val_sent_acc)\n",
    "            self.training_history['val_emotion_accuracy'].append(val_emo_acc)\n",
    "            self.training_history['val_sentiment_f1_macro'].append(val_sent_f1)\n",
    "            self.training_history['val_emotion_f1_macro'].append(val_emo_f1)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Train Sentiment Acc: {train_sent_acc:.4f}, Train Emotion Acc: {train_emo_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val Sentiment Acc: {val_sent_acc:.4f}, F1: {val_sent_f1:.4f}\")\n",
    "            print(f\"  Val Emotion Acc: {val_emo_acc:.4f}, F1: {val_emo_f1:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            combined_f1 = (val_sent_f1 + val_emo_f1) / 2\n",
    "            if combined_f1 > best_combined_f1:\n",
    "                best_combined_f1 = combined_f1\n",
    "                self.save_model(is_best=True)\n",
    "        \n",
    "        print(f\"\\nBERTweet training completed! Best Combined F1: {best_combined_f1:.4f}\")\n",
    "        return self.training_history\n",
    "    \n",
    "    def save_model(self, is_best=False):\n",
    "        suffix = \"_best\" if is_best else \"\"\n",
    "        model_dir = os.path.join(self.config.output_dir, f\"model{suffix}\")\n",
    "        \n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        self.model.save_pretrained(model_dir)\n",
    "        self.tokenizer.save_pretrained(model_dir)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"Best BERTweet model saved to {model_dir}\")\n",
    "\n",
    "print(\"BERTweet Training classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01119cb4-16a3-4b89-94b0-eeb0b9b4da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_pruning(self, data_splits: Dict, trial=None):\n",
    "    print(f\"Starting BERTweet single-task training ({self.config.task_type})...\")\n",
    "    \n",
    "    # Setup data loaders\n",
    "    self.create_data_loaders(data_splits)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(self.config.num_epochs):\n",
    "        print(f\"\\n📍 Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_accuracy = self.train_epoch()\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_accuracy, val_f1_macro = self.evaluate()\n",
    "        \n",
    "        # Track metrics\n",
    "        self.training_history['train_loss'].append(train_loss)\n",
    "        self.training_history['train_accuracy'].append(train_accuracy)\n",
    "        self.training_history['val_loss'].append(val_loss)\n",
    "        self.training_history['val_accuracy'].append(val_accuracy)\n",
    "        self.training_history['val_f1_macro'].append(val_f1_macro)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_macro:.4f}\")\n",
    "        \n",
    "        # Report to ASHA for pruning decision\n",
    "        if trial is not None:\n",
    "            trial.report(val_f1_macro, epoch)\n",
    "            \n",
    "            # Check if trial should be pruned\n",
    "            if trial.should_prune():\n",
    "                print(f\"  🚫 Trial pruned at epoch {epoch + 1}\")\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1_macro > best_f1:\n",
    "            best_f1 = val_f1_macro\n",
    "            self.save_model(is_best=True)\n",
    "    \n",
    "    print(f\"\\nBERTweet training completed! Best F1: {best_f1:.4f}\")\n",
    "    return self.training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d54620-c0ca-4c2f-a88c-5602ceee8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import optuna\n",
    "from typing import Dict\n",
    "\n",
    "def create_tuning_subset(data_splits, subset_ratio=0.03):\n",
    "    print(f\"🔪 Creating {subset_ratio*100:.0f}% subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 50:  # Ensure minimum samples\n",
    "            n_samples = min(50, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'labels': [split_data['labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    # Handle different possible key names for validation set\n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "def create_multitask_tuning_subset(data_splits, subset_ratio=0.03):\n",
    "    print(f\"Creating {subset_ratio*100:.0f}% multitask subset for hyperparameter tuning...\")\n",
    "    \n",
    "    def sample_multitask_split(split_data, ratio):\n",
    "        n_samples = int(len(split_data['texts']) * ratio)\n",
    "        if n_samples < 50:  # Ensure minimum samples\n",
    "            n_samples = min(50, len(split_data['texts']))\n",
    "        indices = np.random.choice(len(split_data['texts']), n_samples, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'texts': [split_data['texts'][i] for i in indices],\n",
    "            'sentiment_labels': [split_data['sentiment_labels'][i] for i in indices],\n",
    "            'emotion_labels': [split_data['emotion_labels'][i] for i in indices]\n",
    "        }\n",
    "    \n",
    "    val_key = 'val' if 'val' in data_splits else ('validation' if 'validation' in data_splits else 'test')\n",
    "    \n",
    "    tuning_data = {\n",
    "        'train': sample_multitask_split(data_splits['train'], subset_ratio),\n",
    "        'val': sample_multitask_split(data_splits[val_key], subset_ratio),\n",
    "        'test': sample_multitask_split(data_splits['test'], subset_ratio) if 'test' in data_splits else sample_multitask_split(data_splits[val_key], subset_ratio)\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Multitask tuning subset created:\")\n",
    "    print(f\"  Train: {len(tuning_data['train']['texts'])} samples\")\n",
    "    print(f\"  Val: {len(tuning_data['val']['texts'])} samples\")\n",
    "    \n",
    "    return tuning_data\n",
    "\n",
    "class FastBERTweetHyperparameterTuner:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        data_splits: Dict,\n",
    "        n_trials: int = 8,\n",
    "        model_name: str = \"vinai/bertweet-base\",\n",
    "        subset_ratio: float = 0.03,\n",
    "        max_epochs_per_trial: int = 2\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.n_trials = n_trials\n",
    "        self.model_name = model_name\n",
    "        self.max_epochs_per_trial = max_epochs_per_trial\n",
    "        \n",
    "        print(f\"🚀 Creating ultra-fast tuning setup for {model_type}...\")\n",
    "        \n",
    "        if model_type == \"multitask\":\n",
    "            self.tuning_data = create_multitask_tuning_subset(data_splits, subset_ratio)\n",
    "        else:\n",
    "            self.tuning_data = create_tuning_subset(data_splits, subset_ratio)\n",
    "        \n",
    "        print(f\"⚡ Speed optimizations:\")\n",
    "        print(f\"  - Using {subset_ratio*100:.0f}% of data ({len(self.tuning_data['train']['texts'])} samples)\")\n",
    "        print(f\"  - Max {max_epochs_per_trial} epochs per trial\")\n",
    "        print(f\"  - {n_trials} total trials\")\n",
    "        print(f\"  - Estimated time: {n_trials * max_epochs_per_trial * 1:.0f}-{n_trials * max_epochs_per_trial * 3:.0f} minutes\")\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \n",
    "        # Fast hyperparameter suggestions\n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 1e-4, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "        num_epochs = self.max_epochs_per_trial\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.1, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1)\n",
    "        hidden_dropout = trial.suggest_float('hidden_dropout_prob', 0.1, 0.3)\n",
    "        classifier_dropout = trial.suggest_float('classifier_dropout', 0.1, 0.3)\n",
    "        max_length = 128\n",
    "        \n",
    "        alpha = trial.suggest_float('alpha', 0.4, 0.6) if self.model_type == \"multitask\" else 0.5\n",
    "        \n",
    "        # Create speed-optimized config (removed unsupported parameters)\n",
    "        config = TrainingConfig(\n",
    "            model_name=self.model_name,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            hidden_dropout_prob=hidden_dropout,\n",
    "            classifier_dropout=classifier_dropout,\n",
    "            max_length=max_length,\n",
    "            alpha=alpha,\n",
    "            task_type=self.model_type,\n",
    "            output_dir=f\"./fast_trial_{trial.number}\"\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            aggressive_memory_cleanup()\n",
    "            \n",
    "            if self.model_type == \"multitask\":\n",
    "                trainer = BERTweetMultiTaskTrainer(config)\n",
    "                history = trainer.train(self.tuning_data)\n",
    "                \n",
    "                # Get scores from general dataset (not Reddit)\n",
    "                best_sentiment_f1 = max(history['val_sentiment_f1_macro']) if history['val_sentiment_f1_macro'] else 0.0\n",
    "                best_emotion_f1 = max(history['val_emotion_f1_macro']) if history['val_emotion_f1_macro'] else 0.0\n",
    "                score = (best_sentiment_f1 + best_emotion_f1) / 2\n",
    "                \n",
    "            else:\n",
    "                if self.model_type == \"sentiment\":\n",
    "                    num_classes = bertweet_model_config.sentiment_num_classes\n",
    "                else:\n",
    "                    num_classes = bertweet_model_config.emotion_num_classes\n",
    "                \n",
    "                trainer = BERTweetSingleTaskTrainer(config, num_classes)\n",
    "                history = trainer.train(self.tuning_data)\n",
    "                \n",
    "                # Get score from general dataset (not Reddit)\n",
    "                score = max(history['val_f1_macro']) if history['val_f1_macro'] else 0.0\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"⚡ Trial {trial.number}: Score={score:.4f}, Time={elapsed/60:.1f}min\")\n",
    "            \n",
    "            return score\n",
    "            \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"❌ Trial {trial.number} failed after {elapsed/60:.1f}min: {str(e)[:100]}...\")\n",
    "            return 0.0\n",
    "            \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if 'trainer' in locals():\n",
    "                del trainer\n",
    "            aggressive_memory_cleanup()\n",
    "    \n",
    "    def tune(self):\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.RandomSampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🚀 Starting FAST hyperparameter tuning for {self.model_type}...\")\n",
    "        print(f\"⚡ Target: Find good hyperparameters in ~{self.n_trials * 2:.0f} minutes\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n🏆 Fast tuning completed in {total_time/60:.1f} minutes!\")\n",
    "        print(f\"🎯 Best score: {study.best_value:.4f}\")\n",
    "        print(f\"📋 Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7433e8-b3d6-4d4b-bd3f-1fa8a2f719dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BERTweet Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bertweet_model(model_path: str, model_type: str, test_data: Dict, model_name: str = \"vinai/bertweet-base\", reddit_data: Dict = None):\n",
    "    \n",
    "    print(f\"📊 Evaluating BERTweet {model_type} model...\")\n",
    "    \n",
    "    # Load BERTweet tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Load BERTweet model\n",
    "    if model_type == \"multitask\":\n",
    "        model = BERTweetMultiTaskTransformer.from_pretrained(model_path)\n",
    "    else:\n",
    "        model = BERTweetSingleTaskTransformer.from_pretrained(model_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate on general dataset\n",
    "    general_results = evaluate_bertweet_on_dataset(model, model_type, test_data, tokenizer, \"General Dataset\")\n",
    "    \n",
    "    # Evaluate on Reddit dataset if available\n",
    "    reddit_results = None\n",
    "    if reddit_data is not None:\n",
    "        reddit_results = evaluate_bertweet_on_dataset(model, model_type, reddit_data, tokenizer, \"Reddit Dataset\")\n",
    "    \n",
    "    return {\n",
    "        'general': general_results,\n",
    "        'reddit': reddit_results\n",
    "    }\n",
    "\n",
    "def evaluate_bertweet_on_dataset(model, model_type: str, data: Dict, tokenizer, dataset_name: str):\n",
    "    \"\"\"Evaluate BERTweet model on a specific dataset\"\"\"\n",
    "    print(f\"Evaluating on {dataset_name}...\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    if model_type == \"multitask\":\n",
    "        test_dataset = BERTweetMultiTaskDataset(\n",
    "            texts=data['texts'],\n",
    "            sentiment_labels=data['sentiment_labels'],\n",
    "            emotion_labels=data['emotion_labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    else:\n",
    "        test_dataset = BERTweetSingleTaskDataset(\n",
    "            texts=data['texts'],\n",
    "            labels=data['labels'],\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    if model_type == \"multitask\":\n",
    "        all_sentiment_predictions = []\n",
    "        all_emotion_predictions = []\n",
    "        all_sentiment_labels = []\n",
    "        all_emotion_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                sentiment_preds = torch.argmax(outputs['sentiment_logits'], dim=-1)\n",
    "                emotion_preds = torch.argmax(outputs['emotion_logits'], dim=-1)\n",
    "                \n",
    "                all_sentiment_predictions.extend(sentiment_preds.cpu().numpy())\n",
    "                all_emotion_predictions.extend(emotion_preds.cpu().numpy())\n",
    "                all_sentiment_labels.extend(batch['sentiment_labels'].numpy())\n",
    "                all_emotion_labels.extend(batch['emotion_labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        sentiment_accuracy = accuracy_score(all_sentiment_labels, all_sentiment_predictions)\n",
    "        emotion_accuracy = accuracy_score(all_emotion_labels, all_emotion_predictions)\n",
    "        sentiment_f1_macro = f1_score(all_sentiment_labels, all_sentiment_predictions, average='macro', zero_division=0)\n",
    "        emotion_f1_macro = f1_score(all_emotion_labels, all_emotion_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        results = {\n",
    "            'sentiment_accuracy': sentiment_accuracy,\n",
    "            'emotion_accuracy': emotion_accuracy,\n",
    "            'sentiment_f1_macro': sentiment_f1_macro,\n",
    "            'emotion_f1_macro': emotion_f1_macro,\n",
    "            'combined_accuracy': (sentiment_accuracy + emotion_accuracy) / 2,\n",
    "            'combined_f1_macro': (sentiment_f1_macro + emotion_f1_macro) / 2\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 BERTweet Multi-task Results on {dataset_name}:\")\n",
    "        print(f\"  Sentiment - Accuracy: {sentiment_accuracy:.4f}, F1: {sentiment_f1_macro:.4f}\")\n",
    "        print(f\"  Emotion - Accuracy: {emotion_accuracy:.4f}, F1: {emotion_f1_macro:.4f}\")\n",
    "        print(f\"  Combined - Accuracy: {results['combined_accuracy']:.4f}, F1: {results['combined_f1_macro']:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        # Single-task evaluation\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_macro': f1_macro\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 BERTweet {model_type.capitalize()} Results on {dataset_name}:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  F1 Macro: {f1_macro:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_bertweet_results_summary(sentiment_results: Dict, emotion_results: Dict, multitask_results: Dict):\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📊 BERTWEET FINAL RESULTS SUMMARY\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🎯 BERTWEET SINGLE-TASK SENTIMENT MODEL:\")\n",
    "    print(f\"  Accuracy: {sentiment_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {sentiment_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 BERTWEET SINGLE-TASK EMOTION MODEL:\")\n",
    "    print(f\"  Accuracy: {emotion_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {emotion_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 BERTWEET MULTI-TASK MODEL:\")\n",
    "    print(f\"  Sentiment - Accuracy: {multitask_results['sentiment_accuracy']:.4f}, F1: {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"  Emotion - Accuracy: {multitask_results['emotion_accuracy']:.4f}, F1: {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"  Combined - Accuracy: {multitask_results['combined_accuracy']:.4f}, F1: {multitask_results['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 BERTWEET COMPARISON:\")\n",
    "    print(f\"  Single-task Sentiment vs Multi-task Sentiment:\")\n",
    "    print(f\"    Accuracy: {sentiment_results['accuracy']:.4f} vs {multitask_results['sentiment_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {sentiment_results['f1_macro']:.4f} vs {multitask_results['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"  Single-task Emotion vs Multi-task Emotion:\")\n",
    "    print(f\"    Accuracy: {emotion_results['accuracy']:.4f} vs {multitask_results['emotion_accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {emotion_results['f1_macro']:.4f} vs {multitask_results['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"✅ BERTweet Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a2219e-0300-4c27-bc26-99fe65cfbed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet Main training pipeline defined!\n"
     ]
    }
   ],
   "source": [
    "def main_bertweet_training_pipeline():\n",
    "    \n",
    "    print(\"STARTING COMPREHENSIVE BERTWEET TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and process datasets for BERTweet\n",
    "    print(\"\\n1️⃣ Loading and processing datasets for BERTweet...\")\n",
    "    sentiment_data, emotion_data = load_and_process_datasets_bertweet()\n",
    "    multitask_data = create_multitask_data_bertweet(sentiment_data, emotion_data)\n",
    "    \n",
    "    # Load Reddit evaluation data\n",
    "    print(\"\\nLoading Reddit evaluation data...\")\n",
    "    reddit_data = load_reddit_evaluation_data()\n",
    "    \n",
    "    # Model configurations\n",
    "    model_name = \"vinai/bertweet-base\"\n",
    "    n_trials = 10 # Number of hyperparameter tuning trials\n",
    "    \n",
    "    # Store results globally\n",
    "    all_results = {}\n",
    "    \n",
    "    print(\"✅ Data loading completed!\")\n",
    "    print(f\"📊 Sentiment data: {len(sentiment_data['train']['texts'])} train samples\")\n",
    "    print(f\"📊 Emotion data: {len(emotion_data['train']['texts'])} train samples\")\n",
    "    print(f\"📊 Multitask data: {len(multitask_data['train']['texts'])} train samples\")\n",
    "    if reddit_data:\n",
    "        print(f\"📊 Reddit data: {len(reddit_data['sentiment']['texts'])} evaluation samples\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 1: INITIAL BERTWEET TRAINING WITH DEFAULT PARAMETERS\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 1: INITIAL BERTWEET TRAINING WITH DEFAULT PARAMETERS\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Default configuration for BERTweet\n",
    "    default_config_sentiment = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        task_type=\"sentiment\",\n",
    "        output_dir=\"./initial_bertweet_sentiment_model\"\n",
    "    )\n",
    "    \n",
    "    default_config_emotion = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        task_type=\"emotion\",\n",
    "        output_dir=\"./initial_bertweet_emotion_model\"\n",
    "    )\n",
    "    \n",
    "    default_config_multitask = TrainingConfig(\n",
    "        model_name=model_name,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        max_length=128,\n",
    "        alpha=0.5,\n",
    "        task_type=\"multitask\",\n",
    "        output_dir=\"./initial_bertweet_multitask_model\"\n",
    "    )\n",
    "    \n",
    "    # 1.1 Train Initial BERTweet Sentiment Model\n",
    "    print(f\"\\n2️⃣ Training Initial BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_sentiment_trainer = BERTweetSingleTaskTrainer(\n",
    "        config=default_config_sentiment,\n",
    "        num_classes=bertweet_model_config.sentiment_num_classes\n",
    "    )\n",
    "    initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet sentiment model on both general and Reddit datasets\n",
    "    initial_sentiment_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_sentiment_model/model_best\",\n",
    "        model_type=\"sentiment\",\n",
    "        test_data=sentiment_data['test'],\n",
    "        model_name=model_name,\n",
    "        reddit_data=reddit_data['sentiment'] if reddit_data else None\n",
    "    )\n",
    "    all_results['initial_sentiment'] = initial_sentiment_results\n",
    "    \n",
    "    # 1.2 Train Initial BERTweet Emotion Model\n",
    "    print(f\"\\n3️⃣ Training Initial BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_emotion_trainer = BERTweetSingleTaskTrainer(\n",
    "        config=default_config_emotion,\n",
    "        num_classes=bertweet_model_config.emotion_num_classes\n",
    "    )\n",
    "    initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "    \n",
    "    # Evaluate initial BERTweet emotion model on both general and Reddit datasets\n",
    "    initial_emotion_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_emotion_model/model_best\",\n",
    "        model_type=\"emotion\",\n",
    "        test_data=emotion_data['test'],\n",
    "        model_name=model_name,\n",
    "        reddit_data=reddit_data['emotion'] if reddit_data else None\n",
    "    )\n",
    "    all_results['initial_emotion'] = initial_emotion_results\n",
    "    \n",
    "    # 1.3 Train Initial BERTweet Multi-task Model\n",
    "    print(f\"\\n4️⃣ Training Initial BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    initial_multitask_trainer = BERTweetMultiTaskTrainer(config=default_config_multitask)\n",
    "    initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "    \n",
    "    # Evaluate initial multitask model on both general and Reddit datasets\n",
    "    initial_multitask_results = evaluate_bertweet_model(\n",
    "        model_path=\"./initial_bertweet_multitask_model/model_best\",\n",
    "        model_type=\"multitask\",\n",
    "        test_data=multitask_data['test'],\n",
    "        model_name=model_name,\n",
    "        reddit_data=reddit_data['multitask'] if reddit_data else None\n",
    "    )\n",
    "    all_results['initial_multitask'] = initial_multitask_results\n",
    "    \n",
    "    # Display initial BERTweet results summary\n",
    "    print(f\"\\n5️⃣ Initial BERTweet Results Summary...\")\n",
    "    print(\"=\"*60)\n",
    "    create_bertweet_initial_results_summary(\n",
    "        sentiment_results=all_results['initial_sentiment'],\n",
    "        emotion_results=all_results['initial_emotion'],\n",
    "        multitask_results=all_results['initial_multitask']\n",
    "    )\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 2: BERTWEET HYPERPARAMETER TUNING\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 2: BERTWEET HYPERPARAMETER TUNING\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # 2.1 Hyperparameter tuning for BERTweet sentiment\n",
    "    print(f\"\\n6️⃣ Hyperparameter Tuning for BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sentiment_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"sentiment\",\n",
    "        data_splits=sentiment_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    sentiment_study = sentiment_tuner.tune()\n",
    "    \n",
    "    # 2.2 Hyperparameter tuning for BERTweet emotion\n",
    "    print(f\"\\n7️⃣ Hyperparameter Tuning for BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    emotion_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"emotion\",\n",
    "        data_splits=emotion_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    emotion_study = emotion_tuner.tune()\n",
    "    \n",
    "    # 2.3 Hyperparameter tuning for BERTweet multi-task\n",
    "    print(f\"\\n8️⃣ Hyperparameter Tuning for BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    multitask_tuner = BERTweetHyperparameterTuner(\n",
    "        model_type=\"multitask\",\n",
    "        data_splits=multitask_data,\n",
    "        n_trials=n_trials,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    multitask_study = multitask_tuner.tune()\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 3: FINAL BERTWEET TRAINING WITH OPTIMIZED PARAMETERS\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 3: FINAL BERTWEET TRAINING WITH OPTIMIZED PARAMETERS\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # 3.1 Train optimized BERTweet sentiment model\n",
    "    print(f\"\\n9️⃣ Training Optimized BERTweet Sentiment Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_sentiment_trainer, optimized_sentiment_history = train_bertweet_with_best_params(\n",
    "        model_type=\"sentiment\",\n",
    "        data_splits=sentiment_data,\n",
    "        best_params=sentiment_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet sentiment model\n",
    "    optimized_sentiment_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_sentiment_model/model_best\",\n",
    "        model_type=\"sentiment\",\n",
    "        test_data=sentiment_data['test'],\n",
    "        model_name=model_name,\n",
    "        reddit_data=reddit_data['sentiment'] if reddit_data else None\n",
    "    )\n",
    "    all_results['optimized_sentiment'] = optimized_sentiment_results\n",
    "    \n",
    "    # 3.2 Train optimized BERTweet emotion model\n",
    "    print(f\"\\n🔟 Training Optimized BERTweet Emotion Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_emotion_trainer, optimized_emotion_history = train_bertweet_with_best_params(\n",
    "        model_type=\"emotion\",\n",
    "        data_splits=emotion_data,\n",
    "        best_params=emotion_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet emotion model\n",
    "    optimized_emotion_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_emotion_model/model_best\",\n",
    "        model_type=\"emotion\",\n",
    "        test_data=emotion_data['test'],\n",
    "        model_name=model_name,\n",
    "        reddit_data=reddit_data['emotion'] if reddit_data else None\n",
    "    )\n",
    "    all_results['optimized_emotion'] = optimized_emotion_results\n",
    "    \n",
    "    # 3.3 Train optimized BERTweet multi-task model\n",
    "    print(f\"\\n1️⃣1️⃣ Training Optimized BERTweet Multi-task Model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    optimized_multitask_trainer, optimized_multitask_history = train_bertweet_with_best_params(\n",
    "        model_type=\"multitask\",\n",
    "        data_splits=multitask_data,\n",
    "        best_params=multitask_study.best_params,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate optimized BERTweet multi-task model\n",
    "    optimized_multitask_results = evaluate_bertweet_model(\n",
    "        model_path=\"./final_bertweet_multitask_model/model_best\",\n",
    "        model_type=\"multitask\",\n",
    "        test_data=multitask_data['test'],\n",
    "        model_name=model_name,\n",
    "        reddit_data=reddit_data['multitask'] if reddit_data else None\n",
    "    )\n",
    "    all_results['optimized_multitask'] = optimized_multitask_results\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 4: COMPREHENSIVE BERTWEET RESULTS COMPARISON\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"📍 PHASE 4: COMPREHENSIVE BERTWEET RESULTS COMPARISON\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Create comprehensive BERTweet comparison\n",
    "    create_comprehensive_bertweet_results_comparison(all_results)\n",
    "    \n",
    "    # Save all BERTweet results\n",
    "    results_summary = {\n",
    "        'model_type': 'BERTweet',\n",
    "        'model_name': model_name,\n",
    "        'initial_models': {\n",
    "            'sentiment': all_results['initial_sentiment'],\n",
    "            'emotion': all_results['initial_emotion'],\n",
    "            'multitask': all_results['initial_multitask']\n",
    "        },\n",
    "        'optimized_models': {\n",
    "            'sentiment': all_results['optimized_sentiment'],\n",
    "            'emotion': all_results['optimized_emotion'],\n",
    "            'multitask': all_results['optimized_multitask']\n",
    "        },\n",
    "        'hyperparameter_studies': {\n",
    "            'sentiment': sentiment_study.best_params,\n",
    "            'emotion': emotion_study.best_params,\n",
    "            'multitask': multitask_study.best_params\n",
    "        },\n",
    "        'improvements': {\n",
    "            'sentiment': {\n",
    "                'accuracy_improvement': all_results['optimized_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy'],\n",
    "                'f1_improvement': all_results['optimized_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro']\n",
    "            },\n",
    "            'emotion': {\n",
    "                'accuracy_improvement': all_results['optimized_emotion']['accuracy'] - all_results['initial_emotion']['accuracy'],\n",
    "                'f1_improvement': all_results['optimized_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro']\n",
    "            },\n",
    "            'multitask': {\n",
    "                'sentiment_accuracy_improvement': all_results['optimized_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy'],\n",
    "                'emotion_accuracy_improvement': all_results['optimized_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy'],\n",
    "                'combined_f1_improvement': all_results['optimized_multitask']['combined_f1_macro'] - all_results['initial_multitask']['combined_f1_macro']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('comprehensive_bertweet_results_summary.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ COMPLETE BERTWEET PIPELINE FINISHED!\")\n",
    "    print(f\"📁 Results saved to: comprehensive_bertweet_results_summary.json\")\n",
    "    print(f\"📁 Initial models saved to: ./initial_bertweet_*_model/\")\n",
    "    print(f\"📁 Optimized models saved to: ./final_bertweet_*_model/\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def create_bertweet_initial_results_summary(sentiment_results: Dict, emotion_results: Dict, multitask_results: Dict):\n",
    "    \n",
    "    print(f\"\\n📊 INITIAL BERTWEET MODELS RESULTS SUMMARY\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n🎯 INITIAL BERTWEET SENTIMENT MODEL:\")\n",
    "    print(f\"  Accuracy: {sentiment_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {sentiment_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 INITIAL BERTWEET EMOTION MODEL:\")\n",
    "    print(f\"  Accuracy: {emotion_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {emotion_results['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 INITIAL BERTWEET MULTI-TASK MODEL:\")\n",
    "    print(f\"  General Dataset:\")\n",
    "    print(f\"    Sentiment - Accuracy: {multitask_results['general']['sentiment_accuracy']:.4f}, F1: {multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Emotion - Accuracy: {multitask_results['general']['emotion_accuracy']:.4f}, F1: {multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Combined - Accuracy: {multitask_results['general']['combined_accuracy']:.4f}, F1: {multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "    if multitask_results.get('reddit'):\n",
    "        print(f\"  Reddit Dataset:\")\n",
    "        print(f\"    Sentiment - Accuracy: {multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "        print(f\"    Emotion - Accuracy: {multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "        print(f\"    Combined - Accuracy: {multitask_results['reddit']['combined_accuracy']:.4f}, F1: {multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n�� These are BERTweet baseline results. Hyperparameter tuning will aim to improve them!\")\n",
    "\n",
    "def create_comprehensive_bertweet_results_comparison(all_results: Dict):\n",
    "    \n",
    "    print(f\"\\n📊 COMPREHENSIVE BERTWEET RESULTS COMPARISON\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🎯 BERTWEET SENTIMENT MODEL COMPARISON:\")\n",
    "    print(f\"  Initial    - Accuracy: {all_results['initial_sentiment']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['f1_macro']:.4f}\")\n",
    "    print(f\"  Optimized  - Accuracy: {all_results['optimized_sentiment']['accuracy']:.4f}, F1: {all_results['optimized_sentiment']['f1_macro']:.4f}\")\n",
    "    \n",
    "    sent_acc_improve = all_results['optimized_sentiment']['accuracy'] - all_results['initial_sentiment']['accuracy']\n",
    "    sent_f1_improve = all_results['optimized_sentiment']['f1_macro'] - all_results['initial_sentiment']['f1_macro']\n",
    "    print(f\"  Improvement - Accuracy: {sent_acc_improve:+.4f}, F1: {sent_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n😊 BERTWEET EMOTION MODEL COMPARISON:\")\n",
    "    print(f\"  Initial    - Accuracy: {all_results['initial_emotion']['accuracy']:.4f}, F1: {all_results['initial_emotion']['f1_macro']:.4f}\")\n",
    "    print(f\"  Optimized  - Accuracy: {all_results['optimized_emotion']['accuracy']:.4f}, F1: {all_results['optimized_emotion']['f1_macro']:.4f}\")\n",
    "    \n",
    "    emo_acc_improve = all_results['optimized_emotion']['accuracy'] - all_results['initial_emotion']['accuracy']\n",
    "    emo_f1_improve = all_results['optimized_emotion']['f1_macro'] - all_results['initial_emotion']['f1_macro']\n",
    "    print(f\"  Improvement - Accuracy: {emo_acc_improve:+.4f}, F1: {emo_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔗 BERTWEET MULTI-TASK MODEL COMPARISON:\")\n",
    "    print(f\"  SENTIMENT TASK:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['optimized_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_sent_acc_improve = all_results['optimized_multitask']['sentiment_accuracy'] - all_results['initial_multitask']['sentiment_accuracy']\n",
    "    mt_sent_f1_improve = all_results['optimized_multitask']['sentiment_f1_macro'] - all_results['initial_multitask']['sentiment_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_sent_acc_improve:+.4f}, F1: {mt_sent_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"  EMOTION TASK:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['emotion_accuracy']:.4f}, F1: {all_results['optimized_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_emo_acc_improve = all_results['optimized_multitask']['emotion_accuracy'] - all_results['initial_multitask']['emotion_accuracy']\n",
    "    mt_emo_f1_improve = all_results['optimized_multitask']['emotion_f1_macro'] - all_results['initial_multitask']['emotion_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_emo_acc_improve:+.4f}, F1: {mt_emo_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"  COMBINED:\")\n",
    "    print(f\"    Initial    - Accuracy: {all_results['initial_multitask']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['combined_f1_macro']:.4f}\")\n",
    "    print(f\"    Optimized  - Accuracy: {all_results['optimized_multitask']['combined_accuracy']:.4f}, F1: {all_results['optimized_multitask']['combined_f1_macro']:.4f}\")\n",
    "    \n",
    "    mt_combined_acc_improve = all_results['optimized_multitask']['combined_accuracy'] - all_results['initial_multitask']['combined_accuracy']\n",
    "    mt_combined_f1_improve = all_results['optimized_multitask']['combined_f1_macro'] - all_results['initial_multitask']['combined_f1_macro']\n",
    "    print(f\"    Improvement - Accuracy: {mt_combined_acc_improve:+.4f}, F1: {mt_combined_f1_improve:+.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 BERTWEET SINGLE-TASK vs MULTI-TASK COMPARISON (OPTIMIZED):\")\n",
    "    print(f\"  SENTIMENT:\")\n",
    "    print(f\"    Single-task: Accuracy: {all_results['optimized_sentiment']['accuracy']:.4f}, F1: {all_results['optimized_sentiment']['f1_macro']:.4f}\")\n",
    "    print(f\"    Multi-task:  Accuracy: {all_results['optimized_multitask']['sentiment_accuracy']:.4f}, F1: {all_results['optimized_multitask']['sentiment_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"  EMOTION:\")\n",
    "    print(f\"    Single-task: Accuracy: {all_results['optimized_emotion']['accuracy']:.4f}, F1: {all_results['optimized_emotion']['f1_macro']:.4f}\")\n",
    "    print(f\"    Multi-task:  Accuracy: {all_results['optimized_multitask']['emotion_accuracy']:.4f}, F1: {all_results['optimized_multitask']['emotion_f1_macro']:.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"BERTweet Main training pipeline defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d7dae6-9b45-488d-af01-f05d24f8aadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING BERTWEET TRAINING PIPELINE\n",
      "================================================================================\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "1️⃣ Loading and processing datasets for BERTweet...\n",
      "Loading datasets for BERTweet...\n",
      "✅ SST-2 loaded: 67349 train, 872 val\n",
      "✅ GoEmotion loaded: 43410 train, 5426 val\n",
      "✅ Loaded existing encoders from enc/ directory for BERTweet\n",
      "Processing sentiment data for BERTweet...\n",
      "BERTweet Sentiment data processed:\n",
      "  Train: 47144 samples\n",
      "  Val: 10102 samples\n",
      "  Test: 10103 samples\n",
      "Processing emotion data for BERTweet...\n",
      "✅ BERTweet Emotion data processed:\n",
      "  Train: 9534 samples\n",
      "  Val: 2043 samples\n",
      "  Test: 2044 samples\n",
      "Creating multi-task dataset for BERTweet...\n",
      "BERTweet Multi-task data created:\n",
      "  Train: 9534 samples\n",
      "  Val: 2043 samples\n",
      "  Test: 2044 samples\n",
      "\n",
      "Loading Reddit evaluation data...\n",
      "Loading Reddit evaluation data...\n",
      "✅ Reddit data loaded: 95 samples\n",
      "✅ Reddit data prepared: 95 samples\n",
      "   Sentiment classes: ['Negative', 'Neutral', 'Positive']\n",
      "   Emotion classes: ['Anger', 'Fear', 'Joy', 'No Emotion', 'Sadness', 'Surprise']\n",
      "✅ Data loading completed!\n",
      "📊 Sentiment data: 47144 train samples\n",
      "📊 Emotion data: 9534 train samples\n",
      "📊 Multitask data: 9534 train samples\n",
      "📊 Reddit data: 95 evaluation samples\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 STARTING BERTWEET TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear memory before starting\n",
    "aggressive_memory_cleanup()\n",
    "\n",
    "# Load and process datasets for BERTweet\n",
    "print(\"\\n1️⃣ Loading and processing datasets for BERTweet...\")\n",
    "sentiment_data, emotion_data = load_and_process_datasets_bertweet()\n",
    "multitask_data = create_multitask_data_bertweet(sentiment_data, emotion_data)\n",
    "\n",
    "# Load Reddit evaluation data\n",
    "print(\"\\nLoading Reddit evaluation data...\")\n",
    "reddit_data = load_reddit_evaluation_data()\n",
    "\n",
    "# Model configurations\n",
    "model_name = \"vinai/bertweet-base\"\n",
    "n_trials = 10 # Number of hyperparameter tuning trials\n",
    "\n",
    "# Store results globally\n",
    "all_results = {}\n",
    "\n",
    "print(\"✅ Data loading completed!\")\n",
    "print(f\"📊 Sentiment data: {len(sentiment_data['train']['texts'])} train samples\")\n",
    "print(f\"📊 Emotion data: {len(emotion_data['train']['texts'])} train samples\")\n",
    "print(f\"📊 Multitask data: {len(multitask_data['train']['texts'])} train samples\")\n",
    "if reddit_data:\n",
    "    print(f\"📊 Reddit data: {len(reddit_data['sentiment']['texts'])} evaluation samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f14cb-0c8e-4cfe-82ab-d7a265e93105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL BERTWEET TRAINING - SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "2️⃣ Training Initial BERTweet Sentiment Model...\n",
      "============================================================\n",
      "Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 0.5535, Train Acc: 0.8103\n",
      "  Val Loss: 0.4513, Val Acc: 0.8640, Val F1: 0.6030\n",
      "BERTweet single-task model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.4059, Train Acc: 0.8778\n",
      "  Val Loss: 0.4223, Val Acc: 0.8727, Val F1: 0.6088\n",
      "BERTweet single-task model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.3395, Train Acc: 0.8955\n",
      "  Val Loss: 0.4642, Val Acc: 0.8746, Val F1: 0.6103\n",
      "BERTweet single-task model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_sentiment_model\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.6103\n",
      "📊 Evaluating BERTweet sentiment model...\n",
      "Evaluating on General Dataset...\n",
      "📊 BERTweet Sentiment Results on General Dataset:\n",
      "  Accuracy: 0.8692\n",
      "  F1 Macro: 0.6065\n",
      "Evaluating on Reddit Dataset...\n",
      "📊 BERTweet Sentiment Results on Reddit Dataset:\n",
      "  Accuracy: 0.5684\n",
      "  F1 Macro: 0.3461\n",
      "\n",
      "✅ Initial Sentiment Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.8692\n",
      "    F1 Macro: 0.6065\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.5684\n",
      "    F1 Macro: 0.3461\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL BERTWEET TRAINING - SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for BERTweet sentiment\n",
    "default_config_sentiment = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./initial_bertweet_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Training Initial BERTweet Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial sentiment model\n",
    "initial_sentiment_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=default_config_sentiment,\n",
    "    num_classes=bertweet_model_config.sentiment_num_classes\n",
    ")\n",
    "initial_sentiment_history = initial_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate initial sentiment model on both general and Reddit datasets\n",
    "initial_sentiment_results = evaluate_bertweet_model(\n",
    "    model_path=\"./initial_bertweet_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['sentiment'] if reddit_data else None\n",
    ")\n",
    "all_results['initial_sentiment'] = initial_sentiment_results\n",
    "\n",
    "print(f\"\\n✅ Initial Sentiment Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {initial_sentiment_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "if initial_sentiment_results.get('reddit'):\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {initial_sentiment_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {initial_sentiment_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5620b29-d78c-44dd-ba58-47c433e6712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL BERTWEET TRAINING - EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "Training Initial BERTweet Emotion Model...\n",
      "============================================================\n",
      "Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 1.0313, Train Acc: 0.6055\n",
      "  Val Loss: 0.7505, Val Acc: 0.7450, Val F1: 0.7106\n",
      "BERTweet single-task model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 0.5803, Train Acc: 0.7962\n",
      "  Val Loss: 0.7472, Val Acc: 0.7445, Val F1: 0.7134\n",
      "BERTweet single-task model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 0.4061, Train Acc: 0.8663\n",
      "  Val Loss: 0.8625, Val Acc: 0.7602, Val F1: 0.7319\n",
      "BERTweet single-task model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_emotion_model\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.7319\n",
      "📊 Evaluating BERTweet emotion model...\n",
      "Evaluating on General Dataset...\n",
      "📊 BERTweet Emotion Results on General Dataset:\n",
      "  Accuracy: 0.7647\n",
      "  F1 Macro: 0.7376\n",
      "Evaluating on Reddit Dataset...\n",
      "📊 BERTweet Emotion Results on Reddit Dataset:\n",
      "  Accuracy: 0.1158\n",
      "  F1 Macro: 0.0673\n",
      "\n",
      "✅ Initial Emotion Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.7647\n",
      "    F1 Macro: 0.7376\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.1158\n",
      "    F1 Macro: 0.0673\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initial Emotion Model Training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL BERTWEET TRAINING - EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for BERTweet emotion\n",
    "default_config_emotion = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./initial_bertweet_emotion_model\"\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Initial BERTweet Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial emotion model\n",
    "initial_emotion_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=default_config_emotion,\n",
    "    num_classes=bertweet_model_config.emotion_num_classes\n",
    ")\n",
    "initial_emotion_history = initial_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate initial emotion model on both general and Reddit datasets\n",
    "initial_emotion_results = evaluate_bertweet_model(\n",
    "    model_path=\"./initial_bertweet_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['emotion'] if reddit_data else None\n",
    ")\n",
    "all_results['initial_emotion'] = initial_emotion_results\n",
    "\n",
    "print(f\"\\n✅ Initial Emotion Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {initial_emotion_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {initial_emotion_results['general']['f1_macro']:.4f}\")\n",
    "if initial_emotion_results.get('reddit'):\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {initial_emotion_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {initial_emotion_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f50a1-d286-468e-988b-61afae14846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 1: INITIAL BERTWEET TRAINING - MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "4️⃣ Training Initial BERTweet Multi-task Model...\n",
      "============================================================\n",
      "Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/3\n",
      "  Train Loss: 1.2452\n",
      "  Train Sentiment Acc: 0.7027, Train Emotion Acc: 0.2620\n",
      "  Val Loss: 1.1247\n",
      "  Val Sentiment Acc: 0.8502, F1: 0.5937\n",
      "  Val Emotion Acc: 0.1919, F1: 0.0578\n",
      "BERTweet multi-task model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "\n",
      "📍 Epoch 2/3\n",
      "  Train Loss: 1.0723\n",
      "  Train Sentiment Acc: 0.8589, Train Emotion Acc: 0.2938\n",
      "  Val Loss: 1.0567\n",
      "  Val Sentiment Acc: 0.8639, F1: 0.6028\n",
      "  Val Emotion Acc: 0.2844, F1: 0.0907\n",
      "BERTweet multi-task model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "Best BERTweet model saved to ./initial_bertweet_multitask_model\\model_best\n",
      "\n",
      "📍 Epoch 3/3\n",
      "  Train Loss: 1.0150\n",
      "  Train Sentiment Acc: 0.8908, Train Emotion Acc: 0.3019\n",
      "  Val Loss: 1.0812\n",
      "  Val Sentiment Acc: 0.8654, F1: 0.6041\n",
      "  Val Emotion Acc: 0.3030, F1: 0.0775\n",
      "\n",
      "BERTweet training completed! Best Combined F1: 0.3467\n",
      "📊 Evaluating BERTweet multitask model...\n",
      "Evaluating on General Dataset...\n",
      "📊 BERTweet Multi-task Results on General Dataset:\n",
      "  Sentiment - Accuracy: 0.8469, F1: 0.5919\n",
      "  Emotion - Accuracy: 0.2886, F1: 0.0909\n",
      "  Combined - Accuracy: 0.5678, F1: 0.3414\n",
      "Evaluating on Reddit Dataset...\n",
      "📊 BERTweet Multi-task Results on Reddit Dataset:\n",
      "  Sentiment - Accuracy: 0.5684, F1: 0.3316\n",
      "  Emotion - Accuracy: 0.2000, F1: 0.0783\n",
      "  Combined - Accuracy: 0.3842, F1: 0.2050\n",
      "\n",
      "✅ Initial Multitask Model Results:\n",
      "  General Dataset:\n",
      "    Sentiment - Accuracy: 0.8469, F1: 0.5919\n",
      "    Emotion - Accuracy: 0.2886, F1: 0.0909\n",
      "    Combined - Accuracy: 0.5678, F1: 0.3414\n",
      "  Reddit Dataset:\n",
      "    Sentiment - Accuracy: 0.5684, F1: 0.3316\n",
      "    Emotion - Accuracy: 0.2000, F1: 0.0783\n",
      "    Combined - Accuracy: 0.3842, F1: 0.2050\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 1: INITIAL BERTWEET TRAINING - MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Default configuration for BERTweet multitask\n",
    "default_config_multitask = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_epochs=3,\n",
    "    max_length=128,\n",
    "    alpha=0.5,\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./initial_bertweet_multitask_model\"\n",
    ")\n",
    "\n",
    "print(\"\\n4️⃣ Training Initial BERTweet Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train initial multitask model\n",
    "initial_multitask_trainer = BERTweetMultiTaskTrainer(config=default_config_multitask)\n",
    "initial_multitask_history = initial_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate initial multitask model on both general and Reddit datasets\n",
    "initial_multitask_results = evaluate_bertweet_model(\n",
    "    model_path=\"./initial_bertweet_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['multitask'] if reddit_data else None\n",
    ")\n",
    "all_results['initial_multitask'] = initial_multitask_results\n",
    "\n",
    "print(f\"\\n✅ Initial Multitask Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Sentiment - Accuracy: {initial_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {initial_multitask_results['general']['emotion_accuracy']:.4f}, F1: {initial_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {initial_multitask_results['general']['combined_accuracy']:.4f}, F1: {initial_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if initial_multitask_results.get('reddit'):\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Sentiment - Accuracy: {initial_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Emotion - Accuracy: {initial_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Combined - Accuracy: {initial_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {initial_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5d5c95b-063c-4fc8-a159-56f363e38323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 12:58:05,469] A new study created in memory with name: no-name-3cd92c5e-6bfc-4f25-bcc1-06b129a7ead3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - SENTIMENT\n",
      "================================================================================\n",
      "\n",
      "6️⃣ Fast Hyperparameter Tuning for BERTweet Sentiment Model...\n",
      "============================================================\n",
      "🚀 Creating ultra-fast tuning setup for sentiment...\n",
      "🔪 Creating 2% subset for hyperparameter tuning...\n",
      "📊 Tuning subset created:\n",
      "  Train: 942 samples\n",
      "  Val: 202 samples\n",
      "⚡ Speed optimizations:\n",
      "  - Using 2% of data (942 samples)\n",
      "  - Max 2 epochs per trial\n",
      "  - 5 total trials\n",
      "  - Estimated time: 10-30 minutes\n",
      "\n",
      "🚀 Starting FAST hyperparameter tuning for sentiment...\n",
      "⚡ Target: Find good hyperparameters in ~10 minutes\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9509, Train Acc: 0.4883\n",
      "  Val Loss: 0.9305, Val Acc: 0.4752, Val F1: 0.2414\n",
      "BERTweet single-task model saved to ./fast_trial_0\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.9088, Train Acc: 0.5042\n",
      "  Val Loss: 0.9226, Val Acc: 0.5050, Val F1: 0.3532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 12:58:37,472] Trial 0 finished with value: 0.3532163742690058 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'warmup_ratio': 0.15986584841970367, 'weight_decay': 0.02404167763981929, 'hidden_dropout_prob': 0.13119890406724052, 'classifier_dropout': 0.1116167224336399}. Best is trial 0 with value: 0.3532163742690058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_0\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.3532\n",
      "⚡ Trial 0: Score=0.3532, Time=0.5min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9650, Train Acc: 0.4618\n",
      "  Val Loss: 1.0175, Val Acc: 0.4356, Val F1: 0.2508\n",
      "BERTweet single-task model saved to ./fast_trial_1\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 12:59:04,019] Trial 1 finished with value: 0.2507798145536897 and parameters: {'learning_rate': 8.062340576073854e-05, 'batch_size': 32, 'warmup_ratio': 0.10205844942958026, 'weight_decay': 0.0972918866945795, 'hidden_dropout_prob': 0.26648852816008434, 'classifier_dropout': 0.14246782213565523}. Best is trial 0 with value: 0.3532163742690058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.9216, Train Acc: 0.4618\n",
      "  Val Loss: 0.9410, Val Acc: 0.4752, Val F1: 0.2148\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.2508\n",
      "⚡ Trial 1: Score=0.2508, Time=0.4min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9882, Train Acc: 0.4692\n",
      "  Val Loss: 0.9191, Val Acc: 0.4554, Val F1: 0.2376\n",
      "BERTweet single-task model saved to ./fast_trial_2\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.8906, Train Acc: 0.5573\n",
      "  Val Loss: 0.8947, Val Acc: 0.6436, Val F1: 0.4503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 12:59:30,450] Trial 2 finished with value: 0.4502876517159913 and parameters: {'learning_rate': 2.679909904436601e-05, 'batch_size': 32, 'warmup_ratio': 0.1524756431632238, 'weight_decay': 0.048875051677790424, 'hidden_dropout_prob': 0.15824582803960838, 'classifier_dropout': 0.22237057894447587}. Best is trial 2 with value: 0.4502876517159913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_2\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.4503\n",
      "⚡ Trial 2: Score=0.4503, Time=0.4min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9763, Train Acc: 0.4459\n",
      "  Val Loss: 0.9356, Val Acc: 0.4356, Val F1: 0.2083\n",
      "BERTweet single-task model saved to ./fast_trial_3\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 0.9088, Train Acc: 0.5117\n",
      "  Val Loss: 0.9154, Val Acc: 0.5594, Val F1: 0.3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 12:59:57,797] Trial 3 finished with value: 0.38995115396488167 and parameters: {'learning_rate': 2.503410215228042e-05, 'batch_size': 32, 'warmup_ratio': 0.1456069984217036, 'weight_decay': 0.08066583652537122, 'hidden_dropout_prob': 0.13993475643167194, 'classifier_dropout': 0.2028468876827223}. Best is trial 2 with value: 0.4502876517159913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_3\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.3900\n",
      "⚡ Trial 3: Score=0.3900, Time=0.5min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 0.9900, Train Acc: 0.4055\n",
      "  Val Loss: 0.9294, Val Acc: 0.5248, Val F1: 0.3632\n",
      "BERTweet single-task model saved to ./fast_trial_4\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:00:24,030] Trial 4 finished with value: 0.363155494756042 and parameters: {'learning_rate': 5.1893147077559305e-05, 'batch_size': 32, 'warmup_ratio': 0.11705241236872915, 'weight_decay': 0.015854643368675158, 'hidden_dropout_prob': 0.28977710745066665, 'classifier_dropout': 0.29312640661491185}. Best is trial 2 with value: 0.4502876517159913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.9021, Train Acc: 0.5212\n",
      "  Val Loss: 0.9188, Val Acc: 0.5396, Val F1: 0.3292\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.3632\n",
      "⚡ Trial 4: Score=0.3632, Time=0.4min\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "🏆 Fast tuning completed in 2.3 minutes!\n",
      "🎯 Best score: 0.4503\n",
      "📋 Best parameters:\n",
      "  learning_rate: 2.679909904436601e-05\n",
      "  batch_size: 32\n",
      "  warmup_ratio: 0.1524756431632238\n",
      "  weight_decay: 0.048875051677790424\n",
      "  hidden_dropout_prob: 0.15824582803960838\n",
      "  classifier_dropout: 0.22237057894447587\n",
      "\n",
      "✅ Sentiment Hyperparameter Tuning Completed!\n",
      "🏆 Best F1 Score: 0.4503\n",
      "📋 Best Parameters:\n",
      "  learning_rate: 2.679909904436601e-05\n",
      "  batch_size: 32\n",
      "  warmup_ratio: 0.1524756431632238\n",
      "  weight_decay: 0.048875051677790424\n",
      "  hidden_dropout_prob: 0.15824582803960838\n",
      "  classifier_dropout: 0.22237057894447587\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n6️⃣ Fast Hyperparameter Tuning for BERTweet Sentiment Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create FAST tuner for sentiment\n",
    "sentiment_tuner = FastBERTweetHyperparameterTuner(\n",
    "    model_type=\"sentiment\",\n",
    "    data_splits=sentiment_data,\n",
    "    n_trials=5,  # Even fewer trials for speed\n",
    "    model_name=model_name,\n",
    "    subset_ratio=0.02,  # Only 2% of data!\n",
    "    max_epochs_per_trial=2  # Only 2 epochs per trial!\n",
    ")\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "sentiment_study = sentiment_tuner.tune()\n",
    "\n",
    "print(f\"\\n✅ Sentiment Hyperparameter Tuning Completed!\")\n",
    "print(f\"🏆 Best F1 Score: {sentiment_study.best_value:.4f}\")\n",
    "print(f\"📋 Best Parameters:\")\n",
    "for key, value in sentiment_study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b1abda-b7df-4746-aaa3-1f549d9c0b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:00:24,142] A new study created in memory with name: no-name-59cc03b8-19d6-4878-8ea5-571b21ce77e9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 2: HYPERPARAMETER TUNING - EMOTION\n",
      "================================================================================\n",
      "\n",
      "7️⃣ Hyperparameter Tuning for BERTweet Emotion Model...\n",
      "============================================================\n",
      "🚀 Creating ultra-fast tuning setup for emotion...\n",
      "🔪 Creating 2% subset for hyperparameter tuning...\n",
      "📊 Tuning subset created:\n",
      "  Train: 190 samples\n",
      "  Val: 50 samples\n",
      "⚡ Speed optimizations:\n",
      "  - Using 2% of data (190 samples)\n",
      "  - Max 2 epochs per trial\n",
      "  - 5 total trials\n",
      "  - Estimated time: 10-30 minutes\n",
      "\n",
      "🚀 Starting FAST hyperparameter tuning for emotion...\n",
      "⚡ Target: Find good hyperparameters in ~10 minutes\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7837, Train Acc: 0.1737\n",
      "  Val Loss: 1.7688, Val Acc: 0.3000, Val F1: 0.0769\n",
      "BERTweet single-task model saved to ./fast_trial_0\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:00:32,650] Trial 0 finished with value: 0.07692307692307693 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'warmup_ratio': 0.15986584841970367, 'weight_decay': 0.02404167763981929, 'hidden_dropout_prob': 0.13119890406724052, 'classifier_dropout': 0.1116167224336399}. Best is trial 0 with value: 0.07692307692307693.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.7148, Train Acc: 0.2737\n",
      "  Val Loss: 1.7955, Val Acc: 0.3000, Val F1: 0.0769\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0769\n",
      "⚡ Trial 0: Score=0.0769, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7840, Train Acc: 0.2316\n",
      "  Val Loss: 1.6966, Val Acc: 0.3000, Val F1: 0.1342\n",
      "BERTweet single-task model saved to ./fast_trial_1\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:00:40,565] Trial 1 finished with value: 0.13418290854572715 and parameters: {'learning_rate': 8.062340576073854e-05, 'batch_size': 32, 'warmup_ratio': 0.10205844942958026, 'weight_decay': 0.0972918866945795, 'hidden_dropout_prob': 0.26648852816008434, 'classifier_dropout': 0.14246782213565523}. Best is trial 1 with value: 0.13418290854572715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.7277, Train Acc: 0.2842\n",
      "  Val Loss: 1.6800, Val Acc: 0.3000, Val F1: 0.0769\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.1342\n",
      "⚡ Trial 1: Score=0.1342, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7897, Train Acc: 0.1211\n",
      "  Val Loss: 1.7249, Val Acc: 0.3000, Val F1: 0.0769\n",
      "BERTweet single-task model saved to ./fast_trial_2\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:00:48,362] Trial 2 finished with value: 0.07692307692307693 and parameters: {'learning_rate': 2.679909904436601e-05, 'batch_size': 32, 'warmup_ratio': 0.1524756431632238, 'weight_decay': 0.048875051677790424, 'hidden_dropout_prob': 0.15824582803960838, 'classifier_dropout': 0.22237057894447587}. Best is trial 1 with value: 0.13418290854572715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.7437, Train Acc: 0.1895\n",
      "  Val Loss: 1.7139, Val Acc: 0.3000, Val F1: 0.0769\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0769\n",
      "⚡ Trial 2: Score=0.0769, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.8218, Train Acc: 0.1474\n",
      "  Val Loss: 1.7712, Val Acc: 0.1800, Val F1: 0.0846\n",
      "BERTweet single-task model saved to ./fast_trial_3\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.7611, Train Acc: 0.2053\n",
      "  Val Loss: 1.7537, Val Acc: 0.1800, Val F1: 0.0847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:00:56,836] Trial 3 finished with value: 0.08465608465608465 and parameters: {'learning_rate': 2.503410215228042e-05, 'batch_size': 32, 'warmup_ratio': 0.1456069984217036, 'weight_decay': 0.08066583652537122, 'hidden_dropout_prob': 0.13993475643167194, 'classifier_dropout': 0.2028468876827223}. Best is trial 1 with value: 0.13418290854572715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet single-task model saved to ./fast_trial_3\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0847\n",
      "⚡ Trial 3: Score=0.0847, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.7620, Train Acc: 0.2105\n",
      "  Val Loss: 1.7214, Val Acc: 0.3000, Val F1: 0.0769\n",
      "BERTweet single-task model saved to ./fast_trial_4\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:01:05,090] Trial 4 finished with value: 0.07692307692307693 and parameters: {'learning_rate': 5.1893147077559305e-05, 'batch_size': 32, 'warmup_ratio': 0.11705241236872915, 'weight_decay': 0.015854643368675158, 'hidden_dropout_prob': 0.28977710745066665, 'classifier_dropout': 0.29312640661491185}. Best is trial 1 with value: 0.13418290854572715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.7571, Train Acc: 0.2474\n",
      "  Val Loss: 1.7173, Val Acc: 0.3000, Val F1: 0.0769\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0769\n",
      "⚡ Trial 4: Score=0.0769, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "🏆 Fast tuning completed in 0.7 minutes!\n",
      "🎯 Best score: 0.1342\n",
      "📋 Best parameters:\n",
      "  learning_rate: 8.062340576073854e-05\n",
      "  batch_size: 32\n",
      "  warmup_ratio: 0.10205844942958026\n",
      "  weight_decay: 0.0972918866945795\n",
      "  hidden_dropout_prob: 0.26648852816008434\n",
      "  classifier_dropout: 0.14246782213565523\n",
      "\n",
      "✅ Emotion Hyperparameter Tuning Completed!\n",
      "🏆 Best F1 Score: 0.1342\n",
      "📋 Best Parameters:\n",
      "  learning_rate: 8.062340576073854e-05\n",
      "  batch_size: 32\n",
      "  warmup_ratio: 0.10205844942958026\n",
      "  weight_decay: 0.0972918866945795\n",
      "  hidden_dropout_prob: 0.26648852816008434\n",
      "  classifier_dropout: 0.14246782213565523\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: HYPERPARAMETER TUNING - EMOTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n7️⃣ Hyperparameter Tuning for BERTweet Emotion Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tuner for emotion\n",
    "emotion_tuner = FastBERTweetHyperparameterTuner(\n",
    "    model_type=\"emotion\",\n",
    "    data_splits=emotion_data,\n",
    "    n_trials=5,  # Even fewer trials for speed\n",
    "    model_name=model_name,\n",
    "    subset_ratio=0.02,  # Only 2% of data!\n",
    "    max_epochs_per_trial=2  # Only 2 epochs per trial!\n",
    ")\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "emotion_study = emotion_tuner.tune()\n",
    "\n",
    "print(f\"\\n✅ Emotion Hyperparameter Tuning Completed!\")\n",
    "print(f\"🏆 Best F1 Score: {emotion_study.best_value:.4f}\")\n",
    "print(f\"📋 Best Parameters:\")\n",
    "for key, value in emotion_study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd4e6bd-9737-46d2-86d3-88c1a6c16275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:01:05,210] A new study created in memory with name: no-name-5e2c65c3-1b26-43f3-a488-26912c88db0c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - MULTITASK\n",
      "================================================================================\n",
      "\n",
      "8️⃣ Fast Hyperparameter Tuning for BERTweet Multi-task Model...\n",
      "============================================================\n",
      "🚀 Creating ultra-fast tuning setup for multitask...\n",
      "Creating 2% multitask subset for hyperparameter tuning...\n",
      "📊 Multitask tuning subset created:\n",
      "  Train: 190 samples\n",
      "  Val: 50 samples\n",
      "⚡ Speed optimizations:\n",
      "  - Using 2% of data (190 samples)\n",
      "  - Max 2 epochs per trial\n",
      "  - 5 total trials\n",
      "  - Estimated time: 10-30 minutes\n",
      "\n",
      "🚀 Starting FAST hyperparameter tuning for multitask...\n",
      "⚡ Target: Find good hyperparameters in ~10 minutes\n",
      "============================================================\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.4307\n",
      "  Train Sentiment Acc: 0.3789, Train Emotion Acc: 0.2211\n",
      "  Val Loss: 1.3449\n",
      "  Val Sentiment Acc: 0.5200, F1: 0.3145\n",
      "  Val Emotion Acc: 0.2400, F1: 0.0714\n",
      "BERTweet multi-task model saved to ./fast_trial_0\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.2648\n",
      "  Train Sentiment Acc: 0.5316, Train Emotion Acc: 0.2526\n",
      "  Val Loss: 1.2611\n",
      "  Val Sentiment Acc: 0.5200, F1: 0.2990\n",
      "  Val Emotion Acc: 0.2400, F1: 0.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:01:15,791] Trial 0 finished with value: 0.21186801964857868 and parameters: {'learning_rate': 3.65445235521325e-05, 'batch_size': 16, 'warmup_ratio': 0.15986584841970367, 'weight_decay': 0.02404167763981929, 'hidden_dropout_prob': 0.13119890406724052, 'classifier_dropout': 0.1116167224336399, 'alpha': 0.573235229154987}. Best is trial 0 with value: 0.21186801964857868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_0\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_0\\model_best\n",
      "\n",
      "BERTweet training completed! Best Combined F1: 0.2042\n",
      "⚡ Trial 0: Score=0.2119, Time=0.2min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.5811\n",
      "  Train Sentiment Acc: 0.4526, Train Emotion Acc: 0.1895\n",
      "  Val Loss: 1.5828\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.2100\n",
      "  Val Emotion Acc: 0.1000, F1: 0.0452\n",
      "BERTweet multi-task model saved to ./fast_trial_1\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.3516\n",
      "  Train Sentiment Acc: 0.5579, Train Emotion Acc: 0.2737\n",
      "  Val Loss: 1.4911\n",
      "  Val Sentiment Acc: 0.5800, F1: 0.3889\n",
      "  Val Emotion Acc: 0.3000, F1: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:01:25,660] Trial 1 finished with value: 0.23350694444444442 and parameters: {'learning_rate': 5.262490902114904e-05, 'batch_size': 16, 'warmup_ratio': 0.19699098521619945, 'weight_decay': 0.08491983767203796, 'hidden_dropout_prob': 0.14246782213565523, 'classifier_dropout': 0.1363649934414201, 'alpha': 0.43668090197068676}. Best is trial 1 with value: 0.23350694444444442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_1\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_1\\model_best\n",
      "\n",
      "BERTweet training completed! Best Combined F1: 0.2335\n",
      "⚡ Trial 1: Score=0.2335, Time=0.2min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.4792\n",
      "  Train Sentiment Acc: 0.4632, Train Emotion Acc: 0.1526\n",
      "  Val Loss: 1.4616\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.3230\n",
      "  Val Emotion Acc: 0.3000, F1: 0.1044\n",
      "BERTweet multi-task model saved to ./fast_trial_2\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.3695\n",
      "  Train Sentiment Acc: 0.5421, Train Emotion Acc: 0.2789\n",
      "  Val Loss: 1.4080\n",
      "  Val Sentiment Acc: 0.5400, F1: 0.3779\n",
      "  Val Emotion Acc: 0.2800, F1: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:01:35,429] Trial 2 finished with value: 0.24113685878391763 and parameters: {'learning_rate': 3.2635193912846855e-05, 'batch_size': 16, 'warmup_ratio': 0.1291229140198042, 'weight_decay': 0.06506676052501416, 'hidden_dropout_prob': 0.12789877213040837, 'classifier_dropout': 0.15842892970704364, 'alpha': 0.47327236865873834}. Best is trial 2 with value: 0.24113685878391763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_2\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_2\\model_best\n",
      "\n",
      "BERTweet training completed! Best Combined F1: 0.2254\n",
      "⚡ Trial 2: Score=0.2411, Time=0.2min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.5102\n",
      "  Train Sentiment Acc: 0.3421, Train Emotion Acc: 0.2947\n",
      "  Val Loss: 1.3922\n",
      "  Val Sentiment Acc: 0.5000, F1: 0.2715\n",
      "  Val Emotion Acc: 0.3000, F1: 0.0769\n",
      "BERTweet multi-task model saved to ./fast_trial_3\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.4433\n",
      "  Train Sentiment Acc: 0.5105, Train Emotion Acc: 0.3158\n",
      "  Val Loss: 1.3837\n",
      "  Val Sentiment Acc: 0.5000, F1: 0.2715\n",
      "  Val Emotion Acc: 0.3000, F1: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:01:45,411] Trial 3 finished with value: 0.17544153811759444 and parameters: {'learning_rate': 4.166863122305896e-05, 'batch_size': 16, 'warmup_ratio': 0.15142344384136117, 'weight_decay': 0.06331731119758383, 'hidden_dropout_prob': 0.10929008254399955, 'classifier_dropout': 0.22150897038028766, 'alpha': 0.4341048247374583}. Best is trial 2 with value: 0.24113685878391763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_3\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_3\\model_best\n",
      "\n",
      "BERTweet training completed! Best Combined F1: 0.1754\n",
      "⚡ Trial 3: Score=0.1754, Time=0.2min\n",
      "🧹 Memory cleaned!\n",
      "🧹 Memory cleaned!\n",
      "Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/2\n",
      "  Train Loss: 1.6003\n",
      "  Train Sentiment Acc: 0.3684, Train Emotion Acc: 0.2632\n",
      "  Val Loss: 1.3758\n",
      "  Val Sentiment Acc: 0.4200, F1: 0.1972\n",
      "  Val Emotion Acc: 0.3000, F1: 0.0769\n",
      "BERTweet multi-task model saved to ./fast_trial_4\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_4\\model_best\n",
      "\n",
      "📍 Epoch 2/2\n",
      "  Train Loss: 1.4129\n",
      "  Train Sentiment Acc: 0.4947, Train Emotion Acc: 0.2684\n",
      "  Val Loss: 1.3645\n",
      "  Val Sentiment Acc: 0.4600, F1: 0.2344\n",
      "  Val Emotion Acc: 0.3000, F1: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-13 13:01:54,408] Trial 4 finished with value: 0.15563681232695317 and parameters: {'learning_rate': 2.2207471217033647e-05, 'batch_size': 32, 'warmup_ratio': 0.1808397348116461, 'weight_decay': 0.037415239225603365, 'hidden_dropout_prob': 0.11953442280127678, 'classifier_dropout': 0.23684660530243137, 'alpha': 0.48803049874792026}. Best is trial 2 with value: 0.24113685878391763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTweet multi-task model saved to ./fast_trial_4\\model_best\n",
      "Best BERTweet model saved to ./fast_trial_4\\model_best\n",
      "\n",
      "BERTweet training completed! Best Combined F1: 0.1556\n",
      "⚡ Trial 4: Score=0.1556, Time=0.1min\n",
      "🧹 Memory cleaned!\n",
      "\n",
      "🏆 Fast tuning completed in 0.8 minutes!\n",
      "🎯 Best score: 0.2411\n",
      "📋 Best parameters:\n",
      "  learning_rate: 3.2635193912846855e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.1291229140198042\n",
      "  weight_decay: 0.06506676052501416\n",
      "  hidden_dropout_prob: 0.12789877213040837\n",
      "  classifier_dropout: 0.15842892970704364\n",
      "  alpha: 0.47327236865873834\n",
      "\n",
      "✅ Multitask Hyperparameter Tuning Completed!\n",
      "🏆 Best Combined F1 Score: 0.2411\n",
      "📋 Best Parameters:\n",
      "  learning_rate: 3.2635193912846855e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.1291229140198042\n",
      "  weight_decay: 0.06506676052501416\n",
      "  hidden_dropout_prob: 0.12789877213040837\n",
      "  classifier_dropout: 0.15842892970704364\n",
      "  alpha: 0.47327236865873834\n",
      "🧹 Memory cleaned!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 2: ULTRA-FAST HYPERPARAMETER TUNING - MULTITASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n8️⃣ Fast Hyperparameter Tuning for BERTweet Multi-task Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "multitask_tuner = FastBERTweetHyperparameterTuner(\n",
    "    model_type=\"multitask\",\n",
    "    data_splits=multitask_data,\n",
    "    n_trials=5,  # Reduced trials for speed\n",
    "    model_name=model_name,\n",
    "    subset_ratio=0.02,  # Only 2% of data!\n",
    "    max_epochs_per_trial=2  # Only 2 epochs per trial!\n",
    ")\n",
    "\n",
    "multitask_study = multitask_tuner.tune()\n",
    "\n",
    "print(f\"\\n✅ Multitask Hyperparameter Tuning Completed!\")\n",
    "print(f\"🏆 Best Combined F1 Score: {multitask_study.best_value:.4f}\")\n",
    "print(f\"📋 Best Parameters:\")\n",
    "for key, value in multitask_study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4fdb5b6-0bb5-44f3-8dd0-7ddb7a4c0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\n",
      "================================================================================\n",
      "\n",
      "9️⃣ Training Final BERTweet Sentiment Model with Best Parameters...\n",
      "============================================================\n",
      "🎯 Using best hyperparameters:\n",
      "  learning_rate: 2.679909904436601e-05\n",
      "  batch_size: 32\n",
      "  warmup_ratio: 0.1524756431632238\n",
      "  weight_decay: 0.048875051677790424\n",
      "  hidden_dropout_prob: 0.15824582803960838\n",
      "  classifier_dropout: 0.22237057894447587\n",
      "\n",
      "🚀 Training final sentiment model:\n",
      "  Dataset: Full sentiment data (47144 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 32\n",
      "  Learning rate: 2.68e-05\n",
      "  Max length: 128\n",
      "Starting BERTweet single-task training (sentiment)...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "  Train Loss: 0.5871, Train Acc: 0.7616\n",
      "  Val Loss: 0.4073, Val Acc: 0.8618, Val F1: 0.6012\n",
      "BERTweet single-task model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "Best BERTweet model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 2/5\n",
      "  Train Loss: 0.3952, Train Acc: 0.8664\n",
      "  Val Loss: 0.3813, Val Acc: 0.8719, Val F1: 0.6079\n",
      "BERTweet single-task model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "Best BERTweet model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 3/5\n",
      "  Train Loss: 0.3445, Train Acc: 0.8856\n",
      "  Val Loss: 0.4269, Val Acc: 0.8703, Val F1: 0.6071\n",
      "\n",
      "📍 Epoch 4/5\n",
      "  Train Loss: 0.3144, Train Acc: 0.8964\n",
      "  Val Loss: 0.4415, Val Acc: 0.8746, Val F1: 0.6097\n",
      "BERTweet single-task model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "Best BERTweet model saved to ./final_bertweet_sentiment_model\\model_best\n",
      "\n",
      "📍 Epoch 5/5\n",
      "  Train Loss: 0.2900, Train Acc: 0.9032\n",
      "  Val Loss: 0.4614, Val Acc: 0.8740, Val F1: 0.6094\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.6097\n",
      "📊 Evaluating BERTweet sentiment model...\n",
      "Evaluating on General Dataset...\n",
      "📊 BERTweet Sentiment Results on General Dataset:\n",
      "  Accuracy: 0.8717\n",
      "  F1 Macro: 0.6077\n",
      "Evaluating on Reddit Dataset...\n",
      "📊 BERTweet Sentiment Results on Reddit Dataset:\n",
      "  Accuracy: 0.5789\n",
      "  F1 Macro: 0.3743\n",
      "\n",
      "✅ Final Sentiment Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.8717\n",
      "    F1 Macro: 0.6077\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.5789\n",
      "    F1 Macro: 0.3743\n",
      "\n",
      "📊 Comparison:\n",
      "  Tuning F1 (on subset): 0.4503\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'f1_macro'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m📊 Comparison:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Tuning F1 (on subset): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment_study.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Final F1 (on full test): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfinal_sentiment_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mf1_macro\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Clean up memory\u001b[39;00m\n\u001b[32m     67\u001b[39m aggressive_memory_cleanup()\n",
      "\u001b[31mKeyError\u001b[39m: 'f1_macro'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED SENTIMENT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n9️⃣ Training Final BERTweet Sentiment Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from sentiment tuning\n",
    "best_sentiment_params = sentiment_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_sentiment_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training (full dataset, more epochs)\n",
    "final_sentiment_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_sentiment_params['learning_rate'],\n",
    "    batch_size=best_sentiment_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_sentiment_params['warmup_ratio'],\n",
    "    weight_decay=best_sentiment_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_sentiment_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_sentiment_params['classifier_dropout'],\n",
    "    max_length=best_sentiment_params.get('max_length', 128),  # Fixed: use .get() with default\n",
    "    task_type=\"sentiment\",\n",
    "    output_dir=\"./final_bertweet_sentiment_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final sentiment model:\")\n",
    "print(f\"  Dataset: Full sentiment data ({len(sentiment_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_sentiment_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_sentiment_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_sentiment_config.learning_rate:.2e}\")\n",
    "print(f\"  Max length: {final_sentiment_config.max_length}\")\n",
    "\n",
    "# Train final sentiment model\n",
    "final_sentiment_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=final_sentiment_config,\n",
    "    num_classes=bertweet_model_config.sentiment_num_classes\n",
    ")\n",
    "final_sentiment_history = final_sentiment_trainer.train(sentiment_data)\n",
    "\n",
    "# Evaluate final sentiment model on both general and Reddit datasets\n",
    "final_sentiment_results = evaluate_bertweet_model(\n",
    "    model_path=\"./final_bertweet_sentiment_model/model_best\",\n",
    "    model_type=\"sentiment\",\n",
    "    test_data=sentiment_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['sentiment'] if reddit_data else None\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final Sentiment Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {final_sentiment_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {final_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "if final_sentiment_results.get('reddit'):\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {final_sentiment_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {final_sentiment_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Compare with tuning results\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"  Tuning F1 (on subset): {sentiment_study.best_value:.4f}\")\n",
    "print(f\"  Final F1 (on full test): {final_sentiment_results['f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae7d967d-eb55-452e-9ce3-743be143cd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\n",
      "================================================================================\n",
      "\n",
      "🔟 Training Final BERTweet Emotion Model with Best Parameters...\n",
      "============================================================\n",
      "🎯 Using best hyperparameters:\n",
      "  learning_rate: 8.062340576073854e-05\n",
      "  batch_size: 32\n",
      "  warmup_ratio: 0.10205844942958026\n",
      "  weight_decay: 0.0972918866945795\n",
      "  hidden_dropout_prob: 0.26648852816008434\n",
      "  classifier_dropout: 0.14246782213565523\n",
      "\n",
      "🚀 Training final emotion model:\n",
      "  Dataset: Full emotion data (9534 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 32\n",
      "  Learning rate: 8.06e-05\n",
      "  Max length: 128\n",
      "Starting BERTweet single-task training (emotion)...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "  Train Loss: 1.7240, Train Acc: 0.2849\n",
      "  Val Loss: 1.7061, Val Acc: 0.3030, Val F1: 0.0775\n",
      "BERTweet single-task model saved to ./final_bertweet_emotion_model\\model_best\n",
      "Best BERTweet model saved to ./final_bertweet_emotion_model\\model_best\n",
      "\n",
      "📍 Epoch 2/5\n",
      "  Train Loss: 1.7093, Train Acc: 0.3006\n",
      "  Val Loss: 1.7074, Val Acc: 0.3030, Val F1: 0.0775\n",
      "\n",
      "📍 Epoch 3/5\n",
      "  Train Loss: 1.7022, Train Acc: 0.3026\n",
      "  Val Loss: 1.7219, Val Acc: 0.3030, Val F1: 0.0775\n",
      "\n",
      "📍 Epoch 4/5\n",
      "  Train Loss: 1.7023, Train Acc: 0.3032\n",
      "  Val Loss: 1.7057, Val Acc: 0.3030, Val F1: 0.0775\n",
      "\n",
      "📍 Epoch 5/5\n",
      "  Train Loss: 1.7036, Train Acc: 0.3030\n",
      "  Val Loss: 1.7056, Val Acc: 0.3030, Val F1: 0.0775\n",
      "\n",
      "✅ BERTweet training completed! Best F1: 0.0775\n",
      "📊 Evaluating BERTweet emotion model...\n",
      "Evaluating on General Dataset...\n",
      "📊 BERTweet Emotion Results on General Dataset:\n",
      "  Accuracy: 0.3033\n",
      "  F1 Macro: 0.0776\n",
      "Evaluating on Reddit Dataset...\n",
      "📊 BERTweet Emotion Results on Reddit Dataset:\n",
      "  Accuracy: 0.2526\n",
      "  F1 Macro: 0.0672\n",
      "\n",
      "✅ Final Emotion Model Results:\n",
      "  General Dataset:\n",
      "    Accuracy: 0.3033\n",
      "    F1 Macro: 0.0776\n",
      "  Reddit Dataset:\n",
      "    Accuracy: 0.2526\n",
      "    F1 Macro: 0.0672\n",
      "🧹 Memory cleaned!\n",
      "💾 Final emotion model saved to: ./final_bertweet_emotion_model/\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED EMOTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🔟 Training Final BERTweet Emotion Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from emotion tuning\n",
    "best_emotion_params = emotion_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_emotion_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training (full dataset, more epochs)\n",
    "final_emotion_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_emotion_params['learning_rate'],\n",
    "    batch_size=best_emotion_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_emotion_params['warmup_ratio'],\n",
    "    weight_decay=best_emotion_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_emotion_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_emotion_params['classifier_dropout'],\n",
    "    max_length=best_emotion_params.get('max_length', 128),  # Fixed: use .get() with default\n",
    "    task_type=\"emotion\",\n",
    "    output_dir=\"./final_bertweet_emotion_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final emotion model:\")\n",
    "print(f\"  Dataset: Full emotion data ({len(emotion_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_emotion_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_emotion_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_emotion_config.learning_rate:.2e}\")\n",
    "print(f\"  Max length: {final_emotion_config.max_length}\")\n",
    "\n",
    "# Train final emotion model\n",
    "final_emotion_trainer = BERTweetSingleTaskTrainer(\n",
    "    config=final_emotion_config,\n",
    "    num_classes=bertweet_model_config.emotion_num_classes\n",
    ")\n",
    "final_emotion_history = final_emotion_trainer.train(emotion_data)\n",
    "\n",
    "# Evaluate final emotion model on both general and Reddit datasets\n",
    "final_emotion_results = evaluate_bertweet_model(\n",
    "    model_path=\"./final_bertweet_emotion_model/model_best\",\n",
    "    model_type=\"emotion\",\n",
    "    test_data=emotion_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['emotion'] if reddit_data else None\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final Emotion Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Accuracy: {final_emotion_results['general']['accuracy']:.4f}\")\n",
    "print(f\"    F1 Macro: {final_emotion_results['general']['f1_macro']:.4f}\")\n",
    "if final_emotion_results.get('reddit'):\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Accuracy: {final_emotion_results['reddit']['accuracy']:.4f}\")\n",
    "    print(f\"    F1 Macro: {final_emotion_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n",
    "print(f\"💾 Final emotion model saved to: ./final_bertweet_emotion_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7965233-e7d0-4c0d-8cd8-5654f3cbd4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\n",
      "================================================================================\n",
      "\n",
      "1️⃣1️⃣ Training Final BERTweet Multi-task Model with Best Parameters...\n",
      "============================================================\n",
      "🎯 Using best hyperparameters:\n",
      "  learning_rate: 3.2635193912846855e-05\n",
      "  batch_size: 16\n",
      "  warmup_ratio: 0.1291229140198042\n",
      "  weight_decay: 0.06506676052501416\n",
      "  hidden_dropout_prob: 0.12789877213040837\n",
      "  classifier_dropout: 0.15842892970704364\n",
      "  alpha: 0.47327236865873834\n",
      "\n",
      "🚀 Training final multitask model:\n",
      "  Dataset: Full multitask data (9534 train samples)\n",
      "  Epochs: 5\n",
      "  Batch size: 16\n",
      "  Learning rate: 3.26e-05\n",
      "  Max length: 128\n",
      "  Alpha (loss weighting): 0.473\n",
      "Starting BERTweet multi-task training...\n",
      "\n",
      "📍 Epoch 1/5\n",
      "  Train Loss: 1.3060\n",
      "  Train Sentiment Acc: 0.6708, Train Emotion Acc: 0.2525\n",
      "  Val Loss: 1.1316\n",
      "  Val Sentiment Acc: 0.8311, F1: 0.5797\n",
      "  Val Emotion Acc: 0.2971, F1: 0.0824\n",
      "BERTweet multi-task model saved to ./final_bertweet_multitask_model\\model_best\n",
      "Best BERTweet model saved to ./final_bertweet_multitask_model\\model_best\n",
      "\n",
      "📍 Epoch 2/5\n",
      "  Train Loss: 1.1219\n",
      "  Train Sentiment Acc: 0.8457, Train Emotion Acc: 0.2876\n",
      "  Val Loss: 1.1161\n",
      "  Val Sentiment Acc: 0.8502, F1: 0.5945\n",
      "  Val Emotion Acc: 0.3030, F1: 0.0775\n",
      "BERTweet multi-task model saved to ./final_bertweet_multitask_model\\model_best\n",
      "Best BERTweet model saved to ./final_bertweet_multitask_model\\model_best\n",
      "\n",
      "📍 Epoch 3/5\n",
      "  Train Loss: 1.0635\n",
      "  Train Sentiment Acc: 0.8855, Train Emotion Acc: 0.2976\n",
      "  Val Loss: 1.1213\n",
      "  Val Sentiment Acc: 0.8492, F1: 0.5936\n",
      "  Val Emotion Acc: 0.3030, F1: 0.0775\n",
      "\n",
      "📍 Epoch 4/5\n",
      "  Train Loss: 1.0340\n",
      "  Train Sentiment Acc: 0.9022, Train Emotion Acc: 0.2983\n",
      "  Val Loss: 1.1455\n",
      "  Val Sentiment Acc: 0.8541, F1: 0.5970\n",
      "  Val Emotion Acc: 0.3030, F1: 0.0775\n",
      "BERTweet multi-task model saved to ./final_bertweet_multitask_model\\model_best\n",
      "Best BERTweet model saved to ./final_bertweet_multitask_model\\model_best\n",
      "\n",
      "📍 Epoch 5/5\n",
      "  Train Loss: 1.0132\n",
      "  Train Sentiment Acc: 0.9099, Train Emotion Acc: 0.3024\n",
      "  Val Loss: 1.1653\n",
      "  Val Sentiment Acc: 0.8536, F1: 0.5965\n",
      "  Val Emotion Acc: 0.3030, F1: 0.0775\n",
      "\n",
      "BERTweet training completed! Best Combined F1: 0.3373\n",
      "📊 Evaluating BERTweet multitask model...\n",
      "Evaluating on General Dataset...\n",
      "📊 BERTweet Multi-task Results on General Dataset:\n",
      "  Sentiment - Accuracy: 0.8381, F1: 0.5856\n",
      "  Emotion - Accuracy: 0.3033, F1: 0.0776\n",
      "  Combined - Accuracy: 0.5707, F1: 0.3316\n",
      "Evaluating on Reddit Dataset...\n",
      "📊 BERTweet Multi-task Results on Reddit Dataset:\n",
      "  Sentiment - Accuracy: 0.6211, F1: 0.4234\n",
      "  Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "  Combined - Accuracy: 0.4368, F1: 0.2453\n",
      "\n",
      "✅ Final Multitask Model Results:\n",
      "  General Dataset:\n",
      "    Sentiment - Accuracy: 0.8381, F1: 0.5856\n",
      "    Emotion - Accuracy: 0.3033, F1: 0.0776\n",
      "    Combined - Accuracy: 0.5707, F1: 0.3316\n",
      "  Reddit Dataset:\n",
      "    Sentiment - Accuracy: 0.6211, F1: 0.4234\n",
      "    Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "    Combined - Accuracy: 0.4368, F1: 0.2453\n",
      "🧹 Memory cleaned!\n",
      "💾 Final multitask model saved to: ./final_bertweet_multitask_model/\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 3: FINAL TRAINING - OPTIMIZED MULTITASK MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1️⃣1️⃣ Training Final BERTweet Multi-task Model with Best Parameters...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get best parameters from multitask tuning\n",
    "best_multitask_params = multitask_study.best_params\n",
    "print(f\"🎯 Using best hyperparameters:\")\n",
    "for key, value in best_multitask_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create optimized config for final training (full dataset, more epochs)\n",
    "final_multitask_config = TrainingConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=best_multitask_params['learning_rate'],\n",
    "    batch_size=best_multitask_params['batch_size'],\n",
    "    num_epochs=5,  # Increase epochs for final training\n",
    "    warmup_ratio=best_multitask_params['warmup_ratio'],\n",
    "    weight_decay=best_multitask_params['weight_decay'],\n",
    "    hidden_dropout_prob=best_multitask_params['hidden_dropout_prob'],\n",
    "    classifier_dropout=best_multitask_params['classifier_dropout'],\n",
    "    max_length=best_multitask_params.get('max_length', 128),  # Fixed: use .get() with default\n",
    "    alpha=best_multitask_params['alpha'],  # Multitask-specific parameter\n",
    "    task_type=\"multitask\",\n",
    "    output_dir=\"./final_bertweet_multitask_model\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🚀 Training final multitask model:\")\n",
    "print(f\"  Dataset: Full multitask data ({len(multitask_data['train']['texts'])} train samples)\")\n",
    "print(f\"  Epochs: {final_multitask_config.num_epochs}\")\n",
    "print(f\"  Batch size: {final_multitask_config.batch_size}\")\n",
    "print(f\"  Learning rate: {final_multitask_config.learning_rate:.2e}\")\n",
    "print(f\"  Max length: {final_multitask_config.max_length}\")\n",
    "print(f\"  Alpha (loss weighting): {final_multitask_config.alpha:.3f}\")\n",
    "\n",
    "# Train final multitask model\n",
    "final_multitask_trainer = BERTweetMultiTaskTrainer(config=final_multitask_config)\n",
    "final_multitask_history = final_multitask_trainer.train(multitask_data)\n",
    "\n",
    "# Evaluate final multitask model on both general and Reddit datasets\n",
    "final_multitask_results = evaluate_bertweet_model(\n",
    "    model_path=\"./final_bertweet_multitask_model/model_best\",\n",
    "    model_type=\"multitask\",\n",
    "    test_data=multitask_data['test'],\n",
    "    model_name=model_name,\n",
    "    reddit_data=reddit_data['multitask'] if reddit_data else None\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final Multitask Model Results:\")\n",
    "print(f\"  General Dataset:\")\n",
    "print(f\"    Sentiment - Accuracy: {final_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"    Emotion - Accuracy: {final_multitask_results['general']['emotion_accuracy']:.4f}, F1: {final_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"    Combined - Accuracy: {final_multitask_results['general']['combined_accuracy']:.4f}, F1: {final_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if final_multitask_results.get('reddit'):\n",
    "    print(f\"  Reddit Dataset:\")\n",
    "    print(f\"    Sentiment - Accuracy: {final_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"    Emotion - Accuracy: {final_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {final_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"    Combined - Accuracy: {final_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {final_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Clean up memory\n",
    "aggressive_memory_cleanup()\n",
    "print(f\"💾 Final multitask model saved to: ./final_bertweet_multitask_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7ce227a-a36c-4863-a8b7-4950aa4e9211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📍 PHASE 4: COMPREHENSIVE RESULTS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "📊 BERTWEET HYPERPARAMETER TUNING & FINAL TRAINING RESULTS\n",
      "============================================================\n",
      "\n",
      "🎯 HYPERPARAMETER TUNING PERFORMANCE (on small subsets):\n",
      "  Sentiment Model:\n",
      "    Best F1 Score: 0.4503\n",
      "    Key Parameters: LR=2.68e-05, Batch=32\n",
      "\n",
      "  Emotion Model:\n",
      "    Best F1 Score: 0.1342\n",
      "    Key Parameters: LR=8.06e-05, Batch=32\n",
      "\n",
      "  Multitask Model:\n",
      "    Best Combined F1 Score: 0.2411\n",
      "    Key Parameters: LR=3.26e-05, Alpha=0.473\n",
      "\n",
      "🏆 FINAL MODEL PERFORMANCE (on full test sets):\n",
      "  Sentiment Model:\n",
      "    General Dataset - Accuracy: 0.8717, F1: 0.6077\n",
      "    Reddit Dataset - Accuracy: 0.5789, F1: 0.3743\n",
      "\n",
      "  Emotion Model:\n",
      "    General Dataset - Accuracy: 0.3033, F1: 0.0776\n",
      "    Reddit Dataset - Accuracy: 0.2526, F1: 0.0672\n",
      "\n",
      "  Multitask Model:\n",
      "    General Dataset:\n",
      "      Sentiment - Accuracy: 0.8381, F1: 0.5856\n",
      "      Emotion - Accuracy: 0.3033, F1: 0.0776\n",
      "      Combined - Accuracy: 0.5707, F1: 0.3316\n",
      "    Reddit Dataset:\n",
      "      Sentiment - Accuracy: 0.6211, F1: 0.4234\n",
      "      Emotion - Accuracy: 0.2526, F1: 0.0672\n",
      "      Combined - Accuracy: 0.4368, F1: 0.2453\n",
      "\n",
      "📈 TUNING vs FINAL PERFORMANCE COMPARISON:\n",
      "  Sentiment:\n",
      "    Tuning F1 (subset): 0.4503\n",
      "    Final F1 (full):    0.6077\n",
      "    Improvement:        +0.1574 ✅\n",
      "\n",
      "  Emotion:\n",
      "    Tuning F1 (subset): 0.1342\n",
      "    Final F1 (full):    0.0776\n",
      "    Improvement:        -0.0566 ⚠️\n",
      "\n",
      "  Multitask:\n",
      "    Tuning Combined F1 (subset): 0.2411\n",
      "    Final Combined F1 (full):    0.3316\n",
      "    Improvement:                 +0.0904 ✅\n",
      "\n",
      "📁 MODEL LOCATIONS:\n",
      "  📦 Sentiment model: ./final_bertweet_sentiment_model/\n",
      "  📦 Emotion model: ./final_bertweet_emotion_model/\n",
      "  📦 Multitask model: ./final_bertweet_multitask_model/\n",
      "\n",
      "📄 RESULTS SAVED:\n",
      "  📊 Comprehensive summary: ./comprehensive_bertweet_results_summary.json\n",
      "\n",
      "🎉 COMPLETE BERTWEET PIPELINE FINISHED!\n",
      "✅ Fast hyperparameter tuning + optimized final training completed!\n",
      "🚀 Pipeline completed in a fraction of the original time!\n",
      "\n",
      "================================================================================\n",
      "🏁 COMPREHENSIVE BERTWEET RESULTS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "📊 BERTWEET MODEL PERFORMANCE COMPARISON:\n",
      "  ============================================================\n",
      "\n",
      "🎯 SENTIMENT MODEL:\n",
      "  Initial Model:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'initial_sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🎯 SENTIMENT MODEL:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Initial Model:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    General Dataset - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mall_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minitial_sentiment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mgeneral\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_results[\u001b[33m'\u001b[39m\u001b[33minitial_sentiment\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgeneral\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_results[\u001b[33m'\u001b[39m\u001b[33minitial_sentiment\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mreddit\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Reddit Dataset - Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_results[\u001b[33m'\u001b[39m\u001b[33minitial_sentiment\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mreddit\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_results[\u001b[33m'\u001b[39m\u001b[33minitial_sentiment\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mreddit\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'initial_sentiment'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📍 PHASE 4: COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 BERTWEET HYPERPARAMETER TUNING & FINAL TRAINING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display hyperparameter tuning results\n",
    "print(f\"\\n🎯 HYPERPARAMETER TUNING PERFORMANCE (on small subsets):\")\n",
    "print(f\"  Sentiment Model:\")\n",
    "print(f\"    Best F1 Score: {sentiment_study.best_value:.4f}\")\n",
    "print(f\"    Key Parameters: LR={sentiment_study.best_params['learning_rate']:.2e}, Batch={sentiment_study.best_params['batch_size']}\")\n",
    "\n",
    "print(f\"\\n  Emotion Model:\")\n",
    "print(f\"    Best F1 Score: {emotion_study.best_value:.4f}\")\n",
    "print(f\"    Key Parameters: LR={emotion_study.best_params['learning_rate']:.2e}, Batch={emotion_study.best_params['batch_size']}\")\n",
    "\n",
    "print(f\"\\n  Multitask Model:\")\n",
    "print(f\"    Best Combined F1 Score: {multitask_study.best_value:.4f}\")\n",
    "print(f\"    Key Parameters: LR={multitask_study.best_params['learning_rate']:.2e}, Alpha={multitask_study.best_params['alpha']:.3f}\")\n",
    "\n",
    "# Display final model results (only using available metrics)\n",
    "print(f\"\\n🏆 FINAL MODEL PERFORMANCE (on full test sets):\")\n",
    "print(f\"  Sentiment Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {final_sentiment_results['general']['accuracy']:.4f}, F1: {final_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "if final_sentiment_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {final_sentiment_results['reddit']['accuracy']:.4f}, F1: {final_sentiment_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Emotion Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {final_emotion_results['general']['accuracy']:.4f}, F1: {final_emotion_results['general']['f1_macro']:.4f}\")\n",
    "if final_emotion_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {final_emotion_results['reddit']['accuracy']:.4f}, F1: {final_emotion_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Multitask Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {final_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {final_multitask_results['general']['emotion_accuracy']:.4f}, F1: {final_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {final_multitask_results['general']['combined_accuracy']:.4f}, F1: {final_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if final_multitask_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {final_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {final_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {final_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {final_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {final_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Performance comparison between tuning and final results\n",
    "print(f\"\\n📈 TUNING vs FINAL PERFORMANCE COMPARISON:\")\n",
    "sentiment_improvement = final_sentiment_results['general']['f1_macro'] - sentiment_study.best_value\n",
    "emotion_improvement = final_emotion_results['general']['f1_macro'] - emotion_study.best_value\n",
    "multitask_improvement = final_multitask_results['general']['combined_f1_macro'] - multitask_study.best_value\n",
    "\n",
    "print(f\"  Sentiment:\")\n",
    "print(f\"    Tuning F1 (subset): {sentiment_study.best_value:.4f}\")\n",
    "print(f\"    Final F1 (full):    {final_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "print(f\"    Improvement:        {sentiment_improvement:+.4f} {'✅' if sentiment_improvement > 0 else '⚠️'}\")\n",
    "\n",
    "print(f\"\\n  Emotion:\")\n",
    "print(f\"    Tuning F1 (subset): {emotion_study.best_value:.4f}\")\n",
    "print(f\"    Final F1 (full):    {final_emotion_results['general']['f1_macro']:.4f}\")\n",
    "print(f\"    Improvement:        {emotion_improvement:+.4f} {'✅' if emotion_improvement > 0 else '⚠️'}\")\n",
    "\n",
    "print(f\"\\n  Multitask:\")\n",
    "print(f\"    Tuning Combined F1 (subset): {multitask_study.best_value:.4f}\")\n",
    "print(f\"    Final Combined F1 (full):    {final_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "print(f\"    Improvement:                 {multitask_improvement:+.4f} {'✅' if multitask_improvement > 0 else '⚠️'}\")\n",
    "\n",
    "# Create comprehensive results summary\n",
    "results_summary = {\n",
    "    'model_type': 'BERTweet',\n",
    "    'model_name': model_name,\n",
    "    'pipeline_type': 'Fast Hyperparameter Tuning + Final Training',\n",
    "    'hyperparameter_tuning': {\n",
    "        'method': 'Fast Random Search',\n",
    "        'subset_ratio': 0.02,\n",
    "        'trials_per_model': 6,\n",
    "        'epochs_per_trial': 2,\n",
    "        'sentiment': {\n",
    "            'best_f1': float(sentiment_study.best_value),\n",
    "            'best_params': sentiment_study.best_params\n",
    "        },\n",
    "        'emotion': {\n",
    "            'best_f1': float(emotion_study.best_value),\n",
    "            'best_params': emotion_study.best_params\n",
    "        },\n",
    "        'multitask': {\n",
    "            'best_combined_f1': float(multitask_study.best_value),\n",
    "            'best_params': multitask_study.best_params\n",
    "        }\n",
    "    },\n",
    "    'final_models': {\n",
    "        'sentiment': final_sentiment_results,\n",
    "        'emotion': final_emotion_results,\n",
    "        'multitask': final_multitask_results\n",
    "    },\n",
    "    'performance_improvements': {\n",
    "        'sentiment_f1_improvement': float(sentiment_improvement),\n",
    "        'emotion_f1_improvement': float(emotion_improvement),\n",
    "        'multitask_f1_improvement': float(multitask_improvement)\n",
    "    },\n",
    "    'model_locations': {\n",
    "        'sentiment': './final_bertweet_sentiment_model/',\n",
    "        'emotion': './final_bertweet_emotion_model/',\n",
    "        'multitask': './final_bertweet_multitask_model/'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "with open('comprehensive_bertweet_results_summary.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n📁 MODEL LOCATIONS:\")\n",
    "print(f\"  📦 Sentiment model: ./final_bertweet_sentiment_model/\")\n",
    "print(f\"  📦 Emotion model: ./final_bertweet_emotion_model/\")\n",
    "print(f\"  📦 Multitask model: ./final_bertweet_multitask_model/\")\n",
    "\n",
    "print(f\"\\n📄 RESULTS SAVED:\")\n",
    "print(f\"  📊 Comprehensive summary: ./comprehensive_bertweet_results_summary.json\")\n",
    "\n",
    "print(f\"\\n🎉 COMPLETE BERTWEET PIPELINE FINISHED!\")\n",
    "print(f\"✅ Fast hyperparameter tuning + optimized final training completed!\")\n",
    "print(f\"🚀 Pipeline completed in a fraction of the original time!\")\n",
    "\n",
    "# Add comprehensive results comparison\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"🏁 COMPREHENSIVE BERTWEET RESULTS COMPARISON\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 BERTWEET MODEL PERFORMANCE COMPARISON:\")\n",
    "print(f\"  {'='*60}\")\n",
    "\n",
    "# Sentiment Model Comparison\n",
    "print(f\"\\n🎯 SENTIMENT MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['initial_sentiment']['general']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['general']['f1_macro']:.4f}\")\n",
    "if all_results['initial_sentiment'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['initial_sentiment']['reddit']['accuracy']:.4f}, F1: {all_results['initial_sentiment']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {final_sentiment_results['general']['accuracy']:.4f}, F1: {final_sentiment_results['general']['f1_macro']:.4f}\")\n",
    "if final_sentiment_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {final_sentiment_results['reddit']['accuracy']:.4f}, F1: {final_sentiment_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "sentiment_general_improvement = final_sentiment_results['general']['accuracy'] - all_results['initial_sentiment']['general']['accuracy']\n",
    "sentiment_f1_improvement = final_sentiment_results['general']['f1_macro'] - all_results['initial_sentiment']['general']['f1_macro']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    General Accuracy: {sentiment_general_improvement:+.4f}\")\n",
    "print(f\"    General F1: {sentiment_f1_improvement:+.4f}\")\n",
    "\n",
    "# Emotion Model Comparison\n",
    "print(f\"\\n🎭 EMOTION MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {all_results['initial_emotion']['general']['accuracy']:.4f}, F1: {all_results['initial_emotion']['general']['f1_macro']:.4f}\")\n",
    "if all_results['initial_emotion'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {all_results['initial_emotion']['reddit']['accuracy']:.4f}, F1: {all_results['initial_emotion']['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset - Accuracy: {final_emotion_results['general']['accuracy']:.4f}, F1: {final_emotion_results['general']['f1_macro']:.4f}\")\n",
    "if final_emotion_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset - Accuracy: {final_emotion_results['reddit']['accuracy']:.4f}, F1: {final_emotion_results['reddit']['f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "emotion_general_improvement = final_emotion_results['general']['accuracy'] - all_results['initial_emotion']['general']['accuracy']\n",
    "emotion_f1_improvement = final_emotion_results['general']['f1_macro'] - all_results['initial_emotion']['general']['f1_macro']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    General Accuracy: {emotion_general_improvement:+.4f}\")\n",
    "print(f\"    General F1: {emotion_f1_improvement:+.4f}\")\n",
    "\n",
    "# Multitask Model Comparison\n",
    "print(f\"\\n🔄 MULTITASK MODEL:\")\n",
    "print(f\"  Initial Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {all_results['initial_multitask']['general']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {all_results['initial_multitask']['general']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {all_results['initial_multitask']['general']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['general']['combined_f1_macro']:.4f}\")\n",
    "if all_results['initial_multitask'].get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {all_results['initial_multitask']['reddit']['sentiment_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {all_results['initial_multitask']['reddit']['emotion_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {all_results['initial_multitask']['reddit']['combined_accuracy']:.4f}, F1: {all_results['initial_multitask']['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"  Final Optimized Model:\")\n",
    "print(f\"    General Dataset:\")\n",
    "print(f\"      Sentiment - Accuracy: {final_multitask_results['general']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['general']['sentiment_f1_macro']:.4f}\")\n",
    "print(f\"      Emotion - Accuracy: {final_multitask_results['general']['emotion_accuracy']:.4f}, F1: {final_multitask_results['general']['emotion_f1_macro']:.4f}\")\n",
    "print(f\"      Combined - Accuracy: {final_multitask_results['general']['combined_accuracy']:.4f}, F1: {final_multitask_results['general']['combined_f1_macro']:.4f}\")\n",
    "if final_multitask_results.get('reddit'):\n",
    "    print(f\"    Reddit Dataset:\")\n",
    "    print(f\"      Sentiment - Accuracy: {final_multitask_results['reddit']['sentiment_accuracy']:.4f}, F1: {final_multitask_results['reddit']['sentiment_f1_macro']:.4f}\")\n",
    "    print(f\"      Emotion - Accuracy: {final_multitask_results['reddit']['emotion_accuracy']:.4f}, F1: {final_multitask_results['reddit']['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"      Combined - Accuracy: {final_multitask_results['reddit']['combined_accuracy']:.4f}, F1: {final_multitask_results['reddit']['combined_f1_macro']:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "multitask_sentiment_improvement = final_multitask_results['general']['sentiment_accuracy'] - all_results['initial_multitask']['general']['sentiment_accuracy']\n",
    "multitask_emotion_improvement = final_multitask_results['general']['emotion_accuracy'] - all_results['initial_multitask']['general']['emotion_accuracy']\n",
    "multitask_combined_improvement = final_multitask_results['general']['combined_accuracy'] - all_results['initial_multitask']['general']['combined_accuracy']\n",
    "print(f\"  Improvements:\")\n",
    "print(f\"    Sentiment Accuracy: {multitask_sentiment_improvement:+.4f}\")\n",
    "print(f\"    Emotion Accuracy: {multitask_emotion_improvement:+.4f}\")\n",
    "print(f\"    Combined Accuracy: {multitask_combined_improvement:+.4f}\")\n",
    "\n",
    "print(f\"\\n🎉 BERTWEET TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   All models trained and evaluated on both general and Reddit datasets\")\n",
    "print(f\"   Hyperparameter optimization completed using macro F1 on general datasets\")\n",
    "print(f\"   Final models saved and ready for deployment\")\n",
    "\n",
    "# Display final summary table\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"📋 FINAL PERFORMANCE SUMMARY\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"{'Model':<12} {'Accuracy':<10} {'F1 Score':<10} {'Improvement':<12} {'Status':<10}\")\n",
    "print(f\"-\" * 65)\n",
    "print(f\"{'Sentiment':<12} {final_sentiment_results['general']['accuracy']:<10.4f} {final_sentiment_results['general']['f1_macro']:<10.4f} {sentiment_improvement:+10.4f} {'✅ Complete':<10}\")\n",
    "print(f\"{'Emotion':<12} {final_emotion_results['general']['accuracy']:<10.4f} {final_emotion_results['general']['f1_macro']:<10.4f} {emotion_improvement:+10.4f} {'✅ Complete':<10}\")\n",
    "print(f\"{'Multitask':<12} {final_multitask_results['general']['combined_accuracy']:<10.4f} {final_multitask_results['general']['combined_f1_macro']:<10.4f} {multitask_improvement:+10.4f} {'✅ Complete':<10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n🐦 BERTweet models are ready for social media text processing!\")\n",
    "print(f\"💡 All models trained with optimized hyperparameters found via fast search!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa876afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
