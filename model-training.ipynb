{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a263aba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset as HFDataset, load_dataset\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix,\n",
    "    classification_report, cohen_kappa_score, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b90630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4bdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaTrainer:\n",
    "    \n",
    "    def __init__(self, model_name: str = \"roberta-base\", max_length: int = 512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.sentiment_model = None\n",
    "        self.emotion_model = None\n",
    "        self.sentiment_label_encoder = LabelEncoder()\n",
    "        self.emotion_label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(\"./roberta_sentiment_model\", exist_ok=True)\n",
    "        os.makedirs(\"./roberta_emotion_model\", exist_ok=True)\n",
    "        os.makedirs(\"./plots/roberta\", exist_ok=True)\n",
    "        os.makedirs(\"./results/roberta\", exist_ok=True)\n",
    "    \n",
    "    def setup_tokenizer(self):\n",
    "        logger.info(\"Setting up RoBERTa tokenizer...\")\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Add padding token if it doesn't exist\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        logger.info(\"‚úÖ Tokenizer ready!\")\n",
    "    \n",
    "    def load_reddit_data(self, file_path: str) -> pd.DataFrame:\n",
    "        logger.info(f\"Loading Reddit data from {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            logger.info(f\"‚úÖ Loaded {len(df)} samples from Reddit dataset\")\n",
    "            \n",
    "            # Check for required columns\n",
    "            required_cols = ['text_content', 'sentiment', 'emotion']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "            \n",
    "            # Remove rows with missing values\n",
    "            df = df.dropna(subset=required_cols)\n",
    "            logger.info(f\"After removing missing values: {len(df)} samples\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Reddit data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_external_datasets(self) -> Tuple[Dict, Dict]:\n",
    "        logger.info(\"Loading external datasets...\")\n",
    "        \n",
    "        # Load SST-2 for sentiment\n",
    "        try:\n",
    "            sst2_dataset = load_dataset(\"sst2\")\n",
    "            sentiment_data = {\n",
    "                'train': sst2_dataset['train'],\n",
    "                'validation': sst2_dataset['validation']\n",
    "            }\n",
    "            logger.info(\"‚úÖ SST-2 dataset loaded for sentiment classification\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load SST-2: {e}. Using dummy data.\")\n",
    "            sentiment_data = self._create_dummy_sentiment_data()\n",
    "        \n",
    "        # Load GoEmotions for emotion\n",
    "        try:\n",
    "            emotions_dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "            emotion_data = {\n",
    "                'train': emotions_dataset['train'],\n",
    "                'validation': emotions_dataset['validation']\n",
    "            }\n",
    "            logger.info(\"‚úÖ GoEmotions dataset loaded for emotion classification\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load GoEmotions: {e}. Using dummy data.\")\n",
    "            emotion_data = self._create_dummy_emotion_data()\n",
    "        \n",
    "        return sentiment_data, emotion_data\n",
    "    \n",
    "    def _create_dummy_sentiment_data(self) -> Dict:\n",
    "        dummy_texts = [\n",
    "            \"I love this product!\", \"This is terrible\", \"It's okay\",\n",
    "            \"Amazing quality\", \"Worst experience ever\", \"Not bad\"\n",
    "        ] * 100\n",
    "        dummy_labels = [1, 0, 1, 1, 0, 1] * 100\n",
    "        \n",
    "        dummy_data = {\n",
    "            'sentence': dummy_texts,\n",
    "            'label': dummy_labels\n",
    "        }\n",
    "        \n",
    "        dataset = HFDataset.from_dict(dummy_data)\n",
    "        return {'train': dataset, 'validation': dataset.select(range(100))}\n",
    "    \n",
    "    def _create_dummy_emotion_data(self) -> Dict:\n",
    "        \"\"\"Create dummy emotion data for testing\"\"\"\n",
    "        dummy_texts = [\n",
    "            \"I'm so happy!\", \"This is sad\", \"I'm angry\", \"That's scary\",\n",
    "            \"What a surprise!\", \"Okay\", \"I admire you\", \"That's funny\"\n",
    "        ] * 100\n",
    "        dummy_labels = [0, 1, 2, 3, 4, 5, 6, 7] * 100\n",
    "        \n",
    "        dummy_data = {\n",
    "            'text': dummy_texts,\n",
    "            'labels': dummy_labels\n",
    "        }\n",
    "        \n",
    "        dataset = HFDataset.from_dict(dummy_data)\n",
    "        return {'train': dataset, 'validation': dataset.select(range(100))}\n",
    "    \n",
    "    def preprocess_external_data(self, dataset: Dict, task: str) -> Dict:\n",
    "        \"\"\"Preprocess external datasets\"\"\"\n",
    "        logger.info(f\"Preprocessing external data for {task}...\")\n",
    "\n",
    "        # --- ADD THIS BLOCK FOR EMOTION TASK ---\n",
    "        if task == 'emotion':\n",
    "            # Only keep samples with label in [0, 1, 2, 3, 4, 5]\n",
    "            def filter_first_6_classes(example):\n",
    "                # Handle both single-label and multi-label\n",
    "                if isinstance(example['labels'], list):\n",
    "                    return example['labels'] and example['labels'][0] in range(6)\n",
    "                else:\n",
    "                    return example['labels'] in range(6)\n",
    "            dataset['train'] = dataset['train'].filter(filter_first_6_classes)\n",
    "            dataset['validation'] = dataset['validation'].filter(filter_first_6_classes)\n",
    "        # --- END BLOCK ---\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            if task == 'sentiment':\n",
    "                texts = examples['sentence']\n",
    "                labels = examples['label']\n",
    "            else:  # emotion\n",
    "                texts = examples['text']\n",
    "                # Handle multi-label to single-label conversion\n",
    "                if isinstance(examples['labels'][0], list):\n",
    "                    labels = [label_list[0] if label_list else 0 for label_list in examples['labels']]\n",
    "                else:\n",
    "                    labels = examples['labels']\n",
    "\n",
    "            tokenized = self.tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=None\n",
    "            )\n",
    "\n",
    "            tokenized['labels'] = labels\n",
    "            return tokenized\n",
    "\n",
    "        # Tokenize datasets\n",
    "        tokenized_train = dataset['train'].map(tokenize_function, batched=True)\n",
    "        tokenized_val = dataset['validation'].map(tokenize_function, batched=True)\n",
    "\n",
    "        # Take subsets for faster training\n",
    "        train_subset = tokenized_train.shuffle(seed=42).select(range(min(10000, len(tokenized_train))))\n",
    "        val_subset = tokenized_val.shuffle(seed=42).select(range(min(2000, len(tokenized_val))))\n",
    "\n",
    "        logger.info(f\"‚úÖ Preprocessed {task} data: {len(train_subset)} train, {len(val_subset)} val\")\n",
    "\n",
    "        return {'train': train_subset, 'validation': val_subset}\n",
    "    \n",
    "    def train_model(self, dataset: Dict, task: str, num_labels: int, \n",
    "                   learning_rate: float = 2e-5, num_epochs: int = 3,\n",
    "                   batch_size: int = 16) -> Tuple[any, any]:\n",
    "        \"\"\"Train RoBERTa model for specific task\"\"\"\n",
    "        logger.info(f\"Training RoBERTa model for {task} classification...\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=num_labels,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        output_dir = f\"./roberta_{task}_model\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f\"./logs_{task}\",\n",
    "            logging_steps=100,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_accuracy\",\n",
    "            learning_rate=learning_rate,\n",
    "            report_to=None,\n",
    "            save_total_limit=2,\n",
    "            dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        \n",
    "        # Compute metrics function\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            \n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                labels, predictions, average='macro', zero_division=0\n",
    "            )\n",
    "            accuracy = accuracy_score(labels, predictions)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset['validation'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        logger.info(f\"üöÄ Starting {task} model training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        final_output_dir = f\"./roberta_{task}_model_final\"\n",
    "        os.makedirs(final_output_dir, exist_ok=True)\n",
    "        trainer.save_model(final_output_dir)\n",
    "        \n",
    "        logger.info(f\"‚úÖ {task.capitalize()} model training completed!\")\n",
    "        \n",
    "        return model, trainer\n",
    "    \n",
    "    def prepare_reddit_data(self, df: pd.DataFrame, task: str) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"Prepare Reddit data for training/evaluation\"\"\"\n",
    "        texts = df['text_content'].tolist()\n",
    "        \n",
    "        if task == 'sentiment':\n",
    "            labels = df['sentiment'].tolist()\n",
    "            encoded_labels = self.sentiment_label_encoder.fit_transform(labels)\n",
    "        else:  # emotion\n",
    "            labels = df['emotion'].tolist()\n",
    "            encoded_labels = self.emotion_label_encoder.fit_transform(labels)\n",
    "        \n",
    "        return texts, encoded_labels.tolist()\n",
    "    \n",
    "    def evaluate_model(self, model, texts: List[str], true_labels: List[int], \n",
    "                      task: str, label_encoder) -> Dict:\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        logger.info(f\"Evaluating {task} model...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        dataset = RobertaDataset(texts, true_labels, self.tokenizer, self.max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        # Get predictions\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        prediction_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Get predictions and probabilities\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                prediction_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "            true_labels, predictions, average='macro', zero_division=0\n",
    "        )\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "            true_labels, predictions, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Cohen's Kappa\n",
    "        kappa = cohen_kappa_score(true_labels, predictions)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(true_labels, predictions)\n",
    "        \n",
    "        # Classification Report\n",
    "        class_report = classification_report(\n",
    "            true_labels, predictions, \n",
    "            target_names=label_encoder.classes_, \n",
    "            output_dict=True, zero_division=0\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'f1_macro': f1_macro,\n",
    "            'precision_weighted': precision_weighted,\n",
    "            'recall_weighted': recall_weighted,\n",
    "            'f1_weighted': f1_weighted,\n",
    "            'cohen_kappa': kappa,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': class_report,\n",
    "            'predictions': predictions,\n",
    "            'prediction_probs': prediction_probs,\n",
    "            'true_labels': true_labels,\n",
    "            'label_names': label_encoder.classes_\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        logger.info(f\"\\n{task.upper()} CLASSIFICATION RESULTS:\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(f\"Accuracy:           {accuracy:.4f}\")\n",
    "        logger.info(f\"Precision (macro):  {precision_macro:.4f}\")\n",
    "        logger.info(f\"Recall (macro):     {recall_macro:.4f}\")\n",
    "        logger.info(f\"F1-score (macro):   {f1_macro:.4f}\")\n",
    "        logger.info(f\"F1-score (weighted): {f1_weighted:.4f}\")\n",
    "        logger.info(f\"Cohen's Kappa:      {kappa:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_confusion_matrix(self, results: Dict, task: str, save_path: str = None):\n",
    "        \"\"\"Create and save confusion matrix\"\"\"\n",
    "        cm = results['confusion_matrix']\n",
    "        labels = results['label_names']\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f'{task.capitalize()} Classification - Confusion Matrix', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Predicted', fontsize=12)\n",
    "        plt.ylabel('True', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Confusion matrix saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def create_roc_curves(self, results: Dict, task: str, save_path: str = None):\n",
    "        \"\"\"Create ROC curves for multi-class classification\"\"\"\n",
    "        y_true = results['true_labels']\n",
    "        y_prob = np.array(results['prediction_probs'])\n",
    "        labels = results['label_names']\n",
    "        n_classes = len(labels)\n",
    "        \n",
    "        # Binarize labels for multi-class ROC\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Calculate ROC curve for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot ROC curves\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, n_classes))\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                    label=f'{labels[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title(f'{task.capitalize()} Classification - ROC Curves', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "        if save_path:\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"ROC curves saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def create_precision_recall_curves(self, results: Dict, task: str, save_path: str = None):\n",
    "        \"\"\"Create Precision-Recall curves\"\"\"\n",
    "        y_true = results['true_labels']\n",
    "        y_prob = np.array(results['prediction_probs'])\n",
    "        labels = results['label_names']\n",
    "        n_classes = len(labels)\n",
    "        \n",
    "        # Binarize labels\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Calculate PR curve for each class\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        ap_score = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            ap_score[i] = average_precision_score(y_true_bin[:, i], y_prob[:, i])\n",
    "        \n",
    "        # Plot PR curves\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, n_classes))\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                    label=f'{labels[i]} (AP = {ap_score[i]:.2f})')\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title(f'{task.capitalize()} Classification - Precision-Recall Curves', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        \n",
    "        if save_path:\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"PR curves saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def hyperparameter_tuning(self, dataset: Dict, task: str, num_labels: int) -> Dict:\n",
    "        \"\"\"Perform hyperparameter tuning\"\"\"\n",
    "        logger.info(f\"Starting hyperparameter tuning for {task}...\")\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'learning_rate': [1e-5, 2e-5, 3e-5, 5e-5],\n",
    "            'batch_size': [8, 16, 32],\n",
    "            'num_epochs': [2, 3, 4],\n",
    "            'weight_decay': [0.01, 0.1]\n",
    "        }\n",
    "        \n",
    "        best_score = 0\n",
    "        best_params = {}\n",
    "        results = []\n",
    "        \n",
    "        # Random search over parameters\n",
    "        from itertools import product\n",
    "        import random\n",
    "        \n",
    "        # Generate all combinations and sample randomly\n",
    "        all_combinations = list(product(*param_grid.values()))\n",
    "        random.shuffle(all_combinations)\n",
    "        \n",
    "        # Try up to 10 combinations\n",
    "        for i, params in enumerate(all_combinations[:10]):\n",
    "            param_dict = dict(zip(param_grid.keys(), params))\n",
    "            logger.info(f\"Trying parameters {i+1}/10: {param_dict}\")\n",
    "            \n",
    "            try:\n",
    "                # Train model with current parameters\n",
    "                model, trainer = self.train_model(\n",
    "                    dataset, task, num_labels,\n",
    "                    learning_rate=param_dict['learning_rate'],\n",
    "                    num_epochs=param_dict['num_epochs'],\n",
    "                    batch_size=param_dict['batch_size']\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                eval_results = trainer.evaluate()\n",
    "                score = eval_results['eval_accuracy']\n",
    "                \n",
    "                results.append({\n",
    "                    'params': param_dict,\n",
    "                    'score': score,\n",
    "                    'eval_results': eval_results\n",
    "                })\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = param_dict\n",
    "                    \n",
    "                logger.info(f\"Score: {score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error with parameters {param_dict}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Best parameters for {task}: {best_params}\")\n",
    "        logger.info(f\"Best score: {best_score:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score,\n",
    "            'all_results': results\n",
    "        }\n",
    "    \n",
    "    def save_results(self, results: Dict, task: str, filename: str = None):\n",
    "        \"\"\"Save evaluation results to file\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"results/roberta_{task}_results_{timestamp}.json\"\n",
    "        \n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        results_copy = results.copy()\n",
    "        for key, value in results_copy.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                results_copy[key] = value.tolist()\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results_copy, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"Results saved to {filename}\")\n",
    "    \n",
    "    def create_learning_curves(self, trainer, task: str, save_path: str = None):\n",
    "        \"\"\"Create learning curves from training history\"\"\"\n",
    "        # Get training history\n",
    "        history = trainer.state.log_history\n",
    "        \n",
    "        train_losses = []\n",
    "        eval_losses = []\n",
    "        eval_accuracies = []\n",
    "        \n",
    "        for entry in history:\n",
    "            if 'train_loss' in entry:\n",
    "                train_losses.append(entry['train_loss'])\n",
    "            if 'eval_loss' in entry:\n",
    "                eval_losses.append(entry['eval_loss'])\n",
    "            if 'eval_accuracy' in entry:\n",
    "                eval_accuracies.append(entry['eval_accuracy'])\n",
    "        \n",
    "        # Create plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss curves\n",
    "        epochs = range(1, len(eval_losses) + 1)\n",
    "        ax1.plot(epochs, eval_losses, 'b-', label='Validation Loss')\n",
    "        if len(train_losses) > 0:\n",
    "            train_epochs = np.linspace(1, len(eval_losses), len(train_losses))\n",
    "            ax1.plot(train_epochs, train_losses, 'r-', label='Training Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'{task.capitalize()} - Learning Curves (Loss)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy curve\n",
    "        ax2.plot(epochs, eval_accuracies, 'g-', label='Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title(f'{task.capitalize()} - Learning Curves (Accuracy)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Learning curves saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def run_complete_pipeline(self, reddit_data_path: str, \n",
    "                             perform_hyperparameter_tuning: bool = True):\n",
    "        \"\"\"Run the complete training and evaluation pipeline\"\"\"\n",
    "        logger.info(\"üöÄ Starting RoBERTa Complete Training Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Setup\n",
    "        self.setup_tokenizer()\n",
    "        \n",
    "        # Load data\n",
    "        reddit_df = self.load_reddit_data(reddit_data_path)\n",
    "        sentiment_data, emotion_data = self.load_external_datasets()\n",
    "        \n",
    "        # Preprocess external data\n",
    "        sentiment_processed = self.preprocess_external_data(sentiment_data, 'sentiment')\n",
    "        emotion_processed = self.preprocess_external_data(emotion_data, 'emotion')\n",
    "        \n",
    "        # Prepare Reddit data\n",
    "        sentiment_texts, sentiment_labels = self.prepare_reddit_data(reddit_df, 'sentiment')\n",
    "        emotion_texts, emotion_labels = self.prepare_reddit_data(reddit_df, 'emotion')\n",
    "        \n",
    "        # Get number of classes\n",
    "        n_sentiment_classes = len(self.sentiment_label_encoder.classes_)\n",
    "        n_emotion_classes = len(self.emotion_label_encoder.classes_)\n",
    "        \n",
    "        logger.info(f\"Sentiment classes: {n_sentiment_classes}\")\n",
    "        logger.info(f\"Emotion classes: {n_emotion_classes}\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # SENTIMENT TRAINING\n",
    "        logger.info(\"\\nüéØ SENTIMENT CLASSIFICATION PIPELINE\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "        # Pre-train on external data\n",
    "        logger.info(\"Pre-training on SST-2 dataset...\")\n",
    "        sentiment_model, sentiment_trainer = self.train_model(\n",
    "            sentiment_processed, 'sentiment', n_sentiment_classes\n",
    "        )\n",
    "        \n",
    "        # Evaluate on Reddit data\n",
    "        sentiment_results = self.evaluate_model(\n",
    "            sentiment_model, sentiment_texts, sentiment_labels, \n",
    "            'sentiment', self.sentiment_label_encoder\n",
    "        )\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.create_confusion_matrix(\n",
    "            sentiment_results, 'sentiment', \n",
    "            'plots/sentiment_confusion_matrix.png'\n",
    "        )\n",
    "        self.create_roc_curves(\n",
    "            sentiment_results, 'sentiment', \n",
    "            'plots/sentiment_roc_curves.png'\n",
    "        )\n",
    "        self.create_precision_recall_curves(\n",
    "            sentiment_results, 'sentiment', \n",
    "            'plots/sentiment_pr_curves.png'\n",
    "        )\n",
    "        self.create_learning_curves(\n",
    "            sentiment_trainer, 'sentiment', \n",
    "            'plots/sentiment_learning_curves.png'\n",
    "        )\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        if perform_hyperparameter_tuning:\n",
    "            logger.info(\"Performing hyperparameter tuning for sentiment...\")\n",
    "            sentiment_tuning_results = self.hyperparameter_tuning(\n",
    "                sentiment_processed, 'sentiment', n_sentiment_classes\n",
    "            )\n",
    "            \n",
    "            # Re-train with best parameters\n",
    "            best_params = sentiment_tuning_results['best_params']\n",
    "            logger.info(f\"Re-training sentiment model with best parameters: {best_params}\")\n",
    "            \n",
    "            sentiment_model_tuned, sentiment_trainer_tuned = self.train_model(\n",
    "                sentiment_processed, 'sentiment', n_sentiment_classes, **best_params\n",
    "            )\n",
    "            \n",
    "            # Evaluate tuned model\n",
    "            sentiment_results_tuned = self.evaluate_model(\n",
    "                sentiment_model_tuned, sentiment_texts, sentiment_labels,\n",
    "                'sentiment', self.sentiment_label_encoder\n",
    "            )\n",
    "            \n",
    "            results['sentiment_tuned'] = sentiment_results_tuned\n",
    "            results['sentiment_tuning'] = sentiment_tuning_results\n",
    "        \n",
    "        results['sentiment'] = sentiment_results\n",
    "        \n",
    "        # EMOTION TRAINING\n",
    "        logger.info(\"\\nüí≠ EMOTION CLASSIFICATION PIPELINE\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "        # Pre-train on external data\n",
    "        logger.info(\"Pre-training on GoEmotions dataset...\")\n",
    "        emotion_model, emotion_trainer = self.train_model(\n",
    "            emotion_processed, 'emotion', n_emotion_classes\n",
    "        )\n",
    "        \n",
    "        # Evaluate on Reddit data\n",
    "        emotion_results = self.evaluate_model(\n",
    "            emotion_model, emotion_texts, emotion_labels,\n",
    "            'emotion', self.emotion_label_encoder\n",
    "        )\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.create_confusion_matrix(\n",
    "            emotion_results, 'emotion', \n",
    "            'plots/emotion_confusion_matrix.png'\n",
    "        )\n",
    "        self.create_roc_curves(\n",
    "            emotion_results, 'emotion', \n",
    "            'plots/emotion_roc_curves.png'\n",
    "        )\n",
    "        self.create_precision_recall_curves(\n",
    "            emotion_results, 'emotion', \n",
    "            'plots/emotion_pr_curves.png'\n",
    "        )\n",
    "        self.create_learning_curves(\n",
    "            emotion_trainer, 'emotion', \n",
    "            'plots/emotion_learning_curves.png'\n",
    "        )\n",
    "        \n",
    "        # Hyperparameter tuning\n",
    "        if perform_hyperparameter_tuning:\n",
    "            logger.info(\"Performing hyperparameter tuning for emotion...\")\n",
    "            emotion_tuning_results = self.hyperparameter_tuning(\n",
    "                emotion_processed, 'emotion', n_emotion_classes\n",
    "            )\n",
    "            \n",
    "            # Re-train with best parameters\n",
    "            best_params = emotion_tuning_results['best_params']\n",
    "            logger.info(f\"Re-training emotion model with best parameters: {best_params}\")\n",
    "            \n",
    "            emotion_model_tuned, emotion_trainer_tuned = self.train_model(\n",
    "                emotion_processed, 'emotion', n_emotion_classes, **best_params\n",
    "            )\n",
    "            \n",
    "            # Evaluate tuned model\n",
    "            emotion_results_tuned = self.evaluate_model(\n",
    "                emotion_model_tuned, emotion_texts, emotion_labels,\n",
    "                'emotion', self.emotion_label_encoder\n",
    "            )\n",
    "            \n",
    "            results['emotion_tuned'] = emotion_results_tuned\n",
    "            results['emotion_tuning'] = emotion_tuning_results\n",
    "        \n",
    "        results['emotion'] = emotion_results\n",
    "        \n",
    "        # FINAL SUMMARY\n",
    "        logger.info(\"\\nüìã FINAL SUMMARY\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        \n",
    "        self.create_final_summary(results)\n",
    "        \n",
    "        # Save all results\n",
    "        self.save_results(results, 'complete_pipeline')\n",
    "        \n",
    "        logger.info(\"\\nüéâ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        logger.info(\"üìÅ Output files:\")\n",
    "        logger.info(\"   - ./roberta_sentiment_model_final/ (sentiment model)\")\n",
    "        logger.info(\"   - ./roberta_emotion_model_final/ (emotion model)\")\n",
    "        logger.info(\"   - ./plots/ (all visualization plots)\")\n",
    "        logger.info(\"   - ./results/ (evaluation results)\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_final_summary(self, results: Dict):\n",
    "        \"\"\"Create a comprehensive summary of all results\"\"\"\n",
    "        summary_data = []\n",
    "        \n",
    "        for task in ['sentiment', 'emotion']:\n",
    "            if task in results:\n",
    "                res = results[task]\n",
    "                summary_data.append({\n",
    "                    'Task': task.capitalize(),\n",
    "                    'Model': 'RoBERTa (Base)',\n",
    "                    'Accuracy': f\"{res['accuracy']:.4f}\",\n",
    "                    'Precision (Macro)': f\"{res['precision_macro']:.4f}\",\n",
    "                    'Recall (Macro)': f\"{res['recall_macro']:.4f}\",\n",
    "                    'F1-Score (Macro)': f\"{res['f1_macro']:.4f}\",\n",
    "                    'F1-Score (Weighted)': f\"{res['f1_weighted']:.4f}\",\n",
    "                    'Cohen\\'s Kappa': f\"{res['cohen_kappa']:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Add tuned results if available\n",
    "            if f'{task}_tuned' in results:\n",
    "                res = results[f'{task}_tuned']\n",
    "                summary_data.append({\n",
    "                    'Task': f\"{task.capitalize()} (Tuned)\",\n",
    "                    'Model': 'RoBERTa (Tuned)',\n",
    "                    'Accuracy': f\"{res['accuracy']:.4f}\",\n",
    "                    'Precision (Macro)': f\"{res['precision_macro']:.4f}\",\n",
    "                    'Recall (Macro)': f\"{res['recall_macro']:.4f}\",\n",
    "                    'F1-Score (Macro)': f\"{res['f1_macro']:.4f}\",\n",
    "                    'F1-Score (Weighted)': f\"{res['f1_weighted']:.4f}\",\n",
    "                    'Cohen\\'s Kappa': f\"{res['cohen_kappa']:.4f}\"\n",
    "                })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ROBERTA EVALUATION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Save summary\n",
    "        summary_df.to_csv('results/roberta_evaluation_summary.csv', index=False)\n",
    "        logger.info(\"Summary saved to: results/roberta_evaluation_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4578393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:Setting up RoBERTa tokenizer...\n",
      "INFO:__main__:‚úÖ Tokenizer ready!\n"
     ]
    }
   ],
   "source": [
    "trainer = RobertaTrainer()\n",
    "trainer.setup_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1b69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Reddit data from annotated_reddit_posts.csv...\n",
      "INFO:__main__:‚úÖ Loaded 95 samples from Reddit dataset\n",
      "INFO:__main__:After removing missing values: 95 samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_content</th>\n",
       "      <th>original_text</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d8kzu3m</td>\n",
       "      <td>ya screw username really looking forward note ...</td>\n",
       "      <td>Ya this screws me over completely. I was reall...</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>0.464167</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5jd0fx</td>\n",
       "      <td>username samsung galaxy note7 still more user ...</td>\n",
       "      <td>Cancelled Samsung Galaxy Note7 still has more ...</td>\n",
       "      <td>post</td>\n",
       "      <td>95</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbfyq3r</td>\n",
       "      <td>traded note 7 s7 edge really hope samsung user...</td>\n",
       "      <td>I traded my Note 7 in for an S7 Edge.  I reall...</td>\n",
       "      <td>comment</td>\n",
       "      <td>9</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbgcdgl</td>\n",
       "      <td>reading username report battery design failed ...</td>\n",
       "      <td>From reading the independent report of why its...</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbftpbx</td>\n",
       "      <td>maybe phone unique username feature explode us...</td>\n",
       "      <td>Maybe the phone's unique exploding feature (or...</td>\n",
       "      <td>comment</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       text_content  \\\n",
       "0  d8kzu3m  ya screw username really looking forward note ...   \n",
       "1   5jd0fx  username samsung galaxy note7 still more user ...   \n",
       "2  dbfyq3r  traded note 7 s7 edge really hope samsung user...   \n",
       "3  dbgcdgl  reading username report battery design failed ...   \n",
       "4  dbftpbx  maybe phone unique username feature explode us...   \n",
       "\n",
       "                                       original_text     type  score  \\\n",
       "0  Ya this screws me over completely. I was reall...  comment      2   \n",
       "1  Cancelled Samsung Galaxy Note7 still has more ...     post     95   \n",
       "2  I traded my Note 7 in for an S7 Edge.  I reall...  comment      9   \n",
       "3  From reading the independent report of why its...  comment      2   \n",
       "4  Maybe the phone's unique exploding feature (or...  comment     -4   \n",
       "\n",
       "   subjectivity sentiment   emotion  \n",
       "0      0.464167  Negative   Sadness  \n",
       "1      0.500000   Neutral  Surprise  \n",
       "2      0.414286  Negative   Sadness  \n",
       "3      0.406250  Positive   Sadness  \n",
       "4      0.800000  Negative     Anger  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = trainer.load_reddit_data('annotated_reddit_posts.csv')\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3ba7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading external datasets...\n",
      "INFO:__main__:‚úÖ SST-2 dataset loaded for sentiment classification\n",
      "INFO:__main__:‚úÖ GoEmotions dataset loaded for emotion classification\n"
     ]
    }
   ],
   "source": [
    "sentiment_data, emotion_data = trainer.load_external_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce275f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Preprocessing external data for sentiment...\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 67349/67349 [00:10<00:00, 6344.15 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 872/872 [00:00<00:00, 3085.67 examples/s]\n",
      "INFO:__main__:‚úÖ Preprocessed sentiment data: 10000 train, 872 val\n",
      "INFO:__main__:Preprocessing external data for emotion...\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13621/13621 [00:02<00:00, 4653.57 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1717/1717 [00:00<00:00, 4309.29 examples/s]\n",
      "INFO:__main__:‚úÖ Preprocessed emotion data: 10000 train, 1717 val\n"
     ]
    }
   ],
   "source": [
    "sentiment_processed = trainer.preprocess_external_data(sentiment_data, 'sentiment')\n",
    "emotion_processed = trainer.preprocess_external_data(emotion_data, 'emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9850abf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment_bertweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment_bertweet'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sentiment_texts, sentiment_labels = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_reddit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreddit_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m emotion_texts, emotion_labels = trainer.prepare_reddit_data(reddit_df, \u001b[33m'\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m n_sentiment_classes = \u001b[38;5;28mlen\u001b[39m(trainer.sentiment_label_encoder.classes_)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 250\u001b[39m, in \u001b[36mRobertaTrainer.prepare_reddit_data\u001b[39m\u001b[34m(self, df, task)\u001b[39m\n\u001b[32m    247\u001b[39m texts = df[\u001b[33m'\u001b[39m\u001b[33mtext_content\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task == \u001b[33m'\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     labels = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment_bertweet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.tolist()\n\u001b[32m    251\u001b[39m     encoded_labels = \u001b[38;5;28mself\u001b[39m.sentiment_label_encoder.fit_transform(labels)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# emotion\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hankaixin\\Desktop\\multitask\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment_bertweet'"
     ]
    }
   ],
   "source": [
    "sentiment_texts, sentiment_labels = trainer.prepare_reddit_data(reddit_df, 'sentiment')\n",
    "emotion_texts, emotion_labels = trainer.prepare_reddit_data(reddit_df, 'emotion')\n",
    "n_sentiment_classes = len(trainer.sentiment_label_encoder.classes_)\n",
    "n_emotion_classes = len(trainer.emotion_label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135f706",
   "metadata": {},
   "source": [
    "#### Train sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model, sentiment_trainer = trainer.train_model(\n",
    "    sentiment_processed, 'sentiment', n_sentiment_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879538ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results = trainer.evaluate_model(\n",
    "    sentiment_model, sentiment_texts, sentiment_labels, \n",
    "    'sentiment', trainer.sentiment_label_encoder\n",
    ")\n",
    "\n",
    "# Sentiment\n",
    "trainer.create_confusion_matrix(sentiment_results, 'sentiment', 'plots/sentiment_confusion_matrix.png')\n",
    "trainer.create_roc_curves(sentiment_results, 'sentiment', 'plots/sentiment_roc_curves.png')\n",
    "trainer.create_precision_recall_curves(sentiment_results, 'sentiment', 'plots/sentiment_pr_curves.png')\n",
    "trainer.create_learning_curves(sentiment_trainer, 'sentiment', 'plots/sentiment_learning_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a7c39",
   "metadata": {},
   "source": [
    "#### Train emotion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model, emotion_trainer = trainer.train_model(\n",
    "    emotion_processed, 'emotion', n_emotion_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd00eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_results = trainer.evaluate_model(\n",
    "    emotion_model, emotion_texts, emotion_labels,\n",
    "    'emotion', trainer.emotion_label_encoder\n",
    ")\n",
    "\n",
    "trainer.create_confusion_matrix(emotion_results, 'emotion', 'plots/emotion_confusion_matrix.png')\n",
    "trainer.create_roc_curves(emotion_results, 'emotion', 'plots/emotion_roc_curves.png')\n",
    "trainer.create_precision_recall_curves(emotion_results, 'emotion', 'plots/emotion_pr_curves.png')\n",
    "trainer.create_learning_curves(emotion_trainer, 'emotion', 'plots/emotion_learning_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821b18d",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tuning_results = trainer.hyperparameter_tuning(\n",
    "    sentiment_processed, 'sentiment', n_sentiment_classes\n",
    ")\n",
    "emotion_tuning_results = trainer.hyperparameter_tuning(\n",
    "    emotion_processed, 'emotion', n_emotion_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_results(sentiment_results, 'sentiment')\n",
    "trainer.save_results(emotion_results, 'emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f0490",
   "metadata": {},
   "source": [
    "## Infer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4435f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Path to your saved model directory\n",
    "model_dir = './roberta_sentiment_model_final/'\n",
    "\n",
    "# Load the model\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Load the tokenizer (if you want to preprocess new text)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b764364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
